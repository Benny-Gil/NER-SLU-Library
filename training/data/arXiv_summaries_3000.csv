summary
"We present a general, consistency-based framework for belief change.
Informally, in revising K by A, we begin with A and incorporate as much of K as
consistently possible. Formally, a knowledge base K and sentence A are
expressed, via renaming propositions in K, in separate languages. Using a
maximization process, we assume the languages are the same insofar as
consistently possible. Lastly, we express the resultant knowledge base in a
single language. There may be more than one way in which A can be so extended
by K: in choice revision, one such ``extension'' represents the revised state;
alternately revision consists of the intersection of all such extensions.
  The most general formulation of our approach is flexible enough to express
other approaches to revision and update, the merging of knowledge bases, and
the incorporation of static and dynamic integrity constraints. Our framework
differs from work based on ordinal conditional functions, notably with respect
to iterated revision. We argue that the approach is well-suited for
implementation: the choice revision operator gives better complexity results
than general revision; the approach can be expressed in terms of a finite
knowledge base; and the scope of a revision can be restricted to just those
propositions mentioned in the sentence for revision A."
"In this paper we present a transformation of finite propositional default
theories into so-called propositional argumentation systems. This
transformation allows to characterize all notions of Reiter's default logic in
the framework of argumentation systems. As a consequence, computing extensions,
or determining wether a given formula belongs to one extension or all
extensions can be answered without leaving the field of classical propositional
logic. The transformation proposed is linear in the number of defaults."
"We consider the integration of existing cone-shaped and projection-based
calculi of cardinal direction relations, well-known in QSR. The more general,
integrating language we consider is based on convex constraints of the
qualitative form $r(x,y)$, $r$ being a cone-shaped or projection-based cardinal
direction atomic relation, or of the quantitative form $(\alpha ,\beta)(x,y)$,
with $\alpha ,\beta\in [0,2\pi)$ and $(\beta -\alpha)\in [0,\pi ]$: the meaning
of the quantitative constraint, in particular, is that point $x$ belongs to the
(convex) cone-shaped area rooted at $y$, and bounded by angles $\alpha$ and
$\beta$. The general form of a constraint is a disjunction of the form
$[r_1\vee...\vee r_{n_1}\vee (\alpha_1,\beta_1)\vee...\vee (\alpha
_{n_2},\beta_{n_2})](x,y)$, with $r_i(x,y)$, $i=1... n_1$, and $(\alpha
_i,\beta_i)(x,y)$, $i=1... n_2$, being convex constraints as described above:
the meaning of such a general constraint is that, for some $i=1... n_1$,
$r_i(x,y)$ holds, or, for some $i=1... n_2$, $(\alpha_i,\beta_i)(x,y)$ holds. A
conjunction of such general constraints is a $\tcsp$-like CSP, which we will
refer to as an $\scsp$ (Spatial Constraint Satisfaction Problem). An effective
solution search algorithm for an $\scsp$ will be described, which uses (1)
constraint propagation, based on a composition operation to be defined, as the
filtering method during the search, and (2) the Simplex algorithm, guaranteeing
completeness, at the leaves of the search tree. The approach is particularly
suited for large-scale high-level vision, such as, e.g., satellite-like
surveillance of a geographic area."
"We introduce Ak, an extension of the action description language A (Gelfond
and Lifschitz, 1993) to handle actions which affect knowledge. We use sensing
actions to increase an agent's knowledge of the world and non-deterministic
actions to remove knowledge. We include complex plans involving conditionals
and loops in our query language for hypothetical reasoning. We also present a
translation of Ak domain descriptions into epistemic logic programs."
"Evolutionary artificial neural networks (EANNs) refer to a special class of
artificial neural networks (ANNs) in which evolution is another fundamental
form of adaptation in addition to learning. Evolutionary algorithms are used to
adapt the connection weights, network architecture and learning algorithms
according to the problem environment. Even though evolutionary algorithms are
well known as efficient global search algorithms, very often they miss the best
local solutions in the complex solution space. In this paper, we propose a
hybrid meta-heuristic learning approach combining evolutionary learning and
local search methods (using 1st and 2nd order error information) to improve the
learning and faster convergence obtained using a direct evolutionary approach.
The proposed technique is tested on three different chaotic time series and the
test results are compared with some popular neuro-fuzzy systems and a recently
developed cutting angle method of global optimization. Empirical results reveal
that the proposed technique is efficient in spite of the computational
complexity."
"Multiple Classifier Systems (MCSs) allow evaluation of the uncertainty of
classification outcomes that is of crucial importance for safety critical
applications. The uncertainty of classification is determined by a trade-off
between the amount of data available for training, the classifier diversity and
the required performance. The interpretability of MCSs can also give useful
information for experts responsible for making reliable classifications. For
this reason Decision Trees (DTs) seem to be attractive classification models
for experts. The required diversity of MCSs exploiting such classification
models can be achieved by using two techniques, the Bayesian model averaging
and the randomised DT ensemble. Both techniques have revealed promising results
when applied to real-world problems. In this paper we experimentally compare
the classification uncertainty of the Bayesian model averaging with a
restarting strategy and the randomised DT ensemble on a synthetic dataset and
some domain problems commonly used in the machine learning community. To make
the Bayesian DT averaging feasible, we use a Markov Chain Monte Carlo
technique. The classification uncertainty is evaluated within an Uncertainty
Envelope technique dealing with the class posterior distribution and a given
confidence probability. Exploring a full posterior distribution, this technique
produces realistic estimates which can be easily interpreted in statistical
terms. In our experiments we found out that the Bayesian DTs are superior to
the randomised DT ensembles within the Uncertainty Envelope technique."
"This paper presents two new promising rules of combination for the fusion of
uncertain and potentially highly conflicting sources of evidences in the
framework of the theory of belief functions in order to palliate the well-know
limitations of Dempster's rule and to work beyond the limits of applicability
of the Dempster-Shafer theory. We present both a new class of adaptive
combination rules (ACR) and a new efficient Proportional Conflict
Redistribution (PCR) rule allowing to deal with highly conflicting sources for
static and dynamic fusion applications."
"In this note we introduce the notion of islands for restricting local search.
We show how we can construct islands for CNF SAT problems, and how much search
space can be eliminated by restricting search to the island."
"Motivation: Profile hidden Markov Models (pHMMs) are a popular and very
useful tool in the detection of the remote homologue protein families.
Unfortunately, their performance is not always satisfactory when proteins are
in the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm
and tool that tries to improve pHMM performance by using structural information
while training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.
Each pHMM is constructed by weighting each residue in an aligned protein
according to a specific structural property of the residue. Properties used
were primary, secondary and tertiary structures, accessibility and packing.
HMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP
database to perform our experiments. Throughout, we apply leave-one-family-out
cross-validation over protein superfamilies. First, we used the MAMMOTH-mult
structural aligner to align the training set proteins. Then, we performed two
sets of experiments. In a first experiment, we compared structure weighted
models against standard pHMMs and against each other. In a second experiment,
we compared the voting model against individual pHMMs. We compare method
performance through ROC curves and through Precision/Recall curves, and assess
significance through the paired two tailed t-test. Our results show significant
performance improvements of all structurally weighted models over default
HMMER, and a significant improvement in sensitivity of the combined models over
both the original model and the structurally weighted models."
"In this study, we reproduce two new hybrid intelligent systems, involve three
prominent intelligent computing and approximate reasoning methods: Self
Organizing feature Map (SOM), Neruo-Fuzzy Inference System and Rough Set Theory
(RST),called SONFIS and SORST. We show how our algorithms can be construed as a
linkage of government-society interactions, where government catches various
states of behaviors: solid (absolute) or flexible. So, transition of society,
by changing of connectivity parameters (noise) from order to disorder is
inferred."
"The paper proposes an analysis on some existent ontologies, in order to point
out ways to resolve semantic heterogeneity in information systems. Authors are
highlighting the tasks in a Knowledge Acquisiton System and identifying aspects
related to the addition of new information to an intelligent system. A solution
is proposed, as a combination of ontology reasoning services and natural
languages generation. A multi-agent system will be conceived with an extractor
agent, a reasoner agent and a competence management agent."
"Symmetry is an important feature of many constraint programs. We show that
any symmetry acting on a set of symmetry breaking constraints can be used to
break symmetry. Different symmetries pick out different solutions in each
symmetry class. We use these observations in two methods for eliminating
symmetry from a problem. These methods are designed to have many of the
advantages of symmetry breaking methods that post static symmetry breaking
constraint without some of the disadvantages. In particular, the two methods
prune the search space using fast and efficient propagation of posted
constraints, whilst reducing the conflict between symmetry breaking and
branching heuristics. Experimental results show that the two methods perform
well on some standard benchmarks."
"The previous decade has brought a remarkable increase of the interest in
applications that deal with querying and mining of time series data. Many of
the research efforts in this context have focused on introducing new
representation methods for dimensionality reduction or novel similarity
measures for the underlying data. In the vast majority of cases, each
individual work introducing a particular method has made specific claims and,
aside from the occasional theoretical justifications, provided quantitative
experimental observations. However, for the most part, the comparative aspects
of these experiments were too narrowly focused on demonstrating the benefits of
the proposed methods over some of the previously introduced ones. In order to
provide a comprehensive validation, we conducted an extensive experimental
study re-implementing eight different time series representations and nine
similarity measures and their variants, and testing their effectiveness on
thirty-eight time series data sets from a wide variety of application domains.
In this paper, we give an overview of these different techniques and present
our comparative experimental findings regarding their effectiveness. In
addition to providing a unified validation of some of the existing
achievements, our experiments also indicate that, in some cases, certain claims
in the literature may be unduly optimistic."
"Phase transitions in many complex combinational problems have been widely
studied in the past decade. In this paper, we investigate phase transitions in
the knowledge compilation empirically, where DFA, OBDD and d-DNNF are chosen as
the target languages to compile random k-SAT instances. We perform intensive
experiments to analyze the sizes of compilation results and draw the following
conclusions: there exists an easy-hard-easy pattern in compilations; the peak
point of sizes in the pattern is only related to the ratio of the number of
clauses to that of variables when k is fixed, regardless of target languages;
most sizes of compilation results increase exponentially with the number of
variables growing, but there also exists a phase transition that separates a
polynomial-increment region from the exponential-increment region; Moreover, we
explain why the phase transition in compilations occurs by analyzing
microstructures of DFAs, and conclude that a kind of solution
interchangeability with more than 2 variables has a sharp transition near the
peak point of the easy-hard-easy pattern, and thus it has a great impact on
sizes of DFAs."
"The notion of class is ubiquitous in computer science and is central in many
formalisms for the representation of structured knowledge used both in
knowledge representation and in databases. In this paper we study the basic
issues underlying such representation formalisms and single out both their
common characteristics and their distinguishing features. Such investigation
leads us to propose a unifying framework in which we are able to capture the
fundamental aspects of several representation languages used in different
contexts. The proposed formalism is expressed in the style of description
logics, which have been introduced in knowledge representation as a means to
provide a semantically well-founded basis for the structural aspects of
knowledge representation systems. The description logic considered in this
paper is a subset of first order logic with nice computational characteristics.
It is quite expressive and features a novel combination of constructs that has
not been studied before. The distinguishing constructs are number restrictions,
which generalize existence and functional dependencies, inverse roles, which
allow one to refer to the inverse of a relationship, and possibly cyclic
assertions, which are necessary for capturing real world domains. We are able
to show that it is precisely such combination of constructs that makes our
logic powerful enough to model the essential set of features for defining class
structures that are common to frame systems, object-oriented database
languages, and semantic data models. As a consequence of the established
correspondences, several significant extensions of each of the above formalisms
become available. The high expressiveness of the logic we propose and the need
for capturing the reasoning in different contexts forces us to distinguish
between unrestricted and finite model reasoning. A notable feature of our
proposal is that reasoning in both cases is decidable. We argue that, by virtue
of the high expressive power and of the associated reasoning capabilities on
both unrestricted and finite models, our logic provides a common core for
class-based representation formalisms."
"Despite the prevalence of the Computational Theory of Mind and the
Connectionist Model, the establishing of the key principles of the Cognitive
Science are still controversy and inconclusive. This paper proposes the concept
of Pattern Recognition as Necessary and Sufficient Principle for a general
cognitive science modeling, in a very ambitious scientific proposal. A formal
physical definition of the pattern recognition concept is also proposed to
solve many key conceptual gaps on the field."
"Domain-independent planning is a hard combinatorial problem. Taking into
account plan quality makes the task even more difficult. This article
introduces Planning by Rewriting (PbR), a new paradigm for efficient
high-quality domain-independent planning. PbR exploits declarative
plan-rewriting rules and efficient local search techniques to transform an
easy-to-generate, but possibly suboptimal, initial plan into a high-quality
plan. In addition to addressing the issues of planning efficiency and plan
quality, this framework offers a new anytime planning algorithm. We have
implemented this planner and applied it to several existing domains. The
experimental results show that the PbR approach provides significant savings in
planning effort while generating high-quality plans."
"This paper presents an implemented system for recognizing the occurrence of
events described by simple spatial-motion verbs in short image sequences. The
semantics of these verbs is specified with event-logic expressions that
describe changes in the state of force-dynamic relations between the
participants of the event. An efficient finite representation is introduced for
the infinite sets of intervals that occur when describing liquid and
semi-liquid events. Additionally, an efficient procedure using this
representation is presented for inferring occurrences of compound events,
described with event-logic expressions, from occurrences of primitive events.
Using force dynamics and event logic to specify the lexical semantics of events
allows the system to be more robust than prior systems based on motion profile."
"In recent years research in the planning community has moved increasingly
toward s application of planners to realistic problems involving both time and
many typ es of resources. For example, interest in planning demonstrated by the
space res earch community has inspired work in observation scheduling,
planetary rover ex ploration and spacecraft control domains. Other temporal and
resource-intensive domains including logistics planning, plant control and
manufacturing have also helped to focus the community on the modelling and
reasoning issues that must be confronted to make planning technology meet the
challenges of application. The International Planning Competitions have acted
as an important motivating fo rce behind the progress that has been made in
planning since 1998. The third com petition (held in 2002) set the planning
community the challenge of handling tim e and numeric resources. This
necessitated the development of a modelling langua ge capable of expressing
temporal and numeric properties of planning domains. In this paper we describe
the language, PDDL2.1, that was used in the competition. We describe the syntax
of the language, its formal semantics and the validation of concurrent plans.
We observe that PDDL2.1 has considerable modelling power --- exceeding the
capabilities of current planning technology --- and presents a number of
important challenges to the research community."
"A time series consists of a series of values or events obtained over repeated
measurements in time. Analysis of time series represents and important tool in
many application areas, such as stock market analysis, process and quality
control, observation of natural phenomena, medical treatments, etc. A vital
component in many types of time-series analysis is the choice of an appropriate
distance/similarity measure. Numerous measures have been proposed to date, with
the most successful ones based on dynamic programming. Being of quadratic time
complexity, however, global constraints are often employed to limit the search
space in the matrix during the dynamic programming procedure, in order to speed
up computation. Furthermore, it has been reported that such constrained
measures can also achieve better accuracy. In this paper, we investigate two
representative time-series distance/similarity measures based on dynamic
programming, Dynamic Time Warping (DTW) and Longest Common Subsequence (LCS),
and the effects of global constraints on them. Through extensive experiments on
a large number of time-series data sets, we demonstrate how global constrains
can significantly reduce the computation time of DTW and LCS. We also show
that, if the constraint parameter is tight enough (less than 10-15% of
time-series length), the constrained measure becomes significantly different
from its unconstrained counterpart, in the sense of producing qualitatively
different 1-nearest neighbor graphs. This observation explains the potential
for accuracy gains when using constrained measures, highlighting the need for
careful tuning of constraint parameters in order to achieve a good trade-off
between speed and accuracy."
"In the theory of belief functions, many measures of uncertainty have been
introduced. However, it is not always easy to understand what these measures
really try to represent. In this paper, we re-interpret some measures of
uncertainty in the theory of belief functions. We present some interests and
drawbacks of the existing measures. On these observations, we introduce a
measure of contradiction. Therefore, we present some degrees of non-specificity
and Bayesianity of a mass. We propose a degree of specificity based on the
distance between a mass and its most specific associated mass. We also show how
to use the degree of specificity to measure the specificity of a fusion rule.
Illustrations on simple examples are given."
"We discuss an attentional model for simultaneous object tracking and
recognition that is driven by gaze data. Motivated by theories of perception,
the model consists of two interacting pathways: identity and control, intended
to mirror the what and where pathways in neuroscience models. The identity
pathway models object appearance and performs classification using deep
(factored)-Restricted Boltzmann Machines. At each point in time the
observations consist of foveated images, with decaying resolution toward the
periphery of the gaze. The control pathway models the location, orientation,
scale and speed of the attended object. The posterior distribution of these
states is estimated with particle filtering. Deeper in the control pathway, we
encounter an attentional mechanism that learns to select gazes so as to
minimize tracking uncertainty. Unlike in our previous work, we introduce gaze
selection strategies which operate in the presence of partial information and
on a continuous action space. We show that a straightforward extension of the
existing approach to the partial information setting results in poor
performance, and we propose an alternative method based on modeling the reward
surface as a Gaussian Process. This approach gives good performance in the
presence of partial information and allows us to expand the action space from a
small, discrete set of fixation points to a continuous domain."
"Open distributed multi-agent systems are gaining interest in the academic
community and in industry. In such open settings, agents are often coordinated
using standardized agent conversation protocols. The representation of such
protocols (for analysis, validation, monitoring, etc) is an important aspect of
multi-agent applications. Recently, Petri nets have been shown to be an
interesting approach to such representation, and radically different approaches
using Petri nets have been proposed. However, their relative strengths and
weaknesses have not been examined. Moreover, their scalability and suitability
for different tasks have not been addressed. This paper addresses both these
challenges. First, we analyze existing Petri net representations in terms of
their scalability and appropriateness for overhearing, an important task in
monitoring open multi-agent systems. Then, building on the insights gained, we
introduce a novel representation using Colored Petri nets that explicitly
represent legal joint conversation states and messages. This representation
approach offers significant improvements in scalability and is particularly
suitable for overhearing. Furthermore, we show that this new representation
offers a comprehensive coverage of all conversation features of FIPA
conversation standards. We also present a procedure for transforming AUML
conversation protocol diagrams (a standard human-readable representation), to
our Colored Petri net representation."
"This paper compares various optimization methods for fuzzy inference system
optimization. The optimization methods compared are genetic algorithm, particle
swarm optimization and simulated annealing. When these techniques were
implemented it was observed that the performance of each technique within the
fuzzy inference system classification was context dependent."
"Conjunctive queries play an important role as an expressive query language
for Description Logics (DLs). Although modern DLs usually provide for
transitive roles, conjunctive query answering over DL knowledge bases is only
poorly understood if transitive roles are admitted in the query. In this paper,
we consider unions of conjunctive queries over knowledge bases formulated in
the prominent DL SHIQ and allow transitive roles in both the query and the
knowledge base. We show decidability of query answering in this setting and
establish two tight complexity bounds: regarding combined complexity, we prove
that there is a deterministic algorithm for query answering that needs time
single exponential in the size of the KB and double exponential in the size of
the query, which is optimal. Regarding data complexity, we prove containment in
co-NP."
"Both humans and artificial systems frequently use trial and error methods to
problem solving. In order to be effective, this type of strategy implies having
high quality control knowledge to guide the quest for the optimal solution.
Unfortunately, this control knowledge is rarely perfect. Moreover, in
artificial systems-as in humans-self-evaluation of one's own knowledge is often
difficult. Yet, this self-evaluation can be very useful to manage knowledge and
to determine when to revise it. The objective of our work is to propose an
automated approach to evaluate the quality of control knowledge in artificial
systems based on a specific trial and error strategy, namely the informed tree
search strategy. Our revision approach consists in analysing the system's
execution logs, and in using the belief theory to evaluate the global quality
of the knowledge. We present a real-world industrial application in the form of
an experiment using this approach in the domain of cartographic generalisation.
Thus far, the results of using our approach have been encouraging."
"In order to involve user knowledge in determining equality of sets, which may
not be equal in the mathematical sense, three types of approximate (rough)
equalities were introduced by Novotny and Pawlak ([8, 9, 10]). These notions
were generalized by Tripathy, Mitra and Ojha ([13]), who introduced the
concepts of approximate (rough) equivalences of sets. Rough equivalences
capture equality of sets at a higher level than rough equalities. More
properties of these concepts were established in [14]. Combining the conditions
for the two types of approximate equalities, two more approximate equalities
were introduced by Tripathy [12] and a comparative analysis of their relative
efficiency was provided. In [15], the four types of approximate equalities were
extended by considering rough fuzzy sets instead of only rough sets. In fact
the concepts of leveled approximate equalities were introduced and properties
were studied. In this paper we proceed further by introducing and studying the
approximate equalities based on rough intuitionistic fuzzy sets instead of
rough fuzzy sets. That is we introduce the concepts of approximate
(rough)equalities of intuitionistic fuzzy sets and study their properties. We
provide some real life examples to show the applications of rough equalities of
fuzzy sets and rough equalities of intuitionistic fuzzy sets."
"In Bayesian networks, a Most Probable Explanation (MPE) is a complete
variable instantiation with a highest probability given the current evidence.
In this paper, we discuss the problem of finding robustness conditions of the
MPE under single parameter changes. Specifically, we ask the question: How much
change in a single network parameter can we afford to apply while keeping the
MPE unchanged? We will describe a procedure, which is the first of its kind,
that computes this answer for each parameter in the Bayesian network variable
in time O(n exp(w)), where n is the number of network variables and w is its
treewidth."
"Linear-time computational techniques have been developed for combining
evidence which is available on a number of contending hypotheses. They offer a
means of making the computation-intensive calculations involved more efficient
in certain circumstances. Unfortunately, they restrict the orthogonal sum of
evidential functions to the dichotomous structure applies only to elements and
their complements. In this paper, we present a novel evidence structure in
terms of a triplet and a set of algorithms for evidential reasoning. The merit
of this structure is that it divides a set of evidence into three subsets,
distinguishing trivial evidential elements from important ones focusing some
particular elements. It avoids the deficits of the dichotomous structure in
representing the preference of evidence and estimating the basic probability
assignment of evidence. We have established a formalism for this structure and
the general formulae for combining pieces of evidence in the form of the
triplet, which have been theoretically justified."
"This paper focuses on the restart strategy of CMA-ES on multi-modal
functions. A first alternative strategy proceeds by decreasing the initial
step-size of the mutation while doubling the population size at each restart. A
second strategy adaptively allocates the computational budget among the restart
settings in the BIPOP scheme. Both restart strategies are validated on the BBOB
benchmark; their generality is also demonstrated on an independent real-world
problem suite related to spacecraft trajectory optimization."
"One of the main problems of importance sampling in Bayesian networks is
representation of the importance function, which should ideally be as close as
possible to the posterior joint distribution. Typically, we represent an
importance function as a factorization, i.e., product of conditional
probability tables (CPTs). Given diagnostic evidence, we do not have explicit
forms for the CPTs in the networks. We first derive the exact form for the CPTs
of the optimal importance function. Since the calculation is hard, we usually
only use their approximations. We review several popular strategies and point
out their limitations. Based on an analysis of the influence of evidence, we
propose a method for approximating the exact form of importance function by
explicitly modeling the most important additional dependence relations
introduced by evidence. Our experimental results show that the new
approximation strategy offers an immediate improvement in the quality of the
importance function."
"This paper describes a Bayesian method for learning causal networks using
samples that were selected in a non-random manner from a population of
interest. Examples of data obtained by non-random sampling include convenience
samples and case-control data in which a fixed number of samples with and
without some condition is collected; such data are not uncommon. The paper
describes a method for combining data under selection with prior beliefs in
order to derive a posterior probability for a model of the causal processes
that are generating the data in the population of interest. The priors include
beliefs about the nature of the non-random sampling procedure. Although exact
application of the method would be computationally intractable for most
realistic datasets, efficient special-case and approximation methods are
discussed. Finally, the paper describes how to combine learning under selection
with previous methods for learning from observational and experimental data
that are obtained on random samples of the population of interest. The net
result is a Bayesian methodology that supports causal modeling and discovery
from a rich mixture of different types of data."
"Bayesian Networks (BN) provide robust probabilistic methods of reasoning
under uncertainty, but despite their formal grounds are strictly based on the
notion of conditional dependence, not much attention has been paid so far to
their use in dependability analysis. The aim of this paper is to propose BN as
a suitable tool for dependability analysis, by challenging the formalism with
basic issues arising in dependability tasks. We will discuss how both modeling
and analysis issues can be naturally dealt with by BN. Moreover, we will show
how some limitations intrinsic to combinatorial dependability methods such as
Fault Trees can be overcome using BN. This will be pursued through the study of
a real-world example concerning the reliability analysis of a redundant digital
Programmable Logic Controller (PLC) with majority voting 2:3"
"This paper discusses belief revision under uncertain inputs in the framework
of possibility theory. Revision can be based on two possible definitions of the
conditioning operation, one based on min operator which requires a purely
ordinal scale only, and another based on product, for which a richer structure
is needed, and which is a particular case of Dempster's rule of conditioning.
Besides, revision under uncertain inputs can be understood in two different
ways depending on whether the input is viewed, or not, as a constraint to
enforce. Moreover, it is shown that M.A. Williams' transmutations, originally
defined in the setting of Spohn's functions, can be captured in this framework,
as well as Boutilier's natural revision."
"While influence diagrams have many advantages as a representation framework
for Bayesian decision problems, they have a serious drawback in handling
asymmetric decision problems. To be represented in an influence diagram, an
asymmetric decision problem must be symmetrized. A considerable amount of
unnecessary computation may be involved when a symmetrized influence diagram is
evaluated by conventional algorithms. In this paper we present an approach for
avoiding such unnecessary computation in influence diagram evaluation."
"Bayesian Belief Networks (BBNs) are a powerful formalism for reasoning under
uncertainty but bear some severe limitations: they require a large amount of
information before any reasoning process can start, they have limited
contradiction handling capabilities, and their ability to provide explanations
for their conclusion is still controversial. There exists a class of reasoning
systems, called Truth Maintenance Systems (TMSs), which are able to deal with
partially specified knowledge, to provide well-founded explanation for their
conclusions, and to detect and handle contradictions. TMSs incorporating
measure of uncertainty are called Belief Maintenance Systems (BMSs). This paper
describes how a BMS based on probabilistic logic can be applied to BBNs, thus
introducing a new class of BBNs, called Ignorant Belief Networks, able to
incrementally deal with partially specified conditional dependencies, to
provide explanations, and to detect and handle contradictions."
"Useless paths are a chronic problem for marker-passing techniques. We use a
probabilistic analysis to justify a method for quickly identifying and
rejecting useless paths. Using the same analysis, we identify key conditions
and assumptions necessary for marker-passing to perform well."
"We present Exponentiated Gradient LINUCB, an algorithm for con-textual
multi-armed bandits. This algorithm uses Exponentiated Gradient to find the
optimal exploration of the LINUCB. Within a deliberately designed offline
simulation framework we conduct evaluations with real online event log data.
The experimental results demonstrate that our algorithm outperforms surveyed
algorithms."
"In this paper, we propose an extension of our Mining for SAT framework to
Constraint satisfaction Problem (CSP). We consider n-ary extensional
constraints (table constraints). Our approach aims to reduce the size of the
CSP by exploiting the structure of the constraints graph and of its associated
microstructure. More precisely, we apply itemset mining techniques to search
for closed frequent itemsets on these two representation. Using Tseitin
extension, we rewrite the whole CSP to another compressed CSP equivalent with
respect to satisfiability. Our approach contrast with previous proposed
approach by Katsirelos and Walsh, as we do not change the structure of the
constraints."
"Clinical decision support systems have been developed to help physicians to
take clinical guidelines into account during consultations. The ASTI critiquing
module is one such systems; it provides the physician with automatic criticisms
when a drug prescription does not follow the guidelines. It was initially
developed for hypertension and type 2 diabetes, but is designed to be generic
enough for application to all chronic diseases. We present here the results of
usability and satisfaction evaluations for the ASTI critiquing module, obtained
with GPs for a newly implemented guideline concerning dyslipaemia, and we
discuss the lessons learnt and the difficulties encountered when building a
generic DSS for critiquing physicians' prescriptions."
"We present Confidence-Based Autonomy (CBA), an interactive algorithm for
policy learning from demonstration. The CBA algorithm consists of two
components which take advantage of the complimentary abilities of humans and
computer agents. The first component, Confident Execution, enables the agent to
identify states in which demonstration is required, to request a demonstration
from the human teacher and to learn a policy based on the acquired data. The
algorithm selects demonstrations based on a measure of action selection
confidence, and our results show that using Confident Execution the agent
requires fewer demonstrations to learn the policy than when demonstrations are
selected by a human teacher. The second algorithmic component, Corrective
Demonstration, enables the teacher to correct any mistakes made by the agent
through additional demonstrations in order to improve the policy and future
task performance. CBA and its individual components are compared and evaluated
in a complex simulated driving domain. The complete CBA algorithm results in
the best overall learning performance, successfully reproducing the behavior of
the teacher while balancing the tradeoff between number of demonstrations and
number of incorrect actions during learning."
"Coordination of distributed agents is required for problems arising in many
areas, including multi-robot systems, networking and e-commerce. As a formal
framework for such problems, we use the decentralized partially observable
Markov decision process (DEC-POMDP). Though much work has been done on optimal
dynamic programming algorithms for the single-agent version of the problem,
optimal algorithms for the multiagent case have been elusive. The main
contribution of this paper is an optimal policy iteration algorithm for solving
DEC-POMDPs. The algorithm uses stochastic finite-state controllers to represent
policies. The solution can include a correlation device, which allows agents to
correlate their actions without communicating. This approach alternates between
expanding the controller and performing value-preserving transformations, which
modify the controller without sacrificing value. We present two efficient
value-preserving transformations: one can reduce the size of the controller and
the other can improve its value while keeping the size fixed. Empirical results
demonstrate the usefulness of value-preserving transformations in increasing
value while keeping controller size to a minimum. To broaden the applicability
of the approach, we also present a heuristic version of the policy iteration
algorithm, which sacrifices convergence to optimality. This algorithm further
reduces the size of the controllers at each step by assuming that probability
distributions over the other agents actions are known. While this assumption
may not hold in general, it helps produce higher quality solutions in our test
problems."
"This paper addresses the open question formulated as: Which levels of
abstraction are appropriate in the synthetic modelling of life and cognition?
within the framework of info-computational constructivism, treating natural
phenomena as computational processes on informational structures. At present we
lack the common understanding of the processes of life and cognition in living
organisms with the details of co-construction of informational structures and
computational processes in embodied, embedded cognizing agents, both living and
artifactual ones. Starting with the definition of an agent as an entity capable
of acting on its own behalf, as an actor in Hewitt Actor model of computation,
even so simple systems as molecules can be modelled as actors exchanging
messages (information). We adopt Kauffmans view of a living agent as something
that can reproduce and undergoes at least one thermodynamic work cycle. This
definition of living agents leads to the Maturana and Varelas identification of
life with cognition. Within the info-computational constructive approach to
living beings as cognizing agents, from the simplest to the most complex living
systems, mechanisms of cognition can be studied in order to construct synthetic
model classes of artifactual cognizing agents on different levels of
organization."
"Major advances in Question Answering technology were needed for IBM Watson to
play Jeopardy! at championship level -- the show requires rapid-fire answers to
challenging natural language questions, broad general knowledge, high
precision, and accurate confidence estimates. In addition, Jeopardy! features
four types of decision making carrying great strategic importance: (1) Daily
Double wagering; (2) Final Jeopardy wagering; (3) selecting the next square
when in control of the board; (4) deciding whether to attempt to answer, i.e.,
""buzz in."" Using sophisticated strategies for these decisions, that properly
account for the game state and future event probabilities, can significantly
boost a players overall chances to win, when compared with simple ""rule of
thumb"" strategies. This article presents our approach to developing Watsons
game-playing strategies, comprising development of a faithful simulation model,
and then using learning and Monte-Carlo methods within the simulator to
optimize Watsons strategic decision-making. After giving a detailed description
of each of our game-strategy algorithms, we then focus in particular on
validating the accuracy of the simulators predictions, and documenting
performance improvements using our methods. Quantitative performance benefits
are shown with respect to both simple heuristic strategies, and actual human
contestant performance in historical episodes. We further extend our analysis
of human play to derive a number of valuable and counterintuitive examples
illustrating how human contestants may improve their performance on the show."
"This paper presents a structured power and energy-flow-based qualitative
modelling approach that is applicable to a variety of system types including
electrical and fluid flow. The modelling is split into two parts. Power flow is
a global phenomenon and is therefore naturally represented and analysed by a
network comprised of the relevant structural elements from the components of a
system. The power flow analysis is a platform for higher-level behaviour
prediction of energy related aspects using local component behaviour models to
capture a state-based representation with a global time. The primary
application is Failure Modes and Effects Analysis (FMEA) and a form of
exaggeration reasoning is used, combined with an order of magnitude
representation to derive the worst case failure modes. The novel aspects of the
work are an order of magnitude(OM) qualitative network analyser to represent
any power domain and topology, including multiple power sources, a feature that
was not required for earlier specialised electrical versions of the approach.
Secondly, the representation of generalised energy related behaviour as
state-based local models is presented as a modelling strategy that can be more
vivid and intuitive for a range of topologically complex applications than
qualitative equation-based representations.The two-level modelling strategy
allows the broad system behaviour coverage of qualitative simulation to be
exploited for the FMEA task, while limiting the difficulties of qualitative
ambiguity explanation that can arise from abstracted numerical models. We have
used the method to support an automated FMEA system with examples of an
aircraft fuel system and domestic a heating system discussed in this paper."
"Answer Set Programming (ASP) is a declarative programming paradigm. The
intrinsic complexity of the evaluation of ASP programs makes the development of
more effective and faster systems a challenging research topic. This paper
reports on the recent improvements of the ASP solver WASP. WASP is undergoing a
refactoring process which will end up in the release of a new and more
performant version of the software. In particular the paper focus on the
improvements to the core evaluation algorithms working on normal programs. A
preliminary experiment on benchmarks from the 3rd ASP competition belonging to
the NP class is reported. The previous version of WASP was often not
competitive with alternative solutions on this class. The new version of WASP
shows a substantial increase in performance."
"We propose a new formal language for the expressive representation of
probabilistic knowledge based on Answer Set Programming (ASP). It allows for
the annotation of first-order formulas as well as ASP rules and facts with
probabilities and for learning of such weights from data (parameter
estimation). Weighted formulas are given a semantics in terms of soft and hard
constraints which determine a probability distribution over answer sets. In
contrast to related approaches, we approach inference by optionally utilizing
so-called streamlining XOR constraints, in order to reduce the number of
computed answer sets. Our approach is prototypically implemented. Examples
illustrate the introduced concepts and point at issues and topics for future
research."
"In this paper we address the problem of planning in rich domains, where
knowledge representation is a key aspect for managing the complexity and size
of the planning domain. We follow the approach of Description Logic (DL) based
Dynamic Knowledge Bases, where a state of the world is represented concisely by
a (possibly changing) ABox and a (fixed) TBox containing the axioms, and
actions that allow to change the content of the ABox. The plan goal is given in
terms of satisfaction of a DL query. In this paper we start from a traditional
forward planning algorithm and we propose a much more efficient variant by
combining backward and forward search. In particular, we propose a Backward
State-space Reduction technique that consists in two phases: first, an Abstract
Planning Graph P is created by using the Abstract Backward Planning Algorithm
(ABP), then the abstract planning graph P is instantiated into a corresponding
planning graph P by using the Forward Plan Instantiation Algorithm (FPI). The
advantage is that in the preliminary ABP phase we produce a symbolic plan that
is a pattern to direct the search of the concrete plan. This can be seen as a
kind of informed search where the preliminary backward phase is useful to
discover properties of the state-space that can be used to direct the
subsequent forward phase. We evaluate the effectiveness of our ABP+FPI
algorithm in the reduction of the explored planning domain by comparing it to a
standard forward planning algorithm and applying both of them to a concrete
business case study."
"Pairwise comparisons between alternatives are a well-established tool to
decompose decision problems into smaller and more easily tractable
sub-problems. However, due to our limited rationality, the subjective
preferences expressed by decision makers over pairs of alternatives can hardly
ever be consistent. Therefore, several inconsistency indices have been proposed
in the literature to quantify the extent of the deviation from complete
consistency. Only recently, a set of properties has been proposed to define a
family of functions representing inconsistency indices. The scope of this paper
is twofold. Firstly, it expands the set of properties by adding and justifying
a new one. Secondly, it continues the study of inconsistency indices to check
whether or not they satisfy the above mentioned properties. Out of the four
indices considered in this paper, in its present form, two fail to satisfy some
properties. An adjusted version of one index is proposed so that it fulfills
them."
"Objective: To treat patients with vascular mild cognitive impairment (VMCI)
using TCM, it is necessary to classify the patients into TCM syndrome types and
to apply different treatments to different types. We investigate how to
properly carry out the classification using a novel data-driven method known as
latent tree analysis.
  Method: A cross-sectional survey on VMCI was carried out in several regions
in northern China from 2008 to 2011, which resulted in a data set that involves
803 patients and 93 symptoms. Latent tree analysis was performed on the data to
reveal symptom co-occurrence patterns, and the patients were partitioned into
clusters in multiple ways based on the patterns. The patient clusters were
matched up with syndrome types, and population statistics of the clusters are
used to quantify the syndrome types and to establish classification rules.
  Results: Eight syndrome types are identified: Qi Deficiency, Qi Stagnation,
Blood Deficiency, Blood Stasis, Phlegm-Dampness, Fire-Heat, Yang Deficiency,
and Yin Deficiency. The prevalence and symptom occurrence characteristics of
each syndrome type are determined. Quantitative classification rules are
established for determining whether a patient belongs to each of the syndrome
types.
  Conclusions: A solution for the TCM syndrome classification problem
associated with VMCI is established based on the latent tree analysis of
unlabeled symptom survey data. The results can be used as a reference in clinic
practice to improve the quality of syndrome differentiation and to reduce
diagnosis variances across physicians. They can also be used for patient
selection in research projects aimed at finding biomarkers for the syndrome
types and in randomized control trials aimed at determining the efficacy of TCM
treatments of VMCI."
"We have developed a program called MUDoS (Maastricht University Domineering
Solver) that solves Domineering positions in a very efficient way. This enables
the solution of known positions so far (up to the 10 x 10 board) much quicker
(measured in number of investigated nodes).
  More importantly, it enables the solution of the 11 x 11 Domineering board, a
board up till now far out of reach of previous Domineering solvers. The
solution needed the investigation of 259,689,994,008 nodes, using almost half a
year of computation time on a single simple desktop computer. The results show
that under optimal play the first player wins the 11 x 11 Domineering game,
irrespective if Vertical or Horizontal starts the game.
  In addition, several other boards hitherto unsolved were solved. Using the
convention that Vertical starts, the 8 x 15, 11 x 9, 12 x 8, 12 x 15, 14 x 8,
and 17 x 6 boards are all won by Vertical, whereas the 6 x 17, 8 x 12, 9 x 11,
and 11 x 10 boards are all won by Horizontal."
"We consider partially observable Markov decision processes (POMDPs) with a
set of target states and positive integer costs associated with every
transition. The traditional optimization objective (stochastic shortest path)
asks to minimize the expected total cost until the target set is reached. We
extend the traditional framework of POMDPs to model energy consumption, which
represents a hard constraint. The energy levels may increase and decrease with
transitions, and the hard constraint requires that the energy level must remain
positive in all steps till the target is reached. First, we present a novel
algorithm for solving POMDPs with energy levels, developing on existing POMDP
solvers and using RTDP as its main method. Our second contribution is related
to policy representation. For larger POMDP instances the policies computed by
existing solvers are too large to be understandable. We present an automated
procedure based on machine learning techniques that automatically extracts
important decisions of the policy allowing us to compute succinct human
readable policies. Finally, we show experimentally that our algorithm performs
well and computes succinct policies on a number of POMDP instances from the
literature that were naturally enhanced with energy levels."
"Written responses can provide a wealth of data in understanding student
reasoning on a topic. Yet they are time- and labor-intensive to score,
requiring many instructors to forego them except as limited parts of summative
assessments at the end of a unit or course. Recent developments in Machine
Learning (ML) have produced computational methods of scoring written responses
for the presence or absence of specific concepts. Here, we compare the scores
from one particular ML program -- EvoGrader -- to human scoring of responses to
structurally- and content-similar questions that are distinct from the ones the
program was trained on. We find that there is substantial inter-rater
reliability between the human and ML scoring. However, sufficient systematic
differences remain between the human and ML scoring that we advise only using
the ML scoring for formative, rather than summative, assessment of student
reasoning."
"Random generators or stochastic engines are a key component in the structure
of metaheuristic algorithms. This work investigates the effects of non-Gaussian
stochastic engines on the performance of metaheuristics when solving a
real-world optimization problem. In this work, the bacteria foraging algorithm
(BFA) was employed in tandem with four random generators (stochastic engines).
The stochastic engines operate using the Weibull distribution, Gamma
distribution, Gaussian distribution and a chaotic mechanism. The two
non-Gaussian distributions are the Weibull and Gamma distributions. In this
work, the approaches developed were implemented on the real-world
multi-objective resin bonded sand mould problem. The Pareto frontiers obtained
were benchmarked using two metrics; the hyper volume indicator (HVI) and the
proposed Average Explorative Rate (AER) metric. Detail discussions from various
perspectives on the effects of non-Gaussian random generators in metaheuristics
are provided."
"Deep Reinforcement Learning (DRL) is a trending field of research, showing
great promise in challenging problems such as playing Atari, solving Go and
controlling robots. While DRL agents perform well in practice we are still
lacking the tools to analayze their performance. In this work we present the
Semi-Aggregated MDP (SAMDP) model. A model best suited to describe policies
exhibiting both spatial and temporal hierarchies. We describe its advantages
for analyzing trained policies over other modeling approaches, and show that
under the right state representation, like that of DQN agents, SAMDP can help
to identify skills. We detail the automatic process of creating it from
recorded trajectories, up to presenting it on t-SNE maps. We explain how to
evaluate its fitness and show surprising results indicating high compatibility
with the policy at hand. We conclude by showing how using the SAMDP model, an
extra performance gain can be squeezed from the agent."
"We investigate the 'Digital Synaptic Neural Substrate' (DSNS) computational
creativity approach further with respect to the size and quality of images that
can be used to seed the process. In previous work we demonstrated how combining
photographs of people and sequences taken from chess games between weak players
can be used to generate chess problems or puzzles of higher aesthetic quality,
on average, compared to alternative approaches. In this work we show
experimentally that using larger images as opposed to smaller ones improves the
output quality even further. The same is also true for using clearer or less
corrupted images. The reasons why these things influence the DSNS process is
presently not well-understood and debatable but the findings are nevertheless
immediately applicable for obtaining better results."
"While the possibility of time travel in physics is still debated, the
explosive growth of virtual-reality simulations opens up new possibilities to
rigorously explore such time travel and its consequences in the digital domain.
Here we provide a computational model of time travel and a computer program
that allows exploring digital time travel. In order to explain our method we
formalize a simplified version of the famous grandfather paradox, show how the
system can allow the participant to go back in time, try to kill their
ancestors before they were born, and experience the consequences. The system
has even come up with scenarios that can be considered consistent ""solutions""
of the grandfather paradox. We discuss the conditions for digital time travel,
which indicate that it has a large number of practical applications."
"Current state of the art in the field of UAV activation relies solely on
human operators for the design and adaptation of the drones' flying routes.
Furthermore, this is being done today on an individual level (one vehicle per
operators), with some exceptions of a handful of new systems, that are
comprised of a small number of self-organizing swarms, manually guided by a
human operator.
  Drones-based monitoring is of great importance in variety of civilian
domains, such as road safety, homeland security, and even environmental
control. In its military aspect, efficiently detecting evading targets by a
fleet of unmanned drones has an ever increasing impact on the ability of modern
armies to engage in warfare. The latter is true both traditional symmetric
conflicts among armies as well as asymmetric ones. Be it a speeding driver, a
polluting trailer or a covert convoy, the basic challenge remains the same --
how can its detection probability be maximized using as little number of drones
as possible.
  In this work we propose a novel approach for the optimization of large scale
swarms of reconnaissance drones -- capable of producing on-demand optimal
coverage strategies for any given search scenario. Given an estimation cost of
the threat's potential damages, as well as types of monitoring drones available
and their comparative performance, our proposed method generates an
analytically provable strategy, stating the optimal number and types of drones
to be deployed, in order to cost-efficiently monitor a pre-defined region for
targets maneuvering using a given roads networks.
  We demonstrate our model using a unique dataset of the Israeli transportation
network, on which different deployment schemes for drones deployment are
evaluated."
"Compositional models were introduce by Jirousek and Shenoy in the general
framework of valuation-based systems. They based their theory on an axiomatic
system of valuations involving not only the operations of combination and
marginalisation, but also of removal. They claimed that this systems covers
besides the classical case of discrete probability distributions, also the
cases of Gaussian densities and belief functions, and many other systems.
  Whereas their results on the compositional operator are correct, the
axiomatic basis is not sufficient to cover the examples claimed above. We
propose here a different axiomatic system of valuation algebras, which permits
a rigorous mathematical theory of compositional operators in valuation-based
systems and covers all the examples mentioned above. It extends the classical
theory of inverses in semigroup theory and places thereby the present theory
into its proper mathematical frame. Also this theory sheds light on the
different structures of valuation-based systems, like regular algebras
(represented by probability potentials), canncellative algebras (Gaussian
potentials) and general separative algebras (density functions)."
"In recent years, work has been done to develop the theory of General
Reinforcement Learning (GRL). However, there are few examples demonstrating
these results in a concrete way. In particular, there are no examples
demonstrating the known results regarding gener- alised discounting. We have
added to the GRL simulation platform AIXIjs the functionality to assign an
agent arbitrary discount functions, and an environment which can be used to
determine the effect of discounting on an agent's policy. Using this, we
investigate how geometric, hyperbolic and power discounting affect an informed
agent in a simple MDP. We experimentally reproduce a number of theoretical
results, and discuss some related subtleties. It was found that the agent's
behaviour followed what is expected theoretically, assuming appropriate
parameters were chosen for the Monte-Carlo Tree Search (MCTS) planning
algorithm."
"In this paper, we present a new algorithm for parallel Monte Carlo tree
search (MCTS). It is based on the pipeline pattern and allows flexible
management of the control flow of the operations in parallel MCTS. The pipeline
pattern provides for the first structured parallel programming approach to
MCTS. Moreover, we propose a new lock-free tree data structure for parallel
MCTS which removes synchronization overhead. The Pipeline Pattern for Parallel
MCTS algorithm (called 3PMCTS), scales very well to higher numbers of cores
when compared to the existing methods."
"In this paper we provide a first analysis of the research questions that
arise when dealing with the problem of communicating pieces of formal
argumentation through natural language interfaces. It is a generally held
opinion that formal models of argumentation naturally capture human argument,
and some preliminary studies have focused on justifying this view.
Unfortunately, the results are not only inconclusive, but seem to suggest that
explaining formal argumentation to humans is a rather articulated task.
Graphical models for expressing argumentation-based reasoning are appealing,
but often humans require significant training to use these tools effectively.
We claim that natural language interfaces to formal argumentation systems offer
a real alternative, and may be the way forward for systems that capture human
argument."
"There is significant concern that technological advances, especially in
Robotics and Artificial Intelligence (AI), could lead to high levels of
unemployment in the coming decades. Studies have estimated that around half of
all current jobs are at risk of automation. To look into this issue in more
depth, we surveyed experts in Robotics and AI about the risk, and compared
their views with those of non-experts. Whilst the experts predicted a
significant number of occupations were at risk of automation in the next two
decades, they were more cautious than people outside the field in predicting
occupations at risk. Their predictions were consistent with their estimates for
when computers might be expected to reach human level performance across a wide
range of skills. These estimates were typically decades later than those of the
non-experts. Technological barriers may therefore provide society with more
time to prepare for an automated future than the public fear. In addition,
public expectations may need to be dampened about the speed of progress to be
expected in Robotics and AI."
"Automated planning remains one of the most general paradigms in Artificial
Intelligence, providing means of solving problems coming from a wide variety of
domains. One of the key factors restricting the applicability of planning is
its computational complexity resulting from exponentially large search spaces.
Heuristic approaches are necessary to solve all but the simplest problems. In
this work, we explore the possibility of obtaining domain-independent heuristic
functions using machine learning. This is a part of a wider research program
whose objective is to improve practical applicability of planning in systems
for which the planning domains evolve at run time. The challenge is therefore
the learning of (corrections of) domain-independent heuristics that can be
reused across different planning domains."
"Sequential pattern mining algorithms are widely used to explore care pathways
database, but they generate a deluge of patterns, mostly redundant or useless.
Clinicians need tools to express complex mining queries in order to generate
less but more significant patterns. These algorithms are not versatile enough
to answer complex clinician queries. This article proposes to apply a
declarative pattern mining approach based on Answer Set Programming paradigm.
It is exemplified by a pharmaco-epidemiological study investigating the
possible association between hospitalization for seizure and antiepileptic drug
switch from a french medico-administrative database."
"Providing elderly and people with special needs, including those suffering
from physical disabilities and chronic diseases, with the possibility of
retaining their independence at best is one of the most important challenges
our society is expected to face. Assistance models based on the home care
paradigm are being adopted rapidly in almost all industrialized and emerging
countries. Such paradigms hypothesize that it is necessary to ensure that the
so-called Activities of Daily Living are correctly and regularly performed by
the assisted person to increase the perception of an improved quality of life.
This chapter describes the computational inference engine at the core of
Arianna, a system able to understand whether an assisted person performs a
given set of ADL and to motivate him/her in performing them through a
speech-mediated motivational dialogue, using a set of nearables to be installed
in an apartment, plus a wearable to be worn or fit in garments."
"An autonomous agent embodied in a humanoid robot, in order to learn from the
overwhelming flow of raw and noisy sensory, has to effectively reduce the high
spatial-temporal data dimensionality. In this paper we propose a novel method
of unsupervised feature extraction and selection with binary space
partitioning, followed by a computation of information gain that is interpreted
as intrinsic reward, then applied as immediate-reward signal for the
reinforcement-learning. The space partitioning is executed by tiny codelets
running on a simulated Turing Machine. The features are represented by concept
nodes arranged in a hierarchy, in which those of a lower level become the input
vectors of a higher level."
"Inspired by findings of sensorimotor coupling in humans and animals, there
has recently been a growing interest in the interaction between action and
perception in robotic systems [Bogh et al., 2016]. Here we consider perception
and action as two serial information channels with limited
information-processing capacity. We follow [Genewein et al., 2015] and
formulate a constrained optimization problem that maximizes utility under
limited information-processing capacity in the two channels. As a solution we
obtain an optimal perceptual channel and an optimal action channel that are
coupled such that perceptual information is optimized with respect to
downstream processing in the action module. The main novelty of this study is
that we propose an online optimization procedure to find bounded-optimal
perception and action channels in parameterized serial perception-action
systems. In particular, we implement the perceptual channel as a multi-layer
neural network and the action channel as a multinomial distribution. We
illustrate our method in a NAO robot simulator with a simplified cup lifting
task."
"We describe an application of Answer Set Programming to the understanding of
narratives about stereotypical activities, demonstrated via question answering.
Substantial work in this direction was done by Erik Mueller, who modeled
stereotypical activities as scripts. His systems were able to understand a good
number of narratives, but could not process texts describing exceptional
scenarios. We propose addressing this problem by using a theory of intentions
developed by Blount, Gelfond, and Balduccini. We present a methodology in which
we substitute scripts by activities (i.e., hierarchical plans associated with
goals) and employ the concept of an intentional agent to reason about both
normal and exceptional scenarios. We exemplify the application of this
methodology by answering questions about a number of restaurant stories. This
paper is under consideration for acceptance in TPLP."
"Many reality tasks such as robot coordination can be naturally modelled as
multi-agent cooperative system where the rewards are sparse. This paper focuses
on learning decentralized policies for such tasks using sub-optimal
demonstration. To learn the multi-agent cooperation effectively and tackle the
sub-optimality of demonstration, a self-improving learning method is proposed:
On the one hand, the centralized state-action values are initialized by the
demonstration and updated by the learned decentralized policy to improve the
sub-optimality. On the other hand, the Nash Equilibrium are found by the
current state-action value and are used as a guide to learn the policy. The
proposed method is evaluated on the combat RTS games which requires a high
level of multi-agent cooperation. Extensive experimental results on various
combat scenarios demonstrate that the proposed method can learn multi-agent
cooperation effectively. It significantly outperforms many state-of-the-art
demonstration based approaches."
"In the literature, the optimization problem to identify a set of composite
hypotheses H, which will yield the $k$ largest $P(H|S_e)$ where a composite
hypothesis is an instantiation of all the nodes in the network except the
evidence nodes \cite{KSy:93} is of significant interest. This problem is called
""finding the $k$ Most Plausible Explanation (MPE) of a given evidence $S_e$ in
a Bayesian belief network"".
  The problem of finding $k$ most probable hypotheses is generally NP-hard
\cite{Cooper:90}. Therefore in the past various simplifications of the task by
restricting $k$ (to 1 or 2), restricting the structure (e.g. to singly
connected networks), or shifting the complexity to spatial domain have been
investigated.
  A genetic algorithm is proposed in this paper to overcome some of these
restrictions while stepping out from probabilistic domain onto the general
Valuation based System (VBS) framework is also proposed by generalizing the
genetic algorithm approach to the realm of Dempster-Shafer belief calculus."
"Constraint Satisfaction Problems (CSPs) typically have many solutions that
satisfy all constraints. Often though, some solutions are preferred over
others, that is, some solutions dominate other solutions. We present solution
dominance as a formal framework to reason about such settings. We define
Constraint Dominance Problems (CDPs) as CSPs with a dominance relation, that
is, a preorder over the solutions of the CSP. This framework captures many
well-known variants of constraint satisfaction, including optimization,
multi-objective optimization, Max-CSP, minimal models, minimum correction
subsets as well as optimization over CP-nets and arbitrary dominance relations.
We extend MiniZinc, a declarative language for modeling CSPs, to CDPs by
introducing dominance nogoods; these can be derived from dominance relations in
a principled way. A generic method for solving arbitrary CDPs incrementally
calls a CSP solver and is compatible with any existing solver that supports
MiniZinc. This encourages experimenting with different solution dominance
relations for a problem, as well as comparing different solvers without having
to modify their implementations."
"We propose a planning-based method to teach an agent to manage portfolio from
scratch. Our approach combines deep reinforcement learning techniques with
search techniques like AlphaGo. By uniting the advantages in A* search
algorithm with Monte Carlo tree search, we come up with a new algorithm named
A* tree search in which best information is returned to guide next search.
Also, the expansion mode of Monte Carlo tree is improved for a higher
utilization of the neural network. The suggested algorithm can also optimize
non-differentiable utility function by combinatorial search. This technique is
then used in our trading system. The major component is a neural network that
is trained by trading experiences from tree search and outputs prior
probability to guide search by pruning away branches in turn. Experimental
results on simulated and real financial data verify the robustness of the
proposed trading system and the trading system produces better strategies than
several approaches based on reinforcement learning."
"Word representations are created using analogy context-based statistics and
lexical relations on words. Word representations are inputs for the learning
models in Natural Language Understanding (NLU) tasks. However, to understand
language, knowing only the context is not sufficient. Reading between the lines
is a key component of NLU. Embedding deeper word relationships which are not
represented in the context enhances the word representation. This paper
presents a word embedding which combines an analogy, context-based statistics
using Word2Vec, and deeper word relationships using Conceptnet, to create an
expanded word representation. In order to fine-tune the word representation,
Self-Organizing Map is used to optimize it. The proposed word representation is
compared with semantic word representations using Simlex 999. Furthermore, the
use of 3D visual representations has shown to be capable of representing the
similarity and association between words. The proposed word representation
shows a Spearman correlation score of 0.886 and provided the best results when
compared to the current state-of-the-art methods, and exceed the human
performance of 0.78."
"Inference and decision making under uncertainty are key processes in every
autonomous system and numerous robotic problems. In recent years, the
similarities between inference and decision making triggered much work, from
developing unified computational frameworks to pondering about the duality
between the two. In spite of these efforts, inference and control, as well as
inference and belief space planning (BSP) are still treated as two separate
processes. In this paper we propose a paradigm shift, a novel approach which
deviates from conventional Bayesian inference and utilizes the similarities
between inference and BSP. We make the key observation that inference can be
efficiently updated using predictions made during the decision making stage,
even in light of inconsistent data association between the two. We developed a
two staged process that implements our novel approach and updates inference
using calculations from the precursory planning phase. Using autonomous
navigation in an unknown environment along with iSAM2 efficient methodologies
as a test case, we benchmarked our novel approach against standard Bayesian
inference, both with synthetic and real-world data (KITTI dataset). Results
indicate that not only our approach improves running time by at least a factor
of two while providing the same estimation accuracy, but it also alleviates the
computational burden of state dimensionality and loop closures."
"This paper presents a novel model-free Reinforcement Learning algorithm for
learning behavior in continuous action, state, and goal spaces. The algorithm
approximates optimal value functions using non-parametric estimators. It is
able to efficiently learn to reach multiple arbitrary goals in deterministic
and nondeterministic environments. To improve generalization in the goal space,
we propose a novel sample augmentation technique. Using these methods, robots
learn faster and overall better controllers. We benchmark the proposed
algorithms using simulation and a real-world voltage controlled robot that
learns to maneuver in a non-observable Cartesian task space."
"Nowadays, model-free reinforcement learning algorithms have achieved
remarkable performance on many decision making and control tasks, but high
sample complexity and low sample efficiency still hinder the wide use of
model-free reinforcement learning algorithms. In this paper, we argue that if
we intend to design an intelligent agent that learns fast and transfers well,
the agent must be able to reflect key elements of intelligence, like intuition,
Memory, PredictionandCuriosity. We propose an agent framework that integrates
off-policy reinforcement learning with world model learning, so as to embody
the important features of intelligence in our algorithm design. We adopt the
state-of-art model-free reinforcement learning algorithm, Soft Actor-Critic, as
the agent intuition, and world model learning through RNN to endow the agent
with memory, curiosity, and the ability to predict. We show that these ideas
can work collaboratively with each other and our agent (RMC) can give new
state-of-art results while maintaining sample efficiency and training
stability. Moreover, our agent framework can be easily extended from MDP to
POMDP problems without performance loss."
"As increasingly complex AI systems are introduced into our daily lives, it
becomes important for such systems to be capable of explaining the rationale
for their decisions and allowing users to contest these decisions. A
significant hurdle to allowing for such explanatory dialogue could be the
vocabulary mismatch between the user and the AI system. This paper introduces
methods for providing contrastive explanations in terms of user-specified
concepts for sequential decision-making settings where the system's model of
the task may be best represented as an inscrutable model. We do this by
building partial symbolic models of a local approximation of the task that can
be leveraged to answer the user queries. We test these methods on a popular
Atari game (Montezuma's Revenge) and variants of Sokoban (a well-known planning
benchmark) and report the results of user studies to evaluate whether people
find explanations generated in this form useful."
"Highly automated driving requires precise models of traffic participants.
Many state of the art models are currently based on machine learning
techniques. Among others, the required amount of labeled data is one major
challenge. An autonomous learning process addressing this problem is proposed.
The initial models are iteratively refined in three steps: (1) detection and
context identification, (2) novelty detection and active learning and (3)
online model adaption."
"This paper presents an overview of the sixth AIBIRDS competition, held at the
26th International Joint Conference on Artificial Intelligence. This
competition tasked participants with developing an intelligent agent which can
play the physics-based puzzle game Angry Birds. This game uses a sophisticated
physics engine that requires agents to reason and predict the outcome of
actions with only limited environmental information. Agents entered into this
competition were required to solve a wide assortment of previously unseen
levels within a set time limit. The physical reasoning and planning required to
solve these levels are very similar to those of many real-world problems. This
year's competition featured some of the best agents developed so far and even
included several new AI techniques such as deep reinforcement learning. Within
this paper we describe the framework, rules, submitted agents and results for
this competition. We also provide some background information on related work
and other video game AI competitions, as well as discussing some potential
ideas for future AIBIRDS competitions and agent improvements."
"Reward shaping allows reinforcement learning (RL) agents to accelerate
learning by receiving additional reward signals. However, these signals can be
difficult to design manually, especially for complex RL tasks. We propose a
simple and general approach that determines the reward of pre-defined events by
their rarity alone. Here events become less rewarding as they are experienced
more often, which encourages the agent to continually explore new types of
events as it learns. The adaptiveness of this reward function results in a form
of automated curriculum learning that does not have to be specified by the
experimenter. We demonstrate that this \emph{Rarity of Events} (RoE) approach
enables the agent to succeed in challenging VizDoom scenarios without access to
the extrinsic reward from the environment. Furthermore, the results demonstrate
that RoE learns a more versatile policy that adapts well to critical changes in
the environment. Rewarding events based on their rarity could help in many
unsolved RL environments that are characterized by sparse extrinsic rewards but
a plethora of known event types."
"Conceptual formalism supported by typical ontologies may not be sufficient to
represent uncertainty information which is caused due to the lack of clear cut
boundaries between concepts of a domain. Fuzzy ontologies are proposed to offer
a way to deal with this uncertainty. This paper describes the state of the art
in developing fuzzy ontologies. The survey is produced by studying about 35
works on developing fuzzy ontologies from a batch of 100 articles in the field
of fuzzy ontologies."
"Given that there exist many different formal and precise treatments of
deontologi- cal and consequentialist ethics, we turn to virtue ethics and
consider what could be a formalization of virtue ethics that makes it amenable
to automation. We present an embroyonic formalization in a cognitive calculus
(which subsumes a quantified first-order logic) that has been previously used
to model robust ethical principles, in both the deontological and
consequentialist traditions."
"The adoption of machine learning in high-stakes applications such as
healthcare and law has lagged in part because predictions are not accompanied
by explanations comprehensible to the domain user, who often holds the ultimate
responsibility for decisions and outcomes. In this paper, we propose an
approach to generate such explanations in which training data is augmented to
include, in addition to features and labels, explanations elicited from domain
users. A joint model is then learned to produce both labels and explanations
from the input features. This simple idea ensures that explanations are
tailored to the complexity expectations and domain knowledge of the consumer.
Evaluation spans multiple modeling techniques on a game dataset, a (visual)
aesthetics dataset, a chemical odor dataset and a Melanoma dataset showing that
our approach is generalizable across domains and algorithms. Results
demonstrate that meaningful explanations can be reliably taught to machine
learning algorithms, and in some cases, also improve modeling accuracy."
"Fuzzy quantification is a subtopic of fuzzy logic which deals with the
modelling of the quantified expressions we can find in natural language. Fuzzy
quantifiers have been successfully applied in several fields like fuzzy,
control, fuzzy databases, information retrieval, natural language generation,
etc. Their ability to model and evaluate linguistic expressions in a
mathematical way, makes fuzzy quantifiers very powerful for data analytics and
data mining applications. In this paper we will give a general overview of the
main applications of fuzzy quantifiers in this field as well as some ideas to
use them in new application contexts."
"This paper presents an artificial intelligence algorithm that can be used to
derive formulas from various scientific disciplines called automatic derivation
machine. First, the formula is abstractly expressed as a multiway tree model,
and then each step of the formula derivation transformation is abstracted as a
mapping of multiway trees. Derivation steps similar can be expressed as a
reusable formula template by a multiway tree map. After that, the formula
multiway tree is eigen-encoded to feature vectors construct the feature space
of formulas, the Q-learning model using in this feature space can achieve the
derivation by making training data from derivation process. Finally, an
automatic formula derivation machine is made to choose the next derivation step
based on the current state and object. We also make an example about the
nuclear reactor physics problem to show how the automatic derivation machine
works."
"Reinforcement learning (RL) problems often feature deceptive local optima,
and learning methods that optimize purely for reward signal often fail to learn
strategies for overcoming them. Deep neuroevolution and novelty search have
been proposed as effective alternatives to gradient-based methods for learning
RL policies directly from pixels. In this paper, we introduce and evaluate the
use of novelty search over agent action sequences by string edit metric
distance as a means for promoting innovation. We also introduce a method for
stagnation detection and population resampling inspired by recent developments
in the RL community that uses the same mechanisms as novelty search to promote
and develop innovative policies. Our methods extend a state-of-the-art method
for deep neuroevolution using a simple-yet-effective genetic algorithm (GA)
designed to efficiently learn deep RL policy network weights. Experiments using
four games from the Atari 2600 benchmark were conducted. Results provide
further evidence that GAs are competitive with gradient-based algorithms for
deep RL. Results also demonstrate that novelty search over action sequences is
an effective source of selection pressure that can be integrated into existing
evolutionary algorithms for deep RL."
"Large probabilistic models are often shaped by a pool of known individuals (a
universe) and relations between them. Lifted inference algorithms handle sets
of known individuals for tractable inference. Universes may not always be
known, though, or may only described by assumptions such as ""small universes
are more likely"". Without a universe, inference is no longer possible for
lifted algorithms, losing their advantage of tractable inference. The aim of
this paper is to define a semantics for models with unknown universes decoupled
from a specific constraint language to enable lifted and thereby, tractable
inference."
"In this paper we present Gelisp, a new library to represent musical
Constraint Satisfaction Problems and search strategies intuitively. Gelisp has
two interfaces, a command-line one for Common Lisp and a graphical one for
OpenMusic. Using Gelisp, we solved a problem of automatic music generation
proposed by composer Michael Jarrell and we found solutions for the
All-interval series."
"In reinforcement learning, we often define goals by specifying rewards within
desirable states. One problem with this approach is that we typically need to
redefine the rewards each time the goal changes, which often requires some
understanding of the solution in the agents environment. When humans are
learning to complete tasks, we regularly utilize alternative sources that guide
our understanding of the problem. Such task representations allow one to
specify goals on their own terms, thus providing specifications that can be
appropriately interpreted across various environments. This motivates our own
work, in which we represent goals in environments that are different from the
agents. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned
rewards that represent the visual similarity between an agents state and a
cross-domain goal image. We report results for learning the CDPRs with a deep
neural network and using them to solve two tasks with deep reinforcement
learning."
"Explanation is necessary for humans to understand and accept decisions made
by an AI system when the system's goal is known. It is even more important when
the AI system makes decisions in multi-agent environments where the human does
not know the systems' goals since they may depend on other agents' preferences.
In such situations, explanations should aim to increase user satisfaction,
taking into account the system's decision, the user's and the other agents'
preferences, the environment settings and properties such as fairness, envy and
privacy. Generating explanations that will increase user satisfaction is very
challenging; to this end, we propose a new research direction: xMASE. We then
review the state of the art and discuss research directions towards efficient
methodologies and algorithms for generating explanations that will increase
users' satisfaction from AI system's decisions in multi-agent environments."
"The personnel rostering problem is the problem of finding an optimal way to
assign employees to shifts, subject to a set of hard constraints which all
valid solutions must follow, and a set of soft constraints which define the
relative quality of valid solutions. The problem has received significant
attention in the literature and is addressed by a large number of exact and
metaheuristic methods. In order to make the complex and costly design of
heuristics for the personnel rostering problem automatic, we propose a new
method combined Deep Neural Network and Tree Search. By treating schedules as
matrices, the neural network can predict the distance between the current
solution and the optimal solution. It can select solution strategies by
analyzing existing (near-)optimal solutions to personnel rostering problem
instances. Combined with branch and bound, the network can give every node a
probability which indicates the distance between it and the optimal one, so
that a well-informed choice can be made on which branch to choose next and to
prune the search tree."
"In existing literature, while approximate approaches based on Monte-Carlo
simulation technique have been proposed to compute the semantics of
probabilistic argumentation, how to improve the efficiency of computation
without using simulation technique is still an open problem. In this paper, we
address this problem from the following two perspectives. First, conceptually,
we define specific properties to characterize the subgraphs of a PrAG with
respect to a given extension, such that the probability of a set of arguments E
being an extension can be defined in terms of these properties, without (or
with less) construction of subgraphs. Second, computationally, we take
preferred semantics as an example, and develop algorithms to evaluate the
efficiency of our approach. The results show that our approach not only
dramatically decreases the time for computing p(E^\sigma), but also has an
attractive property, which is contrary to that of existing approaches: the
denser the edges of a PrAG are or the bigger the size of a given extension E
is, the more efficient our approach computes p(E^\sigma). Meanwhile, it is
shown that under complete and preferred semantics, the problems of determining
p(E^\sigma) are fixed-parameter tractable."
"This volume contains the thesis abstracts presented at the Second Summer
School on Argumentation: Computational and Linguistic Perspectives (SSA'2016)
held on September 8-12 in Potsdam, Germany."
"Motivated by Shannon's model and recent rehabilitation of self-supervised
artificial intelligence having a ""World Model"", this paper propose an unified
intelligence-communication (UIC) model for describing a single agent and any
multi-agent system.
  Firstly, the environment is modelled as the generic communication channel
between agents. Secondly, the UIC model adopts a learning-agent model for
unifying several well-adopted agent architecture, e.g. rule-based agent model
in complex adaptive systems, layered model for describing human-level
intelligence, world-model based agent model. The model may also provide an
unified approach to investigate a multi-agent system (MAS) having multiple
action-perception modalities, e.g. explicitly information transfer and implicit
information transfer.
  This treatise would be divided into three parts, and this first part provides
an overview of the UIC model without introducing cumbersome mathematical
analysis and optimizations. In the second part of this treatise, case studies
with quantitative analysis driven by the UIC model would be provided,
exemplifying the adoption of the UIC model in multi-agent system. Specifically,
two representative cases would be studied, namely the analysis of a natural
multi-agent system, as well as the co-design of communication, perception and
action in an artificial multi-agent system. In the third part of this treatise,
the paper provides further insights and future research directions motivated by
the UIC model, such as unification of single intelligence and collective
intelligence, a possible explanation of intelligence emergence and a dual model
for agent-environment intelligence hypothesis.
  Notes: This paper is a Previewed Version, the extended full-version would be
released after being accepted."
"Safe-interval path planning (SIPP) is a powerful algorithm for finding a path
in the presence of dynamic obstacles. SIPP returns provably optimal solutions.
However, in many practical applications of SIPP such as path planning for
robots, one would like to trade-off optimality for shorter planning time. In
this paper we explore different ways to build a bounded-suboptimal SIPP and
discuss their pros and cons. We compare the different bounded-suboptimal
versions of SIPP experimentally. While there is no universal winner, the
results provide insights into when each method should be used."
"Cooperation among constraint solvers is difficult because different solving
paradigms have different theoretical foundations. Recent works have shown that
abstract interpretation can provide a unifying theory for various constraint
solvers. In particular, it relies on abstract domains which capture constraint
languages as ordered structures. The key insight of this paper is viewing
cooperation schemes as abstract domains combinations. We propose a modular
framework in which solvers and cooperation schemes can be seamlessly added and
combined. This differs from existing approaches such as SMT where the
cooperation scheme is usually fixed (e.g., Nelson-Oppen). We contribute to two
new cooperation schemes: (i) interval propagators completion that allows
abstract domains to exchange bound constraints, and (ii) delayed product which
exchanges over-approximations of constraints between two abstract domains.
Moreover, the delayed product is based on delayed goal of logic programming,
and it shows that abstract domains can also capture control aspects of
constraint solving. Finally, to achieve modularity, we propose the shared
product to combine abstract domains and cooperation schemes. Our approach has
been fully implemented, and we provide various examples on the flexible job
shop scheduling problem. Under consideration for acceptance in TPLP."
"This work is inspired by recent advances in hierarchical reinforcement
learning (HRL) (Barto and Mahadevan 2003; Hengst 2010), and improvements in
learning efficiency from heuristic-based subgoal selection, experience replay
(Lin 1993; Andrychowicz et al. 2017), and task-based curriculum learning
(Bengio et al. 2009; Zaremba and Sutskever 2014). We propose a new method to
integrate HRL, experience replay and effective subgoal selection through an
implicit curriculum design based on human expertise to support sample-efficient
learning and enhance interpretability of the agent's behavior. Human expertise
remains indispensable in many areas such as medicine (Buch, Ahmed, and
Maruthappu 2018) and law (Cath 2018), where interpretability, explainability
and transparency are crucial in the decision making process, for ethical and
legal reasons. Our method simplifies the complex task sets for achieving the
overall objectives by decomposing them into subgoals at different levels of
abstraction. Incorporating relevant subjective knowledge also significantly
reduces the computational resources spent in exploration for RL, especially in
high speed, changing, and complex environments where the transition dynamics
cannot be effectively learned and modelled in a short time. Experimental
results in two StarCraft II (SC2) (Vinyals et al. 2017) minigames demonstrate
that our method can achieve better sample efficiency than flat and end-to-end
RL methods, and provides an effective method for explaining the agent's
performance."
"Long-term forecasting involves predicting a horizon that is far ahead of the
last observation. It is a problem of high practical relevance, for instance for
companies in order to decide upon expensive long-term investments. Despite the
recent progress and success of Gaussian processes (GPs) based on spectral
mixture kernels, long-term forecasting remains a challenging problem for these
kernels because they decay exponentially at large horizons. This is mainly due
to their use of a mixture of Gaussians to model spectral densities.
Characteristics of the signal important for long-term forecasting can be
unravelled by investigating the distribution of the Fourier coefficients of
(the training part of) the signal, which is non-smooth, heavy-tailed, sparse,
and skewed. The heavy tail and skewness characteristics of such distributions
in the spectral domain allow to capture long-range covariance of the signal in
the time domain. Motivated by these observations, we propose to model spectral
densities using a skewed Laplace spectral mixture (SLSM) due to the skewness of
its peaks, sparsity, non-smoothness, and heavy tail characteristics. By
applying the inverse Fourier Transform to this spectral density we obtain a new
GP kernel for long-term forecasting. In addition, we adapt the lottery ticket
method, originally developed to prune weights of a neural network, to GPs in
order to automatically select the number of kernel components. Results of
extensive experiments, including a multivariate time series, show the
beneficial effect of the proposed SLSM kernel for long-term extrapolation and
robustness to the choice of the number of mixture components."
"The extremely sensitive and highly nonlinear search space of interplanetary
transfer trajectory design bring about big challenges on global optimization.
As a representative, the current known best solution of the global trajectory
optimization problem (GTOP) designed by the European space agency (ESA) is very
hard to be found. To deal with this difficulty, a powerful differential
evolution-based optimization tool named COoperative Differential Evolution
(CODE) is proposed in this paper. CODE employs a two-stage evolutionary
process, which concentrates on learning global structure in the earlier
process, and tends to self-adaptively learn the structures of different local
spaces. Besides, considering the spatial distribution of global optimum on
different problems and the gradient information on different variables, a
multiple boundary check technique has been employed. Also, Covariance Matrix
Adaptation Evolutionary Strategies (CMA-ES) is used as a local optimizer. The
previous studies have shown that a specific swarm intelligent optimization
algorithm usually can solve only one or two GTOP problems. However, the
experimental test results show that CODE can find the current known best
solutions of Cassini1 and Sagas directly, and the cooperation with CMA-ES can
solve Cassini2, GTOC1, Messenger (reduced) and Rosetta. For the most
complicated Messenger (full) problem, even though CODE cannot find the current
known best solution, the found best solution with objective function equaling
to 3.38 km/s is still a level that other swarm intelligent algorithms cannot
easily reach."
"In a variety of application settings, the user preference for a planning task
- the precise optimization objective - is difficult to elicit. One possible
remedy is planning as an iterative process, allowing the user to iteratively
refine and modify example plans. A key step to support such a process are
explanations, answering user questions about the current plan. In particular, a
relevant kind of question is ""Why does the plan you suggest not satisfy $p$?"",
where p is a plan property desirable to the user. Note that such a question
pertains to plan space, i.e., the set of possible alternative plans. Adopting
the recent approach to answer such questions in terms of plan-property
dependencies, here we implement a tool and user interface for human-guided
iterative planning including plan-space explanations. The tool runs in standard
Web browsers, and provides simple user interfaces for both developers and
users. We conduct a first user study, whose outcome indicates the usefulness of
plan-property dependency explanations in iterative planning."
"In recent years, the world has witnessed various primitives pertaining to the
complexity of human behavior. Identifying an event in the presence of
insufficient, incomplete, or tentative premises along with the constraints on
resources such as time, data and memory is a vital aspect of an intelligent
system. Data explosion presents one of the most challenging research issues for
intelligent systems; to optimally represent and store this heterogeneous and
voluminous data semantically to provide human behavior. There is a requirement
of intelligent but personalized human behavior subject to constraints on
resources and priority of the user. Knowledge, when represented in the form of
an ontology, procures an intelligent response to a query posed by users; but it
does not offer content in accordance with the user context. To this aim, we
propose a model to quantify the user context and provide semantic contextual
reasoning. A diagnostic belief algorithm (DBA) is also presented that
identifies a given event and also computes the confidence of the decision as a
function of available resources, premises, exceptions, and desired specificity.
We conduct an empirical study in the domain of day-to-day routine queries and
the experimental results show that the answer to queries and also its
confidence varies with user context."
"Case Management has been gradually evolving to support Knowledge-intensive
business process management, which resulted in developing different modeling
languages, e.g., Declare, Dynamic Condition Response (DCR), and Case Management
Model and Notation (CMMN). A language will die if users do not accept and use
it in practice - similar to extinct human languages. Thus, it is important to
evaluate how users perceive languages to determine if there is a need for
improvement. Although some studies have investigated how the process designers
perceived Declare and DCR, there is a lack of research on how they perceive
CMMN. Therefore, this study investigates how the process designers perceive the
usefulness and ease of use of CMMN and DCR based on the Technology Acceptance
Model. DCR is included to enable comparing the study result with previous ones.
The study is performed by educating master level students with these languages
over eight weeks by giving feedback on their assignments to reduce perceptions
biases. The students' perceptions are collected through questionnaires before
and after sending feedback on their final practice in the exam. Thus, the
result shows how the perception of participants can change by receiving
feedback - despite being well trained. The reliability of responses is tested
using Cronbach's alpha, and the result indicates that both languages have an
acceptable level for both perceived usefulness and ease of use."
"Deep neural networks are vulnerable to adversarial examples that mislead the
models with imperceptible perturbations. Though adversarial attacks have
achieved incredible success rates in the white-box setting, most existing
adversaries often exhibit weak transferability in the black-box setting,
especially under the scenario of attacking models with defense mechanisms. In
this work, we propose a new method called variance tuning to enhance the class
of iterative gradient based attack methods and improve their attack
transferability. Specifically, at each iteration for the gradient calculation,
instead of directly using the current gradient for the momentum accumulation,
we further consider the gradient variance of the previous iteration to tune the
current gradient so as to stabilize the update direction and escape from poor
local optima. Empirical results on the standard ImageNet dataset demonstrate
that our method could significantly improve the transferability of
gradient-based adversarial attacks. Besides, our method could be used to attack
ensemble models or be integrated with various input transformations.
Incorporating variance tuning with input transformations on iterative
gradient-based attacks in the multi-model setting, the integrated method could
achieve an average success rate of 90.1% against nine advanced defense methods,
improving the current best attack performance significantly by 85.1% . Code is
available at https://github.com/JHL-HUST/VT."
"Today, intelligent systems that offer artificial intelligence capabilities
often rely on machine learning. Machine learning describes the capacity of
systems to learn from problem-specific training data to automate the process of
analytical model building and solve associated tasks. Deep learning is a
machine learning concept based on artificial neural networks. For many
applications, deep learning models outperform shallow machine learning models
and traditional data analysis approaches. In this article, we summarize the
fundamentals of machine learning and deep learning to generate a broader
understanding of the methodical underpinning of current intelligent systems. In
particular, we provide a conceptual distinction between relevant terms and
concepts, explain the process of automated analytical model building through
machine learning and deep learning, and discuss the challenges that arise when
implementing such intelligent systems in the field of electronic markets and
networked business. These naturally go beyond technological aspects and
highlight issues in human-machine interaction and artificial intelligence
servitization."
"Discovering novel high-level concepts is one of the most important steps
needed for human-level AI. In inductive logic programming (ILP), discovering
novel high-level concepts is known as predicate invention (PI). Although seen
as crucial since the founding of ILP, PI is notoriously difficult and most ILP
systems do not support it. In this paper, we introduce POPPI, an ILP system
that formulates the PI problem as an answer set programming problem. Our
experiments show that (i) PI can drastically improve learning performance when
useful, (ii) PI is not too costly when unnecessary, and (iii) POPPI can
substantially outperform existing ILP systems."
"This paper presents new methods for analyzing and evaluating generalized
plans that can solve broad classes of related planning problems. Although
synthesis and learning of generalized plans has been a longstanding goal in AI,
it remains challenging due to fundamental gaps in methods for analyzing the
scope and utility of a given generalized plan. This paper addresses these gaps
by developing a new conceptual framework along with proof techniques and
algorithmic processes for assessing termination and goal-reachability related
properties of generalized plans. We build upon classic results from graph
theory to decompose generalized plans into smaller components that are then
used to derive hierarchical termination arguments. These methods can be used to
determine the utility of a given generalized plan, as well as to guide the
synthesis and learning processes for generalized plans. We present theoretical
as well as empirical results illustrating the scope of this new approach. Our
analysis shows that this approach significantly extends the class of
generalized plans that can be assessed automatically, thereby reducing barriers
in the synthesis and learning of reliable generalized plans."
"Tic Tac Toe is amongst the most well-known games. It has already been shown
that it is a biased game, giving more chances to win for the first player
leaving only a draw or a loss as possibilities for the opponent, assuming both
the players play optimally. Thus on average majority of the games played result
in a draw. The majority of the latest research on how to solve a tic tac toe
board state employs strategies such as Genetic Algorithms, Neural Networks,
Co-Evolution, and Evolutionary Programming. But these approaches deal with a
trivial board state of 3X3 and very little research has been done for a
generalized algorithm to solve 4X4,5X5,6X6 and many higher states. Even though
an algorithm exists which is Min-Max but it takes a lot of time in coming up
with an ideal move due to its recursive nature of implementation. A Sample has
been created on this link \url{https://bk-tic-tac-toe.herokuapp.com/} to prove
this fact. This is the main problem that this study is aimed at solving i.e
providing a generalized algorithm(Approximate method, Learning-Based) for
higher board states of tic tac toe to make precise moves in a short period.
Also, the code changes needed to accommodate higher board states will be
nominal. The idea is to pose the tic tac toe game as a well-posed learning
problem. The study and its results are promising, giving a high win to draw
ratio with each epoch of training. This study could also be encouraging for
other researchers to apply the same algorithm to other similar board games like
Minesweeper, Chess, and GO for finding efficient strategies and comparing the
results."
"This short paper compiles the big ideas behind some philosophical views,
definitions, and examples of causality. This collection spans the realms of the
four commonly adopted approaches to causality: Humes regularity,
counterfactual, manipulation, and mechanisms. This short review is motivated by
presenting simplified views and definitions and then supplements them with
examples from various fields, including economics, education, medicine,
politics, physics, and engineering. It is the hope that this short review comes
in handy for new and interested readers with little knowledge of causality and
causal inference."
"We propose a logical analysis of the concept of typicality, central in human
cognition (Rosch,1978). We start from a previously proposed extension of the
basic Description Logic ALC (a computationally tractable fragment of First
Order Logic, used to represent concept inclusions and ontologies) with a
typicality operator T that allows to consistently represent the attribution to
classes of individuals of properties with exceptions (as in the classic example
(i) typical birds fly, (ii) penguins are birds but (iii) typical penguins don't
fly). We then strengthen this extension in order to separately reason about the
typicality with respect to different aspects (e.g., flying, having nice
feather: in the previous example, penguins may not inherit the property of
flying, for which they are exceptional, but can nonetheless inherit other
properties, such as having nice feather)."
"It is possible that powerful and potentially dangerous artificial
intelligence (AI) might be developed in the future. An Oracle is a design which
aims to restrain the impact of a potentially dangerous AI by restricting the
agent to no actions besides answering questions. Unfortunately, most Oracles
will be motivated to gain more control over the world by manipulating users
through the content of their answers, and Oracles of potentially high
intelligence might be very successful at this
\citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two
designs for Oracles which, even under pessimistic assumptions, will not
manipulate their users into releasing them and yet will still be incentivised
to provide their users with helpful answers. The first design is the
counterfactual Oracle -- which choses its answer as if it expected nobody to
ever read it. The second design is the low-bandwidth Oracle -- which is limited
by the quantity of information it can transmit."
"Action abstractions restrict the number of legal actions available during
search in multi-unit real-time adversarial games, thus allowing algorithms to
focus their search on a set of promising actions. Optimal strategies derived
from un-abstracted spaces are guaranteed to be no worse than optimal strategies
derived from action-abstracted spaces. In practice, however, due to real-time
constraints and the state space size, one is only able to derive good
strategies in un-abstracted spaces in small-scale games. In this paper we
introduce search algorithms that use an action abstraction scheme we call
asymmetric abstraction. Asymmetric abstractions retain the un-abstracted
spaces' theoretical advantage over regularly abstracted spaces while still
allowing the search algorithms to derive effective strategies, even in
large-scale games. Empirical results on combat scenarios that arise in a
real-time strategy game show that our search algorithms are able to
substantially outperform state-of-the-art approaches."
"Hypernym identification of open-domain entities is crucial for taxonomy
construction as well as many higher-level applications. Current methods suffer
from either low precision or low recall. To decrease the difficulty of this
problem, we adopt a classification-based method. We pre-define a concept
taxonomy and classify an entity to one of its leaf concept, based on the name
and description information of the entity. A convolutional neural network
classifier and a K-means clustering module are adopted for classification. We
applied this system to 2.1 million Baidu Baike entities, and 1.1 million of
them were successfully identified with a precision of 99.36%."
"Models of a phenomenon are often developed by examining it under different
experimental conditions, or measurement contexts. The resultant probabilistic
models assume that the underlying random variables, which define a measurable
set of outcomes, can be defined independent of the measurement context. The
phenomenon is deemed contextual when this assumption fails. Contextuality is an
important issue in quantum physics. However, there has been growing speculation
that it manifests outside the quantum realm with human cognition being a
particularly prominent area of investigation. This article contributes the
foundations of a probabilistic programming language that allows convenient
exploration of contextuality in wide range of applications relevant to
cognitive science and artificial intelligence. Specific syntax is proposed to
allow the specification of ""measurement contexts"". Each such context delivers a
partial model of the phenomenon based on the associated experimental condition
described by the measurement context. The probabilistic program is translated
into a hypergraph in a modular way. Recent theoretical results from the field
of quantum physics show that contextuality can be equated with the possibility
of constructing a probabilistic model on the resulting hypergraph. The use of
hypergraphs opens the door for a theoretically succinct and efficient
computational semantics sensitive to modelling both contextual and
non-contextual phenomena. Finally, this article raises awareness of
contextuality beyond quantum physics and to contribute formal methods to detect
its presence by means of hypergraph semantics."
"We use model-free reinforcement learning, extensive simulation, and transfer
learning to develop a continuous control algorithm that has good zero-shot
performance in a real physical environment. We train a simulated agent to act
optimally across a set of similar environments, each with dynamics drawn from a
prior distribution. We propose that the agent is able to adjust its actions
almost immediately, based on small set of observations. This robust and
adaptive behavior is enabled by using a policy gradient algorithm with an Long
Short Term Memory (LSTM) function approximation. Finally, we train an agent to
navigate a two-dimensional environment with uncertain dynamics and noisy
observations. We demonstrate that this agent has good zero-shot performance in
a real physical environment. Our preliminary results indicate that the agent is
able to infer the environmental dynamics after only a few timesteps, and adjust
its actions accordingly."
"General Video Game Playing (GVGP) aims at designing an agent that is capable
of playing multiple video games with no human intervention. In 2014, The
General Video Game AI (GVGAI) competition framework was created and released
with the purpose of providing researchers a common open-source and easy to use
platform for testing their AI methods with potentially infinity of games
created using Video Game Description Language (VGDL). The framework has been
expanded into several tracks during the last few years to meet the demand of
different research directions. The agents are required either to play multiple
unknown games with or without access to game simulations, or to design new game
levels or rules. This survey paper presents the VGDL, the GVGAI framework,
existing tracks, and reviews the wide use of GVGAI framework in research,
education and competitions five years after its birth. A future plan of
framework improvements is also described."
"With an increasing number of web services, providing an end-to-end Quality of
Service (QoS) guarantee in responding to user queries is becoming an important
concern. Multiple QoS parameters (e.g., response time, latency, throughput,
reliability, availability, success rate) are associated with a service,
thereby, service composition with a large number of candidate services is a
challenging multi-objective optimization problem. In this paper, we study the
multi-constrained multi-objective QoS aware web service composition problem and
propose three different approaches to solve the same, one optimal, based on
Pareto front construction and two other based on heuristically traversing the
solution space. We compare the performance of the heuristics against the
optimal, and show the effectiveness of our proposals over other classical
approaches for the same problem setting, with experiments on WSC-2009 and
ICEBE-2005 datasets."
"In this work, we attempt to answer a critical question: whether there exists
some input sequence that will cause a well-trained discrete-space neural
network sequence-to-sequence (seq2seq) model to generate egregious outputs
(aggressive, malicious, attacking, etc.). And if such inputs exist, how to find
them efficiently. We adopt an empirical methodology, in which we first create
lists of egregious output sequences, and then design a discrete optimization
algorithm to find input sequences that will cause the model to generate them.
Moreover, the optimization algorithm is enhanced for large vocabulary search
and constrained to search for input sequences that are likely to be input by
real-world users. In our experiments, we apply this approach to dialogue
response generation models trained on three real-world dialogue data-sets:
Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate
malicious responses. We demonstrate that given the trigger inputs our algorithm
finds, a significant number of malicious sentences are assigned large
probability by the model, which reveals an undesirable consequence of standard
seq2seq training."
"The paper presents an extension of Shannon entropy for neutrosophic
information. This extension uses a new formula for distance between two
neutrosophic triplets. In addition, the obtained results are particularized for
bifuzzy, intuitionistic and paraconsistent fuzzy information."
"Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018"
"In this chapter tools and techniques from the mathematical theory of formal
concept analysis are applied to hypertext systems in general, and the World
Wide Web in particular. Various processes for the conceptual structuring of
hypertext are discussed: summarization, conceptual scaling, and the creation of
conceptual links. Well-known interchange formats for summarizing networked
information resources as resource meta-information are reviewed, and two new
interchange formats originating from formal concept analysis are advocated.
Also reviewed is conceptual scaling, which provides a principled approach to
the faceted analysis techniques in library science classification. The
important notion of conceptual linkage is introduced as a generalization of a
hyperlink. The automatic hyperization of the content of legacy data is
described, and the composite conceptual structuring with hypertext linkage is
defined. For the conceptual empowerment of the Web user, a new technique called
conceptual browsing is advocated. Conceptual browsing, which browses over
conceptual links, is dual mode (extensional versus intensional) and dual scope
(global versus local)."
"In this paper, we introduce a new heuristic search algorithm based on mean
values for real-time planning, called MHSP. It consists in associating the
principles of UCT, a bandit-based algorithm which gave very good results in
computer games, and especially in Computer Go, with heuristic search in order
to obtain a real-time planner in the context of classical planning. MHSP is
evaluated on different planning problems and compared to existing algorithms
performing on-line search and learning. Besides, our results highlight the
capacity of MHSP to return plans in a real-time manner which tend to an optimal
plan over the time which is faster and of better quality compared to existing
algorithms in the literature."
"""Theorem proving is similar to the game of Go. So, we can probably improve
our provers using deep learning, like DeepMind built the super-human computer
Go program, AlphaGo."" Such optimism has been observed among participants of
AITP2017. But is theorem proving really similar to Go? In this paper, we first
identify the similarities and differences between them and then propose a
system in which various provers keep competing against each other and changing
themselves until they prove conjectures provided by users."
"Belief tracking is a basic problem in planning with sensing. While the
problem is intractable, it has been recently shown that for both deterministic
and non-deterministic systems expressed in compact form, it can be done in time
and space that are exponential in the problem width. The width measures the
maximum number of state variables that are all relevant to a given precondition
or goal. In this work, we extend this result both theoretically and
practically. First, we introduce an alternative decomposition scheme and
algorithm with the same time complexity but different completeness guarantees,
whose space complexity is much smaller: exponential in the causal width of the
problem that measures the number of state variables that are causally relevant
to a given precondition, goal, or observable. Second, we introduce a fast,
meaningful, and powerful approximation that trades completeness by speed, and
is both time and space exponential in the problem causal width. It is then
shown empirically that the algorithm combined with simple heuristics yields
state-of-the-art real-time performance in domains with high widths but low
causal widths such as Minesweeper, Battleship, and Wumpus."
"We describe nearly fifteen years of General Game Playing experimental
research history in the context of reproducibility and fairness of comparisons
between various GGP agents and systems designed to play games described by
different formalisms. We think our survey may provide an interesting
perspective of how chaotic methods were allowed when nothing better was
possible. Finally, from our experience-based view, we would like to propose a
few recommendations of how such specific heterogeneous branch of research
should be handled appropriately in the future. The goal of this note is to
point out common difficulties and problems in the experimental research in the
area. We hope that our recommendations will help in avoiding them in future
works and allow more fair and reproducible comparisons."
"Automated negotiation has been used in a variety of distributed settings,
such as privacy in the Internet of Things (IoT) devices and power distribution
in Smart Grids. The most common protocol under which these agents negotiate is
the Alternating Offers Protocol (AOP). Under this protocol, agents cannot
express any additional information to each other besides a counter offer. This
can lead to unnecessarily long negotiations when, for example, negotiations are
impossible, risking to waste bandwidth that is a precious resource at the edge
of the network. While alternative protocols exist which alleviate this problem,
these solutions are too complex for low power devices, such as IoT sensors
operating at the edge of the network. To improve this bottleneck, we introduce
an extension to AOP called Alternating Constrained Offers Protocol (ACOP), in
which agents can also express constraints to each other. This allows agents to
both search the possibility space more efficiently and recognise impossible
situations sooner. We empirically show that agents using ACOP can significantly
reduce the number of messages a negotiation takes, independently of the
strategy agents choose. In particular, we show our method significantly reduces
the number of messages when an agreement is not possible. Furthermore, when an
agreement is possible it reaches this agreement sooner with no negative effect
on the utility."
"Transportation is quickly evolving in the emerging smart city ecosystem with
personalized ride sharing services quickly advancing. Yet, the public bus
infrastructure has been slow to respond to these trends. With our research, we
propose a semi-dynamic bus routing framework that is data-driven and responsive
to relevant parameters in bus transport. We use newly published bus event data
from a bus line in Boston and several algorithmic heuristics to create this
framework and demonstrate the capabilities and results. We find that this
approach yields a very promising routing infrastructure that is smarter and
more dynamic than the existing system."
"Robot sequential decision-making in the real world is a challenge because it
requires the robots to simultaneously reason about the current world state and
dynamics, while planning actions to accomplish complex tasks. On the one hand,
declarative languages and reasoning algorithms well support representing and
reasoning with commonsense knowledge. But these algorithms are not good at
planning actions toward maximizing cumulative reward over a long, unspecified
horizon. On the other hand, probabilistic planning frameworks, such as Markov
decision processes (MDPs) and partially observable MDPs (POMDPs), well support
planning to achieve long-term goals under uncertainty. But they are
ill-equipped to represent or reason about knowledge that is not directly
related to actions.
  In this article, we present a novel algorithm, called iCORPP, to
simultaneously estimate the current world state, reason about world dynamics,
and construct task-oriented controllers. In this process, robot decision-making
problems are decomposed into two interdependent (smaller) subproblems that
focus on reasoning to ""understand the world"" and planning to ""achieve the goal""
respectively. Contextual knowledge is represented in the reasoning component,
which makes the planning component epistemic and enables active information
gathering. The developed algorithm has been implemented and evaluated both in
simulation and on real robots using everyday service tasks, such as indoor
navigation, dialog management, and object delivery. Results show significant
improvements in scalability, efficiency, and adaptiveness, compared to
competitive baselines including handcrafted action policies."
"Generative adversarial networks (GANs) are quickly becoming a ubiquitous
approach to procedurally generating video game levels. While GAN generated
levels are stylistically similar to human-authored examples, human designers
often want to explore the generative design space of GANs to extract
interesting levels. However, human designers find latent vectors opaque and
would rather explore along dimensions the designer specifies, such as number of
enemies or obstacles. We propose using state-of-the-art quality diversity
algorithms designed to optimize continuous spaces, i.e. MAP-Elites with a
directional variation operator and Covariance Matrix Adaptation MAP-Elites, to
efficiently explore the latent space of a GAN to extract levels that vary
across a set of specified gameplay measures. In the benchmark domain of Super
Mario Bros, we demonstrate how designers may specify gameplay measures to our
system and extract high-quality (playable) levels with a diverse range of level
mechanics, while still maintaining stylistic similarity to human authored
examples. An online user study shows how the different mechanics of the
automatically generated levels affect subjective ratings of their perceived
difficulty and appearance."
"As the current trend of artificial intelligence is shifting towards
self-supervised learning, conventional norms such as highly curated
domain-specific data, application-specific learning models, extrinsic reward
based learning policies etc. might not provide with the suitable ground for
such developments. In this paper, we introduce SEDRo, a Simulated Environment
for Developmental Robotics which allows a learning agent to have similar
experiences that a human infant goes through from the fetus stage up to 12
months. A series of simulated tests based on developmental psychology will be
used to evaluate the progress of a learning model."
"Meta-reinforcement learning (RL) addresses the problem of sample inefficiency
in deep RL by using experience obtained in past tasks for a new task to be
solved.
  However, most meta-RL methods require partially or fully on-policy data,
i.e., they cannot reuse the data collected by past policies, which hinders the
improvement of sample efficiency.
  To alleviate this problem, we propose a novel off-policy meta-RL method,
embedding learning and evaluation of uncertainty (ELUE).
  An ELUE agent is characterized by the learning of a feature embedding space
shared among tasks.
  It learns a belief model over the embedding space and a belief-conditional
policy and Q-function.
  Then, for a new task, it collects data by the pretrained policy, and updates
its belief based on the belief model.
  Thanks to the belief update, the performance can be improved with a small
amount of data.
  In addition, it updates the parameters of the neural networks to adjust the
pretrained relationships when there are enough data.
  We demonstrate that ELUE outperforms state-of-the-art meta RL methods through
experiments on meta-RL benchmarks."
"With the recent advancements in deep learning, neural solvers have gained
promising results in solving math word problems. However, these SOTA solvers
only generate binary expression trees that contain basic arithmetic operators
and do not explicitly use the math formulas. As a result, the expression trees
they produce are lengthy and uninterpretable because they need to use multiple
operators and constants to represent one single formula. In this paper, we
propose sequence-to-general tree (S2G) that learns to generate interpretable
and executable operation trees where the nodes can be formulas with an
arbitrary number of arguments. With nodes now allowed to be formulas, S2G can
learn to incorporate mathematical domain knowledge into problem-solving, making
the results more interpretable. Experiments show that S2G can achieve a better
performance against strong baselines on problems that require domain knowledge."
"The explosion in the sheer magnitude and complexity of financial news data in
recent years makes it increasingly challenging for investment analysts to
extract valuable insights and perform analysis. We propose FactCheck in
finance, a web-based news aggregator with deep learning models, to provide
analysts with a holistic view of important financial events from multilingual
news sources and extract events using an unsupervised clustering method. A web
interface is provided to examine the credibility of news articles using a
transformer-based fact-checker. The performance of the fact checker is
evaluated using a dataset related to merger and acquisition (M\&A) events and
is shown to outperform several strong baselines."
"Federated learning (FL) is a distributed model for deep learning that
integrates client-server architecture, edge computing, and real-time
intelligence. FL has the capability of revolutionizing machine learning (ML)
but lacks in the practicality of implementation due to technological
limitations, communication overhead, non-IID (independent and identically
distributed) data, and privacy concerns. Training a ML model over heterogeneous
non-IID data highly degrades the convergence rate and performance. The existing
traditional and clustered FL algorithms exhibit two main limitations, including
inefficient client training and static hyper-parameter utilization. To overcome
these limitations, we propose a novel hybrid algorithm, namely genetic
clustered FL (Genetic CFL), that clusters edge devices based on the training
hyper-parameters and genetically modifies the parameters cluster-wise. Then, we
introduce an algorithm that drastically increases the individual cluster
accuracy by integrating the density-based clustering and genetic
hyper-parameter optimization. The results are bench-marked using MNIST
handwritten digit dataset and the CIFAR-10 dataset. The proposed genetic CFL
shows significant improvements and works well with realistic cases of non-IID
and ambiguous data."
"Packet routing is a fundamental problem in communication networks that
decides how the packets are directed from their source nodes to their
destination nodes through some intermediate nodes. With the increasing
complexity of network topology and highly dynamic traffic demand, conventional
model-based and rule-based routing schemes show significant limitations, due to
the simplified and unrealistic model assumptions, and lack of flexibility and
adaption. Adding intelligence to the network control is becoming a trend and
the key to achieving high-efficiency network operation. In this paper, we
develop a model-free and data-driven routing strategy by leveraging
reinforcement learning (RL), where routers interact with the network and learn
from the experience to make some good routing configurations for the future.
Considering the graph nature of the network topology, we design a multi-agent
RL framework in combination with Graph Neural Network (GNN), tailored to the
routing problem. Three deployment paradigms, centralized, federated, and
cooperated learning, are explored respectively. Simulation results demonstrate
that our algorithm outperforms some existing benchmark algorithms in terms of
packet transmission delay and affordable load."
"Multimodal sentiment analysis aims to extract and integrate semantic
information collected from multiple modalities to recognize the expressed
emotions and sentiment in multimodal data. This research area's major concern
lies in developing an extraordinary fusion scheme that can extract and
integrate key information from various modalities. However, one issue that may
restrict previous work to achieve a higher level is the lack of proper modeling
for the dynamics of the competition between the independence and relevance
among modalities, which could deteriorate fusion outcomes by causing the
collapse of modality-specific feature space or introducing extra noise. To
mitigate this, we propose the Bi-Bimodal Fusion Network (BBFN), a novel
end-to-end network that performs fusion (relevance increment) and separation
(difference increment) on pairwise modality representations. The two parts are
trained simultaneously such that the combat between them is simulated. The
model takes two bimodal pairs as input due to the known information imbalance
among modalities. In addition, we leverage a gated control mechanism in the
Transformer architecture to further improve the final output. Experimental
results on three datasets (CMU-MOSI, CMU-MOSEI, and UR-FUNNY) verifies that our
model significantly outperforms the SOTA. The implementation of this work is
available at https://github.com/declare-lab/multimodal-deep-learning."
"In this work, we fill the gap in the Semantic Web in the context of Cultural
Symbolism. Building upon earlier work in, we introduce the Simulation Ontology,
an ontology that models the background knowledge of symbolic meanings,
developed by combining the concepts taken from the authoritative theory of
Simulacra and Simulations of Jean Baudrillard with symbolic structures and
content taken from ""Symbolism: a Comprehensive Dictionary"" by Steven Olderr. We
re-engineered the symbolic knowledge already present in heterogeneous resources
by converting it into our ontology schema to create HyperReal, the first
knowledge graph completely dedicated to cultural symbolism. A first experiment
run on the knowledge graph is presented to show the potential of quantitative
research on symbolism."
"This paper optimizes motion planning when there is a known risk that the road
choice suggested by a Satnav (GPS) is not on a shortest path. At every branch
node of a network Q, a Satnav (GPS) points to the arc leading to the
destination, or home node, H - but only with a high known probability p. Always
trusting the Satnav's suggestion may lead to an infinite cycle. If one wishes
to reach H in least expected time, with what probability q=q(Q,p) should one
trust the pointer (if not, one chooses randomly among the other arcs)? We call
this the Faulty Satnav (GPS) Problem. We also consider versions where the trust
probability q can depend on the degree of the current node and a `treasure
hunt' where two searchers try to reach H first. The agent searching for H need
not be a car, that is just a familiar example -- it could equally be a UAV
receiving unreliable GPS information. This problem has its origin not in driver
frustration but in the work of Fonio et al (2017) on ant navigation, where the
pointers correspond to pheromone markers pointing to the nest. Neither the
driver or ant will know the exact process by which a choice (arc) is suggested,
which puts the problem into the domain of how much to trust an option suggested
by AI."
"Representation learning models for Knowledge Graphs (KG) have proven to be
effective in encoding structural information and performing reasoning over KGs.
In this paper, we propose a novel pre-training-then-fine-tuning framework for
knowledge graph representation learning, in which a KG model is firstly
pre-trained with triple classification task, followed by discriminative
fine-tuning on specific downstream tasks such as entity type prediction and
entity alignment. Drawing on the general ideas of learning deep contextualized
word representations in typical pre-trained language models, we propose SCoP to
learn pre-trained KG representations with structural and contextual triples of
the target triple encoded. Experimental results demonstrate that fine-tuning
SCoP not only outperforms results of baselines on a portfolio of downstream
tasks but also avoids tedious task-specific model design and parameter
training."
"In a world of daily emerging scientific inquisition and discovery, the
prolific launch of machine learning across industries comes to little surprise
for those familiar with the potential of ML. Neither so should the congruent
expansion of ethics-focused research that emerged as a response to issues of
bias and unfairness that stemmed from those very same applications. Fairness
research, which focuses on techniques to combat algorithmic bias, is now more
supported than ever before. A large portion of fairness research has gone to
producing tools that machine learning practitioners can use to audit for bias
while designing their algorithms. Nonetheless, there is a lack of application
of these fairness solutions in practice. This systematic review provides an
in-depth summary of the algorithmic bias issues that have been defined and the
fairness solution space that has been proposed. Moreover, this review provides
an in-depth breakdown of the caveats to the solution space that have arisen
since their release and a taxonomy of needs that have been proposed by machine
learning practitioners, fairness researchers, and institutional stakeholders.
These needs have been organized and addressed to the parties most influential
to their implementation, which includes fairness researchers, organizations
that produce ML algorithms, and the machine learning practitioners themselves.
These findings can be used in the future to bridge the gap between
practitioners and fairness experts and inform the creation of usable fair ML
toolkits."
"For problems requiring cooperation, many multiagent systems implement
solutions among either individual agents or across an entire population towards
a common goal. Multiagent teams are primarily studied when in conflict;
however, organizational psychology (OP) highlights the benefits of teams among
human populations for learning how to coordinate and cooperate. In this paper,
we propose a new model of multiagent teams for reinforcement learning (RL)
agents inspired by OP and early work on teams in artificial intelligence. We
validate our model using complex social dilemmas that are popular in recent
multiagent RL and find that agents divided into teams develop cooperative
pro-social policies despite incentives to not cooperate. Furthermore, agents
are better able to coordinate and learn emergent roles within their teams and
achieve higher rewards compared to when the interests of all agents are
aligned."
"In mathematics information is a number that measures uncertainty (entropy)
based on a probabilistic distribution, often of an obscure origin. In real life
language information is a datum, a statement, more precisely, a formula. But
such a formula should be justified by a proof. I try to formalize this
perception of information. The measure of informativeness of a proof is based
on the set of proofs related to the formulas under consideration. This set of
possible proofs (`a knowledge base') defines a probabilistic measure, and
entropic weight is defined using this measure. The paper is mainly conceptual,
it is not clear where and how this approach can be applied."
"Interpretable regression models are important for many application domains,
as they allow experts to understand relations between variables from sparse
data. Symbolic regression addresses this issue by searching the space of all
possible free form equations that can be constructed from elementary algebraic
functions. While explicit mathematical functions can be rediscovered this way,
the determination of unknown numerical constants during search has been an
often neglected issue. We propose a new multi-objective memetic algorithm that
exploits a differentiable Cartesian Genetic Programming encoding to learn
constants during evolutionary loops. We show that this approach is competitive
or outperforms machine learned black box regression models or hand-engineered
fits for two applications from space: the Mars express thermal power estimation
and the determination of the age of stars by gyrochronology."
"The unbiased learning to rank (ULTR) problem has been greatly advanced by
recent deep learning techniques and well-designed debias algorithms. However,
promising results on the existing benchmark datasets may not be extended to the
practical scenario due to the following disadvantages observed from those
popular benchmark datasets: (1) outdated semantic feature extraction where
state-of-the-art large scale pre-trained language models like BERT cannot be
exploited due to the missing of the original text;(2) incomplete display
features for in-depth study of ULTR, e.g., missing the displayed abstract of
documents for analyzing the click necessary bias; (3) lacking real-world user
feedback, leading to the prevalence of synthetic datasets in the empirical
study. To overcome the above disadvantages, we introduce the Baidu-ULTR
dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008
expert annotated queries, which is orders of magnitude larger than the existing
ones. Baidu-ULTR provides:(1) the original semantic feature and a pre-trained
language model for easy usage; (2) sufficient display information such as
position, displayed height, and displayed abstract, enabling the comprehensive
study of different biases with advanced techniques such as causal discovery and
meta-learning; and (3) rich user feedback on search result pages (SERPs) like
dwelling time, allowing for user engagement optimization and promoting the
exploration of multi-task learning in ULTR. In this paper, we present the
design principle of Baidu-ULTR and the performance of benchmark ULTR algorithms
on this new data resource, favoring the exploration of ranking for long-tail
queries and pre-training tasks for ranking. The Baidu-ULTR dataset and
corresponding baseline implementation are available at
https://github.com/ChuXiaokai/baidu_ultr_dataset."
"The extensive adoption of business analytics (BA) has brought financial gains
and increased efficiencies. However, these advances have simultaneously drawn
attention to rising legal and ethical challenges when BA inform decisions with
fairness implications. As a response to these concerns, the emerging study of
algorithmic fairness deals with algorithmic outputs that may result in
disparate outcomes or other forms of injustices for subgroups of the
population, especially those who have been historically marginalized. Fairness
is relevant on the basis of legal compliance, social responsibility, and
utility; if not adequately and systematically addressed, unfair BA systems may
lead to societal harms and may also threaten an organization's own survival,
its competitiveness, and overall performance. This paper offers a
forward-looking, BA-focused review of algorithmic fairness. We first review the
state-of-the-art research on sources and measures of bias, as well as bias
mitigation algorithms. We then provide a detailed discussion of the
utility-fairness relationship, emphasizing that the frequent assumption of a
trade-off between these two constructs is often mistaken or short-sighted.
Finally, we chart a path forward by identifying opportunities for business
scholars to address impactful, open challenges that are key to the effective
and responsible deployment of BA."
"""Unless and until our society recognizes cyber bullying for what it is, the
suffering of thousands of silent victims will continue."" ~ Anna Maria Chavez.
There had been series of research on cyber bullying which are unable to provide
reliable solution to cyber bullying. In this research work, we were able to
provide a permanent solution to this by developing a model capable of detecting
and intercepting bullying incoming and outgoing messages with 92% accuracy. We
also developed a chatbot automation messaging system to test our model leading
to the development of Artificial Intelligence powered anti-cyber bullying
system using machine learning algorithm of Multinomial Naive Bayes (MNB) and
optimized linear Support Vector Machine (SVM). Our model is able to detect and
intercept bullying outgoing and incoming bullying messages and take immediate
action."
"SOTA multiagent reinforcement algorithms distinguish themselves in many ways
from their single-agent equivalences. However, most of them still totally
inherit the single-agent exploration-exploitation strategy. Naively inheriting
this strategy from single-agent algorithms causes potential collaboration
failures, in which the agents blindly follow mainstream behaviors and reject
taking minority responsibility. We name this problem the Responsibility
Diffusion (RD) as it shares similarities with a same-name social psychology
effect. In this work, we start by theoretically analyzing the cause of this RD
problem, which can be traced back to the exploration-exploitation dilemma of
multiagent systems (especially large-scale multiagent systems). We address this
RD problem by proposing a Policy Resonance (PR) approach which modifies the
collaborative exploration strategy of agents by refactoring the joint agent
policy while keeping individual policies approximately invariant. Next, we show
that SOTA algorithms can equip this approach to promote the collaborative
performance of agents in complex cooperative tasks. Experiments are performed
in multiple test benchmark tasks to illustrate the effectiveness of this
approach."
"Humans can generate reasonable answers to novel queries (Schulz, 2012): if I
asked you what kind of food you want to eat for lunch, you would respond with a
food, not a time. The thought that one would respond ""After 4pm"" to ""What would
you like to eat"" is either a joke or a mistake, and seriously entertaining it
as a lunch option would likely never happen in the first place. While
understanding how people come up with new ideas, thoughts, explanations, and
hypotheses that obey the basic constraints of a novel search space is of
central importance to cognitive science, there is no agreed-on formal model for
this kind of reasoning. We propose that a core component of any such reasoning
system is a type theory: a formal imposition of structure on the kinds of
computations an agent can perform, and how they're performed. We motivate this
proposal with three empirical observations: adaptive constraints on learning
and inference (i.e. generating reasonable hypotheses), how people draw
distinctions between improbability and impossibility, and people's ability to
reason about things at varying levels of abstraction."
"The present paper comes across the main steps that laid from Zadeh's
fuzziness ana Atanassov's intuitionistic fuzzy sets to Smarandache's
indeterminacy and to Molodstov's soft sets. Two hybrid methods for assessment
and decision making respectively under fuzzy conditions are also presented
through suitable examples that use soft sets and real intervals as tools. The
decision making method improves an earlier method of Maji et al. Further, it is
described how the concept of topological space, the most general category of
mathematical spaces, can be extended to fuzzy structures and how to generalize
the fundamental mathematical concepts of limit, continuity compactness and
Hausdorff space within such kind of structures. In particular, fuzzy and soft
topological spaces are defined and examples are given to illustrate these
generalizations."
"VAEs are probabilistic graphical models based on neural networks that allow
the coding of input data in a latent space formed by simpler probability
distributions and the reconstruction, based on such latent variables, of the
source data. After training, the reconstruction network, called decoder, is
capable of generating new elements belonging to a close distribution, ideally
equal to the original one. This article has been written in Spanish to
facilitate the arrival of this scientific knowledge to the Spanish-speaking
community."
"Core knowledge about physical objects -- e.g., their permanency, spatial
transformations, and interactions -- is one of the most fundamental building
blocks of biological intelligence across humans and non-human animals. While AI
techniques in certain domains (e.g. vision, NLP) have advanced dramatically in
recent years, no current AI systems can yet match human abilities in flexibly
applying core knowledge to solve novel tasks. We propose a new AI approach to
core knowledge that combines 1) visual representations of core knowledge
inspired by human mental imagery abilities, especially as observed in studies
of neurodivergent individuals; with 2) tree-search-based program synthesis for
flexibly combining core knowledge to form new reasoning strategies on the fly.
We demonstrate our system's performance on the very difficult Abstraction \&
Reasoning Corpus (ARC) challenge, and we share experimental results from
publicly available ARC items as well as from our 4th-place finish on the
private test set during the 2022 global ARCathon challenge."
"Sparsity of rewards while applying a deep reinforcement learning method
negatively affects its sample-efficiency. A viable solution to deal with the
sparsity of rewards is to learn via intrinsic motivation which advocates for
adding an intrinsic reward to the reward function to encourage the agent to
explore the environment and expand the sample space. Though intrinsic
motivation methods are widely used to improve data-efficient learning in the
reinforcement learning model, they also suffer from the so-called detachment
problem. In this article, we discuss the limitations of intrinsic curiosity
module in sparse-reward multi-agent reinforcement learning and propose a method
called I-Go-Explore that combines the intrinsic curiosity module with the
Go-Explore framework to alleviate the detachment problem."
"AI-Powered database (AI-DB) is a novel relational database system that uses a
self-supervised neural network, database embedding, to enable semantic SQL
queries on relational tables. In this paper, we describe an architecture and
implementation of in-database interpretability infrastructure designed to
provide simple, transparent, and relatable insights into ranked results of
semantic SQL queries supported by AI-DB. We introduce a new co-occurrence based
interpretability approach to capture relationships between relational entities
and describe a space-efficient probabilistic Sketch implementation to store and
process co-occurrence counts. Our approach provides both query-agnostic
(global) and query-specific (local) interpretabilities. Experimental evaluation
demonstrate that our in-database probabilistic approach provides the same
interpretability quality as the precise space-inefficient approach, while
providing scalable and space efficient runtime behavior (up to 8X space
savings), without any user intervention."
"We examine the recently proposed language of Logical Credal Networks, in
particular investigating the consequences of various Markov conditions. We
introduce the notion of structure for a Logical Credal Network and show that a
structure without directed cycles leads to a well-known factorization result.
For networks with directed cycles, we analyze the differences between Markov
conditions, factorization results, and specification requirements."
"This work studies non-cooperative Multi-Agent Reinforcement Learning (MARL)
where multiple agents interact in the same environment and whose goal is to
maximize the individual returns. Challenges arise when scaling up the number of
agents due to the resultant non-stationarity that the many agents introduce. In
order to address this issue, Mean Field Games (MFG) rely on the symmetry and
homogeneity assumptions to approximate games with very large populations.
Recently, deep Reinforcement Learning has been used to scale MFG to games with
larger number of states. Current methods rely on smoothing techniques such as
averaging the q-values or the updates on the mean-field distribution. This work
presents a different approach to stabilize the learning based on proximal
updates on the mean-field policy. We name our algorithm Mean Field Proximal
Policy Optimization (MF-PPO), and we empirically show the effectiveness of our
method in the OpenSpiel framework."
"The spread of misinformation in social media outlets has become a prevalent
societal problem and is the cause of many kinds of social unrest. Curtailing
its prevalence is of great importance and machine learning has shown
significant promise. However, there are two main challenges when applying
machine learning to this problem. First, while much too prevalent in one
respect, misinformation, actually, represents only a minor proportion of all
the postings seen on social media. Second, labeling the massive amount of data
necessary to train a useful classifier becomes impractical. Considering these
challenges, we propose a simple semi-supervised learning framework in order to
deal with extreme class imbalances that has the advantage, over other
approaches, of using actual rather than simulated data to inflate the minority
class. We tested our framework on two sets of Covid-related Twitter data and
obtained significant improvement in F1-measure on extremely imbalanced
scenarios, as compared to simple classical and deep-learning data generation
methods such as SMOTE, ADASYN, or GAN-based data generation."
"Standpoint EL is a multi-modal extension of the popular description logic EL
that allows for the integrated representation of domain knowledge relative to
diverse standpoints or perspectives. Advantageously, its satisfiability problem
has recently been shown to be in PTime, making it a promising framework for
large-scale knowledge integration.
  In this paper, we show that we can further push the expressivity of this
formalism, arriving at an extended logic, called Standpoint EL+, which allows
for axiom negation, role chain axioms, self-loops, and other features, while
maintaining tractability. This is achieved by designing a
satisfiability-checking deduction calculus, which at the same time addresses
the need for practical algorithms. We demonstrate the feasibility of our
calculus by presenting a prototypical Datalog implementation of its deduction
rules."
"The problem of model counting, also known as #SAT, is to compute the number
of models or satisfying assignments of a given Boolean formula $F$. Model
counting is a fundamental problem in computer science with a wide range of
applications. In recent years, there has been a growing interest in using
hashing-based techniques for approximate model counting that provide
$(\varepsilon, \delta)$-guarantees: i.e., the count returned is within a
$(1+\varepsilon)$-factor of the exact count with confidence at least
$1-\delta$. While hashing-based techniques attain reasonable scalability for
large enough values of $\delta$, their scalability is severely impacted for
smaller values of $\delta$, thereby preventing their adoption in application
domains that require estimates with high confidence.
  The primary contribution of this paper is to address the Achilles heel of
hashing-based techniques: we propose a novel approach based on rounding that
allows us to achieve a significant reduction in runtime for smaller values of
$\delta$. The resulting counter, called RoundMC, achieves a substantial runtime
performance improvement over the current state-of-the-art counter, ApproxMC. In
particular, our extensive evaluation over a benchmark suite consisting of 1890
instances shows that RoundMC solves 204 more instances than ApproxMC, and
achieves a $4\times$ speedup over ApproxMC."
"Large Language Models (LLMs) do not differentially represent numbers, which
are pervasive in text. In contrast, neuroscience research has identified
distinct neural representations for numbers and words. In this work, we
investigate how well popular LLMs capture the magnitudes of numbers (e.g., that
$4 < 5$) from a behavioral lens. Prior research on the representational
capabilities of LLMs evaluates whether they show human-level performance, for
instance, high overall accuracy on standard benchmarks. Here, we ask a
different question, one inspired by cognitive science: How closely do the
number representations of LLMscorrespond to those of human language users, who
typically demonstrate the distance, size, and ratio effects? We depend on a
linking hypothesis to map the similarities among the model embeddings of number
words and digits to human response times. The results reveal surprisingly
human-like representations across language models of different architectures,
despite the absence of the neural circuitry that directly supports these
representations in the human brain. This research shows the utility of
understanding LLMs using behavioral benchmarks and points the way to future
work on the number representations of LLMs and their cognitive plausibility."
"We discuss the adequacy of tests for intelligent systems and practical
problems raised by their implementation. We propose the replacement test as the
ability of a system to replace successfully another system performing a task in
a given context. We show how it can characterize salient aspects of human
intelligence that cannot be taken into account by the Turing test. We argue
that building intelligent systems passing the replacement test involves a
series of technical problems that are outside the scope of current AI. We
present a framework for implementing the proposed test and validating the
properties of the intelligent systems. We discuss the inherent limitations of
intelligent system validation and advocate new theoretical foundations for
extending existing rigorous test methods. We suggest that the replacement test,
based on the complementarity of skills between human and machine, can lead to a
multitude of intelligence concepts reflecting the ability to combine data-based
and symbolic knowledge to varying degrees."
"We propose and evaluate a system which learns a neuralnetwork heuristic
function for forward search-based, satisficing classical planning. Our system
learns distance-to-goal estimators from scratch, given a single PDDL training
instance. Training data is generated by backward regression search or by
backward search from given or guessed goal states. In domains such as the
24-puzzle where all instances share the same search space, such heuristics can
also be reused across all instances in the domain. We show that this relatively
simple system can perform surprisingly well, sometimes competitive with
well-known domain-independent heuristics."
"The Blackboard Architecture provides a mechanism for storing data and logic
and using it to make decisions that impact the application environment that the
Blackboard Architecture network models. While rule-fact-action networks can
represent numerous types of data, the relationships that can be easily modeled
are limited by the propositional logic nature of the rule-fact network
structure. This paper proposes and evaluates the inclusion of containers and
links in the Blackboard Architecture. These objects are designed to allow them
to model organizational, physical, spatial and other relationships that cannot
be readily or efficiently implemented as Boolean logic rules. Containers group
related facts together and can be nested to implement complex relationships.
Links interconnect containers that have a relationship that is relevant to
their organizational purpose. Both objects, together, facilitate new ways of
using the Blackboard Architecture and enable or simply its use for complex
tasks that have multiple types of relationships that need to be considered
during operations."
"The integration of machine learning in medical image analysis can greatly
enhance the quality of healthcare provided by physicians. The combination of
human expertise and computerized systems can result in improved diagnostic
accuracy. An automated machine learning approach simplifies the creation of
custom image recognition models by utilizing neural architecture search and
transfer learning techniques. Medical imaging techniques are used to
non-invasively create images of internal organs and body parts for diagnostic
and procedural purposes. This article aims to highlight the potential
applications, strategies, and techniques of AutoML in medical imaging through
theoretical and empirical evidence."
"The past three decades have witnessed notable success in designing efficient
SAT solvers, with modern solvers capable of solving industrial benchmarks
containing millions of variables in just a few seconds. The success of modern
SAT solvers owes to the widely-used CDCL algorithm, which lacks comprehensive
theoretical investigation. Furthermore, it has been observed that CDCL solvers
still struggle to deal with specific classes of benchmarks comprising only
hundreds of variables, which contrasts with their widespread use in real-world
applications. Consequently, there is an urgent need to uncover the inner
workings of these seemingly weak yet powerful black boxes.
  In this paper, we present a first step towards this goal by introducing an
approach called CausalSAT, which employs causal reasoning to gain insights into
the functioning of modern SAT solvers. CausalSAT initially generates
observational data from the execution of SAT solvers and learns a structured
graph representing the causal relationships between the components of a SAT
solver. Subsequently, given a query such as whether a clause with low literals
blocks distance (LBD) has a higher clause utility, CausalSAT calculates the
causal effect of LBD on clause utility and provides an answer to the question.
We use CausalSAT to quantitatively verify hypotheses previously regarded as
""rules of thumb"" or empirical findings such as the query above. Moreover,
CausalSAT can address previously unexplored questions, like which branching
heuristic leads to greater clause utility in order to study the relationship
between branching and clause management. Experimental evaluations using
practical benchmarks demonstrate that CausalSAT effectively fits the data,
verifies four ""rules of thumb"", and provides answers to three questions closely
related to implementing modern solvers."
"This paper continues an established line of research about the relations
between argumentation theory, particularly assumption-based argumentation, and
different kinds of logic programs. In particular, we extend known result of
Caminada, Schultz and Toni by showing that assumption-based argumentation can
represent not only normal logic programs, but also disjunctive logic programs
and their extensions. For this, we consider some inference rules for
disjunction that the core logic of the argumentation frameworks should respect,
and show the correspondence to the handling of disjunctions in the heads of the
logic programs' rules."
"Complementary and alternative medicine are commonly used concomitantly with
conventional medications leading to adverse drug reactions and even fatality in
some cases. Furthermore, the vast possibility of herb-drug interactions
prevents health professionals from remembering or manually searching them in a
database. Decision support systems are a powerful tool that can be used to
assist clinicians in making diagnostic and therapeutic decisions in patient
care. Therefore, an original and hybrid decision support system was designed to
identify herb-drug interactions, applying artificial intelligence techniques to
identify new possible interactions. Different machine learning models will be
used to strengthen the typical rules engine used in these cases. Thus, using
the proposed system, the pharmacy community, people's first line of contact
within the Healthcare System, will be able to make better and more accurate
therapeutic decisions and mitigate possible adverse events."
"In recent years, Game AI research has made important breakthroughs using
Reinforcement Learning (RL). Despite this, RL for modern tabletop games has
gained little to no attention, even when they offer a range of unique
challenges compared to video games. To bridge this gap, we introduce PyTAG, a
Python API for interacting with the Tabletop Games framework (TAG). TAG
contains a growing set of more than 20 modern tabletop games, with a common API
for AI agents. We present techniques for training RL agents in these games and
introduce baseline results after training Proximal Policy Optimisation
algorithms on a subset of games. Finally, we discuss the unique challenges
complex modern tabletop games provide, now open to RL research through PyTAG."
"Children possess the ability to learn multiple cognitive tasks sequentially,
which is a major challenge toward the long-term goal of artificial general
intelligence. Existing continual learning frameworks are usually applicable to
Deep Neural Networks (DNNs) and lack the exploration on more brain-inspired,
energy-efficient Spiking Neural Networks (SNNs). Drawing on continual learning
mechanisms during child growth and development, we propose Dynamic Structure
Development of Spiking Neural Networks (DSD-SNN) for efficient and adaptive
continual learning. When learning a sequence of tasks, the DSD-SNN dynamically
assigns and grows new neurons to new tasks and prunes redundant neurons,
thereby increasing memory capacity and reducing computational overhead. In
addition, the overlapping shared structure helps to quickly leverage all
acquired knowledge to new tasks, empowering a single network capable of
supporting multiple incremental tasks (without the separate sub-network mask
for each task). We validate the effectiveness of the proposed model on multiple
class incremental learning and task incremental learning benchmarks. Extensive
experiments demonstrated that our model could significantly improve
performance, learning speed and memory capacity, and reduce computational
overhead. Besides, our DSD-SNN model achieves comparable performance with the
DNNs-based methods, and significantly outperforms the state-of-the-art (SOTA)
performance for existing SNNs-based continual learning methods."
"Wave Function Collapse (WFC) is a widely used tile-based algorithm in
procedural content generation, including textures, objects, and scenes.
However, the current WFC algorithm and related research lack the ability to
generate commercialized large-scale or infinite content due to constraint
conflict and time complexity costs. This paper proposes a Nested WFC (N-WFC)
algorithm framework to reduce time complexity. To avoid conflict and
backtracking problems, we offer a complete and sub-complete tileset preparation
strategy, which requires only a small number of tiles to generate aperiodic and
deterministic infinite content. We also introduce the weight-brush system that
combines N-WFC and sub-complete tileset, proving its suitability for game
design. Our contribution addresses WFC's challenge in massive content
generation and provides a theoretical basis for implementing concrete games."
"We present the results of the second Neural MMO challenge, hosted at IJCAI
2022, which received 1600+ submissions. This competition targets robustness and
generalization in multi-agent systems: participants train teams of agents to
complete a multi-task objective against opponents not seen during training. The
competition combines relatively complex environment design with large numbers
of agents in the environment. The top submissions demonstrate strong success on
this task using mostly standard reinforcement learning (RL) methods combined
with domain-specific engineering. We summarize the competition design and
results and suggest that, as an academic community, competitions may be a
powerful approach to solving hard problems and establishing a solid benchmark
for algorithms. We will open-source our benchmark including the environment
wrapper, baselines, a visualization tool, and selected policies for further
research."
"We are not only observers but also actors of reality. Our capability to
intervene and alter the course of some events in the space and time surrounding
us is an essential component of how we build our model of the world. In this
doctoral thesis we introduce a generic a-priori assessment of each possible
intervention, in order to select the most cost-effective interventions only,
and avoid unnecessary systematic experimentation on the real world. Based on
this a-priori assessment, we propose an active learning algorithm that
identifies the causal relations in any given causal model, using a least cost
sequence of interventions. There are several novel aspects introduced by our
algorithm. It is, in most case scenarios, able to discard many causal model
candidates using relatively inexpensive interventions that only test one value
of the intervened variables. Also, the number of interventions performed by the
algorithm can be bounded by the number of causal model candidates. Hence, fewer
initial candidates (or equivalently, more prior knowledge) lead to fewer
interventions for causal discovery.
  Causality is intimately related to time, as causes appear to precede their
effects. Cyclical causal processes are a very interesting case of causality in
relation to time. In this doctoral thesis we introduce a formal analysis of
time cyclical causal settings by defining a causal analog to the purely
observational Dynamic Bayesian Networks, and provide a sound and complete
algorithm for the identification of causal effects in the cyclic setting. We
introduce the existence of two types of hidden confounder variables in this
framework, which affect in substantially different ways the identification
procedures, a distinction with no analog in either Dynamic Bayesian Networks or
standard causal graphs."
"Reinforcement learning is a powerful technique for learning from trial and
error, but it often requires a large number of interactions to achieve good
performance. In some domains, such as sparse-reward tasks, an oracle that can
provide useful feedback or guidance to the agent during the learning process is
really of great importance. However, querying the oracle too frequently may be
costly or impractical, and the oracle may not always have a clear answer for
every situation. Therefore, we propose a novel method for interacting with the
oracle in a selective and efficient way, using a retrieval-based approach. We
assume that the interaction can be modeled as a sequence of templated questions
and answers, and that there is a large corpus of previous interactions
available. We use a neural network to encode the current state of the agent and
the oracle, and retrieve the most relevant question from the corpus to ask the
oracle. We then use the oracle's answer to update the agent's policy and value
function. We evaluate our method on an object manipulation task. We show that
our method can significantly improve the efficiency of RL by reducing the
number of interactions needed to reach a certain level of performance, compared
to baselines that do not use the oracle or use it in a naive way."
"World-building, the process of developing both the narrative and physical
world of a game, plays a vital role in the game's experience.
Critically-acclaimed independent and AAA video games are praised for strong
world-building, with game maps that masterfully intertwine with and elevate the
narrative, captivating players and leaving a lasting impression. However,
designing game maps that support a desired narrative is challenging, as it
requires satisfying complex constraints from various considerations. Most
existing map generation methods focus on considerations about gameplay
mechanics or map topography, while the need to support the story is typically
neglected. As a result, extensive manual adjustment is still required to design
a game world that facilitates particular stories. In this work, we approach
this problem by introducing an extra layer of plot facility layout design that
is independent of the underlying map generation method in a world-building
pipeline.
  Concretely, we define (plot) facility layout tasks as the tasks of assigning
concrete locations on a game map to abstract locations mentioned in a given
story (plot facilities), following spatial constraints derived from the story.
We present two methods for solving these tasks automatically: an evolutionary
computation based approach through Covariance Matrix Adaptation Evolution
Strategy (CMA-ES), and a Reinforcement Learning (RL) based approach. We develop
a method of generating datasets of facility layout tasks, create a gym-like
environment for experimenting with and evaluating different methods, and
further analyze the two methods with comprehensive experiments, aiming to
provide insights for solving facility layout tasks. We will release the code
and a dataset containing 10, 000 tasks of different scales."
"Intelligent transportation systems are vital for modern traffic management
and optimization, greatly improving traffic efficiency and safety. With the
rapid development of generative artificial intelligence (Generative AI)
technologies in areas like image generation and natural language processing,
generative AI has also played a crucial role in addressing key issues in
intelligent transportation systems (ITS), such as data sparsity, difficulty in
observing abnormal scenarios, and in modeling data uncertainty. In this review,
we systematically investigate the relevant literature on generative AI
techniques in addressing key issues in different types of tasks in ITS tailored
specifically for road transportation. First, we introduce the principles of
different generative AI techniques. Then, we classify tasks in ITS into four
types: traffic perception, traffic prediction, traffic simulation, and traffic
decision-making. We systematically illustrate how generative AI techniques
addresses key issues in these four different types of tasks. Finally, we
summarize the challenges faced in applying generative AI to intelligent
transportation systems, and discuss future research directions based on
different application scenarios."
"Neural-symbolic learning, an intersection of neural networks and symbolic
reasoning, aims to blend neural networks' learning capabilities with symbolic
AI's interpretability and reasoning. This paper introduces an approach designed
to improve the performance of neural models in learning reasoning tasks. It
achieves this by integrating Answer Set Programming (ASP) solvers and
domain-specific expertise, which is an approach that diverges from traditional
complex neural-symbolic models. In this paper, a shallow artificial neural
network (ANN) is specifically trained to solve Sudoku puzzles with minimal
training data. The model has a unique loss function that integrates losses
calculated using the ASP solver outputs, effectively enhancing its training
efficiency. Most notably, the model shows a significant improvement in solving
Sudoku puzzles using only 12 puzzles for training and testing without
hyperparameter tuning. This advancement indicates that the model's enhanced
reasoning capabilities have practical applications, extending well beyond
Sudoku puzzles to potentially include a variety of other domains. The code can
be found on GitHub: https://github.com/Fadi2200/ASPEN."
"Conflict-Driven Clause Learning (CDCL) is the mainstream framework for
solving the Satisfiability problem (SAT), and CDCL solvers typically rely on
various heuristics, which have a significant impact on their performance.
Modern CDCL solvers, such as MiniSat and Kissat, commonly incorporate several
heuristics and select one to use according to simple rules, requiring
significant time and expert effort to fine-tune in practice. The pervasion of
Large Language Models (LLMs) provides a potential solution to address this
issue. However, generating a CDCL solver from scratch is not effective due to
the complexity and context volume of SAT solvers. Instead, we propose AutoSAT,
a framework that automatically optimizes heuristics in a pre-defined modular
search space based on existing CDCL solvers. Unlike existing automated
algorithm design approaches focusing on hyperparameter tuning and operator
selection, AutoSAT can generate new efficient heuristics. In this first attempt
at optimizing SAT solvers using LLMs, several strategies including the greedy
hill climber and (1+1) Evolutionary Algorithm are employed to guide LLMs to
search for better heuristics. Experimental results demonstrate that LLMs can
generally enhance the performance of CDCL solvers. A realization of AutoSAT
outperforms MiniSat on 9 out of 12 datasets and even surpasses the
state-of-the-art hybrid solver Kissat on 4 datasets."
"Matrix completion is an important area of research in recommender systems.
Recent methods view a rating matrix as a user-item bi-partite graph with
labeled edges denoting observed ratings and predict the edges between the user
and item nodes by using the graph neural network (GNN). Despite their
effectiveness, they treat each rating type as an independent relation type and
thus cannot sufficiently consider the ordinal nature of the ratings. In this
paper, we explore a new approach to exploit rating ordinality for GNN, which
has not been studied well in the literature. We introduce a new method, called
ROGMC, to leverage Rating Ordinality in GNN-based Matrix Completion. It uses
cumulative preference propagation to directly incorporate rating ordinality in
GNN's message passing, allowing for users' stronger preferences to be more
emphasized based on inherent orders of rating types. This process is
complemented by interest regularization which facilitates preference learning
using the underlying interest information. Our extensive experiments show that
ROGMC consistently outperforms the existing strategies of using rating types
for GNN. We expect that our attempt to explore the feasibility of utilizing
rating ordinality for GNN may stimulate further research in this direction."
"Industries frequently adjust their facilities network by opening new branches
in promising areas and closing branches in areas where they expect low profits.
In this paper, we examine a particular class of facility location problems. Our
objective is to minimize the loss of sales resulting from the removal of
several retail stores. However, estimating sales accurately is expensive and
time-consuming. To overcome this challenge, we leverage Monte Carlo Tree Search
(MCTS) assisted by a surrogate model that computes evaluations faster. Results
suggest that MCTS supported by a fast surrogate function can generate solutions
faster while maintaining a consistent solution compared to MCTS that does not
benefit from the surrogate function."
"Fully-observable non-deterministic (FOND) planning is at the core of
artificial intelligence planning with uncertainty. It models uncertainty
through actions with non-deterministic effects. A* with Non-Determinism (AND*)
(Messa and Pereira, 2023) is a FOND planner that generalizes A* (Hart et al.,
1968) for FOND planning. It searches for a solution policy by performing an
explicit heuristic search on the policy space of the FOND task. In this paper,
we study and improve the performance of the policy-space search performed by
AND*. We present a polynomial-time procedure that constructs a solution policy
given just the set of states that should be mapped. This procedure, together
with a better understanding of the structure of FOND policies, allows us to
present three concepts of equivalences between policies. We use policy
equivalences to prune part of the policy search space, making AND*
substantially more effective in solving FOND tasks. We also study the impact of
taking into account structural state-space symmetries to strengthen the
detection of equivalence policies and the impact of performing the search with
satisficing techniques. We apply a recent technique from the group theory
literature to better compute structural state-space symmetries. Finally, we
present a solution compressor that, given a policy defined over complete
states, finds a policy that unambiguously represents it using the minimum
number of partial states. AND* with the introduced techniques generates, on
average, two orders of magnitude fewer policies to solve FOND tasks. These
techniques allow explicit policy-space search to be competitive in terms of
both coverage and solution compactness with other state-of-the-art FOND
planners."
"Large Language Models (LLMs) have demonstrated great potential as generalist
assistants, showcasing powerful task understanding and problem-solving
capabilities. To deploy LLMs as AI assistants, it is crucial that these models
exhibit desirable behavioral traits, such as non-toxicity and resilience
against jailbreak attempts. Current methods for detoxification or preventing
jailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement
Learning from Human Feedback (RLHF), which requires finetuning billions of
parameters through gradient descent with substantial computation cost.
Furthermore, models modified through SFT and RLHF may deviate from the
pretrained models, potentially leading to a degradation in foundational LLM
capabilities. In this paper, we observe that surprisingly, directly editing a
small subset of parameters can effectively modulate specific behaviors of LLMs,
such as detoxification and resistance to jailbreaking. Specifically, for a
behavior that we aim to avoid, we employ a linear classifier, which we term the
behavior probe, to classify binary behavior labels within the hidden state
space of the LLM. Using this probe, we introduce an algorithm to identify a
critical subset of LLM parameters that significantly influence this targeted
behavior. Then we directly edit these selected parameters by shifting them
towards the behavior probe. Such a direct parameter editing method necessitates
only inference-level computational resources. Experiments demonstrate that in
the representative detoxification task, our approach achieves reductions of up
to 90.0\% in toxicity on the RealToxicityPrompts dataset and 49.2\% on ToxiGen,
while maintaining the LLM's general capabilities in areas such as common sense,
question answering, and mathematics. Our code is available at
https://github.com/lucywang720/model-surgery."
"To remove redundant components of large language models (LLMs) without
incurring significant computational costs, this work focuses on single-shot
pruning without a retraining phase. We simplify the pruning process for
Transformer-based LLMs by identifying a depth-2 pruning structure that
functions independently. Additionally, we propose two inference-aware pruning
criteria derived from the optimization perspective of output approximation,
which outperforms traditional training-aware metrics such as gradient and
Hessian. We also introduce a two-step reconstruction technique to mitigate
pruning errors without model retraining. Experimental results demonstrate that
our approach significantly reduces computational costs and hardware
requirements while maintaining superior performance across various datasets and
models."
"Comprehensive planning agents have been a long term goal in the field of
artificial intelligence. Recent innovations in Natural Language Processing have
yielded success through the advent of Large Language Models (LLMs). We seek to
improve the travel-planning capability of such LLMs by extending upon the work
of the previous paper TravelPlanner. Our objective is to explore a new method
of using LLMs to improve the travel planning experience. We focus specifically
on the ""sole-planning"" mode of travel planning; that is, the agent is given
necessary reference information, and its goal is to create a comprehensive plan
from the reference information. While this does not simulate the real-world we
feel that an optimization of the sole-planning capability of a travel planning
agent will still be able to enhance the overall user experience. We propose a
semi-automated prompt generation framework which combines the LLM-automated
prompt and ""human-in-the-loop"" to iteratively refine the prompt to improve the
LLM performance. Our result shows that LLM automated prompt has its limitations
and ""human-in-the-loop"" greatly improves the performance by $139\%$ with one
single iteration."
"The unique diagnosis and treatment techniques and remarkable clinical
efficacy of traditional Chinese medicine (TCM) make it play an important role
in the field of elderly care and healthcare, especially in the rehabilitation
of some common chronic diseases of the elderly. Therefore, building a TCM
chatbot for healthcare application will help users obtain consultation services
in a direct and natural way. However, concepts such as acupuncture points
(acupoints) and meridians involved in TCM always appear in the consultation,
which cannot be displayed intuitively. To this end, we develop a
\textbf{h}ealthcare chat\textbf{bot} (HBot) based on a human body model in 3D
and knowledge graph, which provides conversational services such as knowledge
Q\&A, prescription recommendation, moxibustion therapy recommendation, and
acupoint search. When specific acupoints are involved in the conversations
between user and HBot, the 3D body will jump to the corresponding acupoints and
highlight them. Moreover, Hbot can also be used in training scenarios to
accelerate the teaching process of TCM by intuitively displaying acupuncture
points and knowledge cards. The demonstration video is available at
https://www.youtube.com/watch?v=UhQhutSKkTU . Our code and dataset are publicly
available at Gitee: https://gitee.com/plabrolin/interactive-3d-acup.git"
"This paper introduces a novel approach Counterfactual Shapley Values (CSV),
which enhances explainability in reinforcement learning (RL) by integrating
counterfactual analysis with Shapley Values. The approach aims to quantify and
compare the contributions of different state dimensions to various action
choices. To more accurately analyze these impacts, we introduce new
characteristic value functions, the ``Counterfactual Difference Characteristic
Value"" and the ``Average Counterfactual Difference Characteristic Value."" These
functions help calculate the Shapley values to evaluate the differences in
contributions between optimal and non-optimal actions. Experiments across
several RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the
effectiveness of the CSV method. The results show that this method not only
improves transparency in complex RL systems but also quantifies the differences
across various decisions."
"This paper considers a scenario in city navigation: an AI agent is provided
with language descriptions of the goal location with respect to some well-known
landmarks; By only observing the scene around, including recognizing landmarks
and road network connections, the agent has to make decisions to navigate to
the goal location without instructions. This problem is very challenging,
because it requires agent to establish self-position and acquire spatial
representation of complex urban environment, where landmarks are often
invisible. In the absence of navigation instructions, such abilities are vital
for the agent to make high-quality decisions in long-range city navigation.
With the emergent reasoning ability of large language models (LLMs), a tempting
baseline is to prompt LLMs to ""react"" on each observation and make decisions
accordingly. However, this baseline has very poor performance that the agent
often repeatedly visits same locations and make short-sighted, inconsistent
decisions. To address these issues, this paper introduces a novel agentic
workflow featured by its abilities to perceive, reflect and plan. Specifically,
we find LLaVA-7B can be fine-tuned to perceive the direction and distance of
landmarks with sufficient accuracy for city navigation. Moreover, reflection is
achieved through a memory mechanism, where past experiences are stored and can
be retrieved with current perception for effective decision argumentation.
Planning uses reflection results to produce long-term plans, which can avoid
short-sighted decisions in long-range navigation. We show the designed workflow
significantly improves navigation ability of the LLM agent compared with the
state-of-the-art baselines."
"Researchers are investing substantial effort in developing powerful
general-purpose agents, wherein Foundation Models are used as modules within
agentic systems (e.g. Chain-of-Thought, Self-Reflection, Toolformer). However,
the history of machine learning teaches us that hand-designed solutions are
eventually replaced by learned solutions. We formulate a new research area,
Automated Design of Agentic Systems (ADAS), which aims to automatically create
powerful agentic system designs, including inventing novel building blocks
and/or combining them in new ways. We further demonstrate that there is an
unexplored yet promising approach within ADAS where agents can be defined in
code and new agents can be automatically discovered by a meta agent programming
ever better ones in code. Given that programming languages are Turing Complete,
this approach theoretically enables the learning of any possible agentic
system: including novel prompts, tool use, control flows, and combinations
thereof. We present a simple yet effective algorithm named Meta Agent Search to
demonstrate this idea, where a meta agent iteratively programs interesting new
agents based on an ever-growing archive of previous discoveries. Through
extensive experiments across multiple domains including coding, science, and
math, we show that our algorithm can progressively invent agents with novel
designs that greatly outperform state-of-the-art hand-designed agents.
Importantly, we consistently observe the surprising result that agents invented
by Meta Agent Search maintain superior performance even when transferred across
domains and models, demonstrating their robustness and generality. Provided we
develop it safely, our work illustrates the potential of an exciting new
research direction toward automatically designing ever-more powerful agentic
systems to benefit humanity."
"With the emergence of more and more economy-specific LLMS, how to measure
whether they can be safely invested in production becomes a problem. Previous
research has primarily focused on evaluating the performance of LLMs within
specific application scenarios. However, these benchmarks cannot reflect the
theoretical level and generalization ability, and the backward datasets are
increasingly unsuitable for problems in real scenarios. In this paper, we have
compiled a new benchmark, MTFinEval, focusing on the LLMs' basic knowledge of
economics, which can always be used as a basis for judgment. To examine only
theoretical knowledge as much as possible, MTFinEval is build with foundational
questions from university textbooks,and exam papers in economics and management
major. Aware of the overall performance of LLMs do not depend solely on one
subdiscipline of economics, MTFinEval comprise 360 questions refined from six
major disciplines of economics, and reflect capabilities more comprehensively.
Experiment result shows all LLMs perform poorly on MTFinEval, which proves that
our benchmark built on basic knowledge is very successful. Our research not
only offers guidance for selecting the appropriate LLM for specific use cases,
but also put forward increase the rigor reliability of LLMs from the basics."
"This paper presents our research towards a near-term future in which legal
entities, such as individuals and organisations can entrust semi-autonomous
AI-driven agents to carry out online interactions on their behalf. The author's
research concerns the development of semi-autonomous Web agents, which consult
users if and only if the system does not have sufficient context or confidence
to proceed working autonomously. This creates a user-agent dialogue that allows
the user to teach the agent about the information sources they trust, their
data-sharing preferences, and their decision-making preferences. Ultimately,
this enables the user to maximise control over their data and decisions while
retaining the convenience of using agents, including those driven by LLMs.
  In view of developing near-term solutions, the research seeks to answer the
question: ""How do we build a trustworthy and reliable network of
semi-autonomous agents which represent individuals and organisations on the
Web?"". After identifying key requirements, the paper presents a demo for a
sample use case of a generic personal assistant. This is implemented using
(Notation3) rules to enforce safety guarantees around belief, data sharing and
data usage and LLMs to allow natural language interaction with users and
serendipitous dialogues between software agents."
"Understanding molecular structure and related knowledge is crucial for
scientific research. Recent studies integrate molecular graphs with their
textual descriptions to enhance molecular representation learning. However,
they focus on the whole molecular graph and neglect frequently occurring
subgraphs, known as motifs,which are essential for determining molecular
properties. Without such fine-grained knowledge, these models struggle to
generalize to unseen molecules and tasks that require motif-level insights. To
bridge this gap, we propose FineMolTex, a novel Fine-grained Molecular
graph-Text pre-training framework to jointly learn coarse-grained
molecule-level knowledge and fine-grained motif-level knowledge. Specifically,
FineMolTex consists of two pre-training tasks: a contrastive alignment task for
coarse-grained matching and a masked multi-modal modeling task for fine-grained
matching. In particular, the latter predicts the labels of masked motifs and
words, leveraging insights from each other, thereby enabling FineMolTex to
understand the fine-grained matching between motifs and words. Finally, we
conduct extensive experiments across three downstream tasks, achieving up to
230% improvement in the text-based molecule editing task. Additionally, our
case studies reveal that FineMolTex successfully captures fine-grained
knowledge, potentially offering valuable insights for drug discovery and
catalyst design."
"Coronary Heart Disease affects millions of people worldwide and is a
well-studied area of healthcare. There are many viable and accurate methods for
the diagnosis and prediction of heart disease, but they have limiting points
such as invasiveness, late detection, or cost. Supervised learning via machine
learning algorithms presents a low-cost (computationally speaking),
non-invasive solution that can be a precursor for early diagnosis. In this
study, we applied several well-known methods and benchmarked their performance
against each other. It was found that Random Forest with oversampling of the
predictor variable produced the highest accuracy of 84%."
"Large Language Models (LLMs) often struggle with tasks requiring mathematical
reasoning, particularly multiple-choice questions (MCQs). To address this
issue, we developed LLaMa-SciQ, an educational chatbot designed to assist
college students in solving and understanding MCQs in STEM fields. We begin by
fine-tuning and aligning the models to human preferences. After comparing the
performance of Mistral-7B and LLaMa-8B, we selected the latter as the base
model due to its higher evaluation accuracy. To further enhance accuracy, we
implement Retrieval-Augmented Generation (RAG) and apply quantization to
compress the model, reducing inference time and increasing accessibility for
students. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the
GSM8k dataset and 30% on the MATH dataset. However, RAG does not improve
performance and even reduces it, likely due to retriever issues or the model's
unfamiliarity with context. Despite this, the quantized model shows only a 5%
loss in performance, demonstrating significant efficiency improvements."
"Recent regulatory proposals for artificial intelligence emphasize fairness
requirements for machine learning models. However, precisely defining the
appropriate measure of fairness is challenging due to philosophical, cultural
and political contexts. Biases can infiltrate machine learning models in
complex ways depending on the model's context, rendering a single common metric
of fairness insufficient. This ambiguity highlights the need for criteria to
guide the selection of context-aware measures, an issue of increasing
importance given the proliferation of ever tighter regulatory requirements. To
address this, we developed a flowchart to guide the selection of contextually
appropriate fairness measures. Twelve criteria were used to formulate the
flowchart. This included consideration of model assessment criteria, model
selection criteria, and data bias. We also review fairness literature in the
context of machine learning and link it to core regulatory instruments to
assist policymakers, AI developers, researchers, and other stakeholders in
appropriately addressing fairness concerns and complying with relevant
regulatory requirements."
"Current forecasting approaches are largely unimodal and ignore the rich
textual data that often accompany the time series due to lack of well-curated
multimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a
carefully curated, time-aligned text and time dataset for multimodal
forecasting. Our dataset is composed of sequences of numbers and text aligned
to timestamps, and includes data from two different domains: climate science
and healthcare. Our data is a significant contribution to the rare selection of
available multimodal datasets. We also propose the Hybrid Multi-Modal
Forecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and
time series data using shared embeddings. However, contrary to our
expectations, our Hybrid-MMF model does not outperform existing baselines in
our experiments. This negative result highlights the challenges inherent in
multimodal forecasting. Our code and data are available at
https://github.com/Rose-STL-Lab/Multimodal_ Forecasting."
"World model emerges as a key module in decision making, where MuZero and
Dreamer achieve remarkable successes in complex tasks. Recent work leverages
Large Language Models (LLMs) as general world simulators to simulate the
dynamics of the world due to their generalizability. LLMs also serve as the
world model for deliberative reasoning in Reasoning via Planning (RAP) and Tree
of Thought (ToT). However, the world models are either evaluated as a general
world simulator, or as a functional module of the agent, i.e., predicting the
transitions to assist the planning. In this work, we propose a comprehensive
evaluation of the world models with LLMs from the decision making perspective.
Specifically, we leverage the 31 diverse environments from (Wang et al.,
2023;2024) and curate the rule-based policy of each environment for the diverse
evaluation. Then, we design three main tasks, i.e., policy verification, action
proposal, and policy planning, where the world models can be used for decision
making solely. Finally, we conduct the comprehensive evaluation of the advanced
LLMs, i.e., GPT-4o and GPT-4o-mini, on the environments for the three main
tasks under various settings. The key observations include: i) GPT-4o
significantly outperforms GPT-4o-mini on the three main tasks, especially for
the tasks which require the domain knowledge, ii) the performance of the world
model with LLM will be decreased for long-term decision-making tasks, and iii)
the combination of different functionalities of the world model will brings
additional unstabilities of the performance."
"Interpreting human neural signals to decode static speech intentions such as
text or images and dynamic speech intentions such as audio or video is showing
great potential as an innovative communication tool. Human communication
accompanies various features, such as articulatory movements, facial
expressions, and internal speech, all of which are reflected in neural signals.
However, most studies only generate short or fragmented outputs, while
providing informative communication by leveraging various features from neural
signals remains challenging. In this study, we introduce a dynamic neural
communication method that leverages current computer vision and brain-computer
interface technologies. Our approach captures the user's intentions from neural
signals and decodes visemes in short time steps to produce dynamic visual
outputs. The results demonstrate the potential to rapidly capture and
reconstruct lip movements during natural speech attempts from human neural
signals, enabling dynamic neural communication through the convergence of
computer vision and brain--computer interface."
"This paper focuses on the computational complexity of computing empirical
plug-in estimates for causal effect queries. Given a causal graph and
observational data, any identifiable causal query can be estimated from an
expression over the observed variables, called the estimand. The estimand can
then be evaluated by plugging in probabilities computed empirically from data.
In contrast to conventional wisdom, which assumes that high dimensional
probabilistic functions will lead to exponential evaluation time of the
estimand. We show that computation can be done efficiently, potentially in time
linear in the data size, depending on the estimand's hypergraph.
  In particular, we show that both the treewidth and hypertree width of the
estimand's structure bound the evaluation complexity of the plug-in estimands,
analogous to their role in the complexity of probabilistic inference in
graphical models. Often, the hypertree width provides a more effective bound,
since the empirical distributions are sparse."
"Recently, as Large Language Models (LLMs) have shown impressive emerging
capabilities and gained widespread popularity, research on LLM-based search
agents has proliferated. In real-world situations, users often input contextual
and highly personalized queries to chatbots, challenging LLMs to capture
context and generate appropriate answers. However, much of the prior research
has not focused specifically on authentic human-machine dialogue scenarios. It
also ignores the important balance between response quality and computational
cost by forcing all queries to follow the same agent process. To address these
gaps, we propose a Strategy-Router Search Agent (SRSA), routing different
queries to appropriate search strategies and enabling fine-grained serial
searches to obtain high-quality results at a relatively low cost. To evaluate
our work, we introduce a new dataset, Contextual Query Enhancement Dataset
(CQED), comprising contextual queries to simulate authentic and daily
interactions between humans and chatbots. Using LLM-based automatic evaluation
metrics, we assessed SRSA's performance in terms of informativeness,
completeness, novelty, and actionability. To conclude, SRSA provides an
approach that resolves the issue of simple serial searches leading to
degenerate answers for lengthy and contextual queries, effectively and
efficiently parses complex user queries, and generates more comprehensive and
informative responses without fine-tuning an LLM."
"Learning STRIPS action models from action traces alone is a challenging
problem as it involves learning the domain predicates as well. In this work, a
novel approach is introduced which, like the well-known LOCM systems, is
scalable, but like SAT approaches, is sound and complete. Furthermore, the
approach is general and imposes no restrictions on the hidden domain or the
number or arity of the predicates. The new learning method is based on an
\emph{efficient, novel test} that checks whether the assumption that a
predicate is affected by a set of action patterns, namely, actions with
specific argument positions, is consistent with the traces. The predicates and
action patterns that pass the test provide the basis for the learned domain
that is then easily completed with preconditions and static predicates. The new
method is studied theoretically and experimentally. For the latter, the method
is evaluated on traces and graphs obtained from standard classical domains like
the 8-puzzle, which involve hundreds of thousands of states and transitions.
The learned representations are then verified on larger instances."
"Mathematical reasoning is a fundamental capability for large language models
(LLMs), yet achieving high performance in this domain remains a significant
challenge. The auto-regressive generation process often makes LLMs susceptible
to errors, hallucinations, and inconsistencies, particularly during multi-step
reasoning. In this paper, we propose Mars-PO, a novel framework to improve the
mathematical reasoning capabilities of LLMs through a multi-agent system. It
combines high-quality outputs from multiple agents into a hybrid positive
sample set and pairs them with agent-specific negative samples to construct
robust preference pairs for training. By aligning agents with shared positive
samples while addressing individual weaknesses, Mars-PO achieves substantial
performance improvements on mathematical reasoning benchmarks. For example, it
increases the accuracy on the MATH benchmark of the state-of-the-art
instruction-tuned LLM, Llama3.1-8B-Instruct, from 50.38% to 57.82%.
Experimental results further demonstrate that our method consistently
outperforms other baselines, such as supervised fine-tuning, vanilla DPO, and
its enhanced versions, highlighting the effectiveness of our approach."
"Artificial intelligence has, so far, largely automated routine tasks, but
what does it mean for the future of work if Large Language Models (LLMs) show
creativity comparable to humans? To measure the creativity of LLMs
holistically, the current study uses 13 creative tasks spanning three domains.
We benchmark the LLMs against individual humans, and also take a novel approach
by comparing them to the collective creativity of groups of humans. We find
that the best LLMs (Claude and GPT-4) rank in the 52nd percentile against
humans, and overall LLMs excel in divergent thinking and problem solving but
lag in creative writing. When questioned 10 times, an LLM's collective
creativity is equivalent to 8-10 humans. When more responses are requested, two
additional responses of LLMs equal one extra human. Ultimately, LLMs, when
optimally applied, may compete with a small group of humans in the future of
work."
"Recent advancements in autonomous multi-agent systems (MAS) based on large
language models (LLMs) have enhanced the application scenarios and improved the
capability of LLMs to handle complex tasks. Despite demonstrating
effectiveness, existing studies still evidently struggle to evaluate, analysis,
and reproducibility of LLM-based MAS. In this paper, to facilitate the research
on LLM-based MAS, we introduce an open, scalable, and real-time updated
platform for accessing and analyzing the LLM-based MAS based on the games Who
is Spy?"" (WiS). Our platform is featured with three main worths: (1) a unified
model evaluate interface that supports models available on Hugging Face; (2)
real-time updated leaderboard for model evaluation; (3) a comprehensive
evaluation covering game-winning rates, attacking, defense strategies, and
reasoning of LLMs. To rigorously test WiS, we conduct extensive experiments
coverage of various open- and closed-source LLMs, we find that different agents
exhibit distinct and intriguing behaviors in the game. The experimental results
demonstrate the effectiveness and efficiency of our platform in evaluating
LLM-based MAS. Our platform and its documentation are publicly available at
\url{https://whoisspy.ai/}"
"DatalogMTL is a powerful rule-based language for temporal reasoning. Due to
its high expressive power and flexible modeling capabilities, it is suitable
for a wide range of applications, including tasks from industrial and financial
sectors. However, due its high computational complexity, practical reasoning in
DatalogMTL is highly challenging. To address this difficulty, we introduce a
new reasoning method for DatalogMTL which exploits the magic sets technique --
a rewriting approach developed for (non-temporal) Datalog to simulate top-down
evaluation with bottom-up reasoning. We implement this approach and evaluate it
on several publicly available benchmarks, showing that the proposed approach
significantly and consistently outperforms performance of the state-of-the-art
reasoning techniques."
"Obeying precise constraints on top of multiple external attributes is a
common computational problem underlying seemingly different domains, from
controlled text generation to protein engineering. Existing language model (LM)
controllability methods for multi-attribute constraint satisfaction often rely
on specialized architectures or gradient-based classifiers, limiting their
flexibility to work with arbitrary black-box evaluators and pretrained models.
Current general-purpose large language models, while capable, cannot achieve
fine-grained multi-attribute control over external attributes. Thus, we create
Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of
finetuning language models on any sequential domain to satisfy user-specified
constraints on multiple external real-value attributes. Our method trains LMs
as editors by sampling diverse multi-attribute edit pairs from an initial set
of paraphrased outputs. During inference, LM iteratively improves upon its
previous solution to satisfy constraints for all attributes by leveraging our
designed constraint satisfaction reward. We additionally experiment with
reward-weighted behavior cloning to further improve the constraint satisfaction
rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint
Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text
Style Transfer, where the goal is to simultaneously modify the sentiment and
complexity of reviews, and (2) Protein Design, focusing on modulating
fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical
results show that MACS achieves the highest threshold satisfaction in both
FineCS tasks, outperforming strong domain-specific baselines. Our work opens
new avenues for generalized and real-value multi-attribute control, with
implications for diverse applications spanning NLP and bioinformatics."
"LLM test-time compute (or LLM inference) via search has emerged as a
promising research area with rapid developments. However, current frameworks
often adopt distinct perspectives on three key aspects (task definition, LLM
profiling, and search procedures), making direct comparisons challenging.
Moreover, the search algorithms employed often diverge from standard
implementations, and their specific characteristics are not thoroughly
specified. In this survey, we provide a comprehensive technical review that
unifies task definitions and provides modular definitions of LLM profiling and
search procedures. The definitions enable precise comparisons of various LLM
inference frameworks while highlighting their departures from conventional
search algorithms. We also discuss the applicability, performance, and
efficiency of these methods. For further details and ongoing updates, please
refer to our GitHub repository:
https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md"
"We study here constraint satisfaction problems that are based on predefined,
explicitly given finite constraints. To solve them we propose a notion of rule
consistency that can be expressed in terms of rules derived from the explicit
representation of the initial constraints.
  This notion of local consistency is weaker than arc consistency for
constraints of arbitrary arity but coincides with it when all domains are unary
or binary. For Boolean constraints rule consistency coincides with the closure
under the well-known propagation rules for Boolean constraints.
  By generalizing the format of the rules we obtain a characterization of arc
consistency in terms of so-called inclusion rules. The advantage of rule
consistency and this rule based characterization of the arc consistency is that
the algorithms that enforce both notions can be automatically generated, as CHR
rules. So these algorithms could be integrated into constraint logic
programming systems such as Eclipse.
  We illustrate the usefulness of this approach to constraint propagation by
discussing the implementations of both algorithms and their use on various
examples, including Boolean constraints, three valued logic of Kleene,
constraints dealing with Waltz's language for describing polyhedreal scenes,
and Allen's qualitative approach to temporal logic."
"Logics for knowledge representation suffer from over-specialization: while
each logic may provide an ideal representation formalism for some problems, it
is less than optimal for others. A solution to this problem is to choose from
several logics and, when necessary, combine the representations. In general,
such an approach results in a very difficult problem of combination. However,
if we can choose the logics from a uniform framework then the problem of
combining them is greatly simplified. In this paper, we develop such a
framework for defeasible logics. It supports all defeasible logics that satisfy
a strong negation principle. We use logic meta-programs as the basis for the
framework."
"The current document contains a brief description of a system for Reasoning
about Actions and Change called PAL (Pertinence Action Language) which makes
use of several reasoning properties extracted from a Temporal Expert Systems
tool called Medtool."
"The papers gathered in this collection were presented at the 8th
International Workshop on Nonmonotonic Reasoning, NMR2000. The series was
started by John McCarthy in 1978. The first international NMR workshop was held
at Mohonk Mountain House, New Paltz, New York in June, 1984, and was organized
by Ray Reiter and Bonnie Webber.
  In the last 10 years the area of nonmonotonic reasoning has seen a number of
important developments. Significant theoretical advances were made in the
understanding of general abstract principles underlying nonmonotonicity. Key
results on the expressibility and computational complexity of nonmonotonic
logics were established. The role of nonmonotonic reasoning in belief revision,
abduction, reasoning about action, planing and uncertainty was further
clarified. Several successful NMR systems were built and used in applications
such as planning, scheduling, logic programming and constraint satisfaction.
  The papers in the proceedings reflect these recent advances in the field.
They are grouped into sections corresponding to special sessions as they were
held at the workshop:
  1. General NMR track
  2. Abductive reasonig
  3. Belief revision: theory and practice
  4. Representing action and planning
  5. Systems descriptions and demonstrations
  6. Uncertainty frameworks in NMR"
"When reasoning in description, modal or temporal logics it is often useful to
consider axioms representing universal truths in the domain of discourse.
Reasoning with respect to an arbitrary set of axioms is hard, even for
relatively inexpressive logics, and it is essential to deal with such axioms in
an efficient manner if implemented systems are to be effective in real
applications. This is particularly relevant to Description Logics, where
subsumption reasoning with respect to a terminology is a fundamental problem.
Two optimisation techniques that have proved to be particularly effective in
dealing with terminologies are lazy unfolding and absorption. In this paper we
seek to improve our theoretical understanding of these important techniques. We
define a formal framework that allows the techniques to be precisely described,
establish conditions under which they can be safely applied, and prove that,
provided these conditions are respected, subsumption testing algorithms will
still function correctly. These results are used to show that the procedures
used in the FaCT system are correct and, moreover, to show how efficiency can
be significantly improved, while still retaining the guarantee of correctness,
by relaxing the safety conditions for absorption."
"This paper is aimed at providing a uniform framework for reasoning about
beliefs of multiple agents and their fusion. In the first part of the paper, we
develop logics for reasoning about cautiously merged beliefs of agents with
different degrees of reliability. The logics are obtained by combining the
multi-agent epistemic logic and multi-sources reasoning systems. Every ordering
for the reliability of the agents is represented by a modal operator, so we can
reason with the merged results under different situations. The fusion is
cautious in the sense that if an agent's belief is in conflict with those of
higher priorities, then his belief is completely discarded from the merged
result. We consider two strategies for the cautious merging of beliefs. In the
first one, if inconsistency occurs at some level, then all beliefs at the lower
levels are discarded simultaneously, so it is called level cutting strategy.
For the second one, only the level at which the inconsistency occurs is
skipped, so it is called level skipping strategy. The formal semantics and
axiomatic systems for these two strategies are presented. In the second part,
we extend the logics both syntactically and semantically to cover some more
sophisticated belief fusion and revision operators. While most existing
approaches treat belief fusion operators as meta-level constructs, these
operators are directly incorporated into our object logic language. Thus it is
possible to reason not only with the merged results but also about the fusion
process in our logics. The relationship of our extended logics with the
conditional logics of belief revision is also discussed."
"The goal of this paper is to provide a strong integration between constraint
modelling and relational DBMSs. To this end we propose extensions of standard
query languages such as relational algebra and SQL, by adding constraint
modelling capabilities to them. In particular, we propose non-deterministic
extensions of both languages, which are specially suited for combinatorial
problems. Non-determinism is introduced by means of a guessing operator, which
declares a set of relations to have an arbitrary extension. This new operator
results in languages with higher expressive power, able to express all problems
in the complexity class NP. Some syntactical restrictions which make data
complexity polynomial are shown. The effectiveness of both extensions is
demonstrated by means of several examples. The current implementation, written
in Java using local search techniques, is described. To appear in Theory and
Practice of Logic Programming (TPLP)"
"We studied the application of the Pseudo-Zernike features as image parameters
(instead of the Hillas parameters) for the discrimination between the images
produced by atmospheric electromagnetic showers caused by gamma-rays and the
ones produced by atmospheric electromagnetic showers caused by hadrons in the
MAGIC Experiment. We used a Support Vector Machine as classification algorithm
with the computed Pseudo-Zernike features as classification parameters. We
implemented on a FPGA board a kernel function of the SVM and the Pseudo-Zernike
features to build a third level trigger for the gamma-hadron separation task of
the MAGIC Experiment."
"In this paper we derive the equations for Loop Corrected Belief Propagation
on a continuous variable Gaussian model. Using the exactness of the averages
for belief propagation for Gaussian models, a different way of obtaining the
covariances is found, based on Belief Propagation on cavity graphs. We discuss
the relation of this loop correction algorithm to Expectation Propagation
algorithms for the case in which the model is no longer Gaussian, but slightly
perturbed by nonlinear terms."
"Both in the plane and in space, we invert the nonlinear Ullman transformation
for 3 points and 3 orthographic cameras. While Ullman's theorem assures a
unique reconstruction modulo a reflection for 3 cameras and 4 points, we find a
locally unique reconstruction for 3 cameras and 3 points. Explicit
reconstruction formulas allow to decide whether picture data of three cameras
seeing three points can be realized as a point-camera configuration."
"It has previously been shown that a recommender based on immune system
idiotypic principles can out perform one based on correlation alone. This paper
reports the results of work in progress, where we undertake some investigations
into the nature of this beneficial effect. The initial findings are that the
immune system recommender tends to produce different neighbourhoods, and that
the superior performance of this recommender is due partly to the different
neighbourhoods, and partly to the way that the idiotypic effect is used to
weight each neighbours recommendations."
"This paper focuses on the problem of kernelizing an existing supervised
Mahalanobis distance learner. The following features are included in the paper.
Firstly, three popular learners, namely, ""neighborhood component analysis"",
""large margin nearest neighbors"" and ""discriminant neighborhood embedding"",
which do not have kernel versions are kernelized in order to improve their
classification performances. Secondly, an alternative kernelization framework
called ""KPCA trick"" is presented. Implementing a learner in the new framework
gains several advantages over the standard framework, e.g. no mathematical
formulas and no reprogramming are required for a kernel implementation, the
framework avoids troublesome problems such as singularity, etc. Thirdly, while
the truths of representer theorems are just assumptions in previous papers
related to ours, here, representer theorems are formally proven. The proofs
validate both the kernel trick and the KPCA trick in the context of Mahalanobis
distance learning. Fourthly, unlike previous works which always apply brute
force methods to select a kernel, we investigate two approaches which can be
efficiently adopted to construct an appropriate kernel for a given dataset.
Finally, numerical results on various real-world datasets are presented."
"In this paper we examine sorting on the assumption that we do not know in
advance which way to sort a sequence of numbers and we set at work simple local
comparison and swap operators whose repeating application ends up in sorted
sequences. These are the basic elements of Emerge-Sort, our approach to
self-organizing sorting, which we then validate experimentally across a range
of samples. Observing an O(n2) run-time behaviour, we note that the n/logn
delay coefficient that differentiates Emerge-Sort from the classical comparison
based algorithms is an instantiation of the price of anarchy we pay for not
imposing a sorting order and for letting that order emerge through the local
interactions."
"Many reinforcement learning exploration techniques are overly optimistic and
try to explore every state. Such exploration is impossible in environments with
the unlimited number of states. I propose to use simulated exploration with an
optimistic model to discover promising paths for real exploration. This reduces
the needs for the real exploration."
"Given a time series of multicomponent measurements x(t), the usual objective
of nonlinear blind source separation (BSS) is to find a ""source"" time series
s(t), comprised of statistically independent combinations of the measured
components. In this paper, the source time series is required to have a density
function in (s,ds/dt)-space that is equal to the product of density functions
of individual components. This formulation of the BSS problem has a solution
that is unique, up to permutations and component-wise transformations.
Separability is shown to impose constraints on certain locally invariant
(scalar) functions of x, which are derived from local higher-order correlations
of the data's velocity dx/dt. The data are separable if and only if they
satisfy these constraints, and, if the constraints are satisfied, the sources
can be explicitly constructed from the data. The method is illustrated by using
it to separate two speech-like sounds recorded with a single microphone."
"Many state-of-the art visualization techniques must be tailored to the
specific type of dataset, its modality (CT, MRI, etc.), the recorded object or
anatomical region (head, spine, abdomen, etc.) and other parameters related to
the data acquisition process. While parts of the information (imaging modality
and acquisition sequence) may be obtained from the meta-data stored with the
volume scan, there is important information which is not stored explicitly
(anatomical region, tracing compound). Also, meta-data might be incomplete,
inappropriate or simply missing.
  This paper presents a novel and simple method of determining the type of
dataset from previously defined categories. 2D histograms based on intensity
and gradient magnitude of datasets are used as input to a neural network, which
classifies it into one of several categories it was trained with. The proposed
method is an important building block for visualization systems to be used
autonomously by non-experts. The method has been tested on 80 datasets, divided
into 3 classes and a ""rest"" class.
  A significant result is the ability of the system to classify datasets into a
specific class after being trained with only one dataset of that class. Other
advantages of the method are its easy implementation and its high computational
performance."
"As one of the newest members in Artificial Immune Systems (AIS), the
Dendritic Cell Algorithm (DCA) has been applied to a range of problems. These
applications mainly belong to the field of anomaly detection. However,
real-time detection, a new challenge to anomaly detection, requires improvement
on the real-time capability of the DCA. To assess such capability, formal
methods in the research of rea-time systems can be employed. The findings of
the assessment can provide guideline for the future development of the
algorithm. Therefore, in this paper we use an interval logic based method,
named the Duration Calculus (DC), to specify a simplified single-cell model of
the DCA. Based on the DC specifications with further induction, we find that
each individual cell in the DCA can perform its function as a detector in
real-time. Since the DCA can be seen as many such cells operating in parallel,
it is potentially capable of performing real-time detection. However, the
analysis process of the standard DCA constricts its real-time capability. As a
result, we conclude that the analysis process of the standard DCA should be
replaced by a real-time analysis component, which can perform periodic analysis
for the purpose of real-time detection."
"Constraint propagation is one of the basic forms of inference in many
logic-based reasoning systems. In this paper, we investigate constraint
propagation for first-order logic (FO), a suitable language to express a wide
variety of constraints. We present an algorithm with polynomial-time data
complexity for constraint propagation in the context of an FO theory and a
finite structure. We show that constraint propagation in this manner can be
represented by a datalog program and that the algorithm can be executed
symbolically, i.e., independently of a structure. Next, we extend the algorithm
to FO(ID), the extension of FO with inductive definitions. Finally, we discuss
several applications."
"The fastest known exact algorithms for scorebased structure discovery in
Bayesian networks on n nodes run in time and space 2nnO(1). The usage of these
algorithms is limited to networks on at most around 25 nodes mainly due to the
space requirement. Here, we study space-time tradeoffs for finding an optimal
network structure. When little space is available, we apply the Gurevich-Shelah
recurrence-originally proposed for the Hamiltonian path problem-and obtain time
22n-snO(1) in space 2snO(1) for any s = n/2, n/4, n/8, . . .; we assume the
indegree of each node is bounded by a constant. For the more practical setting
with moderate amounts of space, we present a novel scheme. It yields running
time 2n(3/2)pnO(1) in space 2n(3/4)pnO(1) for any p = 0, 1, . . ., n/2; these
bounds hold as long as the indegrees are at most 0.238n. Furthermore, the
latter scheme allows easy and efficient parallelization beyond previous
algorithms. We also explore empirically the potential of the presented
techniques."
"The goal of temporal alignment is to establish time correspondence between
two sequences, which has many applications in a variety of areas such as speech
processing, bioinformatics, computer vision, and computer graphics. In this
paper, we propose a novel temporal alignment method called least-squares
dynamic time warping (LSDTW). LSDTW finds an alignment that maximizes
statistical dependency between sequences, measured by a squared-loss variant of
mutual information. The benefit of this novel information-theoretic formulation
is that LSDTW can align sequences with different lengths, different
dimensionality, high non-linearity, and non-Gaussianity in a computationally
efficient manner. In addition, model parameters such as an initial alignment
matrix can be systematically optimized by cross-validation. We demonstrate the
usefulness of LSDTW through experiments on synthetic and real-world Kinect
action recognition datasets."
"This paper describes a computationally inexpensive and efficient generic
summarization algorithm for Arabic texts. The algorithm belongs to extractive
summarization family, which reduces the problem into representative sentences
identification and extraction sub-problems. Important keyphrases of the
document to be summarized are identified employing combinations of statistical
and linguistic features. The sentence extraction algorithm exploits keyphrases
as the primary attributes to rank a sentence. The present experimental work,
demonstrates different techniques for achieving various summarization goals
including: informative richness, coverage of both main and auxiliary topics,
and keeping redundancy to a minimum. A scoring scheme is then adopted that
balances between these summarization goals. To evaluate the resulted Arabic
summaries with well-established systems, aligned English/Arabic texts are used
through the experiments."
"Decision-theoretic troubleshooting is one of the areas to which Bayesian
networks can be applied. Given a probabilistic model of a malfunctioning
man-made device, the task is to construct a repair strategy with minimal
expected cost. The problem has received considerable attention over the past
two decades. Efficient solution algorithms have been found for simple cases,
whereas other variants have been proven NP-complete. We study several variants
of the problem found in literature, and prove that computing approximate
troubleshooting strategies is NP-hard. In the proofs, we exploit a close
connection to set-covering problems."
"The Semantic Web works on the existing Web which presents the meaning of
information as well-defined vocabularies understood by the people. Semantic
Search, at the same time, works on improving the accuracy if a search by
understanding the intent of the search and providing contextually relevant
results. This paper describes a semantic approach toward web search through a
PHP application. The goal was to parse through a user's browsing history and
return semantically relevant web pages for the search query provided."
"We consider a multilingual weakly supervised learning scenario where
knowledge from annotated corpora in a resource-rich language is transferred via
bitext to guide the learning in other languages. Past approaches project labels
across bitext and use them as features or gold labels for training. We propose
a new method that projects model expectations rather than labels, which
facilities transfer of model uncertainty across language boundaries. We encode
expectations as constraints and train a discriminative CRF model using
Generalized Expectation Criteria (Mann and McCallum, 2010). Evaluated on
standard Chinese-English and German-English NER datasets, our method
demonstrates F1 scores of 64% and 60% when no labeled data is used. Attaining
the same accuracy with supervised CRFs requires 12k and 1.5k labeled sentences.
Furthermore, when combined with labeled examples, our method yields significant
improvements over state-of-the-art supervised methods, achieving best reported
numbers to date on Chinese OntoNotes and German CoNLL-03 datasets."
"In many professional sports leagues, teams from opposing leagues/conferences
compete against one another, playing inter-league games. This is an example of
a bipartite tournament. In this paper, we consider the problem of reducing the
total travel distance of bipartite tournaments, by analyzing inter-league
scheduling from the perspective of discrete optimization. This research has
natural applications to sports scheduling, especially for leagues such as the
National Basketball Association (NBA) where teams must travel long distances
across North America to play all their games, thus consuming much time, money,
and greenhouse gas emissions. We introduce the Bipartite Traveling Tournament
Problem (BTTP), the inter-league variant of the well-studied Traveling
Tournament Problem. We prove that the 2n-team BTTP is NP-complete, but for
small values of n, a distance-optimal inter-league schedule can be generated
from an algorithm based on minimum-weight 4-cycle-covers. We apply our
theoretical results to the 12-team Nippon Professional Baseball (NPB) league in
Japan, producing a provably-optimal schedule requiring 42950 kilometres of
total team travel, a 16% reduction compared to the actual distance traveled by
these teams during the 2010 NPB season. We also develop a nearly-optimal
inter-league tournament for the 30-team NBA league, just 3.8% higher than the
trivial theoretical lower bound."
"In the last decade, a lot of effort has been put into securing software
application during development in the software industry. Software security is a
research field in this area which looks at how security can be weaved into
software at each phase of software development lifecycle (SDLC). The use of
attack patterns is one of the approaches that have been proposed for
integrating security during the design phase of SDLC. While this approach help
developers in identify security flaws in their software designs, the need to
apply the proper security capability that will mitigate the threat identified
is very important. To assist in this area, the uses of security patterns have
been proposed to help developers to identify solutions to recurring security
problems. However due to different types of security patterns and their
taxonomy, software developers are faced with the challenge of finding and
selecting appropriate security patterns that addresses the security risks in
their design. In this paper, we propose a tool based on Neural Network for
proposing solutions in form of security patterns to threats in attack patterns
matching attacking patterns. From the result of performance of the neural
network, we found out that the neural network was able to match attack patterns
to security patterns that can mitigate the threat in the attack pattern. With
this information developers are better informed in making decision on the
solution for securing their application."
"In the context of CSPs, a strong backdoor is a subset of variables such that
every complete assignment yields a residual instance guaranteed to have a
specified property. If the property allows efficient solving, then a small
strong backdoor provides a reasonable decomposition of the original instance
into easy instances. An important challenge is the design of algorithms that
can find quickly a small strong backdoor if one exists. We present a systematic
study of the parameterized complexity of backdoor detection when the target
property is a restricted type of constraint language defined by means of a
family of polymorphisms. In particular, we show that under the weak assumption
that the polymorphisms are idempotent, the problem is unlikely to be FPT when
the parameter is either r (the constraint arity) or k (the size of the
backdoor) unless P = NP or FPT = W[2]. When the parameter is k+r, however, we
are able to identify large classes of languages for which the problem of
finding a small backdoor is FPT."
"We develop a model of abduction in abstract argumentation, where changes to
an argumentation framework act as hypotheses to explain the support of an
observation. We present dialogical proof theories for the main decision
problems (i.e., finding hypothe- ses that explain skeptical/credulous support)
and we show that our model can be instantiated on the basis of abductive logic
programs."
"In this paper, we address the problem of real-time detection of viruses
docking to nanowires, especially when multiple viruses dock to the same
nano-wire. The task becomes more complicated when there is an array of
nanowires coated with different antibodies, where different viruses can dock to
each coated nanowire at different binding strengths. We model the array
response to a viral agent as a pattern of conductance change over nanowires
with known modifier --- this representation permits analysis of the output of
such an array via belief network (Bayes) methods, as well as novel generative
models like the Hidden Semi-Markov Model (HSMM)."
"Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide
efficient automatic analysis of real-world feature models (FM) of systems
ranging from cars to operating systems. It is well-known that solver-based
analysis of real-world FMs scale very well even though SAT instances obtained
from such FMs are large, and the corresponding analysis problems are known to
be NP-complete. To better understand why SAT solvers are so effective, we
systematically studied many syntactic and semantic characteristics of a
representative set of large real-world FMs. We discovered that a key reason why
large real-world FMs are easy-to-analyze is that the vast majority of the
variables in these models are unrestricted, i.e., the models are satisfiable
for both true and false assignments to such variables under the current partial
assignment. Given this discovery and our understanding of CDCL SAT solvers, we
show that solvers can easily find satisfying assignments for such models
without too many backtracks relative to the model size, explaining why solvers
scale so well. Further analysis showed that the presence of unrestricted
variables in these real-world models can be attributed to their high-degree of
variability. Additionally, we experimented with a series of well-known
non-backtracking simplifications that are particularly effective in solving
FMs. The remaining variables/clauses after simplifications, called the core,
are so few that they are easily solved even with backtracking, further
strengthening our conclusions."
"An approach for game bot detection in MMORPGs is proposed based on the
analysis of game playing behavior. Since MMORPGs are large scale games, users
can play in various ways. This variety in playing behavior makes it hard to
detect game bots based on play behaviors. In order to cope with this problem,
the proposed approach observes game playing behaviors of users and groups them
by their behavioral similarities. Then, it develops a local bot detection model
for each player group. Since the locally optimized models can more accurately
detect game bots within each player group, the combination of those models
brings about overall improvement. For a practical purpose of reducing the
workloads of the game servers in service, the game data is collected at a low
resolution in time. Behavioral features are selected and developed to
accurately detect game bots with the low resolution data, considering common
aspects of MMORPG playing. Through the experiment with the real data from a
game currently in service, it is shown that the proposed local model approach
yields more accurate results."
"Sequence-to-sequence neural translation models learn semantic and syntactic
relations between sentence pairs by optimizing the likelihood of the target
given the source, i.e., $p(y|x)$, an objective that ignores other potentially
useful sources of information. We introduce an alternative objective function
for neural MT that maximizes the mutual information between the source and
target sentences, modeling the bi-directional dependency of sources and
targets. We implement the model with a simple re-ranking method, and also
introduce a decoding algorithm that increases diversity in the N-best list
produced by the first pass. Applied to the WMT German/English and
French/English tasks, the proposed models offers a consistent performance boost
on both standard LSTM and attention-based neural MT architectures."
"Making a computational agent 'social' has implications for how it perceives
itself and the environment in which it is situated, including the ability to
recognise the behaviours of others. We point to recent work on social planning,
i.e. planning in settings where the social context is relevant in the
assessment of the beliefs and capabilities of others, and in making appropriate
choices of what to do next."
"To appear in Theory and Practice of Logic Programming (TPLP). In this paper
we propose an extension of logic programming (LP) where each default literal
derived from the well-founded model is associated to a justification
represented as an algebraic expression. This expression contains both causal
explanations (in the form of proof graphs built with rule labels) and terms
under the scope of negation that stand for conditions that enable or disable
the application of causal rules. Using some examples, we discuss how these new
conditions, we respectively call ""enablers"" and ""inhibitors"", are intimately
related to default negation and have an essentially different nature from
regular cause-effect relations. The most important result is a formal
comparison to the recent algebraic approaches for justifications in LP:
""Why-not Provenance"" (WnP) and ""Causal Graphs"" (CG). We show that the current
approach extends both WnP and CG justifications under the Well-Founded
Semantics and, as a byproduct, we also establish a formal relation between
these two approaches."
"Despite progress in perceptual tasks such as image classification, computers
still perform poorly on cognitive tasks such as image description and question
answering. Cognition is core to tasks that involve not just recognizing, but
reasoning about our visual world. However, models used to tackle the rich
content in images for cognitive tasks are still being trained using the same
datasets designed for perceptual tasks. To achieve success at cognitive tasks,
models need to understand the interactions and relationships between objects in
an image. When asked ""What vehicle is the person riding?"", computers will need
to identify the objects in an image as well as the relationships riding(man,
carriage) and pulling(horse, carriage) in order to answer correctly that ""the
person is riding a horse-drawn carriage"".
  In this paper, we present the Visual Genome dataset to enable the modeling of
such relationships. We collect dense annotations of objects, attributes, and
relationships within each image to learn these models. Specifically, our
dataset contains over 100K images where each image has an average of 21
objects, 18 attributes, and 18 pairwise relationships between objects. We
canonicalize the objects, attributes, relationships, and noun phrases in region
descriptions and questions answer pairs to WordNet synsets. Together, these
annotations represent the densest and largest dataset of image descriptions,
objects, attributes, relationships, and question answers."
"We consider a problem of prediction based on opinions elicited from
heterogeneous rational agents with private information. Making an accurate
prediction with a minimal cost requires a joint design of the incentive
mechanism and the prediction algorithm. Such a problem lies at the nexus of
statistical learning theory and game theory, and arises in many domains such as
consumer surveys and mobile crowdsourcing. In order to elicit heterogeneous
agents' private information and incentivize agents with different capabilities
to act in the principal's best interest, we design an optimal joint incentive
mechanism and prediction algorithm called COPE (COst and Prediction
Elicitation), the analysis of which offers several valuable engineering
insights. First, when the costs incurred by the agents are linear in the
exerted effort, COPE corresponds to a ""crowd contending"" mechanism, where the
principal only employs the agent with the highest capability. Second, when the
costs are quadratic, COPE corresponds to a ""crowd-sourcing"" mechanism that
employs multiple agents with different capabilities at the same time. Numerical
simulations show that COPE improves the principal's profit and the network
profit significantly (larger than 30% in our simulations), comparing to those
mechanisms that assume all agents have equal capabilities."
"We train a number of neural networks to play games Bowling, Breakout and
Seaquest using information stored in the memory of a video game console Atari
2600. We consider four models of neural networks which differ in size and
architecture: two networks which use only information contained in the RAM and
two mixed networks which use both information in the RAM and information from
the screen. As the benchmark we used the convolutional model proposed in NIPS
and received comparable results in all considered games. Quite surprisingly, in
the case of Seaquest we were able to train RAM-only agents which behave better
than the benchmark screen-only agent. Mixing screen and RAM did not lead to an
improved performance comparing to screen-only and RAM-only agents."
"Psychological research results have confirmed that people can have different
emotional reactions to different visual stimuli. Several papers have been
published on the problem of visual emotion analysis. In particular, attempts
have been made to analyze and predict people's emotional reaction towards
images. To this end, different kinds of hand-tuned features are proposed. The
results reported on several carefully selected and labeled small image data
sets have confirmed the promise of such features. While the recent successes of
many computer vision related tasks are due to the adoption of Convolutional
Neural Networks (CNNs), visual emotion analysis has not achieved the same level
of success. This may be primarily due to the unavailability of confidently
labeled and relatively large image data sets for visual emotion analysis. In
this work, we introduce a new data set, which started from 3+ million weakly
labeled images of different emotions and ended up 30 times as large as the
current largest publicly available visual emotion data set. We hope that this
data set encourages further research on visual emotion analysis. We also
perform extensive benchmarking analyses on this large data set using the state
of the art methods including CNNs."
"We describe a system to detect objects in three-dimensional space using video
and inertial sensors (accelerometer and gyrometer), ubiquitous in modern mobile
platforms from phones to drones. Inertials afford the ability to impose
class-specific scale priors for objects, and provide a global orientation
reference. A minimal sufficient representation, the posterior of semantic
(identity) and syntactic (pose) attributes of objects in space, can be
decomposed into a geometric term, which can be maintained by a
localization-and-mapping filter, and a likelihood function, which can be
approximated by a discriminatively-trained convolutional neural network. The
resulting system can process the video stream causally in real time, and
provides a representation of objects in the scene that is persistent:
Confidence in the presence of objects grows with evidence, and objects
previously seen are kept in memory even when temporarily occluded, with their
return into view automatically predicted to prime re-detection."
"In this paper, we show how a planning algorithm can be used to automatically
create and update a Behavior Tree (BT), controlling a robot in a dynamic
environment. The planning part of the algorithm is based on the idea of back
chaining. Starting from a goal condition we iteratively select actions to
achieve that goal, and if those actions have unmet preconditions, they are
extended with actions to achieve them in the same way. The fact that BTs are
inherently modular and reactive makes the proposed solution blend acting and
planning in a way that enables the robot to efficiently react to external
disturbances. If an external agent undoes an action the robot reexecutes it
without re-planning, and if an external agent helps the robot, it skips the
corresponding actions, again without replanning. We illustrate our approach in
two different robotics scenarios."
"Automaton models are often seen as interpretable models. Interpretability
itself is not well defined: it remains unclear what interpretability means
without first explicitly specifying objectives or desired attributes. In this
paper, we identify the key properties used to interpret automata and propose a
modification of a state-merging approach to learn variants of finite state
automata. We apply the approach to problems beyond typical grammar inference
tasks. Additionally, we cover several use-cases for prediction, classification,
and clustering on sequential data in both supervised and unsupervised scenarios
to show how the identified key properties are applicable in a wide range of
contexts."
"The concept of uncertainty is posed in almost any complex system including
parallel robots as an outstanding instance of dynamical robotics systems. As
suggested by the name, uncertainty, is some missing information that is beyond
the knowledge of human thus we may tend to handle it properly to minimize the
side-effects through the control process.
  Type-II fuzzy logic has shown its superiority over traditional fuzzy logic
when dealing with uncertainty. Type-II fuzzy logic controllers are however
newer and more promising approaches that have been recently applied to various
fields due to their significant contribution especially when noise (as an
important instance of uncertainty) emerges. During the design of Type-I fuzzy
logic systems, we presume that we are almost certain about the fuzzy membership
functions which is not true in many cases. Thus T2FLS as a more realistic
approach dealing with practical applications might have a lot to offer. Type-II
fuzzy logic takes into account a higher level of uncertainty, in other words,
the membership grade for a type-II fuzzy variable is no longer a crisp number
but rather is itself a type-I linguistic term. In this thesis the effects of
uncertainty in dynamic control of a parallel robot is considered. More
specifically, it is intended to incorporate the Type-II Fuzzy Logic paradigm
into a model based controller, the so-called computed torque control method,
and apply the result to a 3 degrees of freedom parallel manipulator.
  ..."
"Attention-based neural encoder-decoder frameworks have been widely adopted
for image captioning. Most methods force visual attention to be active for
every generated word. However, the decoder likely requires little to no visual
information from the image to predict non-visual words such as ""the"" and ""of"".
Other words that may seem visual can often be predicted reliably just from the
language model e.g., ""sign"" after ""behind a red stop"" or ""phone"" following
""talking on a cell"". In this paper, we propose a novel adaptive attention model
with a visual sentinel. At each time step, our model decides whether to attend
to the image (and if so, to which regions) or to the visual sentinel. The model
decides whether to attend to the image and where, in order to extract
meaningful information for sequential word generation. We test our method on
the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach
sets the new state-of-the-art by a significant margin."
"Connections between relations in relation extraction, which we call class
ties, are common. In distantly supervised scenario, one entity tuple may have
multiple relation facts. Exploiting class ties between relations of one entity
tuple will be promising for distantly supervised relation extraction. However,
previous models are not effective or ignore to model this property. In this
work, to effectively leverage class ties, we propose to make joint relation
extraction with a unified model that integrates convolutional neural network
(CNN) with a general pairwise ranking framework, in which three novel ranking
loss functions are introduced. Additionally, an effective method is presented
to relieve the severe class imbalance problem from NR (not relation) for model
training. Experiments on a widely used dataset show that leveraging class ties
will enhance extraction and demonstrate the effectiveness of our model to learn
class ties. Our model outperforms the baselines significantly, achieving
state-of-the-art performance."
"Supervised learning tends to produce more accurate classifiers than
unsupervised learning in general. This implies that training data is preferred
with annotations. When addressing visual perception challenges, such as
localizing certain object classes within an image, the learning of the involved
classifiers turns out to be a practical bottleneck. The reason is that, at
least, we have to frame object examples with bounding boxes in thousands of
images. A priori, the more complex the model is regarding its number of
parameters, the more annotated examples are required. This annotation task is
performed by human oracles, which ends up in inaccuracies and errors in the
annotations (aka ground truth) since the task is inherently very cumbersome and
sometimes ambiguous. As an alternative we have pioneered the use of virtual
worlds for collecting such annotations automatically and with high precision.
However, since the models learned with virtual data must operate in the real
world, we still need to perform domain adaptation (DA). In this chapter we
revisit the DA of a deformable part-based model (DPM) as an exemplifying case
of virtual- to-real-world DA. As a use case, we address the challenge of
vehicle detection for driver assistance, using different publicly available
virtual-world data. While doing so, we investigate questions such as: how does
the domain gap behave due to virtual-vs-real data with respect to dominant
object appearance per domain, as well as the role of photo-realism in the
virtual world."
"With the popularity of massive open online courses, grading through
crowdsourcing has become a prevalent approach towards large scale classes.
However, for getting grades for complex tasks, which require specific skills
and efforts for grading, crowdsourcing encounters a restriction of insufficient
knowledge of the workers from the crowd. Due to knowledge limitation of the
crowd graders, grading based on partial perspectives becomes a big challenge
for evaluating complex tasks through crowdsourcing. Especially for those tasks
which not only need specific knowledge for grading, but also should be graded
as a whole instead of being decomposed into smaller and simpler subtasks. We
propose a framework for grading complex tasks via multiple views, which are
different grading perspectives defined by experts for the task, to provide
uniformity. Aggregation algorithm based on graders variances are used to
combine the grades for each view. We also detect bias patterns of the graders,
and debias them regarding each view of the task. Bias pattern determines how
the behavior is biased among graders, which is detected by a statistical
technique. The proposed approach is analyzed on a synthetic data set. We show
that our model gives more accurate results compared to the grading approaches
without different views and debiasing algorithm."
"In this paper, the idea of a new artificial intelligence based optimization
algorithm, which is inspired from the nature of vortex, has been provided
briefly. As also a bio-inspired computation algorithm, the idea is generally
focused on a typical vortex flow / behavior in nature and inspires from some
dynamics that are occurred in the sense of vortex nature. Briefly, the
algorithm is also a swarm-oriented evolutional problem solution approach;
because it includes many methods related to elimination of weak swarm members
and trying to improve the solution process by supporting the solution space via
new swarm members. In order have better idea about success of the algorithm; it
has been tested via some benchmark functions. At this point, the obtained
results show that the algorithm can be an alternative to the literature in
terms of single-objective optimization solution ways. Vortex Optimization
Algorithm (VOA) is the name suggestion by the authors; for this new idea of
intelligent optimization approach."
"It is well-known that exploiting label correlations is important to
multi-label learning. Existing approaches either assume that the label
correlations are global and shared by all instances; or that the label
correlations are local and shared only by a data subset. In fact, in the
real-world applications, both cases may occur that some label correlations are
globally applicable and some are shared only in a local group of instances.
Moreover, it is also a usual case that only partial labels are observed, which
makes the exploitation of the label correlations much more difficult. That is,
it is hard to estimate the label correlations when many labels are absent. In
this paper, we propose a new multi-label approach GLOCAL dealing with both the
full-label and the missing-label cases, exploiting global and local label
correlations simultaneously, through learning a latent label representation and
optimizing label manifolds. The extensive experimental studies validate the
effectiveness of our approach on both full-label and missing-label data."
"Despite efforts to increase the supply of organs from living donors, most
kidney transplants performed in Australia still come from deceased donors. The
age of these donated organs has increased substantially in recent decades as
the rate of fatal accidents on roads has fallen. The Organ and Tissue Authority
in Australia is therefore looking to design a new mechanism that better matches
the age of the organ to the age of the patient. I discuss the design,
axiomatics and performance of several candidate mechanisms that respect the
special online nature of this fair division problem."
"We propose USim, a semantic measure for Grammatical Error Correction (GEC)
that measures the semantic faithfulness of the output to the source, thereby
complementing existing reference-less measures (RLMs) for measuring the
output's grammaticality. USim operates by comparing the semantic symbolic
structure of the source and the correction, without relying on manually-curated
references. Our experiments establish the validity of USim, by showing that (1)
semantic annotation can be consistently applied to ungrammatical text; (2)
valid corrections obtain a high USim similarity score to the source; and (3)
invalid corrections obtain a lower score."
"We study feature propagation on graph, an inference process involved in graph
representation learning tasks. It's to spread the features over the whole graph
to the $t$-th orders, thus to expand the end's features. The process has been
successfully adopted in graph embedding or graph neural networks, however few
works studied the convergence of feature propagation. Without convergence
guarantees, it may lead to unexpected numerical overflows and task failures. In
this paper, we first define the concept of feature propagation on graph
formally, and then study its convergence conditions to equilibrium states. We
further link feature propagation to several established approaches such as
node2vec and structure2vec. In the end of this paper, we extend existing
approaches from represent nodes to edges (edge2vec) and demonstrate its
applications on fraud transaction detection in real world scenario. Experiments
show that it is quite competitive."
"Many data mining tasks cannot be completely addressed by auto- mated
processes, such as sentiment analysis and image classification. Crowdsourcing
is an effective way to harness the human cognitive ability to process these
machine-hard tasks. Thanks to public crowdsourcing platforms, e.g., Amazon
Mechanical Turk and Crowd- Flower, we can easily involve hundreds of thousands
of ordinary workers (i.e., the crowd) to address these machine-hard tasks. In
this tutorial, we will survey and synthesize a wide spectrum of existing
studies on crowd-powered data mining. We first give an overview of
crowdsourcing, and then summarize the fundamental techniques, including quality
control, cost control, and latency control, which must be considered in
crowdsourced data mining. Next we review crowd-powered data mining operations,
including classification, clustering, pattern mining, machine learning using
the crowd (including deep learning, transfer learning and semi-supervised
learning) and knowledge discovery. Finally, we provide the emerging challenges
in crowdsourced data mining."
"Evaluating on adversarial examples has become a standard procedure to measure
robustness of deep learning models. Due to the difficulty of creating white-box
adversarial examples for discrete text input, most analyses of the robustness
of NLP models have been done through black-box adversarial examples. We
investigate adversarial examples for character-level neural machine translation
(NMT), and contrast black-box adversaries with a novel white-box adversary,
which employs differentiable string-edit operations to rank adversarial
changes. We propose two novel types of attacks which aim to remove or change a
word in a translation, rather than simply break the NMT. We demonstrate that
white-box adversarial examples are significantly stronger than their black-box
counterparts in different attack scenarios, which show more serious
vulnerabilities than previously known. In addition, after performing
adversarial training, which takes only 3 times longer than regular training, we
can improve the model's robustness significantly."
"Real-time recognition of dynamic hand gestures from video streams is a
challenging task since (i) there is no indication when a gesture starts and
ends in the video, (ii) performed gestures should only be recognized once, and
(iii) the entire architecture should be designed considering the memory and
power budget. In this work, we address these challenges by proposing a
hierarchical structure enabling offline-working convolutional neural network
(CNN) architectures to operate online efficiently by using sliding window
approach. The proposed architecture consists of two models: (1) A detector
which is a lightweight CNN architecture to detect gestures and (2) a classifier
which is a deep CNN to classify the detected gestures. In order to evaluate the
single-time activations of the detected gestures, we propose to use Levenshtein
distance as an evaluation metric since it can measure misclassifications,
multiple detections, and missing detections at the same time. We evaluate our
architecture on two publicly available datasets - EgoGesture and NVIDIA Dynamic
Hand Gesture Datasets - which require temporal detection and classification of
the performed hand gestures. ResNeXt-101 model, which is used as a classifier,
achieves the state-of-the-art offline classification accuracy of 94.04% and
83.82% for depth modality on EgoGesture and NVIDIA benchmarks, respectively. In
real-time detection and classification, we obtain considerable early detections
while achieving performances close to offline operation. The codes and
pretrained models used in this work are publicly available."
"Conversational agents have begun to rise both in the academic (in terms of
research) and commercial (in terms of applications) world. This paper
investigates the task of building a non-goal driven conversational agent, using
neural network generative models and analyzes how the conversation context is
handled. It compares a simpler Encoder-Decoder with a Hierarchical Recurrent
Encoder-Decoder architecture, which includes an additional module to model the
context of the conversation using previous utterances information. We found
that the hierarchical model was able to extract relevant context information
and include them in the generation of the output. However, it performed worse
(35-40%) than the simple Encoder-Decoder model regarding both grammatically
correct output and meaningful response. Despite these results, experiments
demonstrate how conversations about similar topics appear close to each other
in the context space due to the increased frequency of specific topic-related
words, thus leaving promising directions for future research and how the
context of a conversation can be exploited."
"In reality, there is still much to be done for robots to be able to perform
manipulation actions with full autonomy. Complicated manipulation tasks, such
as cooking, may still require a person to perform some actions that are very
risky for a robot to perform. On the other hand, some other actions may be very
risky for a human with physical disabilities to perform. Therefore, it is
necessary to balance the workload of a robot and a human based on their
limitations while minimizing the effort needed from a human in a collaborative
robot (cobot) set-up. This paper proposes a new version of our functional
object-oriented network (FOON) that integrates weights in its functional units
to reflect a robot's chance of successfully executing an action of that
functional unit. The paper also presents a task planning algorithm for the
weighted FOON to allocate manipulation action load to the robot and human to
achieve optimal performance while minimizing human effort. Through a number of
experiments, this paper shows several successful cases in which using the
proposed weighted FOON and the task planning algorithm allow a robot and a
human to successfully complete complicated tasks together with higher success
rates than a robot doing them alone."
"Currently, the text document retrieval systems have many challenges in
exploring the semantics of queries and documents. Each query implies
information which does not appear in the query but the documents related with
the information are also expected by user. The disadvantage of the previous
spreading activation algorithms could be many irrelevant concepts added to the
query. In this paper, a proposed novel algorithm is only activate and add to
the query named entities which are related with original entities in the query
and explicit relations in the query."
"The post-modern novel 'Wittgenstein's Mistress' by David Markson (1988)
presents the reader with a very challenging non linear narrative, that itself
appears to one of the novel's themes. We present a distant reading of this work
designed to complement a close reading of it by David Foster Wallace (1990).
Using a combination of text analysis, entity recognition and networks, we plot
repetitive structures in the novel's narrative relating them to its critical
analysis."
"For reentry or near space communication, owing to the influence of the
time-varying plasma sheath channel environment, the received IQ baseband
signals are severely rotated on the constellation. Researches have shown that
the frequency of electron density varies from 20kHz to 100 kHz which is on the
same order as the symbol rate of most TT\&C communication systems and a mass of
bandwidth will be consumed to track the time-varying channel with traditional
estimation. In this paper, motivated by principal curve analysis, we propose a
deep learning (DL) algorithm which called symmetric manifold network (SMN) to
extract the curves on the constellation and classify the signals based on the
curves. The key advantage is that SMN can achieve joint optimization of
demodulation and channel estimation. From our simulation results, the new
algorithm significantly reduces the symbol error rate (SER) compared to
existing algorithms and enables accurate estimation of fading with extremely
high bandwith utilization rate."
"The use of deep neural networks to make high risk decisions creates a need
for global and local explanations so that users and experts have confidence in
the modeling algorithms. We introduce a novel technique to find global and
local explanations for time series data used in binary classification machine
learning systems. We identify the most salient of the original features used by
a black box model to distinguish between classes. The explanation can be made
on categorical, continuous, and time series data and can be generalized to any
binary classification model. The analysis is conducted on time series data to
train a long short-term memory deep neural network and uses the time dependent
structure of the underlying features in the explanation. The proposed technique
attributes weights to features to explain an observations risk of belonging to
a class as a multiplicative factor of a base hazard rate. We use a variation of
the Cox Proportional Hazards regression, a Generalized Additive Model, to
explain the effect of variables upon the probability of an in-class response
for a score output from the black box model. The covariates incorporate time
dependence structure in the features so the explanation is inclusive of the
underlying time series data structure."
"Reinforcement learning has shown great promise in the training of robot
behavior due to the sequential decision making characteristics. However, the
required enormous amount of interactive and informative training data provides
the major stumbling block for progress. In this study, we focus on accelerating
reinforcement learning (RL) training and improving the performance of
multi-goal reaching tasks. Specifically, we propose a precision-based
continuous curriculum learning (PCCL) method in which the requirements are
gradually adjusted during the training process, instead of fixing the parameter
in a static schedule. To this end, we explore various continuous curriculum
strategies for controlling a training process. This approach is tested using a
Universal Robot 5e in both simulation and real-world multi-goal reach
experiments. Experimental results support the hypothesis that a static training
schedule is suboptimal, and using an appropriate decay function for curriculum
learning provides superior results in a faster way."
"The sigma point (SP) filter, also known as unscented Kalman filter, is an
attractive alternative to the extended Kalman filter and the particle filter.
Here, we extend the SP filter to nonsequential Bayesian inference corresponding
to loopy factor graphs. We propose sigma point belief propagation (SPBP) as a
low-complexity approximation of the belief propagation (BP) message passing
scheme. SPBP achieves approximate marginalizations of posterior distributions
corresponding to (generally) loopy factor graphs. It is well suited for
decentralized inference because of its low communication requirements. For a
decentralized, dynamic sensor localization problem, we demonstrate that SPBP
can outperform nonparametric (particle-based) BP while requiring significantly
less computations and communications."
"There has been an ever-increasing interest in multidisciplinary research on
representing and reasoning with imperfect data. Possibilistic networks present
one of the powerful frameworks of interest for representing uncertain and
imprecise information. This paper covers the problem of their parameters
learning from imprecise datasets, i.e., containing multi-valued data. We
propose in the rst part of this paper a possibilistic networks sampling
process. In the second part, we propose a likelihood function which explores
the link between random sets theory and possibility theory. This function is
then deployed to parametrize possibilistic networks."
"Combinatorial optimization problems are typically NP-hard, and thus very
challenging to solve. In this paper, we present the random key cuckoo search
(RKCS) algorithm for solving the famous Travelling Salesman Problem (TSP). We
used a simplified random-key encoding scheme to pass from a continuous space
(real numbers) to a combinatorial space. We also consider the displacement of a
solution in both spaces using Levy flights. The performance of the proposed
RKCS is tested against a set of benchmarks of symmetric TSP from the well-known
TSPLIB library. The results of the tests show that RKCS is superior to some
other metaheuristic algorithms."
"One way of carving up the broad ""AI ethics and society"" research space that
has emerged in recent years is to distinguish between ""near-term"" and
""long-term"" research. While such ways of breaking down the research space can
be useful, we put forward several concerns about the near/long-term distinction
gaining too much prominence in how research questions and priorities are
framed. We highlight some ambiguities and inconsistencies in how the
distinction is used, and argue that while there are differing priorities within
this broad research community, these differences are not well-captured by the
near/long-term distinction. We unpack the near/long-term distinction into four
different dimensions, and propose some ways that researchers can communicate
more clearly about their work and priorities using these dimensions. We suggest
that moving towards a more nuanced conversation about research priorities can
help establish new opportunities for collaboration, aid the development of more
consistent and coherent research agendas, and enable identification of
previously neglected research areas."
"Constructing general knowledge by learning task-independent models of the
world can help agents solve challenging problems. However, both constructing
and evaluating such models remains an open challenge. The most common
approaches to evaluating models is to assess their accuracy with respect to
observable values. However, the prevailing reliance on estimator accuracy as a
proxy for the usefulness of the knowledge has the potential to lead us astray.
We demonstrate the conflict between accuracy and usefulness through a series of
illustrative examples including both a thought experiment and empirical example
in MineCraft, using the General Value Function framework (GVF). Having
identified challenges in assessing an agent's knowledge, we propose an
alternate evaluation approach that arises continually in the online continual
learning setting we recommend evaluation by examining internal learning
processes, specifically the relevance of a GVF's features to the prediction
task at hand. This paper contributes a first look into evaluation of
predictions through their use, an integral component of predictive knowledge
which is as of yet unexplored."
"In various domains and cases, we observe the creation and usage of
information elements which are unnamed. Such elements do not have a name, or
may have a name that is not externally referable (usually meaningless and not
persistent over time). This paper discusses why we will never `escape' from the
problem of having to construct mappings between such unnamed elements in
information systems. Since unnamed elements nowadays occur very often in the
framework of the Semantic Web and Linked Data as blank nodes, the paper
describes scenarios that can benefit from methods that compute mappings between
the unnamed elements. For each scenario, the corresponding bnode matching
problem is formally defined. Based on this analysis, we try to reach to more a
general formulation of the problem, which can be useful for guiding the
required technological advances. To this end, the paper finally discusses
methods to realize blank node matching, the implementations that exist, and
identifies open issues and challenges."
"We give a detailed characterization of optimal trades under budget
constraints in a prediction market with a cost-function-based automated market
maker. We study how the budget constraints of individual traders affect their
ability to impact the market price. As a concrete application of our
characterization, we give sufficient conditions for a property we call budget
additivity: two traders with budgets B and B' and the same beliefs would have a
combined impact equal to a single trader with budget B+B'. That way, even if a
single trader cannot move the market much, a crowd of like-minded traders can
have the same desired effect. When the set of payoff vectors associated with
outcomes, with coordinates corresponding to securities, is affinely
independent, we obtain that a generalization of the heavily-used logarithmic
market scoring rule is budget additive, but the quadratic market scoring rule
is not. Our results may be used both descriptively, to understand if a
particular market maker is affected by budget constraints or not, and
prescriptively, as a recipe to construct markets."
"Subspace clustering aims to group data points into multiple clusters of which
each corresponds to one subspace. Most existing subspace clustering approaches
assume that input data lie on linear subspaces. In practice, however, this
assumption usually does not hold. To achieve nonlinear subspace clustering, we
propose a novel method, called kernel truncated regression representation. Our
method consists of the following four steps: 1) projecting the input data into
a hidden space, where each data point can be linearly represented by other data
points; 2) calculating the linear representation coefficients of the data
representations in the hidden space; 3) truncating the trivial coefficients to
achieve robustness and block-diagonality; and 4) executing the graph cutting
operation on the coefficient matrix by solving a graph Laplacian problem. Our
method has the advantages of a closed-form solution and the capacity of
clustering data points that lie on nonlinear subspaces. The first advantage
makes our method efficient in handling large-scale datasets, and the second one
enables the proposed method to conquer the nonlinear subspace clustering
challenge. Extensive experiments on six benchmarks demonstrate the
effectiveness and the efficiency of the proposed method in comparison with
current state-of-the-art approaches."
"A broad range of on-line behaviors are mediated by interfaces in which people
make choices among sets of options. A rich and growing line of work in the
behavioral sciences indicate that human choices follow not only from the
utility of alternatives, but also from the choice set in which alternatives are
presented. In this work we study comparison-based choice functions, a simple
but surprisingly rich class of functions capable of exhibiting so-called
choice-set effects. Motivated by the challenge of predicting complex choices,
we study the query complexity of these functions in a variety of settings. We
consider settings that allow for active queries or passive observation of a
stream of queries, and give analyses both at the granularity of individuals or
populations that might exhibit heterogeneous choice behavior. Our main result
is that any comparison-based choice function in one dimension can be inferred
as efficiently as a basic maximum or minimum choice function across many query
contexts, suggesting that choice-set effects need not entail any fundamental
algorithmic barriers to inference. We also introduce a class of choice
functions we call distance-comparison-based functions, and briefly discuss the
analysis of such functions. The framework we outline provides intriguing
connections between human choice behavior and a range of questions in the
theory of sorting."
"With increasing physicians' workload and patients' needs for care, there is a
need for technology that facilitates physicians work and performs continues
follow-up with patients. Existing approaches focus merely on improving
patient's condition, and none have considered managing physician's workload.
This paper presents an initial evaluation of a conversational agent assisted
coaching platform intended to manage physicians' fatigue and provide continuous
follow-up to patients. We highlight the approach adapted to build the chatbot
dialogue and the coaching platform. We will particularly discuss the activity
recommender algorithms used to suggest insights about patients' condition and
activities based on previously collected data. The paper makes three
contributions: (1) present the conversational agent as an assistive virtual
coach, (2) decrease physicians workload and continuous follow up with patients,
all by handling some repetitive physician tasks and performing initial follow
up with the patient, (3) present the activity recommender that tracks previous
activities and patient information and provides useful insights about possible
activity and patient match to the coach. Future work focuses on integrating the
recommender model with the CoachAI platform and test the prototype with
patient's in collaboration with an ambulatory clinic."
"Data-driven, knowledge-grounded neural conversation models are capable of
generating more informative responses. However, these models have not yet
demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs.
This paper proposes a new task about how to apply dynamic knowledge graphs in
neural conversation model and presents a novel TV series conversation corpus
(DyKgChat) for the task. Our new task and corpus aids in understanding the
influence of dynamic knowledge graphs on responses generation. Also, we propose
a preliminary model that selects an output from two networks at each time step:
a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in
order to support dynamic knowledge graphs. To benchmark this new task and
evaluate the capability of adaptation, we introduce several evaluation metrics
and the experiments show that our proposed approach outperforms previous
knowledge-grounded conversation models. The proposed corpus and model can
motivate the future research directions."
"Multimodality is one of the biggest difficulties for optimization as local
optima are often preventing algorithms from making progress. This does not only
challenge local strategies that can get stuck. It also hinders meta-heuristics
like evolutionary algorithms in convergence to the global optimum. In this
paper we present a new concept of gradient descent, which is able to escape
local traps. It relies on multiobjectivization of the original problem and
applies the recently proposed and here slightly modified multi-objective local
search mechanism MOGSA. We use a sophisticated visualization technique for
multi-objective problems to prove the working principle of our idea. As such,
this work highlights the transfer of new insights from the multi-objective to
the single-objective domain and provides first visual evidence that
multiobjectivization can link single-objective local optima in multimodal
landscapes."
"As one kind of architecture from the deep learning family, deep semantic
segmentation network (DSSN) achieves a certain degree of success on the
semantic segmentation task and obviously outperforms the traditional methods
based on hand-crafted features. As a classic data-driven technique, DSSN can be
trained by an end-to-end mechanism and competent for employing the low-level
and mid-level cues (i.e., the discriminative image structure) to understand
images, but lacks the high-level inference ability. By contrast, human beings
have an excellent inference capacity and can be able to reliably interpret the
RS imagery only when human beings master the basic RS domain knowledge. In
literature, ontological modeling and reasoning is an ideal way to imitate and
employ the domain knowledge of human beings, but is still rarely explored and
adopted in the RS domain. To remedy the aforementioned critical limitation of
DSSN, this paper proposes a collaboratively boosting framework (CBF) to combine
data-driven deep learning module and knowledge-guided ontological reasoning
module in an iterative way."
"In this study, the effects of different class labels created as a result of
multiple conceptual meanings on localization using Weakly Supervised Learning
presented on Car Dataset. In addition, the generated labels are included in the
comparison, and the solution turned into Unsupervised Learning. This paper
investigates multiple setups for car localization in the images with other
approaches rather than Supervised Learning. To predict localization labels,
Class Activation Mapping (CAM) is implemented and from the results, the
bounding boxes are extracted by using morphological edge detection. Besides the
original class labels, generated class labels also employed to train CAM on
which turn to a solution to Unsupervised Learning example. In the experiments,
we first analyze the effects of class labels in Weakly Supervised localization
on the Compcars dataset. We then show that the proposed Unsupervised approach
outperforms the Weakly Supervised method in this particular dataset by
approximately %6."
"The increasing application of machine learning techniques in everyday
decision-making processes has brought concerns about the fairness of
algorithmic decision-making. This paper concerns the problem of collider bias
which produces spurious associations in fairness assessment and develops
theorems to guide fairness assessment avoiding the collider bias. We consider a
real-world application of auditing a trained classifier by an audit agency. We
propose an unbiased assessment algorithm by utilising the developed theorems to
reduce collider biases in the assessment. Experiments and simulations show the
proposed algorithm reduces collider biases significantly in the assessment and
is promising in auditing trained classifiers."
"This paper describes a new research project that aims to develop a social
robot designed to help children cope with painful and distressing medical
procedures in a clinical setting. While robots have previously been trialled
for this task, with promising initial results, the systems have tended to be
teleoperated, limiting their flexibility and robustness. This project will use
epistemic planning techniques as a core component for action selection in the
robot system, in order to generate plans that include physical, sensory, and
social actions for interacting with humans. The robot will operate in a task
environment where appropriate and safe interaction with children,
parents/caregivers, and healthcare professionals is required. In addition to
addressing the core technical challenge of building an autonomous social robot,
the project will incorporate co-design techniques involving all participant
groups, and the final robot system will be evaluated in a two-site clinical
trial."
"Many decision-making processes involve solving a combinatorial optimization
problem with uncertain input that can be estimated from historic data.
Recently, problems in this class have been successfully addressed via
end-to-end learning approaches, which rely on solving one optimization problem
for each training instance at every epoch. In this context, we provide two
distinct contributions. First, we use a Noise Contrastive approach to motivate
a family of surrogate loss functions, based on viewing non-optimal solutions as
negative examples. Second, we address a major bottleneck of all
predict-and-optimize approaches, i.e. the need to frequently recompute optimal
solutions at training time. This is done via a solver-agnostic solution caching
scheme, and by replacing optimization calls with a lookup in the solution
cache. The method is formally based on an inner approximation of the feasible
space and, combined with a cache lookup strategy, provides a controllable
trade-off between training time and accuracy of the loss approximation. We
empirically show that even a very slow growth rate is enough to match the
quality of state-of-the-art methods, at a fraction of the computational cost."
"In this work, we present an ensemble of descriptors for the classification of
transmission electron microscopy images of viruses. We propose to combine
handcrafted and deep learning approaches for virus image classification. The
set of handcrafted is mainly based on Local Binary Pattern variants, for each
descriptor a different Support Vector Machine is trained, then the set of
classifiers is combined by sum rule. The deep learning approach is a
densenet201 pretrained on ImageNet and then tuned in the virus dataset, the net
is used as features extractor for feeding another Support Vector Machine, in
particular the last average pooling layer is used as feature extractor.
Finally, classifiers trained on handcrafted features and classifier trained on
deep learning features are combined by sum rule. The proposed fusion strongly
boosts the performance obtained by each stand-alone approach, obtaining state
of the art performance."
"Modern deep learning algorithms tend to optimize an objective metric, such as
minimize a cross entropy loss on a training dataset, to be able to learn. The
problem is that the single metric is an incomplete description of the real
world tasks. The single metric cannot explain why the algorithm learn. When an
erroneous happens, the lack of interpretability causes a hardness of
understanding and fixing the error. Recently, there are works done to tackle
the problem of interpretability to provide insights into neural networks
behavior and thought process. The works are important to identify potential
bias and to ensure algorithm fairness as well as expected performance."
"Since the label collecting is prohibitive and time-consuming, unsupervised
methods are preferred in applications such as fraud detection. Meanwhile, such
applications usually require modeling the intrinsic clusters in
high-dimensional data, which usually displays heterogeneous statistical
patterns as the patterns of different clusters may appear in different
dimensions. Existing methods propose to model the data clusters on selected
dimensions, yet globally omitting any dimension may damage the pattern of
certain clusters. To address the above issues, we propose a novel unsupervised
generative framework called FIRD, which utilizes adversarial distributions to
fit and disentangle the heterogeneous statistical patterns. When applying to
discrete spaces, FIRD effectively distinguishes the synchronized fraudsters
from normal users. Besides, FIRD also provides superior performance on anomaly
detection datasets compared with SOTA anomaly detection methods (over 5%
average AUC improvement). The significant experiment results on various
datasets verify that the proposed method can better model the heterogeneous
statistical patterns in high-dimensional data and benefit downstream
applications."
"Most deep neural networks are considered to be black boxes, meaning their
output is hard to interpret. In contrast, logical expressions are considered to
be more comprehensible since they use symbols that are semantically close to
natural language instead of distributed representations. However, for
high-dimensional input data such as images, the individual symbols, i.e.
pixels, are not easily interpretable. We introduce the concept of first-order
convolutional rules, which are logical rules that can be extracted using a
convolutional neural network (CNN), and whose complexity depends on the size of
the convolutional filter and not on the dimensionality of the input. Our
approach is based on rule extraction from binary neural networks with
stochastic local search. We show how to extract rules that are not necessarily
short, but characteristic of the input, and easy to visualize. Our experiments
show that the proposed approach is able to model the functionality of the
neural network while at the same time producing interpretable logical rules."
"A large number of individuals are suffering from suicidal ideation in the
world. There are a number of causes behind why an individual might suffer from
suicidal ideation. As the most popular platform for self-expression, emotion
release, and personal interaction, individuals may exhibit a number of symptoms
of suicidal ideation on social media. Nevertheless, challenges from both data
and knowledge aspects remain as obstacles, constraining the social media-based
detection performance. Data implicitness and sparsity make it difficult to
discover the inner true intentions of individuals based on their posts.
Inspired by psychological studies, we build and unify a high-level
suicide-oriented knowledge graph with deep neural networks for suicidal
ideation detection on social media. We further design a two-layered attention
mechanism to explicitly reason and establish key risk factors to individual's
suicidal ideation. The performance study on microblog and Reddit shows that: 1)
with the constructed personal knowledge graph, the social media-based suicidal
ideation detection can achieve over 93% accuracy; and 2) among the six
categories of personal factors, post, personality, and experience are the top-3
key indicators. Under these categories, posted text, stress level, stress
duration, posted image, and ruminant thinking contribute to one's suicidal
ideation detection."
"Our brains extract durable, generalizable knowledge from transient
experiences of the world. Artificial neural networks come nowhere close to this
ability. When tasked with learning to classify objects by training on
non-repeating video frames in temporal order (online stream learning), models
that learn well from shuffled datasets catastrophically forget old knowledge
upon learning new stimuli. We propose a new continual learning algorithm,
Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by
replaying feature maps reconstructed by combining generic parts. CRUMB
concatenates trainable and re-usable ""memory block"" vectors to compositionally
reconstruct feature map tensors in convolutional neural networks. Storing the
indices of memory blocks used to reconstruct new stimuli enables memories of
the stimuli to be replayed during later tasks. This reconstruction mechanism
also primes the neural network to minimize catastrophic forgetting by biasing
it towards attending to information about object shapes more than information
about image textures, and stabilizes the network during stream learning by
providing a shared feature-level basis for all training examples. These
properties allow CRUMB to outperform an otherwise identical algorithm that
stores and replays raw images, while occupying only 3.6% as much memory. We
stress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.
To address the limited number of existing online stream learning datasets, we
introduce 2 new benchmarks by adapting existing datasets for stream learning.
With only 3.7-4.1% as much memory and 15-43% as much runtime, CRUMB mitigates
catastrophic forgetting more effectively than the state-of-the-art. Our code is
available at https://github.com/MorganBDT/crumb.git."
"Tensor Decomposition Networks (TDNs) prevail for their inherent compact
architectures. To give more researchers a flexible way to exploit TDNs, we
present a Pytorch toolkit named TedNet. TedNet implements 5 kinds of tensor
decomposition(i.e., CANDECOMP/PARAFAC (CP), Block-Term Tucker (BTT), Tucker-2,
Tensor Train (TT) and Tensor Ring (TR) on traditional deep neural layers, the
convolutional layer and the fully-connected layer. By utilizing the basic
layers, it is simple to construct a variety of TDNs. TedNet is available at
https://github.com/tnbar/tednet."
"We present a comparison of several online machine learning techniques for
tactical learning and proving in the Coq proof assistant. This work builds on
top of Tactician, a plugin for Coq that learns from proofs written by the user
to synthesize new proofs. Learning happens in an online manner, meaning that
Tactician's machine learning model is updated immediately every time the user
performs a step in an interactive proof. This has important advantages compared
to the more studied offline learning systems: (1) it provides the user with a
seamless, interactive experience with Tactician and, (2) it takes advantage of
locality of proof similarity, which means that proofs similar to the current
proof are likely to be found close by. We implement two online methods, namely
approximate k-nearest neighbors based on locality sensitive hashing forests and
random decision forests. Additionally, we conduct experiments with gradient
boosted trees in an offline setting using XGBoost. We compare the relative
performance of Tactician using these three learning methods on Coq's standard
library."
"Automatic ICD coding is the task of assigning codes from the International
Classification of Diseases (ICD) to medical notes. These codes describe the
state of the patient and have multiple applications, e.g., computer-assisted
diagnosis or epidemiological studies. ICD coding is a challenging task due to
the complexity and length of medical notes. Unlike the general trend in
language processing, no transformer model has been reported to reach high
performance on this task. Here, we investigate in detail ICD coding using
PubMedBERT, a state-of-the-art transformer model for biomedical language
understanding. We find that the difficulty of fine-tuning the model on long
pieces of text is the main limitation for BERT-based models on ICD coding. We
run extensive experiments and show that despite the gap with current
state-of-the-art, pretrained transformers can reach competitive performance
using relatively small portions of text. We point at better methods to
aggregate information from long texts as the main need for improving BERT-based
ICD coding."
"We propose TalkNet, a non-autoregressive convolutional neural model for
speech synthesis with explicit pitch and duration prediction. The model
consists of three feed-forward convolutional networks. The first network
predicts grapheme durations. An input text is expanded by repeating each symbol
according to the predicted duration. The second network predicts pitch value
for every mel frame. The third network generates a mel-spectrogram from the
expanded text conditioned on predicted pitch. All networks are based on 1D
depth-wise separable convolutional architecture. The explicit duration
prediction eliminates word skipping and repeating. The quality of the generated
speech nearly matches the best auto-regressive models - TalkNet trained on the
LJSpeech dataset got MOS 4.08. The model has only 13.2M parameters, almost 2x
less than the present state-of-the-art text-to-speech models. The
non-autoregressive architecture allows for fast training and inference. The
small model size and fast inference make the TalkNet an attractive candidate
for embedded speech synthesis."
"In this study, we address the problem of chaotic synchronization over a noisy
channel by introducing a novel Deep Chaos Synchronization (DCS) system using a
Convolutional Neural Network (CNN). Conventional Deep Learning (DL) based
communication strategies are extremely powerful but training on large data sets
is usually a difficult and time-consuming procedure. To tackle this challenge,
DCS does not require prior information or large data sets. In addition, we
provide a novel Recurrent Neural Network (RNN)-based chaotic synchronization
system for comparative analysis. The results show that the proposed DCS
architecture is competitive with RNN-based synchronization in terms of
robustness against noise, convergence, and training. Hence, with these
features, the DCS scheme will open the door for a new class of modulator
schemes and meet the robustness against noise, convergence, and training
requirements of the Ultra Reliable Low Latency Communications (URLLC) and
Industrial Internet of Things (IIoT)."
"Diagrams are often used in scholarly communication. We analyse a corpus of
diagrams found in scholarly computational linguistics conference proceedings
(ACL 2017), and find inclusion of a system diagram to be correlated with higher
numbers of citations after 3 years. Inclusion of over three diagrams in this
8-page limit conference was found to correlate with a lower citation count.
Focusing on neural network system diagrams, we find a correlation between
highly cited papers and ""good diagramming practice"" quantified by level of
compliance with a set of diagramming guidelines. Two diagram classification
types (one visually based, one mental model based) were not found to correlate
with number of citations, but enabled quantification of heterogeneity in those
dimensions. Exploring scholarly paper-writing guides, we find diagrams to be a
neglected media. This study suggests that diagrams may be a useful source of
quality data for predicting citations, and that ""graphicacy"" is a key skill for
scholars with insufficient support at present."
"Text classification is a primary task in natural language processing (NLP).
Recently, graph neural networks (GNNs) have developed rapidly and been applied
to text classification tasks. As a special kind of graph data, the tree has a
simpler data structure and can provide rich hierarchical information for text
classification. Inspired by the structural entropy, we construct the coding
tree of the graph by minimizing the structural entropy and propose HINT, which
aims to make full use of the hierarchical information contained in the text for
the task of text classification. Specifically, we first establish a dependency
parsing graph for each text. Then we designed a structural entropy minimization
algorithm to decode the key information in the graph and convert each graph to
its corresponding coding tree. Based on the hierarchical structure of the
coding tree, the representation of the entire graph is obtained by updating the
representation of non-leaf nodes in the coding tree layer by layer. Finally, we
present the effectiveness of hierarchical information in text classification.
Experimental results show that HINT outperforms the state-of-the-art methods on
popular benchmarks while having a simple structure and few parameters."
"Reinforcement learning (RL) is an effective framework for solving sequential
decision-making tasks. However, applying RL methods in medical care settings is
challenging in part due to heterogeneity in treatment response among patients.
Some patients can be treated with standard protocols whereas others, such as
those with chronic diseases, need personalized treatment planning. Traditional
RL methods often fail to account for this heterogeneity, because they assume
that all patients respond to the treatment in the same way (i.e., transition
dynamics are shared). We introduce Compositional Fitted $Q$-iteration (CFQI),
which uses a compositional task structure to represent heterogeneous treatment
responses in medical care settings. A compositional task consists of several
variations of the same task, each progressing in difficulty; solving simpler
variants of the task can enable efficient solving of harder variants. CFQI uses
a compositional $Q$-value function with separate modules for each task variant,
allowing it to take advantage of shared knowledge while learning distinct
policies for each variant. We validate CFQI's performance using a Cartpole
environment and use CFQI to recommend electrolyte repletion for patients with
and without renal disease. Our results demonstrate that CFQI is robust even in
the presence of class imbalance, enabling effective information usage across
patient sub-populations. CFQI exhibits great promise for clinical applications
in scenarios characterized by known compositional structures."
"Transforming large deep neural network (DNN) models into the multi-exit
architectures can overcome the overthinking issue and distribute a large DNN
model on resource-constrained scenarios (e.g. IoT frontend devices and backend
servers) for inference and transmission efficiency. Nevertheless, intellectual
property (IP) protection for the multi-exit models in the wild is still an
unsolved challenge. Previous efforts to verify DNN model ownership mainly rely
on querying the model with specific samples and checking the responses, e.g.,
DNN watermarking and fingerprinting. However, they are vulnerable to
adversarial settings such as adversarial training and are not suitable for the
IP verification for multi-exit DNN models. In this paper, we propose a novel
approach to fingerprint multi-exit models via inference time rather than
inference predictions. Specifically, we design an effective method to generate
a set of fingerprint samples to craft the inference process with a unique and
robust inference time cost as the evidence for model ownership. We conduct
extensive experiments to prove the uniqueness and robustness of our method on
three structures (ResNet-56, VGG-16, and MobileNet) and three datasets
(CIFAR-10, CIFAR-100, and Tiny-ImageNet) under comprehensive adversarial
settings."
"In temporal Knowledge Graphs (tKGs), the temporal dimension is attached to
facts in a knowledge base resulting in quadruples between entities such as
(Nintendo, released, Super Mario, Sep-13-1985), where the predicate holds
within a time interval or at a timestamp. We propose a reinforcement learning
agent gathering temporal relevant information about the query entities'
neighborhoods, simultaneously. We refer to the encodings of the explored graph
structures as fingerprints which are used as input to a Q-network. Our agent
decides sequentially which relation type needs to be explored next to expand
the local subgraphs of the query entities. Our evaluation shows that the
proposed method yields competitive results compared to state-of-the-art
embedding algorithms for tKGs, and we additionally gain information about the
relevant structures between subjects and objects."
"Humans are universal decision makers: we reason causally to understand the
world; we act competitively to gain advantage in commerce, games, and war; and
we are able to learn to make better decisions through trial and error. In this
paper, we propose Universal Decision Model (UDM), a mathematical formalism
based on category theory. Decision objects in a UDM correspond to instances of
decision tasks, ranging from causal models and dynamical systems such as Markov
decision processes and predictive state representations, to network multiplayer
games and Witsenhausen's intrinsic models, which generalizes all these previous
formalisms. A UDM is a category of objects, which include decision objects,
observation objects, and solution objects. Bisimulation morphisms map between
decision objects that capture structure-preserving abstractions. We formulate
universal properties of UDMs, including information integration, decision
solvability, and hierarchical abstraction. We describe universal functorial
representations of UDMs, and propose an algorithm for computing the minimal
object in a UDM using algebraic topology. We sketch out an application of UDMs
to causal inference in network economics, using a complex multiplayer
producer-consumer two-sided marketplace."
"To alleviate the practical constraints for deploying deep neural networks
(DNNs) on edge devices, quantization is widely regarded as one promising
technique. It reduces the resource requirements for computational power and
storage space by quantizing the weights and/or activation tensors of a DNN into
lower bit-width fixed-point numbers, resulting in quantized neural networks
(QNNs). While it has been empirically shown to introduce minor accuracy loss,
critical verified properties of a DNN might become invalid once quantized.
Existing verification methods focus on either individual neural networks (DNNs
or QNNs) or quantization error bound for partial quantization. In this work, we
propose a quantization error bound verification method, named QEBVerif, where
both weights and activation tensors are quantized. QEBVerif consists of two
parts, i.e., a differential reachability analysis (DRA) and a mixed-integer
linear programming (MILP) based verification method. DRA performs difference
analysis between the DNN and its quantized counterpart layer-by-layer to
compute a tight quantization error interval efficiently. If DRA fails to prove
the error bound, then we encode the verification problem into an equivalent
MILP problem which can be solved by off-the-shelf solvers. Thus, QEBVerif is
sound, complete, and reasonably efficient. We implement QEBVerif and conduct
extensive experiments, showing its effectiveness and efficiency."
"For humans and computers, the first step in answering an open-domain question
is retrieving a set of relevant documents from a large corpus. However, the
strategies that computers use fundamentally differ from those of humans. To
better understand these differences, we design a gamified interface for data
collection -- Cheater's Bowl -- where a human answers complex questions with
access to both traditional and modern search tools. We collect a dataset of
human search sessions, analyze human search strategies, and compare them to
state-of-the-art multi-hop QA models. Humans query logically, apply dynamic
search chains, and use world knowledge to boost searching. We demonstrate how
human queries can improve the accuracy of existing systems and propose
improving the future design of QA models."
"In many domains, the exploration process of reinforcement learning will be
too costly as it requires trying out suboptimal policies, resulting in a need
for off-policy evaluation, in which a target policy is evaluated based on data
collected from a known behaviour policy. In this context, importance sampling
estimators provide estimates for the expected return by weighting the
trajectory based on the probability ratio of the target policy and the
behaviour policy. Unfortunately, such estimators have a high variance and
therefore a large mean squared error. This paper proposes state-based
importance sampling estimators which reduce the variance by dropping certain
states from the computation of the importance weight. To illustrate their
applicability, we demonstrate state-based variants of ordinary importance
sampling, weighted importance sampling, per-decision importance sampling,
incremental importance sampling, doubly robust off-policy evaluation, and
stationary density ratio estimation. Experiments in four domains show that
state-based methods consistently yield reduced variance and improved accuracy
compared to their traditional counterparts."
"The development of deep learning models in medical image analysis is majorly
limited by the lack of large-sized and well-annotated datasets. Unsupervised
learning does not require labels and is more suitable for solving medical image
analysis problems. However, most of the current unsupervised learning methods
need to be applied to large datasets. To make unsupervised learning applicable
to small datasets, we proposed Swin MAE, which is a masked autoencoder with
Swin Transformer as its backbone. Even on a dataset of only a few thousand
medical images and without using any pre-trained models, Swin MAE is still able
to learn useful semantic features purely from images. It can equal or even
slightly outperform the supervised model obtained by Swin Transformer trained
on ImageNet in terms of the transfer learning results of downstream tasks. The
code is publicly available at https://github.com/Zian-Xu/Swin-MAE."
"Joint attention is a core, early-developing form of social interaction. It is
based on our ability to discriminate the third party objects that other people
are looking at. While it has been shown that people can accurately determine
whether another person is looking directly at them versus away, little is known
about human ability to discriminate a third person gaze directed towards
objects that are further away, especially in unconstraint cases where the
looker can move her head and eyes freely. In this paper we address this
question by jointly exploring human psychophysics and a cognitively motivated
computer vision model, which can detect the 3D direction of gaze from 2D face
images. The synthesis of behavioral study and computer vision yields several
interesting discoveries. (1) Human accuracy of discriminating targets
8{\deg}-10{\deg} of visual angle apart is around 40% in a free looking gaze
task; (2) The ability to interpret gaze of different lookers vary dramatically;
(3) This variance can be captured by the computational model; (4) Human
outperforms the current model significantly. These results collectively show
that the acuity of human joint attention is indeed highly impressive, given the
computational challenge of the natural looking task. Moreover, the gap between
human and model performance, as well as the variability of gaze interpretation
across different lookers, require further understanding of the underlying
mechanisms utilized by humans for this challenging task."
"We present a method for the classification of multi-labelled text documents
explicitly designed for data stream applications that require to process a
virtually infinite sequence of data using constant memory and constant
processing time. Our method is composed of an online procedure used to
efficiently map text into a low-dimensional feature space and a partition of
this space into a set of regions for which the system extracts and keeps
statistics used to predict multi-label text annotations. Documents are fed into
the system as a sequence of words, mapped to a region of the partition, and
annotated using the statistics computed from the labelled instances colliding
in the same region. This approach is referred to as clashing. We illustrate the
method in real-world text data, comparing the results with those obtained using
other text classifiers. In addition, we provide an analysis about the effect of
the representation space dimensionality on the predictive performance of the
system. Our results show that the online embedding indeed approximates the
geometry of the full corpus-wise TF and TF-IDF space. The model obtains
competitive F measures with respect to the most accurate methods, using
significantly fewer computational resources. In addition, the method achieves a
higher macro-averaged F measure than methods with similar running time.
Furthermore, the system is able to learn faster than the other methods from
partially labelled streams."
"Neural program embeddings have shown much promise recently for a variety of
program analysis tasks, including program synthesis, program repair, fault
localization, etc. However, most existing program embeddings are based on
syntactic features of programs, such as raw token sequences or abstract syntax
trees. Unlike images and text, a program has an unambiguous semantic meaning
that can be difficult to capture by only considering its syntax (i.e.
syntactically similar pro- grams can exhibit vastly different run-time
behavior), which makes syntax-based program embeddings fundamentally limited.
This paper proposes a novel semantic program embedding that is learned from
program execution traces. Our key insight is that program states expressed as
sequential tuples of live variable values not only captures program semantics
more precisely, but also offer a more natural fit for Recurrent Neural Networks
to model. We evaluate different syntactic and semantic program embeddings on
predicting the types of errors that students make in their submissions to an
introductory programming class and two exercises on the CodeHunt education
platform. Evaluation results show that our new semantic program embedding
significantly outperforms the syntactic program embeddings based on token
sequences and abstract syntax trees. In addition, we augment a search-based
program repair system with the predictions obtained from our se- mantic
embedding, and show that search efficiency is also significantly improved."
"Natural language programming is a promising approach to enable end users to
instruct new tasks for intelligent agents. However, our formative study found
that end users would often use unclear, ambiguous or vague concepts when
naturally instructing tasks in natural language, especially when specifying
conditionals. Existing systems have limited support for letting the user teach
agents new concepts or explaining unclear concepts. In this paper, we describe
a new multi-modal domain-independent approach that combines natural language
programming and programming-by-demonstration to allow users to first naturally
describe tasks and associated conditions at a high level, and then collaborate
with the agent to recursively resolve any ambiguities or vagueness through
conversations and demonstrations. Users can also define new procedures and
concepts by demonstrating and referring to contents within GUIs of existing
mobile apps. We demonstrate this approach in PUMICE, an end-user programmable
agent that implements this approach. A lab study with 10 users showed its
usability."
"Several real-world applications could be modeled as Mixed-Integer Non-Linear
Programming (MINLP) problems, and some prominent examples include portfolio
optimization, remote sensing technology, and so on. Most of the models for
these applications are non-convex and always involve some conflicting
objectives. The mathematical and heuristic methods have their advantages in
solving this category of problems. In this work, we turn to Multi-Objective
Evolutionary Algorithms (MOEAs) for finding elegant solutions for such
problems. In this framework, we investigate a multi-objective constrained
portfolio optimization problem, which can be cast as a classical financial
problem and can also be naturally modeled as an MINLP problem. Consequently, we
point out one challenge, faced by a direct coding scheme for MOEAs, to this
problem. It is that the dependence among variables, like the selection and
weights for one same asset, will likely make the search difficult. We thus,
propose a Compressed Coding Scheme (CCS), compressing the two dependent
variables into one variable to utilize the dependence and thereby meeting this
challenge. Subsequently, we carry out a detailed empirical study on two sets of
instances. The first part consists of 5 instances from OR-Library, which is
solvable for the general mathematical optimizer, like CPLEX, while the
remaining 15 instances from NGINX are addressed only by MOEAs. The two
benchmarks, involving the number of assets from 31 to 2235, consistently
indicate that CCS is not only efficient but also robust for dealing with the
constrained multi-objective portfolio optimization."
"Learning a good representation is an essential component for deep
reinforcement learning (RL). Representation learning is especially important in
multitask and partially observable settings where building a representation of
the unknown environment is crucial to solve the tasks. Here we introduce
Prediction of Bootstrap Latents (PBL), a simple and flexible self-supervised
representation learning algorithm for multitask deep RL. PBL builds on
multistep predictive representations of future observations, and focuses on
capturing structured information about environment dynamics. Specifically, PBL
trains its representation by predicting latent embeddings of future
observations. These latent embeddings are themselves trained to be predictive
of the aforementioned representations. These predictions form a bootstrapping
effect, allowing the agent to learn more about the key aspects of the
environment dynamics. In addition, by defining prediction tasks completely in
latent space, PBL provides the flexibility of using multimodal observations
involving pixel images, language instructions, rewards and more. We show in our
experiments that PBL delivers across-the-board improved performance over state
of the art deep RL agents in the DMLab-30 and Atari-57 multitask setting."
"Question answering (QA) tasks have been posed using a variety of formats,
such as extractive span selection, multiple choice, etc. This has led to
format-specialized models, and even to an implicit division in the QA
community. We argue that such boundaries are artificial and perhaps
unnecessary, given the reasoning abilities we seek to teach are not governed by
the format. As evidence, we use the latest advances in language modeling to
build a single pre-trained QA model, UnifiedQA, that performs surprisingly well
across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par
with 9 different models that were trained on individual datasets themselves.
Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs
surprisingly well, showing strong generalization from its out-of-format
training data. Finally, simply fine-tuning this pre-trained QA model into
specialized models results in a new state of the art on 6 datasets,
establishing UnifiedQA as a strong starting point for building QA systems."
"In this paper, we~present a novel scheduling solution for a class of
System-on-Chip (SoC) systems where heterogeneous chip resources (DSP, FPGA,
GPU, etc.) must be efficiently scheduled for continuously arriving hierarchical
jobs with their tasks represented by a directed acyclic graph. Traditionally,
heuristic algorithms have been widely used for many resource scheduling
domains, and Heterogeneous Earliest Finish Time (HEFT) has been a dominating
state-of-the-art technique across a broad range of heterogeneous resource
scheduling domains over many years. Despite their long-standing popularity,
HEFT-like algorithms are known to be vulnerable to a small amount of noise
added to the environment. Our Deep Reinforcement Learning (DRL)-based SoC
Scheduler (DeepSoCS), capable of learning the ""best"" task ordering under
dynamic environment changes, overcomes the brittleness of rule-based schedulers
such as HEFT with significantly higher performance across different types of
jobs. We~describe a DeepSoCS design process using a real-time heterogeneous SoC
scheduling emulator, discuss major challenges, and present two novel neural
network design features that lead to outperforming HEFT: (i) hierarchical job-
and task-graph embedding; and (ii) efficient use of real-time task information
in the state space. Furthermore, we~introduce effective techniques to address
two fundamental challenges present in our environment: delayed consequences and
joint actions. Through an extensive simulation study, we~show that our DeepSoCS
exhibits the significantly higher performance of job execution time than that
of HEFT with a higher level of robustness under realistic noise conditions.
We~conclude with a discussion of the potential improvements for our DeepSoCS
neural scheduler."
"Traceability is a fundamental component of the modern software development
process that helps to ensure properly functioning, secure programs. Due to the
high cost of manually establishing trace links, researchers have developed
automated approaches that draw relationships between pairs of textual software
artifacts using similarity measures. However, the effectiveness of such
techniques are often limited as they only utilize a single measure of artifact
similarity and cannot simultaneously model (implicit and explicit)
relationships across groups of diverse development artifacts.
  In this paper, we illustrate how these limitations can be overcome through
the use of a tailored probabilistic model. To this end, we design and implement
a HierarchiCal PrObabilistic Model for SoftwarE Traceability (Comet) that is
able to infer candidate trace links. Comet is capable of modeling relationships
between artifacts by combining the complementary observational prowess of
multiple measures of textual similarity. Additionally, our model can
holistically incorporate information from a diverse set of sources, including
developer feedback and transitive (often implicit) relationships among groups
of software artifacts, to improve inference accuracy. We conduct a
comprehensive empirical evaluation of Comet that illustrates an improvement
over a set of optimally configured baselines of $\approx$14% in the best case
and $\approx$5% across all subjects in terms of average precision. The
comparative effectiveness of Comet in practice, where optimal configuration is
typically not possible, is likely to be higher. Finally, we illustrate Comets
potential for practical applicability in a survey with developers from Cisco
Systems who used a prototype Comet Jenkins plugin."
"Various neural-based methods have been proposed so far for joint mention
detection and coreference resolution. However, existing works on coreference
resolution are mainly dependent on filtered mention representation, while other
spans are largely neglected. In this paper, we aim at increasing the
utilization rate of data and investigating whether those eliminated spans are
totally useless, or to what extent they can improve the performance of
coreference resolution. To achieve this, we propose a mention representation
refining strategy where spans highly related to mentions are well leveraged
using a pointer network for representation enhancing. Notably, we utilize an
additional loss term in this work to encourage the diversity between entity
clusters. Experimental results on the document-level CoNLL-2012 Shared Task
English dataset show that eliminated spans are indeed much effective and our
approach can achieve competitive results when compared with previous
state-of-the-art in coreference resolution."
"Learning a robot motor skill from scratch is impractically slow; so much so
that in practice, learning must be bootstrapped using a good skill policy
obtained from human demonstration. However, relying on human demonstration
necessarily degrades the autonomy of robots that must learn a wide variety of
skills over their operational lifetimes. We propose using kinematic motion
planning as a completely autonomous, sample efficient way to bootstrap motor
skill learning for object manipulation. We demonstrate the use of motion
planners to bootstrap motor skills in two complex object manipulation scenarios
with different policy representations: opening a drawer with a dynamic movement
primitive representation, and closing a microwave door with a deep neural
network policy. We also show how our method can bootstrap a motor skill for the
challenging dynamic task of learning to hit a ball off a tee, where a kinematic
plan based on treating the scene as static is insufficient to solve the task,
but sufficient to bootstrap a more dynamic policy. In all three cases, our
method is competitive with human-demonstrated initialization, and significantly
outperforms starting with a random policy. This approach enables robots to to
efficiently and autonomously learn motor policies for dynamic tasks without
human demonstration."
"In today's world, Neural Style Transfer (NST) has become a trendsetting term.
NST combines two pictures, a content picture and a reference image in style
(such as the work of a renowned painter) in a way that makes the output image
look like an image of the material, but rendered with the form of a reference
picture. However, there is no study using the artwork or painting of
Bangladeshi painters. Bangladeshi painting has a long history of more than two
thousand years and is still being practiced by Bangladeshi painters. This study
generates NST stylized image on Bangladeshi paintings and analyzes the human
point of view regarding the aesthetic preference of NST on Bangladeshi
paintings. To assure our study's acceptance, we performed qualitative human
evaluations on generated stylized images by 60 individual humans of different
age and gender groups. We have explained how NST works for Bangladeshi
paintings and assess NST algorithms, both qualitatively \& quantitatively. Our
study acts as a pre-requisite for the impact of NST stylized image using
Bangladeshi paintings on mobile UI/GUI and material translation from the human
perspective. We hope that this study will encourage new collaborations to
create more NST related studies and expand the use of Bangladeshi artworks."
"While often assumed a gold standard, effective human evaluation of text
generation remains an important, open area for research. We revisit this
problem with a focus on producing consistent evaluations that are reproducible
-- over time and across different populations. We study this goal in different
stages of the human evaluation pipeline. In particular, we consider design
choices for the annotation interface used to elicit human judgments and their
impact on reproducibility. Furthermore, we develop an automated mechanism for
maintaining annotator quality via a probabilistic model that detects and
excludes noisy annotators. Putting these lessons together, we introduce GENIE:
a system for running standardized human evaluations across different generation
tasks. We instantiate GENIE with datasets representing four core challenges in
text generation: machine translation, summarization, commonsense reasoning, and
machine comprehension. For each task, GENIE offers a leaderboard that
automatically crowdsources annotations for submissions, evaluating them along
axes such as correctness, conciseness, and fluency. We have made the GENIE
leaderboards publicly available, and have already ranked 50 submissions from 10
different research groups. We hope GENIE encourages further progress toward
effective, standardized evaluations for text generation."
"Explicitly alerting users is not always an optimal intervention, especially
when they are not motivated to obey. For example, in video-based learning,
learners who are distracted from the video would not follow an alert asking
them to pay attention. Inspired by the concept of Mindless Computing, we
propose a novel intervention approach, Mindless Attractor, that leverages the
nature of human speech communication to help learners refocus their attention
without relying on their motivation. Specifically, it perturbs the voice in the
video to direct their attention without consuming their conscious awareness.
Our experiments not only confirmed the validity of the proposed approach but
also emphasized its advantages in combination with a machine learning-based
sensing module. Namely, it would not frustrate users even though the
intervention is activated by false-positive detection of their attentive state.
Our intervention approach can be a reliable way to induce behavioral change in
human-AI symbiosis."
"The optimization of submodular functions constitutes a viable way to perform
clustering. Strong approximation guarantees and feasible optimization w.r.t.
streaming data make this clustering approach favorable. Technically, submodular
functions map subsets of data to real values, which indicate how
""representative"" a specific subset is. Optimal sets might then be used to
partition the data space and to infer clusters. Exemplar-based clustering is
one of the possible submodular functions, but suffers from high computational
complexity. However, for practical applications, the particular real-time or
wall-clock run-time is decisive. In this work, we present a novel way to
evaluate this particular function on GPUs, which keeps the necessities of
optimizers in mind and reduces wall-clock run-time. To discuss our GPU
algorithm, we investigated both the impact of different run-time critical
problem properties, like data dimensionality and the number of data points in a
subset, and the influence of required floating-point precision. In reproducible
experiments, our GPU algorithm was able to achieve competitive speedups of up
to 72x depending on whether multi-threaded computation on CPUs was used for
comparison and the type of floating-point precision required. Half-precision
GPU computation led to large speedups of up to 452x compared to
single-precision, single-thread CPU computations."
"Data is evolving with the rapid progress of population and communication for
various types of devices such as networks, cloud computing, Internet of Things
(IoT), actuators, and sensors. The increment of data and communication content
goes with the equivalence of velocity, speed, size, and value to provide the
useful and meaningful knowledge that helps to solve the future challenging
tasks and latest issues. Besides, multicriteria based decision making is one of
the key issues to solve for various issues related to the alternative effects
in big data analysis. It tends to find a solution based on the latest machine
learning techniques that include algorithms like decision making and deep
learning mechanism based on multicriteria in providing insights to big data. On
the other hand, the derivations are made for it to go with the approximations
to increase the duality of runtime and improve the entire system's potentiality
and efficacy. In essence, several fields, including business, agriculture,
information technology, and computer science, use deep learning and
multicriteria-based decision-making problems. This paper aims to provide
various applications that involve the concepts of deep learning techniques and
exploiting the multicriteria approaches for issues that are facing in big data
analytics by proposing new studies with the fusion approaches of data-driven
techniques."
"Cyber-defense systems are being developed to automatically ingest Cyber
Threat Intelligence (CTI) that contains semi-structured data and/or text to
populate knowledge graphs. A potential risk is that fake CTI can be generated
and spread through Open-Source Intelligence (OSINT) communities or on the Web
to effect a data poisoning attack on these systems. Adversaries can use fake
CTI examples as training input to subvert cyber defense systems, forcing the
model to learn incorrect inputs to serve their malicious needs.
  In this paper, we automatically generate fake CTI text descriptions using
transformers. We show that given an initial prompt sentence, a public language
model like GPT-2 with fine-tuning, can generate plausible CTI text with the
ability of corrupting cyber-defense systems. We utilize the generated fake CTI
text to perform a data poisoning attack on a Cybersecurity Knowledge Graph
(CKG) and a cybersecurity corpus. The poisoning attack introduced adverse
impacts such as returning incorrect reasoning outputs, representation
poisoning, and corruption of other dependent AI-based cyber defense systems. We
evaluate with traditional approaches and conduct a human evaluation study with
cybersecurity professionals and threat hunters. Based on the study,
professional threat hunters were equally likely to consider our fake generated
CTI as true."
"Efficiently solving problems with large action spaces using A* search has
been of importance to the artificial intelligence community for decades. This
is because the computation and memory requirements of A* search grow linearly
with the size of the action space. This burden becomes even more apparent when
A* search uses a heuristic function learned by computationally expensive
function approximators, such as deep neural networks. To address this problem,
we introduce Q* search, a search algorithm that uses deep Q-networks to guide
search in order to take advantage of the fact that the sum of the transition
costs and heuristic values of the children of a node can be computed with a
single forward pass through a deep Q-network without explicitly generating
those children. This significantly reduces computation time and requires only
one node to be generated per iteration. We use Q* search to solve the Rubik's
cube when formulated with a large action space that includes 1872 meta-actions
and find that this 157-fold increase in the size of the action space incurs
less than a 4-fold increase in computation time and less than a 3-fold increase
in number of nodes generated when performing Q* search. Furthermore, Q* search
is up to 129 times faster and generates up to 1288 times fewer nodes than A*
search. Finally, although obtaining admissible heuristic functions from deep
neural networks is an ongoing area of research, we prove that Q* search is
guaranteed to find a shortest path given a heuristic function that neither
overestimates the cost of a shortest path nor underestimates the transition
cost."
"Deep learning is revolutionizing predictive healthcare, including
recommending medications to patients with complex health conditions. Existing
approaches focus on predicting all medications for the current visit, which
often overlaps with medications from previous visits. A more clinically
relevant task is to identify medication changes.
  In this paper, we propose a new recurrent residual network, named MICRON, for
medication change prediction. MICRON takes the changes in patient health
records as input and learns to update a hidden medication vector and the
medication set recurrently with a reconstruction design. The medication vector
is like the memory cell that encodes longitudinal information of medications.
Unlike traditional methods that require the entire patient history for
prediction, MICRON has a residual-based inference that allows for sequential
updating based only on new patient features (e.g., new diagnoses in the recent
visit) more efficiently.
  We evaluated MICRON on real inpatient and outpatient datasets. MICRON
achieves 3.5% and 7.8% relative improvements over the best baseline in F1
score, respectively. MICRON also requires fewer parameters, which significantly
reduces the training time to 38.3s per epoch with 1.5x speed-up."
"Recent technological advances, especially in the field of machine learning,
provide astonishing progress on the road towards artificial general
intelligence. However, tasks in current real-world business applications cannot
yet be solved by machines alone. We, therefore, identify the need for
developing socio-technological ensembles of humans and machines. Such systems
possess the ability to accomplish complex goals by combining human and
artificial intelligence to collectively achieve superior results and
continuously improve by learning from each other. Thus, the need for structured
design knowledge for those systems arises. Following a taxonomy development
method, this article provides three main contributions: First, we present a
structured overview of interdisciplinary research on the role of humans in the
machine learning pipeline. Second, we envision hybrid intelligence systems and
conceptualize the relevant dimensions for system design for the first time.
Finally, we offer useful guidance for system developers during the
implementation of such applications."
"In this research article, we have reported periodic cellular automata rules
for different gait state prediction and classification of the gait data using
extreme machine Leaning (ELM). This research is the first attempt to use
cellular automaton to understand the complexity of bipedal walk. Due to
nonlinearity, varying configurations throughout the gait cycle and the passive
joint located at the unilateral foot-ground contact in bipedal walk resulting
variation of dynamic descriptions and control laws from phase to phase for
human gait is making difficult to predict the bipedal walk states. We have
designed the cellular automata rules which will predict the next gait state of
bipedal steps based on the previous two neighbour states. We have designed
cellular automata rules for normal walk. The state prediction will help to
correctly design the bipedal walk. The normal walk depends on next two states
and has total 8 states. We have considered the current and previous states to
predict next state. So we have formulated 16 rules using cellular automata, 8
rules for each leg. The priority order maintained using the fact that if right
leg in swing phase then left leg will be in stance phase. To validate the model
we have classified the gait Data using ELM [1] and achieved accuracy 60%. We
have explored the trajectories and compares with another gait trajectories.
Finally we have presented the error analysis for different joints."
"Memristor crossbar arrays are used in a wide range of in-memory and
neuromorphic computing applications. However, memristor devices suffer from
non-idealities that result in the variability of conductive states, making
programming them to a desired analog conductance value extremely difficult as
the device ages. In theory, memristors can be a nonlinear programmable analog
resistor with memory properties that can take infinite resistive states. In
practice, such memristors are hard to make, and in a crossbar, it is confined
to a limited set of stable conductance values. The number of conductance levels
available for a node in the crossbar is defined as the crossbar's resolution.
This paper presents a technique to improve the resolution by building a
super-resolution memristor crossbar with nodes having multiple memristors to
generate r-simplicial sequence of unique conductance values. The wider the
range and number of conductance values, the higher the crossbar's resolution.
This is particularly useful in building analog neural network (ANN) layers,
which are proven to be one of the go-to approaches for forming a neural network
layer in implementing neuromorphic computations."
"Graph self-supervised learning has gained increasing attention due to its
capacity to learn expressive node representations. Many pretext tasks, or loss
functions have been designed from distinct perspectives. However, we observe
that different pretext tasks affect downstream tasks differently cross
datasets, which suggests that searching pretext tasks is crucial for graph
self-supervised learning. Different from existing works focusing on designing
single pretext tasks, this work aims to investigate how to automatically
leverage multiple pretext tasks effectively. Nevertheless, evaluating
representations derived from multiple pretext tasks without direct access to
ground truth labels makes this problem challenging. To address this obstacle,
we make use of a key principle of many real-world graphs, i.e., homophily, or
the principle that ""like attracts like,"" as the guidance to effectively search
various self-supervised pretext tasks. We provide theoretical understanding and
empirical evidence to justify the flexibility of homophily in this search task.
Then we propose the AutoSSL framework which can automatically search over
combinations of various self-supervised tasks. By evaluating the framework on 7
real-world datasets, our experimental results show that AutoSSL can
significantly boost the performance on downstream tasks including node
clustering and node classification compared with training under individual
tasks. Code is released at https://github.com/ChandlerBang/AutoSSL."
"Hand-annotated data can vary due to factors such as subjective differences,
intra-rater variability, and differing annotator expertise. We study
annotations from different experts who labelled the same behavior classes on a
set of animal behavior videos, and observe a variation in annotation styles. We
propose a new method using program synthesis to help interpret annotation
differences for behavior analysis. Our model selects relevant trajectory
features and learns a temporal filter as part of a program, which corresponds
to estimated importance an annotator places on that feature at each timestamp.
Our experiments on a dataset from behavioral neuroscience demonstrate that
compared to baseline approaches, our method is more accurate at capturing
annotator labels and learns interpretable temporal filters. We believe that our
method can lead to greater reproducibility of behavior annotations used in
scientific studies. We plan to release our code."
"Network Traffic Classification (NTC) has become an important feature in
various network management operations, e.g., Quality of Service (QoS)
provisioning and security services. Machine Learning (ML) algorithms as a
popular approach for NTC can promise reasonable accuracy in classification and
deal with encrypted traffic. However, ML-based NTC techniques suffer from the
shortage of labeled traffic data which is the case in many real-world
applications. This study investigates the applicability of an active form of
ML, called Active Learning (AL), in NTC. AL reduces the need for a large number
of labeled examples by actively choosing the instances that should be labeled.
The study first provides an overview of NTC and its fundamental challenges
along with surveying the literature on ML-based NTC methods. Then, it
introduces the concepts of AL, discusses it in the context of NTC, and review
the literature in this field. Further, challenges and open issues in AL-based
classification of network traffic are discussed. Moreover, as a technical
survey, some experiments are conducted to show the broad applicability of AL in
NTC. The simulation results show that AL can achieve high accuracy with a small
amount of data."
"A central goal of machine learning is to learn robust representations that
capture the causal relationship between inputs features and output labels.
However, minimizing empirical risk over finite or biased datasets often results
in models latching on to spurious correlations between the training
input/output pairs that are not fundamental to the problem at hand. In this
paper, we define and analyze robust and spurious representations using the
information-theoretic concept of minimal sufficient statistics. We prove that
even when there is only bias of the input distribution (i.e. covariate shift),
models can still pick up spurious features from their training data. Group
distributionally robust optimization (DRO) provides an effective tool to
alleviate covariate shift by minimizing the worst-case training loss over a set
of pre-defined groups. Inspired by our analysis, we demonstrate that group DRO
can fail when groups do not directly account for various spurious correlations
that occur in the data. To address this, we further propose to minimize the
worst-case losses over a more flexible set of distributions that are defined on
the joint distribution of groups and instances, instead of treating each group
as a whole at optimization time. Through extensive experiments on one image and
two language tasks, we show that our model is significantly more robust than
comparable baselines under various partitions. Our code is available at
https://github.com/violet-zct/group-conditional-DRO."
"In this paper, we investigate few-shot joint learning for dialogue language
understanding. Most existing few-shot models learn a single task each time with
only a few examples. However, dialogue language understanding contains two
closely related tasks, i.e., intent detection and slot filling, and often
benefits from jointly learning the two tasks. This calls for new few-shot
learning techniques that are able to capture task relations from only a few
examples and jointly learn multiple tasks. To achieve this, we propose a
similarity-based few-shot learning scheme, named Contrastive Prototype Merging
network (ConProm), that learns to bridge metric spaces of intent and slot on
data-rich domains, and then adapt the bridged metric space to the specific
few-shot domain. Experiments on two public datasets, Snips and FewJoint, show
that our model significantly outperforms the strong baselines in one and five
shots settings."
"For the task of conversation emotion recognition, recent works focus on
speaker relationship modeling but ignore the role of utterance's emotional
tendency.In this paper, we propose a new expression paradigm of sentence-level
emotion orientation vector to model the potential correlation of emotions
between sentence vectors. Based on it, we design an emotion recognition model,
which extracts the sentence-level emotion orientation vectors from the language
model and jointly learns from the dialogue sentiment analysis model and
extracted sentence-level emotion orientation vectors to identify the speaker's
emotional orientation during the conversation. We conduct experiments on two
benchmark datasets and compare them with the five baseline models.The
experimental results show that our model has better performance on all data
sets."
"The last decade has seen a significant increase of interest in deep learning
research, with many public successes that have demonstrated its potential. As
such, these systems are now being incorporated into commercial products. With
this comes an additional challenge: how can we build AI systems that solve
tasks where there is not a crisp, well-defined specification? While multiple
solutions have been proposed, in this competition we focus on one in
particular: learning from human feedback. Rather than training AI systems using
a predefined reward function or using a labeled dataset with a predefined set
of categories, we instead train the AI system using a learning signal derived
from some form of human feedback, which can evolve over time as the
understanding of the task changes, or as the capabilities of the AI system
improve.
  The MineRL BASALT competition aims to spur forward research on this important
class of techniques. We design a suite of four tasks in Minecraft for which we
expect it will be hard to write down hardcoded reward functions. These tasks
are defined by a paragraph of natural language: for example, ""create a
waterfall and take a scenic picture of it"", with additional clarifying details.
Participants must train a separate agent for each task, using any method they
want. Agents are then evaluated by humans who have read the task description.
To help participants get started, we provide a dataset of human demonstrations
on each of the four tasks, as well as an imitation learning baseline that
leverages these demonstrations.
  Our hope is that this competition will improve our ability to build AI
systems that do what their designers intend them to do, even when the intent
cannot be easily formalized. Besides allowing AI to solve more tasks, this can
also enable more effective regulation of AI systems, as well as making progress
on the value alignment problem."
"Strategies based on Explainable Artificial Intelligence - XAI have emerged in
computing to promote a better understanding of predictions made by black box
models. Most XAI measures used today explain these types of models, generating
attribute rankings aimed at explaining the model, that is, the analysis of
Attribute Importance of Model. There is no consensus on which XAI measure
generates an overall explainability rank. For this reason, several proposals
for tools have emerged (Ciu, Dalex, Eli5, Lofo, Shap and Skater). An
experimental benchmark of explainable AI techniques capable of producing global
explainability ranks based on tabular data related to different problems and
ensemble models are presented herein. Seeking to answer questions such as ""Are
the explanations generated by the different measures the same, similar or
different?"" and ""How does data complexity play along model explainability?"" The
results from the construction of 82 computational models and 592 ranks shed
some light on the other side of the problem of explainability: dataset
complexity!"
"The high-quality translation results produced by machine translation (MT)
systems still pose a huge challenge for automatic evaluation. Current MT
evaluation pays the same attention to each sentence component, while the
questions of real-world examinations (e.g., university examinations) have
different difficulties and weightings. In this paper, we propose a novel
difficulty-aware MT evaluation metric, expanding the evaluation dimension by
taking translation difficulty into consideration. A translation that fails to
be predicted by most MT systems will be treated as a difficult one and assigned
a large weight in the final score function, and conversely. Experimental
results on the WMT19 English-German Metrics shared tasks show that our proposed
method outperforms commonly used MT metrics in terms of human correlation. In
particular, our proposed method performs well even when all the MT systems are
very competitive, which is when most existing metrics fail to distinguish
between them. The source code is freely available at
https://github.com/NLP2CT/Difficulty-Aware-MT-Evaluation."
"Graph Neural Networks (GNNs) have already been widely applied in various
graph mining tasks. However, they suffer from the shallow architecture issue,
which is the key impediment that hinders the model performance improvement.
Although several relevant approaches have been proposed, none of the existing
studies provides an in-depth understanding of the root causes of performance
degradation in deep GNNs. In this paper, we conduct the first systematic
experimental evaluation to present the fundamental limitations of shallow
architectures. Based on the experimental results, we answer the following two
essential questions: (1) what actually leads to the compromised performance of
deep GNNs; (2) when we need and how to build deep GNNs. The answers to the
above questions provide empirical insights and guidelines for researchers to
design deep and well-performed GNNs. To show the effectiveness of our proposed
guidelines, we present Deep Graph Multi-Layer Perceptron (DGMLP), a powerful
approach (a paradigm in its own right) that helps guide deep GNN designs.
Experimental results demonstrate three advantages of DGMLP: 1) high accuracy --
it achieves state-of-the-art node classification performance on various
datasets; 2) high flexibility -- it can flexibly choose different propagation
and transformation depths according to graph size and sparsity; 3) high
scalability and efficiency -- it supports fast training on large-scale graphs.
Our code is available in https://github.com/zwt233/DGMLP."
"Deep learning models have recently demonstrated remarkable results in a
variety of tasks, which is why they are being increasingly applied in
high-stake domains, such as industry, medicine, and finance. Considering that
automatic predictions in these domains might have a substantial impact on the
well-being of a person, as well as considerable financial and legal
consequences to an individual or a company, all actions and decisions that
result from applying these models have to be accountable. Given that a
substantial amount of data that is collected in high-stake domains are in the
form of time series, in this paper we examine the current state of eXplainable
AI (XAI) methods with a focus on approaches for opening up deep learning black
boxes for the task of time series classification. Finally, our contribution
also aims at deriving promising directions for future work, to advance XAI for
deep learning on time series data."
"Manufacturing industries have widely adopted the reuse of machine parts as a
method to reduce costs and as a sustainable manufacturing practice.
Identification of reusable features from the design of the parts and finding
their similar features from the database is an important part of this process.
In this project, with the help of fully convolutional geometric features, we
are able to extract and learn the high level semantic features from CAD models
with inductive transfer learning. The extracted features are then compared with
that of other CAD models from the database using Frobenius norm and identical
features are retrieved. Later we passed the extracted features to a deep
convolutional neural network with a spatial pyramid pooling layer and the
performance of the feature retrieval increased significantly. It was evident
from the results that the model could effectively capture the geometrical
elements from machining features."
"Deep learning models are modern tools for spatio-temporal graph (STG)
forecasting. Though successful, we argue that data scarcity is a key factor
limiting their recent improvements. Meanwhile, contrastive learning has been an
effective method for providing self-supervision signals and addressing data
scarcity in various domains. In view of this, one may ask: can we leverage the
additional signals from contrastive learning to alleviate data scarcity, so as
to benefit STG forecasting? To answer this question, we present the first
systematic exploration on incorporating contrastive learning into STG
forecasting. Specifically, we first elaborate two potential schemes for
integrating contrastive learning. We then propose two feasible and efficient
designs of contrastive tasks that are performed on the node or graph level. The
empirical study on STG benchmarks demonstrates that integrating graph-level
contrast with the joint learning scheme achieves the best performance. In
addition, we introduce four augmentations for STG data, which perturb the data
in terms of graph structure, time domain, and frequency domain. Experimental
results reveal that the model is not sensitive to the proposed augmentations'
semantics. Lastly, we extend the classic contrastive loss via a rule-based
strategy that filters out the most semantically similar negatives, yielding
performance gains. We also provide explanations and insights based on the above
experimental findings. Code is available at https://github.com/liuxu77/STGCL."
"Text classification tasks have improved substantially during the last years
by the usage of transformers. However, the majority of researches focus on
prose texts, with poetry receiving less attention, specially for Spanish
language. In this paper, we propose a semi-supervised learning approach for
inferring 21 psychological categories evoked by a corpus of 4572 sonnets, along
with 10 affective and lexico-semantic multiclass ones. The subset of poems used
for training an evaluation includes 270 sonnets. With our approach, we achieve
an AUC beyond 0.7 for 76% of the psychological categories, and an AUC over 0.65
for 60% on the multiclass ones. The sonnets are modelled using transformers,
through sentence embeddings, along with lexico-semantic and affective features,
obtained by using external lexicons. Consequently, we see that this approach
provides an AUC increase of up to 0.12, as opposed to using transformers alone."
"A key trait of daily conversations between individuals is the ability to
express empathy towards others, and exploring ways to implement empathy is a
crucial step towards human-like dialogue systems. Previous approaches on this
topic mainly focus on detecting and utilizing the user's emotion for generating
empathetic responses. However, since empathy includes both aspects of affection
and cognition, we argue that in addition to identifying the user's emotion,
cognitive understanding of the user's situation should also be considered. To
this end, we propose a novel approach for empathetic response generation, which
leverages commonsense to draw more information about the user's situation and
uses this additional information to further enhance the empathy expression in
generated responses. We evaluate our approach on EmpatheticDialogues, which is
a widely-used benchmark dataset for empathetic response generation. Empirical
results demonstrate that our approach outperforms the baseline models in both
automatic and human evaluations and can generate more informative and
empathetic responses."
"Semantic maps represent the environment using a set of semantically
meaningful objects. This representation is storage-efficient, less ambiguous,
and more informative, thus facilitating large-scale autonomy and the
acquisition of actionable information in highly unstructured, GPS-denied
environments. In this letter, we propose an integrated system that can perform
large-scale autonomous flights and real-time semantic mapping in challenging
under-canopy environments. We detect and model tree trunks and ground planes
from LiDAR data, which are associated across scans and used to constrain robot
poses as well as tree trunk models. The autonomous navigation module utilizes a
multi-level planning and mapping framework and computes dynamically feasible
trajectories that lead the UAV to build a semantic map of the user-defined
region of interest in a computationally and storage efficient manner. A
drift-compensation mechanism is designed to minimize the odometry drift using
semantic SLAM outputs in real time, while maintaining planner optimality and
controller stability. This leads the UAV to execute its mission accurately and
safely at scale."
"With the ever-increasing use of web APIs in modern-day applications, it is
becoming more important to test the system as a whole. In the last decade,
tools and approaches have been proposed to automate the creation of
system-level test cases for these APIs using evolutionary algorithms (EAs). One
of the limiting factors of EAs is that the genetic operators (crossover and
mutation) are fully randomized, potentially breaking promising patterns in the
sequences of API requests discovered during the search. Breaking these patterns
has a negative impact on the effectiveness of the test case generation process.
To address this limitation, this paper proposes a new approach that uses
agglomerative hierarchical clustering (AHC) to infer a linkage tree model,
which captures, replicates, and preserves these patterns in new test cases. We
evaluate our approach, called LT-MOSA, by performing an empirical study on 7
real-world benchmark applications w.r.t. branch coverage and real-fault
detection capability. We also compare LT-MOSA with the two existing
state-of-the-art white-box techniques (MIO, MOSA) for REST API testing. Our
results show that LT-MOSA achieves a statistically significant increase in test
target coverage (i.e., lines and branches) compared to MIO and MOSA in 4 and 5
out of 7 applications, respectively. Furthermore, LT-MOSA discovers 27 and 18
unique real-faults that are left undetected by MIO and MOSA, respectively."
"Despite the recent progress in the field of causal inference, to date there
is no agreed upon methodology to glean treatment effect estimation from
observational data. The consequence on clinical practice is that, when lacking
results from a randomized trial, medical personnel is left without guidance on
what seems to be effective in a real-world scenario. This article proposes a
pragmatic methodology to obtain preliminary but robust estimation of treatment
effect from observational studies, to provide front-line clinicians with a
degree of confidence in their treatment strategy. Our study design is applied
to an open problem, the estimation of treatment effect of the proning maneuver
on COVID-19 Intensive Care patients."
"In computer graphics, animation compression is essential for efficient
storage, streaming and reproduction of animated meshes. Previous work has
presented efficient techniques for compression by deriving skinning
transformations and weights using clustering of vertices based on geometric
features of vertices over time. In this work we present a novel approach that
assigns vertices to bone-influenced clusters and derives weights using deep
learning through a training set that consists of pairs of vertex trajectories
(temporal vertex sequences) and the corresponding weights drawn from fully
rigged animated characters. The approximation error of the resulting linear
blend skinning scheme is significantly lower than the error of competent
previous methods by producing at the same time a minimal number of bones.
Furthermore, the optimal set of transformation and vertices is derived in fewer
iterations due to the better initial positioning in the multidimensional
variable space. Our method requires no parameters to be determined or tuned by
the user during the entire process of compressing a mesh animation sequence."
"Weakly-supervised table question-answering(TableQA) models have achieved
state-of-art performance by using pre-trained BERT transformer to jointly
encoding a question and a table to produce structured query for the question.
However, in practical settings TableQA systems are deployed over table corpora
having topic and word distributions quite distinct from BERT's pretraining
corpus. In this work we simulate the practical topic shift scenario by
designing novel challenge benchmarks WikiSQL-TS and WikiTQ-TS, consisting of
train-dev-test splits in five distinct topic groups, based on the popular
WikiSQL and WikiTableQuestions datasets. We empirically show that, despite
pre-training on large open-domain text, performance of models degrades
significantly when they are evaluated on unseen topics. In response, we propose
T3QA (Topic Transferable Table Question Answering) a pragmatic adaptation
framework for TableQA comprising of: (1) topic-specific vocabulary injection
into BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2)
based natural language question generation pipeline focused on generating topic
specific training data, and (3) a logical form reranker. We show that T3QA
provides a reasonably good baseline for our topic shift benchmarks. We believe
our topic split benchmarks will lead to robust TableQA solutions that are
better suited for practical deployment."
"Many constraint satisfaction problems involve synthesizing subgraphs that
satisfy certain reachability constraints. This paper presents programs in Picat
for four problems selected from the recent LP/CP programming competitions. The
programs demonstrate the modeling capabilities of the Picat language and the
solving efficiency of the cutting-edge SAT solvers empowered with effective
encodings."
"In this paper, we propose a distributed multi-stage optimization method for
planning complex missions for heterogeneous multi-robot teams. This class of
problems involves tasks that can be executed in different ways and are
associated with cross-schedule dependencies that constrain the schedules of the
different robots in the system. The proposed approach involves a
multi-objective heuristic search of the mission, represented as a hierarchical
tree that defines the mission goal. This procedure outputs several favorable
ways to fulfill the mission, which directly feed into the next stage of the
method. We propose a distributed metaheuristic based on evolutionary
computation to allocate tasks and generate schedules for the set of chosen
decompositions. The method is evaluated in a simulation setup of an automated
greenhouse use case, where we demonstrate the method's ability to adapt the
planning strategy depending on the available robots and the given optimization
criteria."
"We study the problem of large-scale network embedding, which aims to learn
low-dimensional latent representations for network mining applications. Recent
research in the field of network embedding has led to significant progress such
as DeepWalk, LINE, NetMF, NetSMF. However, the huge size of many real-world
networks makes it computationally expensive to learn network embedding from the
entire network. In this work, we present a novel network embedding method
called ""NES"", which learns network embedding from a small representative
subgraph. NES leverages theories from graph sampling to efficiently construct
representative subgraph with smaller size which can be used to make inferences
about the full network, enabling significantly improved efficiency in embedding
learning. Then, NES computes the network embedding from this representative
subgraph, efficiently. Compared with well-known methods, extensive experiments
on networks of various scales and types demonstrate that NES achieves
comparable performance and significant efficiency superiority."
"Four-variable-independent-regression localization losses, such as
Smooth-$\ell_1$ Loss, are used by default in modern detectors. Nevertheless,
this kind of loss is oversimplified so that it is inconsistent with the final
evaluation metric, intersection over union (IoU). Directly employing the
standard IoU is also not infeasible, since the constant-zero plateau in the
case of non-overlapping boxes and the non-zero gradient at the minimum may make
it not trainable. Accordingly, we propose a systematic method to address these
problems. Firstly, we propose a new metric, the extended IoU (EIoU), which is
well-defined when two boxes are not overlapping and reduced to the standard IoU
when overlapping. Secondly, we present the convexification technique (CT) to
construct a loss on the basis of EIoU, which can guarantee the gradient at the
minimum to be zero. Thirdly, we propose a steady optimization technique (SOT)
to make the fractional EIoU loss approaching the minimum more steadily and
smoothly. Fourthly, to fully exploit the capability of the EIoU based loss, we
introduce an interrelated IoU-predicting head to further boost localization
accuracy. With the proposed contributions, the new method incorporated into
Faster R-CNN with ResNet50+FPN as the backbone yields \textbf{4.2 mAP} gain on
VOC2007 and \textbf{2.3 mAP} gain on COCO2017 over the baseline Smooth-$\ell_1$
Loss, at almost \textbf{no training and inferencing computational cost}.
Specifically, the stricter the metric is, the more notable the gain is,
improving \textbf{8.2 mAP} on VOC2007 and \textbf{5.4 mAP} on COCO2017 at
metric $AP_{90}$."
"Using a collection of publicly available links to short form video clips of
an average of 6 seconds duration each, 1,275 users manually annotated each
video multiple times to indicate both long-term and short-term memorability of
the videos. The annotations were gathered as part of an online memory game and
measured a participant's ability to recall having seen the video previously
when shown a collection of videos. The recognition tasks were performed on
videos seen within the previous few minutes for short-term memorability and
within the previous 24 to 72 hours for long-term memorability. Data includes
the reaction times for each recognition of each video. Associated with each
video are text descriptions (captions) as well as a collection of image-level
features applied to 3 frames extracted from each video (start, middle and end).
Video-level features are also provided. The dataset was used in the Video
Memorability task as part of the MediaEval benchmark in 2020."
"Model-based reinforcement learning (MBRL) achieves significant sample
efficiency in practice in comparison to model-free RL, but its performance is
often limited by the existence of model prediction error. To reduce the model
error, standard MBRL approaches train a single well-designed network to fit the
entire environment dynamics, but this wastes rich information on multiple
sub-dynamics which can be modeled separately, allowing us to construct the
world model more accurately. In this paper, we propose the Environment Dynamics
Decomposition (ED2), a novel world model construction framework that models the
environment in a decomposing manner. ED2 contains two key components:
sub-dynamics discovery (SD2) and dynamics decomposition prediction (D2P). SD2
discovers the sub-dynamics in an environment automatically and then D2P
constructs the decomposed world model following the sub-dynamics. ED2 can be
easily combined with existing MBRL algorithms and empirical results show that
ED2 significantly reduces the model error, increases the sample efficiency, and
achieves higher asymptotic performance when combined with the state-of-the-art
MBRL algorithms on various continuous control tasks. Our code is open source
and available at https://github.com/ED2-source-code/ED2."
"Traffic forecasting is one of the most popular spatio-temporal tasks in the
field of machine learning. A prevalent approach in the field is to combine
graph convolutional networks and recurrent neural networks for the
spatio-temporal processing. There has been fierce competition and many novel
methods have been proposed. In this paper, we present the method of
spatio-temporal graph neural controlled differential equation (STG-NCDE).
Neural controlled differential equations (NCDEs) are a breakthrough concept for
processing sequential data. We extend the concept and design two NCDEs: one for
the temporal processing and the other for the spatial processing. After that,
we combine them into a single framework. We conduct experiments with 6
benchmark datasets and 20 baselines. STG-NCDE shows the best accuracy in all
cases, outperforming all those 20 baselines by non-trivial margins."
"Visual exploration is a task that seeks to visit all the navigable areas of
an environment as quickly as possible. The existing methods employ deep
reinforcement learning (RL) as the standard tool for the task. However, they
tend to be vulnerable to statistical shifts between the training and test data,
resulting in poor generalization over novel environments that are
out-of-distribution (OOD) from the training data. In this paper, we attempt to
improve the generalization ability by utilizing the inductive biases available
for the task. Employing the active neural SLAM (ANS) that learns exploration
policies with the advantage actor-critic (A2C) method as the base framework, we
first point out that the mappings represented by the actor and the critic
should satisfy specific symmetries. We then propose a network design for the
actor and the critic to inherently attain these symmetries. Specifically, we
use $G$-convolution instead of the standard convolution and insert the
semi-global polar pooling (SGPP) layer, which we newly design in this study, in
the last section of the critic network. Experimental results show that our
method increases area coverage by $8.1 m^2$ when trained on the Gibson dataset
and tested on the MP3D dataset, establishing the new state-of-the-art."
"Subgraph recognition aims at discovering a compressed substructure of a graph
that is most informative to the graph property. It can be formulated by
optimizing Graph Information Bottleneck (GIB) with a mutual information
estimator. However, GIB suffers from training instability and degenerated
results due to its intrinsic optimization process. To tackle these issues, we
reformulate the subgraph recognition problem into two steps: graph perturbation
and subgraph selection, leading to a novel Variational Graph Information
Bottleneck (VGIB) framework. VGIB first employs the noise injection to modulate
the information flow from the input graph to the perturbed graph. Then, the
perturbed graph is encouraged to be informative to the graph property. VGIB
further obtains the desired subgraph by filtering out the noise in the
perturbed graph. With the customized noise prior for each input, the VGIB
objective is endowed with a tractable variational upper bound, leading to a
superior empirical performance as well as theoretical properties. Extensive
experiments on graph interpretation, explainability of Graph Neural Networks,
and graph classification show that VGIB finds better subgraphs than existing
methods. Code is avaliable at https://github.com/Samyu0304/VGIB"
"Adversarial examples (AEs) pose severe threats to the applications of deep
neural networks (DNNs) to safety-critical domains, e.g., autonomous driving.
While there has been a vast body of AE defense solutions, to the best of our
knowledge, they all suffer from some weaknesses, e.g., defending against only a
subset of AEs or causing a relatively high accuracy loss for legitimate inputs.
Moreover, most existing solutions cannot defend against adaptive attacks,
wherein attackers are knowledgeable about the defense mechanisms and craft AEs
accordingly. In this paper, we propose a novel AE detection framework based on
the very nature of AEs, i.e., their semantic information is inconsistent with
the discriminative features extracted by the target DNN model. To be specific,
the proposed solution, namely ContraNet, models such contradiction by first
taking both the input and the inference result to a generator to obtain a
synthetic output and then comparing it against the original input. For
legitimate inputs that are correctly inferred, the synthetic output tries to
reconstruct the input. On the contrary, for AEs, instead of reconstructing the
input, the synthetic output would be created to conform to the wrong label
whenever possible. Consequently, by measuring the distance between the input
and the synthetic output with metric learning, we can differentiate AEs from
legitimate inputs. We perform comprehensive evaluations under various AE attack
scenarios, and experimental results show that ContraNet outperforms existing
solutions by a large margin, especially under adaptive attacks. Moreover, our
analysis shows that successful AEs that can bypass ContraNet tend to have
much-weakened adversarial semantics. We have also shown that ContraNet can be
easily combined with adversarial training techniques to achieve further
improved AE defense capabilities."
"Long short-term memory (LSTM) is a robust recurrent neural network
architecture for learning spatiotemporal sequential data. However, it requires
significant computational power for learning and implementing from both
software and hardware aspects. This paper proposes a novel LiteLSTM
architecture based on reducing the computation components of the LSTM using the
weights sharing concept to reduce the overall architecture cost and maintain
the architecture performance. The proposed LiteLSTM can be significant for
learning big data where time-consumption is crucial such as the security of IoT
devices and medical data. Moreover, it helps to reduce the CO2 footprint. The
proposed model was evaluated and tested empirically on two different datasets
from computer vision and cybersecurity domains."
"Graph convolutional networks (GCNs) allow us to learn topologically-aware
node embeddings, which can be useful for classification or link prediction.
However, they are unable to capture long-range dependencies between nodes
without adding additional layers -- which in turn leads to over-smoothing and
increased time and space complexity. Further, the complex dependencies between
nodes make mini-batching challenging, limiting their applicability to large
graphs. We propose a Scalable Multi-resolution Graph Representation Learning
(SMGRL) framework that enables us to learn multi-resolution node embeddings
efficiently. Our framework is model-agnostic and can be applied to any existing
GCN model. We dramatically reduce training costs by training only on a
reduced-dimension coarsening of the original graph, then exploit
self-similarity to apply the resulting algorithm at multiple resolutions. The
resulting multi-resolution embeddings can be aggregated to yield high-quality
node embeddings that capture both long- and short-range dependencies. Our
experiments show that this leads to improved classification accuracy, without
incurring high computational costs."
"Message passing algorithms, whose iterative nature captures well complicated
interactions among interconnected variables in complex systems and extracts
information from the fixed point of iterated messages, provide a powerful
toolkit in tackling hard computational tasks in optimization, inference, and
learning problems. In the context of constraint satisfaction problems (CSPs),
when a control parameter (such as constraint density) is tuned, multiple
threshold phenomena emerge, signaling fundamental structural transitions in
their solution space. Finding solutions around these transition points is
exceedingly challenging for algorithm design, where message passing algorithms
suffer from a large message fluctuation far from convergence. Here we introduce
a residual-based updating step into message passing algorithms, in which
messages varying large between consecutive steps are given high priority in the
updating process. For the specific example of model RB, a typical prototype of
random CSPs with growing domains, we show that our algorithm improves the
convergence of message updating and increases the success probability in
finding solutions around the satisfiability threshold with a low computational
cost. Our approach to message passing algorithms should be of value for
exploring their power in developing algorithms to find ground-state solutions
and understand the detailed structure of solution space of hard optimization
problems."
"Machine learning (ML) approaches have demonstrated promising results in a
wide range of healthcare applications. Data plays a crucial role in developing
ML-based healthcare systems that directly affect people's lives. Many of the
ethical issues surrounding the use of ML in healthcare stem from structural
inequalities underlying the way we collect, use, and handle data. Developing
guidelines to improve documentation practices regarding the creation, use, and
maintenance of ML healthcare datasets is therefore of critical importance. In
this work, we introduce Healthsheet, a contextualized adaptation of the
original datasheet questionnaire ~\cite{gebru2018datasheets} for
health-specific applications. Through a series of semi-structured interviews,
we adapt the datasheets for healthcare data documentation. As part of the
Healthsheet development process and to understand the obstacles researchers
face in creating datasheets, we worked with three publicly-available healthcare
datasets as our case studies, each with different types of structured data:
Electronic health Records (EHR), clinical trial study data, and
smartphone-based performance outcome measures. Our findings from the
interviewee study and case studies show 1) that datasheets should be
contextualized for healthcare, 2) that despite incentives to adopt
accountability practices such as datasheets, there is a lack of consistency in
the broader use of these practices 3) how the ML for health community views
datasheets and particularly \textit{Healthsheets} as diagnostic tool to surface
the limitations and strength of datasets and 4) the relative importance of
different fields in the datasheet to healthcare concerns."
"This paper studies the problem of multi-step manipulative attacks in
Stackelberg security games, in which a clever attacker attempts to orchestrate
its attacks over multiple time steps to mislead the defender's learning of the
attacker's behavior. This attack manipulation eventually influences the
defender's patrol strategy towards the attacker's benefit. Previous work along
this line of research only focuses on one-shot games in which the defender
learns the attacker's behavior and then designs a corresponding strategy only
once. Our work, on the other hand, investigates the long-term impact of the
attacker's manipulation in which current attack and defense choices of players
determine the future learning and patrol planning of the defender. This paper
has three key contributions. First, we introduce a new multi-step manipulative
attack game model that captures the impact of sequential manipulative attacks
carried out by the attacker over the entire time horizon. Second, we propose a
new algorithm to compute an optimal manipulative attack plan for the attacker,
which tackles the challenge of multiple connected optimization components
involved in the computation across multiple time steps. Finally, we present
extensive experimental results on the impact of such misleading attacks,
showing a significant benefit for the attacker and loss for the defender."
"Point cloud data have been widely explored due to its superior accuracy and
robustness under various adverse situations. Meanwhile, deep neural networks
(DNNs) have achieved very impressive success in various applications such as
surveillance and autonomous driving. The convergence of point cloud and DNNs
has led to many deep point cloud models, largely trained under the supervision
of large-scale and densely-labelled point cloud data. Unsupervised point cloud
representation learning, which aims to learn general and useful point cloud
representations from unlabelled point cloud data, has recently attracted
increasing attention due to the constraint in large-scale point cloud
labelling. This paper provides a comprehensive review of unsupervised point
cloud representation learning using DNNs. It first describes the motivation,
general pipelines as well as terminologies of the recent studies. Relevant
background including widely adopted point cloud datasets and DNN architectures
is then briefly presented. This is followed by an extensive discussion of
existing unsupervised point cloud representation learning methods according to
their technical approaches. We also quantitatively benchmark and discuss the
reviewed methods over multiple widely adopted point cloud datasets. Finally, we
share our humble opinion about several challenges and problems that could be
pursued in future research in unsupervised point cloud representation learning.
A project associated with this survey has been built at
https://github.com/xiaoaoran/3d_url_survey."
"Knowledge amalgamation (KA) is a novel deep model reusing task aiming to
transfer knowledge from several well-trained teachers to a multi-talented and
compact student. Currently, most of these approaches are tailored for
convolutional neural networks (CNNs). However, there is a tendency that
transformers, with a completely different architecture, are starting to
challenge the domination of CNNs in many computer vision tasks. Nevertheless,
directly applying the previous KA methods to transformers leads to severe
performance degradation. In this work, we explore a more effective KA scheme
for transformer-based object detection models. Specifically, considering the
architecture characteristics of transformers, we propose to dissolve the KA
into two aspects: sequence-level amalgamation (SA) and task-level amalgamation
(TA). In particular, a hint is generated within the sequence-level amalgamation
by concatenating teacher sequences instead of redundantly aggregating them to a
fixed-size one as previous KA works. Besides, the student learns heterogeneous
detection tasks through soft targets with efficiency in the task-level
amalgamation. Extensive experiments on PASCAL VOC and COCO have unfolded that
the sequence-level amalgamation significantly boosts the performance of
students, while the previous methods impair the students. Moreover, the
transformer-based students excel in learning amalgamated knowledge, as they
have mastered heterogeneous detection tasks rapidly and achieved superior or at
least comparable performance to those of the teachers in their specializations."
"The detection and identification of toxic comments are conducive to creating
a civilized and harmonious Internet environment. In this experiment, we
collected various data sets related to toxic comments. Because of the
characteristics of comment data, we perform data cleaning and feature
extraction operations on it from different angles to obtain different toxic
comment training sets. In terms of model construction, we used the training set
to train the models based on TFIDF and finetuned the Bert model separately.
Finally, we encapsulated the code into software to score toxic comments in
real-time."
"The Spotted Lanternfly (SLF) is an invasive planthopper that threatens the
local biodiversity and agricultural economy of regions such as the Northeastern
United States and Japan. As researchers scramble to study the insect, there is
a great potential for computer vision tasks such as detection, pose estimation,
and accurate identification to have important downstream implications in
containing the SLF. However, there is currently no publicly available dataset
for training such AI models. To enable computer vision applications and
motivate advancements to challenge the invasive SLF problem, we propose
LANTERN-RD, the first curated image dataset of the spotted lanternfly and its
look-alikes, featuring images with varied lighting conditions, diverse
backgrounds, and subjects in assorted poses. A VGG16-based baseline CNN
validates the potential of this dataset for stimulating fresh computer vision
applications to accelerate invasive SLF research. Additionally, we implement
the trained model in a simple mobile classification application in order to
directly empower responsible public mitigation efforts. The overarching mission
of this work is to introduce a novel SLF image dataset and release a
classification framework that enables computer vision applications, boosting
studies surrounding the invasive SLF and assisting in minimizing its
agricultural and economic damage."
"The pre-trained language model is trained on large-scale unlabeled text and
can achieve state-of-the-art results in many different downstream tasks.
However, the current pre-trained language model is mainly concentrated in the
Chinese and English fields. For low resource language such as Tibetan, there is
lack of a monolingual pre-trained model. To promote the development of Tibetan
natural language processing tasks, this paper collects the large-scale training
data from Tibetan websites and constructs a vocabulary that can cover 99.95$\%$
of the words in the corpus by using Sentencepiece. Then, we train the Tibetan
monolingual pre-trained language model named TiBERT on the data and vocabulary.
Finally, we apply TiBERT to the downstream tasks of text classification and
question generation, and compare it with classic models and multilingual
pre-trained models, the experimental results show that TiBERT can achieve the
best performance. Our model is published in http://tibert.cmli-nlp.com/"
"From CNN, RNN, to ViT, we have witnessed remarkable advancements in video
prediction, incorporating auxiliary inputs, elaborate neural architectures, and
sophisticated training strategies. We admire these progresses but are confused
about the necessity: is there a simple method that can perform comparably well?
This paper proposes SimVP, a simple video prediction model that is completely
built upon CNN and trained by MSE loss in an end-to-end fashion. Without
introducing any additional tricks and complicated strategies, we can achieve
state-of-the-art performance on five benchmark datasets. Through extended
experiments, we demonstrate that SimVP has strong generalization and
extensibility on real-world datasets. The significant reduction of training
cost makes it easier to scale to complex scenarios. We believe SimVP can serve
as a solid baseline to stimulate the further development of video prediction.
The code is available at
\href{https://github.com/gaozhangyang/SimVP-Simpler-yet-Better-Video-Prediction}{Github}."
"Different from conventional image matting, which either requires user-defined
scribbles/trimap to extract a specific foreground object or directly extracts
all the foreground objects in the image indiscriminately, we introduce a new
task named Referring Image Matting (RIM) in this paper, which aims to extract
the meticulous alpha matte of the specific object that best matches the given
natural language description, thus enabling a more natural and simpler
instruction for image matting. First, we establish a large-scale challenging
dataset RefMatte by designing a comprehensive image composition and expression
generation engine to automatically produce high-quality images along with
diverse text attributes based on public datasets. RefMatte consists of 230
object categories, 47,500 images, 118,749 expression-region entities, and
474,996 expressions. Additionally, we construct a real-world test set with 100
high-resolution natural images and manually annotate complex phrases to
evaluate the out-of-domain generalization abilities of RIM methods.
Furthermore, we present a novel baseline method CLIPMat for RIM, including a
context-embedded prompt, a text-driven semantic pop-up, and a multi-level
details extractor. Extensive experiments on RefMatte in both keyword and
expression settings validate the superiority of CLIPMat over representative
methods. We hope this work could provide novel insights into image matting and
encourage more follow-up studies. The dataset, code and models are available at
https://github.com/JizhiziLi/RIM."
"This paper aims to advance the mathematical intelligence of machines by
presenting the first Chinese mathematical pre-trained language model~(PLM) for
effectively understanding and representing mathematical problems. Unlike other
standard NLP tasks, mathematical texts are difficult to understand, since they
involve mathematical terminology, symbols and formulas in the problem
statement. Typically, it requires complex mathematical logic and background
knowledge for solving mathematical problems.
  Considering the complex nature of mathematical texts, we design a novel
curriculum pre-training approach for improving the learning of mathematical
PLMs, consisting of both basic and advanced courses. Specially, we first
perform token-level pre-training based on a position-biased masking strategy,
and then design logic-based pre-training tasks that aim to recover the shuffled
sentences and formulas, respectively. Finally, we introduce a more difficult
pre-training task that enforces the PLM to detect and correct the errors in its
generated solutions. We conduct extensive experiments on offline evaluation
(including nine math-related tasks) and online $A/B$ test. Experimental results
demonstrate the effectiveness of our approach compared with a number of
competitive baselines. Our code is available at:
\textcolor{blue}{\url{https://github.com/RUCAIBox/JiuZhang}}."
"Polarimetric imaging, along with deep learning, has shown improved
performances on different tasks including scene analysis. However, its
robustness may be questioned because of the small size of the training
datasets. Though the issue could be solved by data augmentation, polarization
modalities are subject to physical feasibility constraints unaddressed by
classical data augmentation techniques. To address this issue, we propose to
use CycleGAN, an image translation technique based on deep generative models
that solely relies on unpaired data, to transfer large labeled road scene
datasets to the polarimetric domain. We design several auxiliary loss terms
that, alongside the CycleGAN losses, deal with the physical constraints of
polarimetric images. The efficiency of this solution is demonstrated on road
scene object detection tasks where generated realistic polarimetric images
allow to improve performances on cars and pedestrian detection up to 9%. The
resulting constrained CycleGAN is publicly released, allowing anyone to
generate their own polarimetric images."
"Despite its importance in both industrial and service robotics, mobile
manipulation remains a significant challenge as it requires a seamless
integration of end-effector trajectory generation with navigation skills as
well as reasoning over long-horizons. Existing methods struggle to control the
large configuration space, and to navigate dynamic and unknown environments. In
previous work, we proposed to decompose mobile manipulation tasks into a
simplified motion generator for the end-effector in task space and a trained
reinforcement learning agent for the mobile base to account for kinematic
feasibility of the motion. In this work, we introduce Neural Navigation for
Mobile Manipulation (N$^2$M$^2$) which extends this decomposition to complex
obstacle environments and enables it to tackle a broad range of tasks in real
world settings. The resulting approach can perform unseen, long-horizon tasks
in unexplored environments while instantly reacting to dynamic obstacles and
environmental changes. At the same time, it provides a simple way to define new
mobile manipulation tasks. We demonstrate the capabilities of our proposed
approach in extensive simulation and real-world experiments on multiple
kinematically diverse mobile manipulators. Code and videos are publicly
available at http://mobile-rl.cs.uni-freiburg.de."
"Bayesian optimization (BO) is a widely-used sequential method for
zeroth-order optimization of complex and expensive-to-compute black-box
functions. The existing BO methods assume that the function evaluation
(feedback) is available to the learner immediately or after a fixed delay. Such
assumptions may not be practical in many real-life problems like online
recommendations, clinical trials, and hyperparameter tuning where feedback is
available after a random delay. To benefit from the experimental
parallelization in these problems, the learner needs to start new function
evaluations without waiting for delayed feedback. In this paper, we consider
the BO under stochastic delayed feedback problem. We propose algorithms with
sub-linear regret guarantees that efficiently address the dilemma of selecting
new function queries while waiting for randomly delayed feedback. Building on
our results, we also make novel contributions to batch BO and contextual
Gaussian process bandits. Experiments on synthetic and real-life datasets
verify the performance of our algorithms."
"Dust storms may remarkably degrade the imaging quality of Martian orbiters
and delay the progress of mapping the global topography and geomorphology. To
address this issue, this paper presents an approach that reuses the image
dehazing knowledge obtained on Earth to resolve the dust-removal problem on
Mars. In this approach, we collect remote-sensing images captured by Tianwen-1
and manually select hundreds of clean and dusty images. Inspired by the haze
formation process on Earth, we formulate a similar visual degradation process
on clean images and synthesize dusty images sharing a similar feature
distribution with realistic dusty images. These realistic clean and synthetic
dusty image pairs are used to train a deep model that inherently encodes dust
irrelevant features and decodes them into dust-free images. Qualitative and
quantitative results show that dust storms can be effectively eliminated by the
proposed approach, leading to obviously improved topographical and
geomorphological details of Mars."
"The last decades have witnessed a rapid increase of Earth observation
satellites (EOSs), leading to the increasing complexity of EOSs scheduling. On
account of the widespread applications of large region observation, this paper
aims to address the EOSs observation scheduling problem for large region
targets. A rapid coverage calculation method employing a projection reference
plane and a polygon clipping technique is first developed. We then formulate a
nonlinear integer programming model for the scheduling problem, where the
objective function is calculated based on the developed coverage calculation
method. A greedy initialization-based resampling particle swarm optimization
(GI-RPSO) algorithm is proposed to solve the model. The adopted greedy
initialization strategy and particle resampling method contribute to generating
efficient and effective solutions during the evolution process. In the end,
extensive experiments are conducted to illustrate the effectiveness and
reliability of the proposed method. Compared to the traditional particle swarm
optimization and the widely used greedy algorithm, the proposed GI-RPSO can
improve the scheduling result by 5.42% and 15.86%, respectively."
"Most black-box adversarial attack schemes for object detectors mainly face
two shortcomings: requiring access to the target model and generating
inefficient adversarial examples (failing to make objects disappear in large
numbers). To overcome these shortcomings, we propose a black-box adversarial
attack scheme based on semantic segmentation and model inversion (SSMI). We
first locate the position of the target object using semantic segmentation
techniques. Next, we design a neighborhood background pixel replacement to
replace the target region pixels with background pixels to ensure that the
pixel modifications are not easily detected by human vision. Finally, we
reconstruct a machine-recognizable example and use the mask matrix to select
pixels in the reconstructed example to modify the benign image to generate an
adversarial example. Detailed experimental results show that SSMI can generate
efficient adversarial examples to evade human-eye perception and make objects
of interest disappear. And more importantly, SSMI outperforms existing same
kinds of attacks. The maximum increase in new and disappearing labels is 16%,
and the maximum decrease in mAP metrics for object detection is 36%."
"Spoken Language Understanding (SLU), a core component of the task-oriented
dialogue system, expects a shorter inference facing the impatience of human
users. Existing work increases inference speed by designing non-autoregressive
models for single-turn SLU tasks but fails to apply to multi-turn SLU in
confronting the dialogue history. The intuitive idea is to concatenate all
historical utterances and utilize the non-autoregressive models directly.
However, this approach seriously misses the salient historical information and
suffers from the uncoordinated-slot problems. To overcome those shortcomings,
we propose a novel model for multi-turn SLU named Salient History Attention
with Layer-Refined Transformer (SHA-LRT), which composes of an SHA module, a
Layer-Refined Mechanism (LRM), and a Slot Label Generation (SLG) task. SHA
captures salient historical information for the current dialogue from both
historical utterances and results via a well-designed history-attention
mechanism. LRM predicts preliminary SLU results from Transformer's middle
states and utilizes them to guide the final prediction, and SLG obtains the
sequential dependency information for the non-autoregressive encoder.
Experiments on public datasets indicate that our model significantly improves
multi-turn SLU performance (17.5% on Overall) with accelerating (nearly 15
times) the inference process over the state-of-the-art baseline as well as
effective on the single-turn SLU tasks."
"Researchers typically resort to numerical methods to understand and predict
ocean dynamics, a key task in mastering environmental phenomena. Such methods
may not be suitable in scenarios where the topographic map is complex,
knowledge about the underlying processes is incomplete, or the application is
time critical. On the other hand, if ocean dynamics are observed, they can be
exploited by recent machine learning methods. In this paper we describe a
data-driven method to predict environmental variables such as current velocity
and sea surface height in the region of Santos-Sao Vicente-Bertioga Estuarine
System in the southeastern coast of Brazil. Our model exploits both temporal
and spatial inductive biases by joining state-of-the-art sequence models (LSTM
and Transformers) and relational models (Graph Neural Networks) in an
end-to-end framework that learns both the temporal features and the spatial
relationship shared among observation sites. We compare our results with the
Santos Operational Forecasting System (SOFS). Experiments show that better
results are attained by our model, while maintaining flexibility and little
domain knowledge dependency."
"Vanilla Federated learning (FL) relies on the centralized global aggregation
mechanism and assumes that all clients are honest. This makes it a challenge
for FL to alleviate the single point of failure and dishonest clients. These
impending challenges in the design philosophy of FL call for blockchain-based
federated learning (BFL) due to the benefits of coupling FL and blockchain
(e.g., democracy, incentive, and immutability). However, one problem in vanilla
BFL is that its capabilities do not follow adopters' needs in a dynamic
fashion. Besides, vanilla BFL relies on unverifiable clients' self-reported
contributions like data size because checking clients' raw data is not allowed
in FL for privacy concerns. We design and evaluate a novel BFL framework, and
resolve the identified challenges in vanilla BFL with greater flexibility and
incentive mechanism called FAIR-BFL. In contrast to existing works, FAIR-BFL
offers unprecedented flexibility via the modular design, allowing adopters to
adjust its capabilities following business demands in a dynamic fashion. Our
design accounts for BFL's ability to quantify each client's contribution to the
global learning process. Such quantification provides a rational metric for
distributing the rewards among federated clients and helps discover malicious
participants that may poison the global model."
"In this paper we present the probabilistic typed natural deduction calculus
TPTND, designed to reason about and derive trustworthiness properties of
probabilistic computational processes, like those underlying current AI
applications. Derivability in TPTND is interpreted as the process of extracting
$n$ samples of possibly complex outputs with a certain frequency from a given
categorical distribution. We formalize trust for such outputs as a form of
hypothesis testing on the distance between such frequency and the intended
probability. The main advantage of the calculus is to render such notion of
trustworthiness checkable. We present a computational semantics for the terms
over which we reason and then the semantics of TPTND, where logical operators
as well as a Trust operator are defined through introduction and elimination
rules. We illustrate structural and metatheoretical properties, with particular
focus on the ability to establish under which term evolutions and logical rules
applications the notion of trustworhtiness can be preserved."
"Designing reinforcement learning (RL) agents is typically a difficult process
that requires numerous design iterations. Learning can fail for a multitude of
reasons, and standard RL methods provide too few tools to provide insight into
the exact cause. In this paper, we show how to integrate value decomposition
into a broad class of actor-critic algorithms and use it to assist in the
iterative agent-design process. Value decomposition separates a reward function
into distinct components and learns value estimates for each. These value
estimates provide insight into an agent's learning and decision-making process
and enable new training methods to mitigate common problems. As a
demonstration, we introduce SAC-D, a variant of soft actor-critic (SAC) adapted
for value decomposition. SAC-D maintains similar performance to SAC, while
learning a larger set of value predictions. We also introduce
decomposition-based tools that exploit this information, including a new reward
influence metric, which measures each reward component's effect on agent
decision-making. Using these tools, we provide several demonstrations of
decomposition's use in identifying and addressing problems in the design of
both environments and agents. Value decomposition is broadly applicable and
easy to incorporate into existing algorithms and workflows, making it a
powerful tool in an RL practitioner's toolbox."
"Precision agriculture is rapidly attracting research to efficiently introduce
automation and robotics solutions to support agricultural activities. Robotic
navigation in vineyards and orchards offers competitive advantages in
autonomously monitoring and easily accessing crops for harvesting, spraying and
performing time-consuming necessary tasks. Nowadays, autonomous navigation
algorithms exploit expensive sensors which also require heavy computational
cost for data processing. Nonetheless, vineyard rows represent a challenging
outdoor scenario where GPS and Visual Odometry techniques often struggle to
provide reliable positioning information. In this work, we combine Edge AI with
Deep Reinforcement Learning to propose a cutting-edge lightweight solution to
tackle the problem of autonomous vineyard navigation without exploiting precise
localization data and overcoming task-tailored algorithms with a flexible
learning-based approach. We train an end-to-end sensorimotor agent which
directly maps noisy depth images and position-agnostic robot state information
to velocity commands and guides the robot to the end of a row, continuously
adjusting its heading for a collision-free central trajectory. Our extensive
experimentation in realistic simulated vineyards demonstrates the effectiveness
of our solution and the generalization capabilities of our agent."
"Image-based virtual try-on aims to synthesize an image of a person wearing a
given clothing item. To solve the task, the existing methods warp the clothing
item to fit the person's body and generate the segmentation map of the person
wearing the item before fusing the item with the person. However, when the
warping and the segmentation generation stages operate individually without
information exchange, the misalignment between the warped clothes and the
segmentation map occurs, which leads to the artifacts in the final image. The
information disconnection also causes excessive warping near the clothing
regions occluded by the body parts, so-called pixel-squeezing artifacts. To
settle the issues, we propose a novel try-on condition generator as a unified
module of the two stages (i.e., warping and segmentation generation stages). A
newly proposed feature fusion block in the condition generator implements the
information exchange, and the condition generator does not create any
misalignment or pixel-squeezing artifacts. We also introduce discriminator
rejection that filters out the incorrect segmentation map predictions and
assures the performance of virtual try-on frameworks. Experiments on a
high-resolution dataset demonstrate that our model successfully handles the
misalignment and occlusion, and significantly outperforms the baselines. Code
is available at https://github.com/sangyun884/HR-VITON."
"Click-through rate (CTR) prediction, whose goal is to predict the probability
of the user to click on an item, has become increasingly significant in the
recommender systems. Recently, some deep learning models with the ability to
automatically extract the user interest from his/her behaviors have achieved
great success. In these work, the attention mechanism is used to select the
user interested items in historical behaviors, improving the performance of the
CTR predictor. Normally, these attentive modules can be jointly trained with
the base predictor by using gradient descents. In this paper, we regard user
interest modeling as a feature selection problem, which we call user interest
selection. For such a problem, we propose a novel approach under the framework
of the wrapper method, which is named Meta-Wrapper. More specifically, we use a
differentiable module as our wrapping operator and then recast its learning
problem as a continuous bilevel optimization. Moreover, we use a meta-learning
algorithm to solve the optimization and theoretically prove its convergence.
Meanwhile, we also provide theoretical analysis to show that our proposed
method 1) efficiencies the wrapper-based feature selection, and 2) achieves
better resistance to overfitting. Finally, extensive experiments on three
public datasets manifest the superiority of our method in boosting the
performance of CTR prediction."
"Cost-effective asset management is an area of interest across several
industries. Specifically, this paper develops a deep reinforcement learning
(DRL) solution to automatically determine an optimal rehabilitation policy for
continuously deteriorating water pipes. We approach the problem of
rehabilitation planning in an online and offline DRL setting. In online DRL,
the agent interacts with a simulated environment of multiple pipes with
distinct lengths, materials, and failure rate characteristics. We train the
agent using deep Q-learning (DQN) to learn an optimal policy with minimal
average costs and reduced failure probability. In offline learning, the agent
uses static data, e.g., DQN replay data, to learn an optimal policy via a
conservative Q-learning algorithm without further interactions with the
environment. We demonstrate that DRL-based policies improve over standard
preventive, corrective, and greedy planning alternatives. Additionally,
learning from the fixed DQN replay dataset in an offline setting further
improves the performance. The results warrant that the existing deterioration
profiles of water pipes consisting of large and diverse states and action
trajectories provide a valuable avenue to learn rehabilitation policies in the
offline setting, which can be further fine-tuned using the simulator."
"We address the following action-effect prediction task. Given an image
depicting an initial state of the world and an action expressed in text,
predict an image depicting the state of the world following the action. The
prediction should have the same scene context as the input image. We explore
the use of the recently proposed GLIDE model for performing this task. GLIDE is
a generative neural network that can synthesize (inpaint) masked areas of an
image, conditioned on a short piece of text. Our idea is to mask-out a region
of the input image where the effect of the action is expected to occur. GLIDE
is then used to inpaint the masked region conditioned on the required action.
In this way, the resulting image has the same background context as the input
image, updated to show the effect of the action. We give qualitative results
from experiments using the EPIC dataset of ego-centric videos labelled with
actions."
"The promise of interaction between intelligent conversational agents and
humans is that models can learn from such feedback in order to improve.
Unfortunately, such exchanges in the wild will not always involve human
utterances that are benign or of high quality, and will include a mixture of
engaged (helpers) and unengaged or even malicious users (trolls). In this work
we study how to perform robust learning in such an environment. We introduce a
benchmark evaluation, SafetyMix, which can evaluate methods that learn safe vs.
toxic language in a variety of adversarial settings to test their robustness.
We propose and analyze several mitigating learning algorithms that identify
trolls either at the example or at the user level. Our main finding is that
user-based methods, that take into account that troll users will exhibit
adversarial behavior across multiple examples, work best in a variety of
settings on our benchmark. We then test these methods in a further real-life
setting of conversations collected during deployment, with similar results."
"Entity linking (EL) is the process of linking entity mentions appearing in
text with their corresponding entities in a knowledge base. EL features of
entities (e.g., prior probability, relatedness score, and entity embedding) are
usually estimated based on Wikipedia. However, for newly emerging entities
(EEs) which have just been discovered in news, they may still not be included
in Wikipedia yet. As a consequence, it is unable to obtain required EL features
for those EEs from Wikipedia and EL models will always fail to link ambiguous
mentions with those EEs correctly as the absence of their EL features. To deal
with this problem, in this paper we focus on a new task of learning EL features
for emerging entities in a general way. We propose a novel approach called
STAMO to learn high-quality EL features for EEs automatically, which needs just
a small number of labeled documents for each EE collected from the Web, as it
could further leverage the knowledge hidden in the unlabeled data. STAMO is
mainly based on self-training, which makes it flexibly integrated with any EL
feature or EL model, but also makes it easily suffer from the error
reinforcement problem caused by the mislabeled data. Instead of some common
self-training strategies that try to throw the mislabeled data away explicitly,
we regard self-training as a multiple optimization process with respect to the
EL features of EEs, and propose both intra-slot and inter-slot optimizations to
alleviate the error reinforcement problem implicitly. We construct two EL
datasets involving selected EEs to evaluate the quality of obtained EL features
for EEs, and the experimental results show that our approach significantly
outperforms other baseline methods of learning EL features."
"We present an overview of the design and first proof-of-concept
implementation for AIDA, an autonomous intelligent developer agent that
develops software from scratch. AIDA takes a software requirements
specification and uses reasoning over a semantic knowledge graph to interpret
the requirements, then designs and writes software to satisfy them. AIDA uses
both declarative and procedural knowledge in the core domains of data,
algorithms, and code, plus some general knowledge. The reasoning codebase uses
this knowledge to identify needed components, then designs and builds the
necessary information structures around them that become the software. These
structures, the motivating requirements, and the resulting source code itself
are all new knowledge that are added to the knowledge graph, becoming available
for future reasoning. In this way, AIDA also learns as she writes code and
becomes more efficient when writing subsequent code."
"This text is concerned with a hypothetical flavour of cognitive blindness
referred to in this paper as \textit{C-Causal Blindness} or C-CB. A cognitive
blindness where the policy to obtain the objective leads to the state to be
avoided. A literal example of C-CB would be \textit{Kurt G\""odel's} decision to
starve for \textit{""fear of being poisoned""} - take this to be premise
\textbf{A}. The objective being \textit{""to avoid being poisoned (so as to not
die)""}: \textbf{C}, the plan or policy being \textit{""don't eat""}: \textbf{B},
and the actual outcome having been \textit{""dying""}: $\lnot$\textbf{C} - the
state that G\""odel wanted to avoid to begin with. Like many, G\""odel pursued a
strategy that caused the result he wanted to avoid. An experimental
computational framework is proposed to show the isomorphic relationship between
C-CB in brain computations, logic, and computer computations using hidden
Markov models."
"Knowledge distillation has recently become popular as a method of model
aggregation on the server for federated learning. It is generally assumed that
there are abundant public unlabeled data on the server. However, in reality,
there exists a domain discrepancy between the datasets of the server domain and
a client domain, which limits the performance of knowledge distillation. How to
improve the aggregation under such a domain discrepancy setting is still an
open problem. In this paper, we first analyze the generalization bound of the
aggregation model produced from knowledge distillation for the client domains,
and then describe two challenges, server-to-client discrepancy and
client-to-client discrepancy, brought to the aggregation model by the domain
discrepancies. Following our analysis, we propose an adaptive knowledge
aggregation algorithm FedD3A based on domain discrepancy aware distillation to
lower the bound. FedD3A performs adaptive weighting at the sample level in each
round of FL. For each sample in the server domain, only the client models of
its similar domains will be selected for playing the teacher role. To achieve
this, we show that the discrepancy between the server-side sample and the
client domain can be approximately measured using a subspace projection matrix
calculated on each client without accessing its raw data. The server can thus
leverage the projection matrices from multiple clients to assign weights to the
corresponding teacher models for each server-side sample. We validate FedD3A on
two popular cross-domain datasets and show that it outperforms the compared
competitors in both cross-silo and cross-device FL settings."
"We present Phenaki, a model capable of realistic video synthesis, given a
sequence of textual prompts. Generating videos from text is particularly
challenging due to the computational cost, limited quantities of high quality
text-video data and variable length of videos. To address these issues, we
introduce a new model for learning video representation which compresses the
video to a small representation of discrete tokens. This tokenizer uses causal
attention in time, which allows it to work with variable-length videos. To
generate video tokens from text we are using a bidirectional masked transformer
conditioned on pre-computed text tokens. The generated video tokens are
subsequently de-tokenized to create the actual video. To address data issues,
we demonstrate how joint training on a large corpus of image-text pairs as well
as a smaller number of video-text examples can result in generalization beyond
what is available in the video datasets. Compared to the previous video
generation methods, Phenaki can generate arbitrary long videos conditioned on a
sequence of prompts (i.e. time variable text or a story) in open domain. To the
best of our knowledge, this is the first time a paper studies generating videos
from time variable prompts. In addition, compared to the per-frame baselines,
the proposed video encoder-decoder computes fewer tokens per video but results
in better spatio-temporal consistency."
"In large-scale e-commerce platforms like Taobao, it is a big challenge to
retrieve products that satisfy users from billions of candidates. This has been
a common concern of academia and industry. Recently, plenty of works in this
domain have achieved significant improvements by enhancing embedding-based
retrieval (EBR) methods, including the Multi-Grained Deep Semantic Product
Retrieval (MGDSPR) model [16] in Taobao search engine. However, we find that
MGDSPR still has problems of poor relevance and weak personalization compared
to other retrieval methods in our online system, such as lexical matching and
collaborative filtering. These problems promote us to further strengthen the
capabilities of our EBR model in both relevance estimation and personalized
retrieval. In this paper, we propose a novel Multi-Objective Personalized
Product Retrieval (MOPPR) model with four hierarchical optimization objectives:
relevance, exposure, click and purchase. We construct entire-space
multi-positive samples to train MOPPR, rather than the single-positive samples
for existing EBR models.We adopt a modified softmax loss for optimizing
multiple objectives. Results of extensive offline and online experiments show
that MOPPR outperforms the baseline MGDSPR on evaluation metrics of relevance
estimation and personalized retrieval. MOPPR achieves 0.96% transaction and
1.29% GMV improvements in a 28-day online A/B test. Since the Double-11
shopping festival of 2021, MOPPR has been fully deployed in mobile Taobao
search, replacing the previous MGDSPR. Finally, we discuss several advanced
topics of our deeper explorations on multi-objective retrieval and ranking to
contribute to the community."
"The advances in Artificial Intelligence (AI) have led to technological
advancements in a plethora of domains. Healthcare, education, and smart city
services are now enriched with AI capabilities. These technological
advancements would not have been realized without the assistance of fast,
secure, and fault-tolerant communication media. Traditional processing,
communication and storage technologies cannot maintain high levels of
scalability and user experience for immersive services. The metaverse is an
immersive three-dimensional (3D) virtual world that integrates fantasy and
reality into a virtual environment using advanced virtual reality (VR) and
augmented reality (AR) devices. Such an environment is still being developed
and requires extensive research in order for it to be realized to its highest
attainable levels. In this article, we discuss some of the key issues required
in order to attain realization of metaverse services. We propose a framework
that integrates digital twin (DT) with other advanced technologies such as the
sixth generation (6G) communication network, blockchain, and AI, to maintain
continuous end-to-end metaverse services. This article also outlines
requirements for an integrated, DT-enabled metaverse framework and provides a
look ahead into the evolving topic."
"In this paper, we introduce the Vehicle Claims dataset, consisting of
fraudulent insurance claims for automotive repairs. The data belongs to the
more broad category of Auditing data, which includes also Journals and Network
Intrusion data. Insurance claim data are distinctively different from other
auditing data (such as network intrusion data) in their high number of
categorical attributes. We tackle the common problem of missing benchmark
datasets for anomaly detection: datasets are mostly confidential, and the
public tabular datasets do not contain relevant and sufficient categorical
attributes. Therefore, a large-sized dataset is created for this purpose and
referred to as Vehicle Claims (VC) dataset. The dataset is evaluated on shallow
and deep learning methods. Due to the introduction of categorical attributes,
we encounter the challenge of encoding them for the large dataset. As One Hot
encoding of high cardinal dataset invokes the ""curse of dimensionality"", we
experiment with GEL encoding and embedding layer for representing categorical
attributes. Our work compares competitive learning, reconstruction-error,
density estimation and contrastive learning approaches for Label, One Hot, GEL
encoding and embedding layer to handle categorical values."
"We develop a novel data-driven nonlinear mixup mechanism for graph data
augmentation and present different mixup functions for sample pairs and their
labels. Mixup is a data augmentation method to create new training data by
linearly interpolating between pairs of data samples and their labels. Mixup of
graph data is challenging since the interpolation between graphs of potentially
different sizes is an ill-posed operation. Hence, a promising approach for
graph mixup is to first project the graphs onto a common latent feature space
and then explore linear and nonlinear mixup strategies in this latent space. In
this context, we propose to (i) project graphs onto the latent space of
continuous random graph models known as graphons, (ii) leverage convex
clustering in this latent space to generate nonlinear data-driven mixup
functions, and (iii) investigate the use of different mixup functions for
labels and data samples. We evaluate our graph data augmentation performance on
benchmark datasets and demonstrate that nonlinear data-driven mixup functions
can significantly improve graph classification."
"Membership Inference Attacks (MIAs) infer whether a data point is in the
training data of a machine learning model. It is a threat while being in the
training data is private information of a data point. MIA correctly infers some
data points as members or non-members of the training data. Intuitively, data
points that MIA accurately detects are vulnerable. Considering those data
points may exist in different target models susceptible to multiple MIAs, the
vulnerability of data points under multiple MIAs and target models is worth
exploring.
  This paper defines new metrics that can reflect the actual situation of data
points' vulnerability and capture vulnerable data points under multiple MIAs
and target models. From the analysis, MIA has an inference tendency to some
data points despite a low overall inference performance. Additionally, we
implement 54 MIAs, whose average attack accuracy ranges from 0.5 to 0.9, to
support our analysis with our scalable and flexible platform, Membership
Inference Attacks Platform (VMIAP). Furthermore, previous methods are
unsuitable for finding vulnerable data points under multiple MIAs and different
target models. Finally, we observe that the vulnerability is not characteristic
of the data point but related to the MIA and target model."
"Potential advancements in artificial intelligence (AI) could have profound
implications for how countries research and develop weapons systems, and how
militaries deploy those systems on the battlefield. The idea of AI-enabled
military systems has motivated some activists to call for restrictions or bans
on some weapon systems, while others have argued that AI may be too diffuse to
control. This paper argues that while a ban on all military applications of AI
is likely infeasible, there may be specific cases where arms control is
possible. Throughout history, the international community has attempted to ban
or regulate weapons or military systems for a variety of reasons. This paper
analyzes both successes and failures and offers several criteria that seem to
influence why arms control works in some cases and not others. We argue that
success or failure depends on the desirability (i.e., a weapon's military value
versus its perceived horribleness) and feasibility (i.e., sociopolitical
factors that influence its success) of arms control. Based on these criteria,
and the historical record of past attempts at arms control, we analyze the
potential for AI arms control in the future and offer recommendations for what
policymakers can do today."
"Goal-conditioned reinforcement learning (RL) is a promising direction for
training agents that are capable of solving multiple tasks and reach a diverse
set of objectives. How to \textit{specify} and \textit{ground} these goals in
such a way that we can both reliably reach goals during training as well as
generalize to new goals during evaluation remains an open area of research.
Defining goals in the space of noisy and high-dimensional sensory inputs poses
a challenge for training goal-conditioned agents, or even for generalization to
novel goals. We propose to address this by learning factorial representations
of goals and processing the resulting representation via a discretization
bottleneck, for coarser goal specification, through an approach we call DGRL.
We show that applying a discretizing bottleneck can improve performance in
goal-conditioned RL setups, by experimentally evaluating this method on tasks
ranging from maze environments to complex robotic navigation and manipulation.
Additionally, we prove a theorem lower-bounding the expected return on
out-of-distribution goals, while still allowing for specifying goals with
expressive combinatorial structure."
"The cornerstone of multilingual neural translation is shared representations
across languages. Given the theoretically infinite representation power of
neural networks, semantically identical sentences are likely represented
differently. While representing sentences in the continuous latent space
ensures expressiveness, it introduces the risk of capturing of irrelevant
features which hinders the learning of a common representation. In this work,
we discretize the encoder output latent space of multilingual models by
assigning encoder states to entries in a codebook, which in effect represents
source sentences in a new artificial language. This discretization process not
only offers a new way to interpret the otherwise black-box model
representations, but, more importantly, gives potential for increasing
robustness in unseen testing conditions. We validate our approach on
large-scale experiments with realistic data volumes and domains. When tested in
zero-shot conditions, our approach is competitive with two strong alternatives
from the literature. We also use the learned artificial language to analyze
model behavior, and discover that using a similar bridge language increases
knowledge-sharing among the remaining languages."
"The ability of knowledge graphs to represent complex relationships at scale
has led to their adoption for various needs including knowledge representation,
question-answering, and recommendation systems. Knowledge graphs are often
incomplete in the information they represent, necessitating the need for
knowledge graph completion tasks. Pre-trained and fine-tuned language models
have shown promise in these tasks although these models ignore the intrinsic
information encoded in the knowledge graph, namely the entity and relation
types. In this work, we propose the Knowledge Graph Language Model (KGLM)
architecture, where we introduce a new entity/relation embedding layer that
learns to differentiate distinctive entity and relation types, therefore
allowing the model to learn the structure of the knowledge graph. In this work,
we show that further pre-training the language models with this additional
embedding layer using the triples extracted from the knowledge graph, followed
by the standard fine-tuning phase sets a new state-of-the-art performance for
the link prediction task on the benchmark datasets."
"Missing data is a common concern in health datasets, and its impact on good
decision-making processes is well documented. Our study's contribution is a
methodology for tackling missing data problems using a combination of synthetic
dataset generation, missing data imputation and deep learning methods to
resolve missing data challenges. Specifically, we conducted a series of
experiments with these objectives; $a)$ generating a realistic synthetic
dataset, $b)$ simulating data missingness, $c)$ recovering the missing data,
and $d)$ analyzing imputation performance. Our methodology used a gaussian
mixture model whose parameters were learned from a cleaned subset of a real
demographic and health dataset to generate the synthetic data. We simulated
various missingness degrees ranging from $10 \%$, $20 \%$, $30 \%$, and $40\%$
under the missing completely at random scheme MCAR. We used an integrated
performance analysis framework involving clustering, classification and direct
imputation analysis. Our results show that models trained on synthetic and
imputed datasets could make predictions with an accuracy of $83 \%$ and $80 \%$
on $a) $ an unseen real dataset and $b)$ an unseen reserved synthetic test
dataset, respectively. Moreover, the models that used the DAE method for
imputed yielded the lowest log loss an indication of good performance, even
though the accuracy measures were slightly lower. In conclusion, our work
demonstrates that using our methodology, one can reverse engineer a solution to
resolve missingness on an unseen dataset with missingness. Moreover, though we
used a health dataset, our methodology can be utilized in other contexts."
"With the recently increasing capabilities of modern vehicles, novel
approaches for interaction emerged that go beyond traditional touch-based and
voice command approaches. Therefore, hand gestures, head pose, eye gaze, and
speech have been extensively investigated in automotive applications for object
selection and referencing. Despite these significant advances, existing
approaches mostly employ a one-model-fits-all approach unsuitable for varying
user behavior and individual differences. Moreover, current referencing
approaches either consider these modalities separately or focus on a stationary
situation, whereas the situation in a moving vehicle is highly dynamic and
subject to safety-critical constraints. In this paper, I propose a research
plan for a user-centered adaptive multimodal fusion approach for referencing
external objects from a moving vehicle. The proposed plan aims to provide an
open-source framework for user-centered adaptation and personalization using
user observations and heuristics, multimodal fusion, clustering,
transfer-of-learning for model adaptation, and continuous learning, moving
towards trusted human-centered artificial intelligence."
"Graph-structured data are widespread in real-world applications, such as
social networks, recommender systems, knowledge graphs, chemical molecules etc.
Despite the success of Euclidean space for graph-related learning tasks, its
ability to model complex patterns is essentially constrained by its
polynomially growing capacity. Recently, hyperbolic spaces have emerged as a
promising alternative for processing graph data with tree-like structure or
power-law distribution, owing to the exponential growth property. Different
from Euclidean space, which expands polynomially, the hyperbolic space grows
exponentially which makes it gains natural advantages in abstracting tree-like
or scale-free graphs with hierarchical organizations.
  In this tutorial, we aim to give an introduction to this emerging field of
graph representation learning with the express purpose of being accessible to
all audiences. We first give a brief introduction to graph representation
learning as well as some preliminary Riemannian and hyperbolic geometry. We
then comprehensively revisit the hyperbolic embedding techniques, including
hyperbolic shallow models and hyperbolic neural networks. In addition, we
introduce the technical details of the current hyperbolic graph neural networks
by unifying them into a general framework and summarizing the variants of each
component. Moreover, we further introduce a series of related applications in a
variety of fields. In the last part, we discuss several advanced topics about
hyperbolic geometry for graph representation learning, which potentially serve
as guidelines for further flourishing the non-Euclidean graph learning
community."
"Recently, neural network based methods have shown their power in learning
more expressive features on the task of knowledge graph embedding (KGE).
However, the performance of deep methods often falls behind the shallow ones on
simple graphs. One possible reason is that deep models are difficult to train,
while shallow models might suffice for accurately representing the structure of
the simple KGs.
  In this paper, we propose a neural network based model, named DeepE, to
address the problem, which stacks multiple building blocks to predict the tail
entity based on the head entity and the relation. Each building block is an
addition of a linear and a non-linear function. The stacked building blocks are
equivalent to a group of learning functions with different non-linear depth.
Hence, DeepE allows deep functions to learn deep features, and shallow
functions to learn shallow features. Through extensive experiments, we find
DeepE outperforms other state-of-the-art baseline methods. A major advantage of
DeepE is the robustness. DeepE achieves a Mean Rank (MR) score that is 6%, 30%,
65% lower than the best baseline methods on FB15k-237, WN18RR and YAGO3-10. Our
design makes it possible to train much deeper networks on KGE, e.g. 40 layers
on FB15k-237, and without scarifying precision on simple relations."
"Pathfinding makes up an important sub-component of a broad range of complex
tasks in AI, such as robot path planning, transport routing, and game playing.
While classical algorithms can efficiently compute shortest paths, neural
networks could be better suited to adapting these sub-routines to more complex
and intractable tasks. As a step toward developing such networks, we hand-code
and learn models for Breadth-First Search (BFS), i.e. shortest path finding,
using the unified architectural framework of Neural Cellular Automata, which
are iterative neural networks with equal-size inputs and outputs. Similarly, we
present a neural implementation of Depth-First Search (DFS), and outline how it
can be combined with neural BFS to produce an NCA for computing diameter of a
graph. We experiment with architectural modifications inspired by these
hand-coded NCAs, training networks from scratch to solve the diameter problem
on grid mazes while exhibiting strong generalization ability. Finally, we
introduce a scheme in which data points are mutated adversarially during
training. We find that adversarially evolving mazes leads to increased
generalization on out-of-distribution examples, while at the same time
generating data-sets with significantly more complex solutions for reasoning
tasks."
"Understanding the dynamics of financial transactions among people is
critically important for various applications such as fraud detection. One
important aspect of financial transaction networks is temporality. The order
and repetition of transactions can offer new insights when considered within
the graph structure. Temporal motifs, defined as a set of nodes that interact
with each other in a short time period, are a promising tool in this context.
In this work, we study three unique temporal financial networks: transactions
in Mercari, an online marketplace, payments in a synthetic network generated by
J.P. Morgan Chase, and payments and friendships among Venmo users. We consider
the fraud detection problem on the Mercari and J.P. Morgan Chase networks, for
which the ground truth is available. We show that temporal motifs offer
superior performance than a previous method that considers simple graph
features. For the Venmo network, we investigate the interplay between financial
and social relations on three tasks: friendship prediction, vendor
identification, and analysis of temporal cycles. For friendship prediction,
temporal motifs yield better results than general heuristics, such as Jaccard
and Adamic-Adar measures. We are also able to identify vendors with high
accuracy and observe interesting patterns in rare motifs, like temporal cycles.
We believe that the analysis, datasets, and lessons from this work will be
beneficial for future research on financial transaction networks."
"Developing models to automatically score students' written responses to
science problems is critical for science education. However, collecting and
labeling sufficient student responses for training models is time and
cost-consuming. Recent studies suggest that pre-trained language models can be
adapted to downstream tasks without fine-tuning with prompts. However, no
research has employed such a prompt approach in science education. As student
responses are presented with natural language, aligning the scoring procedure
as the next sentence prediction task using prompts can skip the costly
fine-tuning stage. In this study, we developed a zero-shot approach to
automatically score student responses via Matching Exemplars as Next Sentence
Prediction (MeNSP). This approach employs no training samples. We first apply
MeNSP in scoring three assessment tasks of scientific argumentation and found
machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and
F1 score ranges from 0.54 to 0.81. To improve the performance, we extend our
research to the few-shots setting, either randomly selecting labeled student
responses or manually constructing responses to fine-tune the models. We find
that one task's performance is improved with more samples, Cohen's Kappa from
0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring
performance is not improved. We also find that randomly selected few-shots
perform better than the human expert-crafted approach. This study suggests that
MeNSP can yield referable automatic scoring for student responses while
significantly reducing the cost of model training. This method can benefit
low-stakes classroom assessment practices in science education. Future research
should further explore the applicability of the MeNSP in different types of
assessment tasks in science education and improve the model performance."
"Perception, localization, planning, and control, high-level functions often
organized in a so-called pipeline, are amongst the core building blocks of
modern autonomous (ground, air, and underwater) vehicle architectures. These
functions are increasingly being implemented using learning-enabled components
(LECs), i.e., (software) components leveraging knowledge acquisition and
learning processes such as deep learning. Providing quantified component-level
assurance as part of a wider (dynamic) assurance case can be useful in
supporting both pre-operational approval of LECs (e.g., by regulators), and
runtime hazard mitigation, e.g., using assurance-based failover configurations.
This paper develops a notion of assurance for LECs based on i) identifying the
relevant dependability attributes, and ii) quantifying those attributes and the
associated uncertainty, using probabilistic techniques. We give a practical
grounding for our work using an example from the aviation domain: an autonomous
taxiing capability for an unmanned aircraft system (UAS), focusing on the
application of LECs as sensors in the perception function. We identify the
applicable quantitative measures of assurance, and characterize the associated
uncertainty using a non-parametric Bayesian approach, namely Gaussian process
regression. We additionally discuss the relevance and contribution of LEC
assurance to system-level assurance, the generalizability of our approach, and
the associated challenges."
"As reinforcement learning methods increasingly amass accomplishments, the
need for comprehending their solutions becomes more crucial. Most explainable
reinforcement learning (XRL) methods generate a static explanation depicting
their developers' intuition of what should be explained and how. In contrast,
literature from the social sciences proposes that meaningful explanations are
structured as a dialog between the explainer and the explainee, suggesting a
more active role for the user and her communication with the agent. In this
paper, we present ASQ-IT -- an interactive tool that presents video clips of
the agent acting in its environment based on queries given by the user that
describe temporal properties of behaviors of interest. Our approach is based on
formal methods: queries in ASQ-IT's user interface map to a fragment of Linear
Temporal Logic over finite traces (LTLf), which we developed, and our algorithm
for query processing is based on automata theory. User studies show that
end-users can understand and formulate queries in ASQ-IT, and that using ASQ-IT
assists users in identifying faulty agent behaviors."
"PPO (Proximal Policy Optimization) is a state-of-the-art policy gradient
algorithm that has been successfully applied to complex computer games such as
Dota 2 and Honor of Kings. In these environments, an agent makes compound
actions consisting of multiple sub-actions. PPO uses clipping to restrict
policy updates. Although clipping is simple and effective, it is not efficient
in its sample use. For compound actions, most PPO implementations consider the
joint probability (density) of sub-actions, which means that if the ratio of a
sample (state compound-action pair) exceeds the range, the gradient the sample
produces is zero. Instead, for each sub-action we calculate the loss
separately, which is less prone to clipping during updates thereby making
better use of samples. Further, we propose a multi-action mixed loss that
combines joint and separate probabilities. We perform experiments in
Gym-$\mu$RTS and MuJoCo. Our hybrid model improves performance by more than
50\% in different MuJoCo environments compared to OpenAI's PPO benchmark
results. And in Gym-$\mu$RTS, we find the sub-action loss outperforms the
standard PPO approach, especially when the clip range is large. Our findings
suggest this method can better balance the use-efficiency and quality of
samples."
"Knowledge graphs represent known facts using triplets. While existing
knowledge graph embedding methods only consider the connections between
entities, we propose considering the relationships between triplets. For
example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is
(Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins,
Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a
prerequisite for $T_2$. In this paper, we define a higher-level triplet to
represent a relationship between triplets, e.g., $\langle T_1$,
PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation.
We define a bi-level knowledge graph that consists of the base-level and the
higher-level triplets. We also propose a data augmentation strategy based on
the random walks on the bi-level knowledge graph to augment plausible triplets.
Our model called BiVE learns embeddings by taking into account the structures
of the base-level and the higher-level triplets, with additional consideration
of the augmented triplets. We propose two new tasks: triplet prediction and
conditional link prediction. Given a triplet $T_1$ and a higher-level relation,
the triplet prediction predicts a triplet that is likely to be connected to
$T_1$ by the higher-level relation, e.g., $\langle T_1$, PrerequisiteFor,
?$\rangle$. The conditional link prediction predicts a missing entity in a
triplet conditioned on another triplet, e.g., $\langle T_1$, PrerequisiteFor,
(Avatar, Wins, ?)$\rangle$. Experimental results show that BiVE significantly
outperforms all other methods in the two new tasks and the typical base-level
link prediction in real-world bi-level knowledge graphs."
"One approach for interpreting black-box machine learning models is to find a
global approximation of the model using simple interpretable functions, which
is called a metamodel (a model of the model). Approximating the black-box with
a metamodel can be used to 1) estimate instance-wise feature importance; 2)
understand the functional form of the model; 3) analyze feature interactions.
In this work, we propose a new method for finding interpretable metamodels. Our
approach utilizes Kolmogorov superposition theorem, which expresses
multivariate functions as a composition of univariate functions (our primitive
parameterized functions). This composition can be represented in the form of a
tree. Inspired by symbolic regression, we use a modified form of genetic
programming to search over different tree configurations. Gradient descent (GD)
is used to optimize the parameters of a given configuration. Our method is a
novel memetic algorithm that uses GD not only for training numerical constants
but also for the training of building blocks. Using several experiments, we
show that our method outperforms recent metamodeling approaches suggested for
interpreting black-boxes."
"Large pre-training language models (PLMs) have shown promising in-context
learning abilities. However, due to the backbone transformer architecture,
existing PLMs are bottlenecked by the memory and computational cost when
scaling up to a large context size, leaving instruction tuning and in-context
learning of many demonstration examples, as well as long-range language
modeling under-explored. In this study, we propose a long-range language model
EVALM based on an efficient transformer mechanism. EVALM is trained with 8k
tokens per batch line and can test up to 256k-lengthed contexts with
extrapolation, 128 times to the limit of existing PLMs (e.g. GPT3). Based on
EVALM, we scale up the size of examples efficiently in both instruction tuning
and in-context learning to explore the boundary of the benefits from more
annotated data. Experimental results on a diverse set of tasks show that EVALM
achieves 4.1% higher accuracy on average, and the average length of achieving
the best accuracy score over tasks is around 12k. We find that in-context
learning can achieve higher performance with more demonstrations under
many-shot instruction tuning (8k), and further extending the length of
instructions (16k) can further improve the upper bound of scaling in-context
learning."
"One essential feature of an autonomous train is minimizing collision risks
with third-party objects. To estimate the risk, the control system must
identify topological information of all the rail routes ahead on which the
train can possibly move, especially within merging or diverging rails. This
way, the train can figure out the status of potential obstacles with respect to
its route and hence, make a timely decision. Numerous studies have successfully
extracted all rail tracks as a whole within forward-looking images without
considering element instances. Still, some image-based methods have employed
hard-coded prior knowledge of railway geometry on 3D data to associate
left-right rails and generate rail route instances. However, we propose a rail
path extraction pipeline in which left-right rail pixels of each rail route
instance are extracted and associated through a fully convolutional
encoder-decoder architecture called TPE-Net. Two different regression branches
for TPE-Net are proposed to regress the locations of center points of each rail
route, along with their corresponding left-right pixels. Extracted rail pixels
are then spatially clustered to generate topological information of all the
possible train routes (ego-paths), discarding non-ego-path ones. Experimental
results on a challenging, publicly released benchmark show true-positive-pixel
level average precision and recall of 0.9207 and 0.8721, respectively, at about
12 frames per second. Even though our evaluation results are not higher than
the SOTA, the proposed regression pipeline performs remarkably in extracting
the correspondences by looking once at the image. It generates strong rail
route hypotheses without reliance on camera parameters, 3D data, and
geometrical constraints."
"Multi-domain recommender systems benefit from cross-domain representation
learning and positive knowledge transfer. Both can be achieved by introducing a
specific modeling of input data (i.e. disjoint history) or trying dedicated
training regimes. At the same time, treating domains as separate input sources
becomes a limitation as it does not capture the interplay that naturally exists
between domains. In this work, we efficiently learn multi-domain representation
of sequential users' interactions using graph neural networks. We use temporal
intra- and inter-domain interactions as contextual information for our method
called MAGRec (short for Multi-domAin Graph-based Recommender). To better
capture all relations in a multi-domain setting, we learn two graph-based
sequential representations simultaneously: domain-guided for recent user
interest, and general for long-term interest. This approach helps to mitigate
the negative knowledge transfer problem from multiple domains and improve
overall representation. We perform experiments on publicly available datasets
in different scenarios where MAGRec consistently outperforms state-of-the-art
methods. Furthermore, we provide an ablation study and discuss further
extensions of our method."
"Road object detection is an important branch of automatic driving technology,
The model with higher detection accuracy is more conducive to the safe driving
of vehicles. In road object detection, the omission of small objects and
occluded objects is an important problem. therefore, reducing the missed rate
of the object is of great significance for safe driving. In the work of this
paper, based on the YOLOX object detection algorithm to improve, proposes
DecIoU boundary box regression loss function to improve the shape consistency
of the predicted and real box, and Push Loss is introduced to further optimize
the boundary box regression loss function, in order to detect more occluded
objects. In addition, the dynamic anchor box mechanism is also used to improve
the accuracy of the confidence label, improve the label inaccuracy of object
detection model without anchor box. A large number of experiments on KITTI
dataset demonstrate the effectiveness of the proposed method, the improved
YOLOX-s achieved 88.9% mAP and 91.0% mAR on the KITTI dataset, compared to the
baseline version improvements of 2.77% and 4.24%; the improved YOLOX-m achieved
89.1% mAP and 91.4% mAR, compared to the baseline version improvements of 2.30%
and 4.10%."
"Reinforcement Learning has suffered from poor reward specification, and
issues for reward hacking even in simple enough domains. Preference Based
Reinforcement Learning attempts to solve the issue by utilizing binary
feedbacks on queried trajectory pairs by a human in the loop indicating their
preferences about the agent's behavior to learn a reward model. In this work,
we present a state augmentation technique that allows the agent's reward model
to be robust and follow an invariance consistency that significantly improved
performance, i.e. the reward recovery and subsequent return computed using the
learned policy over our baseline PEBBLE. We validate our method on three
domains, Mountain Car, a locomotion task of Quadruped-Walk, and a robotic
manipulation task of Sweep-Into, and find that using the proposed augmentation
the agent not only benefits in the overall performance but does so, quite early
in the agent's training phase."
"Failure and resilience are important aspects of gameplay. This is especially
important for serious and competitive games, where players need to adapt and
cope with failure frequently. In such situations, emotion regulation -- the
active process of modulating ones' emotions to cope and adapt to challenging
situations -- becomes essential. It is one of the prominent aspects of human
intelligence and promotes mental health and well-being. While there has been
work on developing artificial emotional regulation assistants to help users
cope with emotion regulation in the field of Intelligent Tutoring systems,
little is done to incorporate such systems or ideas into (serious) video games.
In this paper, we introduce a data-driven 6-phase approach to establish
empathetic artificial intelligence (EAI), which operates on raw chat log data
to detect key affective states, identify common sequences and emotion
regulation strategies and generalizes these to make them applicable for
intervention systems."
"The goal of the present paper is to develop and validate a questionnaire to
assess AI literacy. In particular, the questionnaire should be deeply grounded
in the existing literature on AI literacy, should be modular (i.e., including
different facets that can be used independently of each other) to be flexibly
applicable in professional life depending on the goals and use cases, and
should meet psychological requirements and thus includes further psychological
competencies in addition to the typical facets of AIL. We derived 60 items to
represent different facets of AI Literacy according to Ng and colleagues
conceptualisation of AI literacy and additional 12 items to represent
psychological competencies such as problem solving, learning, and emotion
regulation in regard to AI. For this purpose, data were collected online from
300 German-speaking adults. The items were tested for factorial structure in
confirmatory factor analyses. The result is a measurement instrument that
measures AI literacy with the facets Use & apply AI, Understand AI, Detect AI,
and AI Ethics and the ability to Create AI as a separate construct, and AI
Self-efficacy in learning and problem solving and AI Self-management. This
study contributes to the research on AI literacy by providing a measurement
instrument relying on profound competency models. In addition, higher-order
psychological competencies are included that are particularly important in the
context of pervasive change through AI systems."
"The rapid expansion of the Internet of Things (IoT) and Edge Computing has
presented challenges for centralized Machine and Deep Learning (ML/DL) methods
due to the presence of distributed data silos that hold sensitive information.
To address concerns regarding data privacy, collaborative and
privacy-preserving ML/DL techniques like Federated Learning (FL) have emerged.
However, ensuring data privacy and performance alone is insufficient since
there is a growing need to establish trust in model predictions. Existing
literature has proposed various approaches on trustworthy ML/DL (excluding data
privacy), identifying robustness, fairness, explainability, and accountability
as important pillars. Nevertheless, further research is required to identify
trustworthiness pillars and evaluation metrics specifically relevant to FL
models, as well as to develop solutions that can compute the trustworthiness
level of FL models. This work examines the existing requirements for evaluating
trustworthiness in FL and introduces a comprehensive taxonomy consisting of six
pillars (privacy, robustness, fairness, explainability, accountability, and
federation), along with over 30 metrics for computing the trustworthiness of FL
models. Subsequently, an algorithm named FederatedTrust is designed based on
the pillars and metrics identified in the taxonomy to compute the
trustworthiness score of FL models. A prototype of FederatedTrust is
implemented and integrated into the learning process of FederatedScope, a
well-established FL framework. Finally, five experiments are conducted using
different configurations of FederatedScope to demonstrate the utility of
FederatedTrust in computing the trustworthiness of FL models. Three experiments
employ the FEMNIST dataset, and two utilize the N-BaIoT dataset considering a
real-world IoT security use case."
"Recently, a large number of studies have shown that the introduction of
visual information can effectively improve the effect of neural machine
translation (NMT). Its effectiveness largely depends on the availability of a
large number of bilingual parallel sentence pairs and manual image annotation.
The lack of images and the effectiveness of images have been difficult to
solve. In this paper, a multimodal pre-training generalization algorithm for
self-supervised training is proposed, which overcomes the lack of visual
information and inaccuracy, and thus extends the applicability of images on
NMT. Specifically, we will search for many pictures from the existing sentences
through the search engine, and then through the relationship between visual
information and text, do the self-supervised training task of graphics and text
to obtain more effective visual information for text. We show that when the
filtered information is used as multimodal machine translation for fine-tuning,
the effect of translation in the global voice dataset is 0.5 BLEU higher than
the baseline."
"The goal of Feature Selection - comprising filter, wrapper, and embedded
approaches - is to find the optimal feature subset for designated downstream
tasks. Nevertheless, current feature selection methods are limited by: 1) the
selection criteria of these methods are varied for different domains, making
them hard to generalize; 2) the selection performance of these approaches drops
significantly when processing high-dimensional feature space coupled with small
sample size. In light of these challenges, we pose the question: can selected
feature subsets be more robust, accurate, and input dimensionality agnostic? In
this paper, we reformulate the feature selection problem as a deep
differentiable optimization task and propose a new research perspective:
conceptualizing discrete feature subsetting as continuous embedding space
optimization. We introduce a novel and principled framework that encompasses a
sequential encoder, an accuracy evaluator, a sequential decoder, and a gradient
ascent optimizer. This comprehensive framework includes four important steps:
preparation of features-accuracy training data, deep feature subset embedding,
gradient-optimized search, and feature subset reconstruction. Specifically, we
utilize reinforcement feature selection learning to generate diverse and
high-quality training data and enhance generalization. By optimizing
reconstruction and accuracy losses, we embed feature selection knowledge into a
continuous space using an encoder-evaluator-decoder model structure. We employ
a gradient ascent search algorithm to find better embeddings in the learned
embedding space. Furthermore, we reconstruct feature selection solutions using
these embeddings and select the feature subset with the highest performance for
downstream tasks as the optimal subset."
"The application of pattern mining algorithms to extract movement patterns
from sports big data can improve training specificity by facilitating a more
granular evaluation of movement. As there are various pattern mining
algorithms, this study aimed to validate which algorithm discovers the best set
of movement patterns for player movement profiling in professional rugby league
and the similarity in extracted movement patterns between the algorithms. Three
pattern mining algorithms (l-length Closed Contiguous [LCCspm], Longest Common
Subsequence [LCS] and AprioriClose) were used to profile elite rugby football
league hookers (n = 22 players) and wingers (n = 28 players) match-games
movements across 319 matches. Machine learning classification algorithms were
used to identify which algorithm gives the best set of movement patterns to
separate playing positions with Jaccard similarity score identifying the extent
of similarity between algorithms' movement patterns. LCCspm and LCS movement
patterns shared a 0.19 Jaccard similarity score. AprioriClose movement patterns
shared no significant similarity with LCCspm and LCS patterns. The closed
contiguous movement patterns profiled by LCCspm best-separated players into
playing positions. Multi-layered Perceptron algorithm achieved the highest
accuracy of 91.02% and precision, recall and F1 scores of 0.91 respectively.
Therefore, we recommend the extraction of closed contiguous (consecutive) over
non-consecutive movement patterns for separating groups of players."
"Multi-Object Tracking (MOT) has gained extensive attention in recent years
due to its potential applications in traffic and pedestrian detection. We note
that tracking by detection may suffer from errors generated by noise detectors,
such as an imprecise bounding box before the occlusions, and observed that in
most tracking scenarios, objects tend to move and lost within specific
locations. To counter this, we present a novel tracker to deal with the bad
detector and occlusions. Firstly, we proposed a location-wise sub-region
recognition method which equally divided the frame, which we called mesh. Then
we proposed corresponding location-wise loss management strategies and
different matching strategies. The resulting Mesh-SORT, ablation studies
demonstrate its effectiveness and made 3% fragmentation 7.2% ID switches drop
and 0.4% MOTA improvement compared to the baseline on MOT17 datasets. Finally,
we analyze its limitation on the specific scene and discussed what future works
can be extended."
"Query-focused meeting summarization (QFMS) aims to generate summaries from
meeting transcripts in response to a given query. Previous works typically
concatenate the query with meeting transcripts and implicitly model the query
relevance only at the token level with attention mechanism. However, due to the
dilution of key query-relevant information caused by long meeting transcripts,
the original transformer-based model is insufficient to highlight the key parts
related to the query. In this paper, we propose a query-aware framework with
joint modeling token and utterance based on Query-Utterance Attention. It
calculates the utterance-level relevance to the query with a dense retrieval
module. Then both token-level query relevance and utterance-level query
relevance are combined and incorporated into the generation process with
attention mechanism explicitly. We show that the query relevance of different
granularities contributes to generating a summary more related to the query.
Experimental results on the QMSum dataset show that the proposed model achieves
new state-of-the-art performance."
"Despite the success of deep neural network (DNN) on sequential data (i.e.,
scene text and speech) recognition, it suffers from the over-confidence problem
mainly due to overfitting in training with the cross-entropy loss, which may
make the decision-making less reliable. Confidence calibration has been
recently proposed as one effective solution to this problem. Nevertheless, the
majority of existing confidence calibration methods aims at non-sequential
data, which is limited if directly applied to sequential data since the
intrinsic contextual dependency in sequences or the class-specific statistical
prior is seldom exploited. To the end, we propose a Context-Aware Selective
Label Smoothing (CASLS) method for calibrating sequential data. The proposed
CASLS fully leverages the contextual dependency in sequences to construct
confusion matrices of contextual prediction statistics over different classes.
Class-specific error rates are then used to adjust the weights of smoothing
strength in order to achieve adaptive calibration. Experimental results on
sequence recognition tasks, including scene text recognition and speech
recognition, demonstrate that our method can achieve the state-of-the-art
performance."
"Nowadays, numerous applications incorporate machine learning (ML) algorithms
due to their prominent achievements. However, many studies in the field of
computer vision have shown that ML can be fooled by intentionally crafted
instances, called adversarial examples. These adversarial examples take
advantage of the intrinsic vulnerability of ML models. Recent research raises
many concerns in the cybersecurity field. An increasing number of researchers
are studying the feasibility of such attacks on security systems based on ML
algorithms, such as Intrusion Detection Systems (IDS). The feasibility of such
adversarial attacks would be influenced by various domain-specific constraints.
This can potentially increase the difficulty of crafting adversarial examples.
Despite the considerable amount of research that has been done in this area,
much of it focuses on showing that it is possible to fool a model using
features extracted from the raw data but does not address the practical side,
i.e., the reverse transformation from theory to practice. For this reason, we
propose a review browsing through various important papers to provide a
comprehensive analysis. Our analysis highlights some challenges that have not
been addressed in the reviewed papers."
"We develop an elementary method to compute spaces of equivariant maps from a
homogeneous space $G/H$ of a Lie group $G$ to a module of this group. The Lie
group is not required to be compact. More generally, we study spaces of
invariant sections in homogeneous vector bundles, and take a special interest
in the case where the fibres are algebras. These latter cases have a natural
global algebra structure. We classify these automorphic algebras for the case
where the homogeneous space has compact stabilisers. This work has applications
in the theoretical development of geometric deep learning and also in the
theory of automorphic Lie algebras."
"The emergence of large language models (LLMs) has resulted in the production
of LLM-generated texts that is highly sophisticated and almost
indistinguishable from texts written by humans. However, this has also sparked
concerns about the potential misuse of such texts, such as spreading
misinformation and causing disruptions in the education system. Although many
detection approaches have been proposed, a comprehensive understanding of the
achievements and challenges is still lacking. This survey aims to provide an
overview of existing LLM-generated text detection techniques and enhance the
control and regulation of language generation models. Furthermore, we emphasize
crucial considerations for future research, including the development of
comprehensive evaluation metrics and the threat posed by open-source LLMs, to
drive progress in the area of LLM-generated text detection."
"Dense prediction tasks are a fundamental class of problems in computer
vision. As supervised methods suffer from high pixel-wise labeling cost, a
few-shot learning solution that can learn any dense task from a few labeled
images is desired. Yet, current few-shot learning methods target a restricted
set of tasks such as semantic segmentation, presumably due to challenges in
designing a general and unified model that is able to flexibly and efficiently
adapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching
(VTM), a universal few-shot learner for arbitrary dense prediction tasks. It
employs non-parametric matching on patch-level embedded tokens of images and
labels that encapsulates all tasks. Also, VTM flexibly adapts to any task with
a tiny amount of task-specific parameters that modulate the matching algorithm.
We implement VTM as a powerful hierarchical encoder-decoder architecture
involving ViT backbones where token matching is performed at multiple feature
hierarchies. We experiment VTM on a challenging variant of Taskonomy dataset
and observe that it robustly few-shot learns various unseen dense prediction
tasks. Surprisingly, it is competitive with fully supervised baselines using
only 10 labeled examples of novel tasks (0.004% of full supervision) and
sometimes outperforms using 0.1% of full supervision. Codes are available at
https://github.com/GitGyun/visual_token_matching."
"Recent studies show that models trained by continual learning can achieve the
comparable performances as the standard supervised learning and the learning
flexibility of continual learning models enables their wide applications in the
real world. Deep learning models, however, are shown to be vulnerable to
adversarial attacks. Though there are many studies on the model robustness in
the context of standard supervised learning, protecting continual learning from
adversarial attacks has not yet been investigated. To fill in this research
gap, we are the first to study adversarial robustness in continual learning and
propose a novel method called \textbf{T}ask-\textbf{A}ware \textbf{B}oundary
\textbf{A}ugmentation (TABA) to boost the robustness of continual learning
models. With extensive experiments on CIFAR-10 and CIFAR-100, we show the
efficacy of adversarial training and TABA in defending adversarial attacks."
"The innovation of Wi-Fi 6, IEEE 802.11ax, was be approved as the next
sixth-generation (6G) technology of wireless local area networks (WLANs) by
improving the fundamental performance of latency, throughput, and so on. The
main technical feature of orthogonal frequency division multiple access (OFDMA)
supports multi-users to transmit respective data concurrently via the
corresponding access points (APs). However, the conventional IEEE 802.11
protocol for Wi-Fi roaming selects the target AP only depending on received
signal strength indication (RSSI) which is obtained by the received Response
frame from the APs. In the long term, it may lead to congestion in a single
channel under the scenarios of dense users further increasing the association
delay and packet drop rate, even reducing the quality of service (QoS) of the
overall system. In this paper, we propose a multi-agent deep Q-learning for
fast roaming (MADAR) algorithm to effectively minimize the latency during the
station roaming for Smart Warehouse in Wi-Fi 6 system. The MADAR algorithm
considers not only RSSI but also channel state information (CSI), and through
online neural network learning and weighting adjustments to maximize the reward
of the action selected from Epsilon-Greedy. Compared to existing benchmark
methods, the MADAR algorithm has been demonstrated for improved roaming latency
by analyzing the simulation result and realistic dataset."
"Contrastive language-image pretraining (CLIP) has demonstrated remarkable
success in various image tasks. However, how to extend CLIP with effective
temporal modeling is still an open and crucial problem. Existing factorized or
joint spatial-temporal modeling trades off between the efficiency and
performance. While modeling temporal information within straight through tube
is widely adopted in literature, we find that simple frame alignment already
provides enough essence without temporal attention. To this end, in this paper,
we proposed a novel Implicit Learnable Alignment (ILA) method, which minimizes
the temporal modeling effort while achieving incredibly high performance.
Specifically, for a frame pair, an interactive point is predicted in each
frame, serving as a mutual information rich region. By enhancing the features
around the interactive point, two frames are implicitly aligned. The aligned
features are then pooled into a single token, which is leveraged in the
subsequent spatial self-attention. Our method allows eliminating the costly or
insufficient temporal self-attention in video. Extensive experiments on
benchmarks demonstrate the superiority and generality of our module.
Particularly, the proposed ILA achieves a top-1 accuracy of 88.7% on
Kinetics-400 with much fewer FLOPs compared with Swin-L and ViViT-H. Code is
released at https://github.com/Francis-Rings/ILA ."
"The agent learns to organize decision behavior to achieve a behavioral goal,
such as reward maximization, and reinforcement learning is often used for this
optimization. Learning an optimal behavioral strategy is difficult under the
uncertainty that events necessary for learning are only partially observable,
called as Partially Observable Markov Decision Process (POMDP). However, the
real-world environment also gives many events irrelevant to reward delivery and
an optimal behavioral strategy. The conventional methods in POMDP, which
attempt to infer transition rules among the entire observations, including
irrelevant states, are ineffective in such an environment. Supposing
Redundantly Observable Markov Decision Process (ROMDP), here we propose a
method for goal-oriented reinforcement learning to efficiently learn state
transition rules among reward-related ""core states'' from redundant
observations. Starting with a small number of initial core states, our model
gradually adds new core states to the transition diagram until it achieves an
optimal behavioral strategy consistent with the Bellman equation. We
demonstrate that the resultant inference model outperforms the conventional
method for POMDP. We emphasize that our model only containing the core states
has high explainability. Furthermore, the proposed method suits online learning
as it suppresses memory consumption and improves learning speed."
"The widespread dissemination of toxic online posts is increasingly damaging
to society. However, research on detecting toxic language in Chinese has lagged
significantly. Existing datasets lack fine-grained annotation of toxic types
and expressions, and ignore the samples with indirect toxicity. In addition, it
is crucial to introduce lexical knowledge to detect the toxicity of posts,
which has been a challenge for researchers. In this paper, we facilitate the
fine-grained detection of Chinese toxic language. First, we built Monitor Toxic
Frame, a hierarchical taxonomy to analyze toxic types and expressions. Then, a
fine-grained dataset ToxiCN is presented, including both direct and indirect
toxic samples. We also build an insult lexicon containing implicit profanity
and propose Toxic Knowledge Enhancement (TKE) as a benchmark, incorporating the
lexical feature to detect toxic language. In the experimental stage, we
demonstrate the effectiveness of TKE. After that, a systematic quantitative and
qualitative analysis of the findings is given."
"We study the causal bandit problem that entails identifying a near-optimal
intervention from a specified set $A$ of (possibly non-atomic) interventions
over a given causal graph. Here, an optimal intervention in ${A}$ is one that
maximizes the expected value for a designated reward variable in the graph, and
we use the standard notion of simple regret to quantify near optimality.
Considering Bernoulli random variables and for causal graphs on $N$ vertices
with constant in-degree, prior work has achieved a worst case guarantee of
$\widetilde{O} (N/\sqrt{T})$ for simple regret. The current work utilizes the
idea of covering interventions (which are not necessarily contained within
${A}$) and establishes a simple regret guarantee of
$\widetilde{O}(\sqrt{N/T})$. Notably, and in contrast to prior work, our simple
regret bound depends only on explicit parameters of the problem instance. We
also go beyond prior work and achieve a simple regret guarantee for causal
graphs with unobserved variables. Further, we perform experiments to show
improvements over baselines in this setting."
"The Smatch metric is a popular method for evaluating graph distances, as is
necessary, for instance, to assess the performance of semantic graph parsing
systems. However, we observe some issues in the metric that jeopardize
meaningful evaluation. E.g., opaque pre-processing choices can affect results,
and current graph-alignment solvers do not provide us with upper-bounds.
Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore,
adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)
are spread out, and lack a unifying framework.
  For better inspection, we divide the metric into three modules:
pre-processing, alignment, and scoring. Examining each module, we specify its
goals and diagnose potential issues, for which we discuss and test mitigation
strategies. For pre-processing, we show how to fully conform to annotation
guidelines that allow structurally deviating but valid graphs. For safer and
enhanced alignment, we show the feasibility of optimal alignment in a standard
evaluation setup, and develop a lossless graph compression method that shrinks
the search space and significantly increases efficiency. For improved scoring,
we propose standardized and extended metric calculation of fine-grained
sub-graph meaning aspects. Our code is available at
https://github.com/flipz357/smatchpp"
"We propose the GFlowNets with Human Feedback (GFlowHF) framework to improve
the exploration ability when training AI models. For tasks where the reward is
unknown, we fit the reward function through human evaluations on different
trajectories. The goal of GFlowHF is to learn a policy that is strictly
proportional to human ratings, instead of only focusing on human favorite
ratings like RLHF. Experiments show that GFlowHF can achieve better exploration
ability than RLHF."
"Large language models often necessitate grounding on external knowledge to
generate faithful and reliable answers. Yet even with the correct groundings in
the reference, they can ignore them and rely on wrong groundings or their
inherent biases to hallucinate when users, being largely unaware of the
specifics of the stored information, pose questions that might not directly
correlate with the retrieved groundings. In this work, we formulate this
knowledge alignment problem and introduce MixAlign, a framework that interacts
with both the human user and the knowledge base to obtain and integrate
clarifications on how the user question relates to the stored information.
MixAlign employs a language model to achieve automatic knowledge alignment and,
if necessary, further enhances this alignment through human user
clarifications. Experimental results highlight the crucial role of knowledge
alignment in boosting model performance and mitigating hallucination, with
improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the
effectiveness of MixAlign in improving knowledge alignment by producing
high-quality, user-centered clarifications."
"Large Language Models (LLMs) are claimed to be capable of Natural Language
Inference (NLI), necessary for applied tasks like question answering and
summarization. We present a series of behavioral studies on several LLM
families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled
experiments. We establish two biases originating from pretraining which predict
much of their behavior, and show that these are major sources of hallucination
in generative LLMs. First, memorization at the level of sentences: we show
that, regardless of the premise, models falsely label NLI test samples as
entailing when the hypothesis is attested in training data, and that entities
are used as ``indices'' to access the memorized data. Second, statistical
patterns of usage learned at the level of corpora: we further show a similar
effect when the premise predicate is less frequent than that of the hypothesis
in the training data, a bias following from previous studies. We demonstrate
that LLMs perform significantly worse on NLI test samples which do not conform
to these biases than those which do, and we offer these as valuable controls
for future LLM evaluation."
"Most existing stylistic text rewriting methods and evaluation metrics operate
on a sentence level, but ignoring the broader context of the text can lead to
preferring generic, ambiguous, and incoherent rewrites. In this paper, we
investigate integrating the preceding textual context into both the
$\textit{rewriting}$ and $\textit{evaluation}$ stages of stylistic text
rewriting, and introduce a new composite contextual evaluation metric
$\texttt{CtxSimFit}$ that combines similarity to the original sentence with
contextual cohesiveness. We comparatively evaluate non-contextual and
contextual rewrites in formality, toxicity, and sentiment transfer tasks. Our
experiments show that humans significantly prefer contextual rewrites as more
fitting and natural over non-contextual ones, yet existing sentence-level
automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences
($\rho$=0--0.3). In contrast, human preferences are much better reflected by
both our novel $\texttt{CtxSimFit}$ ($\rho$=0.7--0.9) as well as proposed
context-infused versions of common metrics ($\rho$=0.4--0.7). Overall, our
findings highlight the importance of integrating context into the generation
and especially the evaluation stages of stylistic text rewriting."
"Automatic literature review generation is one of the most challenging tasks
in natural language processing. Although large language models have tackled
literature review generation, the absence of large-scale datasets has been a
stumbling block to the progress. We release SciReviewGen, consisting of over
10,000 literature reviews and 690,000 papers cited in the reviews. Based on the
dataset, we evaluate recent transformer-based summarization models on the
literature review generation task, including Fusion-in-Decoder extended for
literature review generation. Human evaluation results show that some
machine-generated summaries are comparable to human-written reviews, while
revealing the challenges of automatic literature review generation such as
hallucinations and a lack of detailed information. Our dataset and code are
available at https://github.com/tetsu9923/SciReviewGen."
"Language-based colorization produces plausible and visually pleasing colors
under the guidance of user-friendly natural language descriptions. Previous
methods implicitly assume that users provide comprehensive color descriptions
for most of the objects in the image, which leads to suboptimal performance. In
this paper, we propose a unified model to perform language-based colorization
with any-level descriptions. We leverage the pretrained cross-modality
generative model for its robust language understanding and rich color priors to
handle the inherent ambiguity of any-level descriptions. We further design
modules to align with input conditions to preserve local spatial structures and
prevent the ghosting effect. With the proposed novel sampling strategy, our
model achieves instance-aware colorization in diverse and complex scenarios.
Extensive experimental results demonstrate our advantages of effectively
handling any-level descriptions and outperforming both language-based and
automatic colorization methods. The code and pretrained models are available
at: https://github.com/changzheng123/L-CAD."
"Unsupervised sentence representation learning is one of the fundamental
problems in natural language processing with various downstream applications.
Recently, contrastive learning has been widely adopted which derives
high-quality sentence representations by pulling similar semantics closer and
pushing dissimilar ones away. However, these methods fail to capture the
fine-grained ranking information among the sentences, where each sentence is
only treated as either positive or negative. In many real-world scenarios, one
needs to distinguish and rank the sentences based on their similarities to a
query sentence, e.g., very relevant, moderate relevant, less relevant,
irrelevant, etc. In this paper, we propose a novel approach, RankCSE, for
unsupervised sentence representation learning, which incorporates ranking
consistency and ranking distillation with contrastive learning into a unified
framework. In particular, we learn semantically discriminative sentence
representations by simultaneously ensuring ranking consistency between two
representations with different dropout masks, and distilling listwise ranking
knowledge from the teacher. An extensive set of experiments are conducted on
both semantic textual similarity (STS) and transfer (TR) tasks. Experimental
results demonstrate the superior performance of our approach over several
state-of-the-art baselines."
"We can usually assume others have goals analogous to our own. This assumption
can also, at times, be applied to multi-agent games - e.g. Agent 1's attraction
to green pellets is analogous to Agent 2's attraction to red pellets. This
""analogy"" assumption is tied closely to the cognitive process known as empathy.
Inspired by empathy, we design a simple and explainable architecture to model
another agent's action-value function. This involves learning an ""Imagination
Network"" to transform the other agent's observed state in order to produce a
human-interpretable ""empathetic state"" which, when presented to the learning
agent, produces behaviours that mimic the other agent. Our approach is
applicable to multi-agent scenarios consisting of a single learning agent and
other (independent) agents acting according to fixed policies. This
architecture is particularly beneficial for (but not limited to) algorithms
using a composite value or reward function. We show our method produces better
performance in multi-agent games, where it robustly estimates the other's model
in different environment configurations. Additionally, we show that the
empathetic states are human interpretable, and thus verifiable."
"In quality control, microstructures are investigated rigorously to ensure
structural integrity, exclude the presence of critical volume defects, and
validate the formation of the target microstructure. For quenched,
hierarchically-structured steels, the morphology of the bainitic and
martensitic microstructures are of major concern to guarantee the reliability
of the material under service conditions. Therefore, industries conduct small
sample-size inspections of materials cross-sections through metallographers to
validate the needle morphology of such microstructures. We demonstrate
round-robin test results revealing that this visual grading is afflicted by
pronounced subjectivity despite the thorough training of personnel. Instead, we
propose a deep learning image classification approach that distinguishes steels
based on their microstructure type and classifies their needle length alluding
to the ISO 643 grain size assessment standard. This classification approach
facilitates the reliable, objective, and automated classification of
hierarchically structured steels. Specifically, an accuracy of 96% and roughly
91% is attained for the distinction of martensite/bainite subtypes and needle
length, respectively. This is achieved on an image dataset that contains
significant variance and labeling noise as it is acquired over more than ten
years from multiple plants, alloys, etchant applications, and light optical
microscopes by many metallographers (raters). Interpretability analysis gives
insights into the decision-making of these models and allows for estimating
their generalization capability."
"To enhance the generalization ability of the model and improve the
effectiveness of the transformer for named entity recognition tasks, the
XLNet-Transformer-R model is proposed in this paper. The XLNet pre-trained
model and the Transformer encoder with relative positional encodings are
combined to enhance the model's ability to process long text and learn
contextual information to improve robustness. To prevent overfitting, the
R-Drop structure is used to improve the generalization capability and enhance
the accuracy of the model in named entity recognition tasks. The model in this
paper performs ablation experiments on the MSRA dataset and comparison
experiments with other models on four datasets with excellent performance,
demonstrating the strategic effectiveness of the XLNet-Transformer-R model."
"Text-adventure games and text role-playing games are grand challenges for
reinforcement learning game playing agents. Text role-playing games are
open-ended environments where an agent must faithfully play a particular
character. We consider the distinction between characters and actors, where an
actor agent has the ability to play multiple characters. We present a framework
we call a thespian agent that can learn to emulate multiple characters along
with a soft prompt that can be used to direct it as to which character to play
at any time. We further describe an attention mechanism that allows the agent
to learn new characters that are based on previously learned characters in a
few-shot fashion. We show that our agent outperforms the state of the art agent
framework in multi-character learning and few-shot learning."
"As a subset of machine learning, meta-learning, or learning to learn, aims at
improving the model's capabilities by employing prior knowledge and experience.
A meta-learning paradigm can appropriately tackle the conventional challenges
of traditional learning approaches, such as insufficient number of samples,
domain shifts, and generalization. These unique characteristics position
meta-learning as a suitable choice for developing influential solutions in
various healthcare contexts, where the available data is often insufficient,
and the data collection methodologies are different. This survey discusses
meta-learning broad applications in the healthcare domain to provide insight
into how and where it can address critical healthcare challenges. We first
describe the theoretical foundations and pivotal methods of meta-learning. We
then divide the employed meta-learning approaches in the healthcare domain into
two main categories of multi/single-task learning and many/few-shot learning
and survey the studies. Finally, we highlight the current challenges in
meta-learning research, discuss the potential solutions, and provide future
perspectives on meta-learning in healthcare."
"Assurance cases can be used to argue for the safety of products in safety
engineering. In safety-critical areas, the construction of assurance cases is
indispensable. Trustworthiness Derivation Trees (TDTs) enhance assurance cases
by incorporating formal methods, rendering it possible for automatic reasoning
about assurance cases. We present Trustworthiness Derivation Tree Analyzer
(Trusta), a desktop application designed to automatically construct and verify
TDTs. The tool has a built-in Prolog interpreter in its backend, and is
supported by the constraint solvers Z3 and MONA. Therefore, it can solve
constraints about logical formulas involving arithmetic, sets, Horn clauses
etc. Trusta also utilizes large language models to make the creation and
evaluation of assurance cases more convenient. It allows for interactive human
examination and modification. We evaluated top language models like
ChatGPT-3.5, ChatGPT-4, and PaLM 2 for generating assurance cases. Our tests
showed a 50%-80% similarity between machine-generated and human-created cases.
In addition, Trusta can extract formal constraints from text in natural
languages, facilitating an easier interpretation and validation process. This
extraction is subject to human review and correction, blending the best of
automated efficiency with human insight. To our knowledge, this marks the first
integration of large language models in automatic creating and reasoning about
assurance cases, bringing a novel approach to a traditional challenge. Through
several industrial case studies, Trusta has proven to quickly find some subtle
issues that are typically missed in manual inspection, demonstrating its
practical value in enhancing the assurance case development process."
"Integrating first-order logic constraints (FOLCs) with neural networks is a
crucial but challenging problem since it involves modeling intricate
correlations to satisfy the constraints. This paper proposes a novel neural
layer, LogicMP, whose layers perform mean-field variational inference over an
MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs
while retaining modularity and efficiency. By exploiting the structure and
symmetries in MLNs, we theoretically demonstrate that our well-designed,
efficient mean-field iterations effectively mitigate the difficulty of MLN
inference, reducing the inference from sequential calculation to a series of
parallel tensor operations. Empirical results in three kinds of tasks over
graphs, images, and text show that LogicMP outperforms advanced competitors in
both performance and efficiency."
"Large language models (LLMs) have demonstrated impressive reasoning abilities
in complex tasks. However, they lack up-to-date knowledge and experience
hallucinations during reasoning, which can lead to incorrect reasoning
processes and diminish their performance and trustworthiness. Knowledge graphs
(KGs), which capture vast amounts of facts in a structured format, offer a
reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM
reasoning methods only treat KGs as factual knowledge bases and overlook the
importance of their structural information for reasoning. In this paper, we
propose a novel method called reasoning on graphs (RoG) that synergizes LLMs
with KGs to enable faithful and interpretable reasoning. Specifically, we
present a planning-retrieval-reasoning framework, where RoG first generates
relation paths grounded by KGs as faithful plans. These plans are then used to
retrieve valid reasoning paths from the KGs for LLMs to conduct faithful
reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the
reasoning ability of LLMs through training but also allows seamless integration
with any arbitrary LLMs during inference. Extensive experiments on two
benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art
performance on KG reasoning tasks and generates faithful and interpretable
reasoning results."
"Graphs are ubiquitous due to their flexibility in representing social and
technological systems as networks of interacting elements. Graph representation
learning methods, such as node embeddings, are powerful approaches to map nodes
into a latent vector space, allowing their use for various graph tasks. Despite
their success, only few studies have focused on explaining node embeddings
locally. Moreover, global explanations of node embeddings remain unexplored,
limiting interpretability and debugging potentials. We address this gap by
developing human-understandable explanations for dimensions in node embeddings.
Towards that, we first develop new metrics that measure the global
interpretability of embedding vectors based on the marginal contribution of the
embedding dimensions to predicting graph structure. We say that an embedding
dimension is more interpretable if it can faithfully map to an understandable
sub-structure in the input graph - like community structure. Having observed
that standard node embeddings have low interpretability, we then introduce DINE
(Dimension-based Interpretable Node Embedding), a novel approach that can
retrofit existing node embeddings by making them more interpretable without
sacrificing their task performance. We conduct extensive experiments on
synthetic and real-world graphs and show that we can simultaneously learn
highly interpretable node embeddings with effective performance in link
prediction."
"Task-oriented dialogue systems (TODS) have become crucial for users to
interact with machines and computers using natural language. One of its key
components is the dialogue manager, which guides the conversation towards a
good goal for the user by providing the best possible response. Previous works
have proposed rule-based systems (RBS), reinforcement learning (RL), and
supervised learning (SL) as solutions for the correct dialogue management; in
other words, select the best response given input by the user. However, this
work argues that the leading cause of DMs not achieving maximum performance
resides in the quality of the datasets rather than the models employed thus
far; this means that dataset errors, like mislabeling, originate a large
percentage of failures in dialogue management. We studied the main errors in
the most widely used datasets, Multiwoz 2.1 and SGD, to demonstrate this
hypothesis. To do this, we have designed a synthetic dialogue generator to
fully control the amount and type of errors introduced in the dataset. Using
this generator, we demonstrated that errors in the datasets contribute
proportionally to the performance of the models"
"As innovation in deep learning continues, many engineers seek to adopt
Pre-Trained Models (PTMs) as components in computer systems. Researchers
publish PTMs, which engineers adapt for quality or performance prior to
deployment. PTM authors should choose appropriate names for their PTMs, which
would facilitate model discovery and reuse. However, prior research has
reported that model names are not always well chosen - and are sometimes
erroneous. The naming for PTM packages has not been systematically studied.
  In this paper, we frame and conduct the first empirical investigation of PTM
naming practices in the Hugging Face PTM registry. We initiated our study with
a survey of 108 Hugging Face users to understand the practices in PTM naming.
From our survey analysis, we highlight discrepancies from traditional software
package naming, and present findings on naming practices. Our findings indicate
there is a great mismatch between engineers' preferences and practical
practices of PTM naming. We also present practices on detecting naming
anomalies and introduce a novel automated DNN ARchitecture Assessment technique
(DARA), capable of detecting PTM naming anomalies. We envision future works on
leveraging meta-features of PTMs to improve model reuse and trustworthiness."
"We present a novel method for intraoperative patient-to-image registration by
learning Expected Appearances. Our method uses preoperative imaging to
synthesize patient-specific expected views through a surgical microscope for a
predicted range of transformations. Our method estimates the camera pose by
minimizing the dissimilarity between the intraoperative 2D view through the
optical microscope and the synthesized expected texture. In contrast to
conventional methods, our approach transfers the processing tasks to the
preoperative stage, reducing thereby the impact of low-resolution, distorted,
and noisy intraoperative images, that often degrade the registration accuracy.
We applied our method in the context of neuronavigation during brain surgery.
We evaluated our approach on synthetic data and on retrospective data from 6
clinical cases. Our method outperformed state-of-the-art methods and achieved
accuracies that met current clinical standards."
"Robotic technology has been widely used in nowadays society, which has made
great progress in various fields such as agriculture, manufacturing and
entertainment. In this paper, we focus on the topic of drumming robots in
entertainment. To this end, we introduce an improving drumming robot that can
automatically complete music transcription based on the popular vision
transformer network based on the attention mechanism. Equipped with the
attention transformer network, our method can efficiently handle the sequential
audio embedding input and model their global long-range dependencies. Massive
experimental results demonstrate that the improving algorithm can help the
drumming robot promote drum classification performance, which can also help the
robot to enjoy a variety of smart applications and services."
"A significant amount of protein function requires binding small molecules,
including enzymatic catalysis. As such, designing binding pockets for small
molecules has several impactful applications ranging from drug synthesis to
energy storage. Towards this goal, we first develop HarmonicFlow, an improved
generative process over 3D protein-ligand binding structures based on our
self-conditioned flow matching objective. FlowSite extends this flow model to
jointly generate a protein pocket's discrete residue types and the molecule's
binding 3D structure. We show that HarmonicFlow improves upon state-of-the-art
generative processes for docking in simplicity, generality, and average sample
quality in pocket-level docking. Enabled by this structure modeling, FlowSite
designs binding sites substantially better than baseline approaches."
"We simulate behaviour of two independent reinforcement learning algorithms
playing the Crawford and Sobel (1982) game of strategic information
transmission. We adopt memoryless algorithms to capture learning in a static
game where a large population interacts anonymously. We show that sender and
receiver converge to Nash equilibrium play. The level of informativeness of the
sender's cheap talk decreases as the bias increases and, at intermediate level
of the bias, it matches the level predicted by the Pareto optimal equilibrium
or by the second best one. Conclusions are robust to alternative specifications
of the learning hyperparameters and of the game."
"The generative pre-trained transformer (GPT)-based chatbot software ChatGPT
possesses excellent natural language processing capabilities but is inadequate
for solving arithmetic problems, especially multiplication. Its GPT structure
uses a computational graph for multiplication, which has limited accuracy
beyond simple multiplication operations. We developed a graph-based
multiplication algorithm that emulated human-like numerical operations by
incorporating a 10k operator, where k represents the maximum power to base 10
of the larger of two input numbers. Our proposed algorithm attained 100%
accuracy for 1,000,000 large number multiplication tasks, effectively solving
the multiplication challenge of GPT-based and other large language models. Our
work highlights the importance of blending simple human insights into the
design of artificial intelligence algorithms. Keywords: Graph-based
multiplication; ChatGPT; Multiplication problem"
"Despite Multi-modal Large Language Models (MM-LLMs) have made exciting
strides recently, they are still struggling to efficiently model the
interactions among multi-modal inputs and the generation in non-textual
modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an
approach to treat the input from any modality as a token sequence and learn a
joint embedding space for all modalities. Specifically, for the input from any
modality, TEAL first discretizes it into a token sequence with the
off-the-shelf tokenizer and embeds the token sequence into a joint embedding
space with a learnable embedding matrix. MM-LLMs just need to predict the
multi-modal tokens autoregressively as the textual LLMs do. Finally, the
corresponding de-tokenizer is applied to generate the output in each modality
based on the predicted token sequence. With the joint embedding space, TEAL
enables the frozen LLMs to perform both understanding and generation tasks
involving non-textual modalities, such as image and audio. Thus, the textual
LLM can just work as an interface and maintain its high performance in textual
understanding and generation. Experiments show that TEAL achieves substantial
improvements in multi-modal understanding, and implements a simple scheme for
multi-modal generations."
"Deep learning techniques have greatly enhanced the performance of fire
detection in videos. However, video-based fire detection models heavily rely on
labeled data, and the process of data labeling is particularly costly and
time-consuming, especially when dealing with videos. Considering the limited
quantity of labeled video data, we propose a semi-supervised fire detection
model called FireMatch, which is based on consistency regularization and
adversarial distribution alignment. Specifically, we first combine consistency
regularization with pseudo-label. For unlabeled data, we design video data
augmentation to obtain corresponding weakly augmented and strongly augmented
samples. The proposed model predicts weakly augmented samples and retains
pseudo-label above a threshold, while training on strongly augmented samples to
predict these pseudo-labels for learning more robust feature representations.
Secondly, we generate video cross-set augmented samples by adversarial
distribution alignment to expand the training data and alleviate the decline in
classification performance caused by insufficient labeled data. Finally, we
introduce a fairness loss to help the model produce diverse predictions for
input samples, thereby addressing the issue of high confidence with the
non-fire class in fire classification scenarios. The FireMatch achieved an
accuracy of 76.92% and 91.81% on two real-world fire datasets, respectively.
The experimental results demonstrate that the proposed method outperforms the
current state-of-the-art semi-supervised classification methods."
"The growing awareness of safety concerns in large language models (LLMs) has
sparked considerable interest in the evaluation of safety. This study
investigates an under-explored issue about the evaluation of LLMs, namely the
substantial discrepancy in performance between multiple-choice questions and
open-ended questions. Inspired by research on jailbreak attack patterns, we
argue this is caused by mismatched generalization. That is, LLM only remembers
the answer style for open-ended safety questions, which makes it unable to
solve other forms of safety tests. We refer to this phenomenon as fake
alignment and construct a comparative benchmark to empirically verify its
existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework
and two novel metrics--Consistency Score (CS) and Consistent Safety Score
(CSS), which jointly assess two complementary forms of evaluation to quantify
fake alignment and obtain corrected performance estimation. Applying FINE to 14
widely-used LLMs reveals several models with purported safety are poorly
aligned in practice. Subsequently, we found that multiple-choice format data
can also be used as high-quality contrast distillation-based fine-tuning data,
which can strongly improve the alignment consistency of LLMs with minimal
fine-tuning overhead. For data and code, see
https://github.com/AIFlames/Fake-Alignment."
"In Cyber-Physical Systems (CPS) research, anomaly detection (detecting
abnormal behavior) and diagnosis (identifying the underlying root cause) are
often treated as distinct, isolated tasks. However, diagnosis algorithms
require symptoms, i.e. temporally and spatially isolated anomalies, as input.
Thus, anomaly detection and diagnosis must be developed together to provide a
holistic solution for diagnosis in CPS. We therefore propose a method for
utilizing deep learning-based anomaly detection to generate inputs for
Consistency-Based Diagnosis (CBD). We evaluate our approach on a simulated and
a real-world CPS dataset, where our model demonstrates strong performance
relative to other state-of-the-art models."
"Soil organic carbon (SOC) plays a pivotal role in the global carbon cycle,
impacting climate dynamics and necessitating accurate estimation for
sustainable land and agricultural management. While traditional methods of SOC
estimation face resolution and accuracy challenges, recent technological
solutions harness remote sensing, machine learning, and high-resolution
satellite mapping. Graph Neural Networks (GNNs), especially when integrated
with positional encoders, can capture complex relationships between soil and
climate. Using the LUCAS database, this study compared four GNN operators in
the positional encoder framework. Results revealed that the PESAGE and
PETransformer models outperformed others in SOC estimation, indicating their
potential in capturing the complex relationship between SOC and climate
features. Our findings confirm the feasibility of applications of GNN
architectures in SOC prediction, establishing a framework for future
explorations of this topic with more advanced GNN models."
"The COVID-19 pandemic has severely disrupted the retail landscape and has
accelerated the adoption of innovative technologies. A striking example relates
to the proliferation of online grocery orders and the technology deployed to
facilitate such logistics. In fact, for many retailers, this disruption was a
wake-up call after which they started recognizing the power of data analytics
and artificial intelligence (AI). In this article, we discuss the opportunities
that AI can offer to retailers in the new normal retail landscape. Some of the
techniques described have been applied at scale to adapt previously deployed AI
models, whereas in other instances, fresh solutions needed to be developed to
help retailers cope with recent disruptions, such as unexpected panic buying,
retraining predictive models, and leveraging online-offline synergies."
"This paper examines the capacity of LLMs to reason with knowledge graphs
using their internal knowledge graph, i.e., the knowledge graph they learned
during pre-training. Two research questions are formulated to investigate the
accuracy of LLMs in recalling information from pre-training knowledge graphs
and their ability to infer knowledge graph relations from context. To address
these questions, we employ LLMs to perform four distinct knowledge graph
reasoning tasks. Furthermore, we identify two types of hallucinations that may
occur during knowledge reasoning with LLMs: content and ontology hallucination.
Our experimental results demonstrate that LLMs can successfully tackle both
simple and complex knowledge graph reasoning tasks from their own memory, as
well as infer from input context."
"Normalization techniques have been widely used in the field of deep learning
due to their capability of enabling higher learning rates and are less careful
in initialization. However, the effectiveness of popular normalization
technologies is typically limited to specific areas. Unlike the standard Batch
Normalization (BN) and Layer Normalization (LN), where BN computes the mean and
variance along the (N,H,W) dimensions and LN computes the mean and variance
along the (C,H,W) dimensions (N, C, H and W are the batch, channel, spatial
height and width dimension, respectively), this paper presents a novel
normalization technique called Batch Channel Normalization (BCN). To exploit
both the channel and batch dependence and adaptively and combine the advantages
of BN and LN based on specific datasets or tasks, BCN separately normalizes
inputs along the (N, H, W) and (C, H, W) axes, then combines the normalized
outputs based on adaptive parameters. As a basic block, BCN can be easily
integrated into existing models for various applications in the field of
computer vision. Empirical results show that the proposed technique can be
seamlessly applied to various versions of CNN or Vision Transformer
architecture. The code is publicly available at
https://github.com/AfifaKhaled/BatchChannel-Normalization"
"An increasing number of studies use gender information to understand
phenomena such as gender bias, inequity in access and participation, or the
impact of the Covid pandemic response. Unfortunately, most datasets do not
include self-reported gender information, making it necessary for researchers
to infer gender from other information, such as names or names and country
information. An important limitation of these tools is that they fail to
appropriately capture the fact that gender exists on a non-binary scale,
however, it remains important to evaluate and compare how well these tools
perform in a variety of contexts. In this paper, we compare the performance of
a generative Artificial Intelligence (AI) tool ChatGPT with three commercially
available list-based and machine learning-based gender inference tools (Namsor,
Gender-API, and genderize.io) on a unique dataset. Specifically, we use a large
Olympic athlete dataset and report how variations in the input (e.g., first
name and first and last name, with and without country information) impact the
accuracy of their predictions. We report results for the full set, as well as
for the subsets: medal versus non-medal winners, athletes from the largest
English-speaking countries, and athletes from East Asia. On these sets, we find
that Namsor is the best traditional commercially available tool. However,
ChatGPT performs at least as well as Namsor and often outperforms it,
especially for the female sample when country and/or last name information is
available. All tools perform better on medalists versus non-medalists and on
names from English-speaking countries. Although not designed for this purpose,
ChatGPT may be a cost-effective tool for gender prediction. In the future, it
might even be possible for ChatGPT or other large scale language models to
better identify self-reported gender rather than report gender on a binary
scale."
"The large language models represented by ChatGPT have a disruptive impact on
the field of artificial intelligence. But it mainly focuses on natural language
processing, speech recognition, machine learning and natural language
understanding. This paper innovatively applies the large language model to the
field of intelligent decision-making, places the large language model in the
decision-making center, and constructs an agent architecture with the large
language model as the core. Based on this, it further proposes a two-layer
agent task planning, issues and executes decision commands through the
interaction of natural language, and carries out simulation verification
through the wargame simulation environment. Through the game confrontation
simulation experiment, it is found that the intelligent decision-making ability
of the large language model is significantly stronger than the commonly used
reinforcement learning AI and rule AI, and the intelligence, understandability
and generalization are all better. And through experiments, it was found that
the intelligence of the large language model is closely related to prompt. This
work also extends the large language model from previous human-computer
interaction to the field of intelligent decision-making, which has important
reference value and significance for the development of intelligent
decision-making."
"Explainable Artificial Intelligence (XAI) is widely regarding as a
cornerstone of trustworthy AI. Unfortunately, most work on XAI offers no
guarantees of rigor. In high-stakes domains, e.g. uses of AI that impact
humans, the lack of rigor of explanations can have disastrous consequences.
Formal abductive explanations offer crucial guarantees of rigor and so are of
interest in high-stakes uses of machine learning (ML). One drawback of
abductive explanations is explanation size, justified by the cognitive limits
of human decision-makers. Probabilistic abductive explanations (PAXps) address
this limitation, but their theoretical and practical complexity makes their
exact computation most often unrealistic. This paper proposes novel efficient
algorithms for the computation of locally-minimal PXAps, which offer
high-quality approximations of PXAps in practice. The experimental results
demonstrate the practical efficiency of the proposed algorithms."
"The advent of foundation models (FMs) as an emerging suite of AI techniques
has struck a wave of opportunities in computational healthcare. The interactive
nature of these models, guided by pre-training data and human instructions, has
ignited a data-centric AI paradigm that emphasizes better data
characterization, quality, and scale. In healthcare AI, obtaining and
processing high-quality clinical data records has been a longstanding
challenge, ranging from data quantity, annotation, patient privacy, and ethics.
In this survey, we investigate a wide range of data-centric approaches in the
FM era (from model pre-training to inference) towards improving the healthcare
workflow. We discuss key perspectives in AI security, assessment, and alignment
with human values. Finally, we offer a promising outlook of FM-based analytics
to enhance the performance of patient outcome and clinical workflow in the
evolving landscape of healthcare and medicine. We provide an up-to-date list of
healthcare-related foundation models and datasets at
https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare ."
"This survey delves into the application of diffusion models in time-series
forecasting. Diffusion models are demonstrating state-of-the-art results in
various fields of generative AI. The paper includes comprehensive background
information on diffusion models, detailing their conditioning methods and
reviewing their use in time-series forecasting. The analysis covers 11 specific
time-series implementations, the intuition and theory behind them, the
effectiveness on different datasets, and a comparison among each other. Key
contributions of this work are the thorough exploration of diffusion models'
applications in time-series forecasting and a chronologically ordered overview
of these models. Additionally, the paper offers an insightful discussion on the
current state-of-the-art in this domain and outlines potential future research
directions. This serves as a valuable resource for researchers in AI and
time-series analysis, offering a clear view of the latest advancements and
future potential of diffusion models."
"The present study puts forward a novel biographical knowledge graph (KG) on
Prof. S. R. Ranganathan, one of the pioneering figures in the Library and
Information Science (LIS) domain. It has been found that most of the relevant
facts about Ranganathan exist in a variety of resources (e.g., books, essays,
journal articles, websites, blogs, etc.), offering information in a fragmented
and piecemeal way. With this dedicated KG (henceforth known as RKG), we hope to
furnish a 360-degree view of his life and achievements. To the best of our
knowledge, such a dedicated representation is unparalleled in its scope and
coverage: using state-of-the-art technology for anyone to openly access,
use/re-use, and contribute. Inspired by Ranganathan's theories and ideas, the
KG was developed using a ""facet-based methodology"" at two levels: in the
identification of the vital biographical aspects and the development of the
ontological model. Finally, with this study, we call for a community-driven
effort to enhance the KG and pay homage to the Father of Library Science on the
hundredth anniversary of his revitalizing the LIS domain through his enduring
participation."
"Critique, as a natural language description for assessing the quality of
model-generated content, has played a vital role in the training, evaluation,
and refinement of LLMs. However, a systematic method to evaluate the quality of
critique is lacking. In this paper, we pioneer the critique of critique, termed
MetaCritique, which builds specific quantification criteria. To achieve a
reliable evaluation outcome, we propose Atomic Information Units (AIUs), which
describe the critique in a more fine-grained manner. MetaCritique aggregates
each AIU's judgment for the overall score. Moreover, MetaCritique delivers a
natural language rationale for the intricate reasoning within each judgment.
Lastly, we construct a meta-evaluation dataset covering 4 tasks across 16
public datasets involving human-written and LLM-generated critiques.
Experiments demonstrate that MetaCritique can achieve near-human performance.
Our study can facilitate future research in LLM critiques based on our
following observations and released resources: (1) superior critiques judged by
MetaCritique can lead to better refinements, indicating that it can potentially
enhance the alignment of existing LLMs; (2) the leaderboard of critique models
reveals that open-source critique models commonly suffer from factuality
issues; (3) relevant code and data are publicly available at
https://github.com/GAIR-NLP/MetaCritique to support deeper exploration; (4) an
API at PyPI with the usage documentation in Appendix C allows users to assess
the critique conveniently."
"We study universal deepfake detection. Our goal is to detect synthetic images
from a range of generative AI approaches, particularly from emerging ones which
are unseen during training of the deepfake detector. Universal deepfake
detection requires outstanding generalization capability. Motivated by recently
proposed masked image modeling which has demonstrated excellent generalization
in self-supervised pre-training, we make the first attempt to explore masked
image modeling for universal deepfake detection. We study spatial and frequency
domain masking in training deepfake detectors. Based on empirical analysis, we
propose a novel deepfake detector via frequency masking. Our focus on frequency
domain is different from the majority, which primarily target spatial domain
detection. Our comparative analyses reveal substantial performance gains over
existing methods. Code and models are publicly available."
"Statutory reasoning refers to the application of legislative provisions to a
series of case facts described in natural language. We re-frame statutory
reasoning as an analogy task, where each instance of the analogy task involves
a combination of two instances of statutory reasoning. This increases the
dataset size by two orders of magnitude, and introduces an element of
interpretability. We show that this task is roughly as difficult to Natural
Language Processing models as the original task. Finally, we come back to
statutory reasoning, solving it with a combination of a retrieval mechanism and
analogy models, and showing some progress on prior comparable work."
"The recent surge in jailbreaking attacks has revealed significant
vulnerabilities in Large Language Models (LLMs) when exposed to malicious
inputs. While various defense strategies have been proposed to mitigate these
threats, there has been limited research into the underlying mechanisms that
make LLMs vulnerable to such attacks. In this study, we suggest that the
self-safeguarding capability of LLMs is linked to specific activity patterns
within their representation space. Although these patterns have little impact
on the semantic content of the generated text, they play a crucial role in
shaping LLM behavior under jailbreaking attacks. Our findings demonstrate that
these patterns can be detected with just a few pairs of contrastive queries.
Extensive experimentation shows that the robustness of LLMs against
jailbreaking can be manipulated by weakening or strengthening these patterns.
Further visual analysis provides additional evidence for our conclusions,
providing new insights into the jailbreaking phenomenon. These findings
highlight the importance of addressing the potential misuse of open-source LLMs
within the community."
"We posit that to achieve superhuman agents, future models require superhuman
feedback in order to provide an adequate training signal. Current approaches
commonly train reward models from human preferences, which may then be
bottlenecked by human performance level, and secondly these separate frozen
reward models cannot then learn to improve during LLM training. In this work,
we study Self-Rewarding Language Models, where the language model itself is
used via LLM-as-a-Judge prompting to provide its own rewards during training.
We show that during Iterative DPO training that not only does instruction
following ability improve, but also the ability to provide high-quality rewards
to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a
model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,
including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still
to explore, this work opens the door to the possibility of models that can
continually improve in both axes."
"Knowledge Organization (KO) and Knowledge Representation (KR) have been the
two mainstream methodologies of knowledge modelling in the Information Science
community and the Artificial Intelligence community, respectively. The
facet-analytical tradition of KO has developed an exhaustive set of guiding
canons for ensuring quality in organising and managing knowledge but has
remained limited in terms of technology-driven activities to expand its scope
and services beyond the bibliographic universe of knowledge. KR, on the other
hand, boasts of a robust ecosystem of technologies and technology-driven
service design which can be tailored to model any entity or scale to any
service in the entire universe of knowledge. This paper elucidates both the
facet-analytical KO and KR methodologies in detail and provides a functional
mapping between them. Out of the mapping, the paper proposes an integrated
KR-enriched KO methodology with all the standard components of a KO methodology
plus the advanced technologies provided by the KR approach. The practical
benefits of the methodological integration has been exemplified through the
flagship application of the Digital University at the University of Trento,
Italy."
"The pursue of what are properties that can be identified to permit an
automated reasoning program to generate and find new and interesting theorems
is an interesting research goal (pun intended). The automatic discovery of new
theorems is a goal in itself, and it has been addressed in specific areas, with
different methods. The separation of the ""weeds"", uninteresting, trivial facts,
from the ""wheat"", new and interesting facts, is much harder, but is also being
addressed by different authors using different approaches. In this paper we
will focus on geometry. We present and discuss different approaches for the
automatic discovery of geometric theorems (and properties), and different
metrics to find the interesting theorems among all those that were generated.
After this description we will introduce the first result of this article: an
undecidability result proving that having an algorithmic procedure that decides
for every possible Turing Machine that produces theorems, whether it is able to
produce also interesting theorems, is an undecidable problem. Consequently, we
will argue that judging whether a theorem prover is able to produce interesting
theorems remains a non deterministic task, at best a task to be addressed by
program based in an algorithm guided by heuristics criteria. Therefore, as a
human, to satisfy this task two things are necessary: an expert survey that
sheds light on what a theorem prover/finder of interesting geometric theorems
is, and - to enable this analysis - other surveys that clarify metrics and
approaches related to the interestingness of geometric theorems. In the
conclusion of this article we will introduce the structure of two of these
surveys - the second result of this article - and we will discuss some future
work."
"In this study, we aim to reduce generation latency for Named Entity
Recognition (NER) with Large Language Models (LLMs). The main cause of high
latency in LLMs is the sequential decoding process, which autoregressively
generates all labels and mentions for NER, significantly increase the sequence
length. To this end, we introduce Parallel Decoding in LLM for NE}
(PaDeLLM-NER), a approach that integrates seamlessly into existing generative
model frameworks without necessitating additional modules or architectural
modifications. PaDeLLM-NER allows for the simultaneous decoding of all
mentions, thereby reducing generation latency. Experiments reveal that
PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times
faster than the autoregressive approach for both English and Chinese.
Simultaneously it maintains the quality of predictions as evidenced by the
performance that is on par with the state-of-the-art across various datasets."
"In the rapidly evolving landscape of artificial intelligence, multimodal
learning systems (MMLS) have gained traction for their ability to process and
integrate information from diverse modality inputs. Their expanding use in
vital sectors such as healthcare has made safety assurance a critical concern.
However, the absence of systematic research into their safety is a significant
barrier to progress in this field. To bridge the gap, we present the first
taxonomy that systematically categorizes and assesses MMLS safety. This
taxonomy is structured around four fundamental pillars that are critical to
ensuring the safety of MMLS: robustness, alignment, monitoring, and
controllability. Leveraging this taxonomy, we review existing methodologies,
benchmarks, and the current state of research, while also pinpointing the
principal limitations and gaps in knowledge. Finally, we discuss unique
challenges in MMLS safety. In illuminating these challenges, we aim to pave the
way for future research, proposing potential directions that could lead to
significant advancements in the safety protocols of MMLS."
"The increasing popularity of AI, particularly Large Language Models (LLMs),
has significantly impacted various domains, including Software Engineering.
This study explores the integration of AI tools in software engineering
practices within a large organization. We focus on ANZ Bank, which employs over
5000 engineers covering all aspects of the software development life cycle.
This paper details an experiment conducted using GitHub Copilot, a notable AI
tool, within a controlled environment to evaluate its effectiveness in
real-world engineering tasks. Additionally, this paper shares initial findings
on the productivity improvements observed after GitHub Copilot was adopted on a
large scale, with about 1000 engineers using it. ANZ Bank's six-week experiment
with GitHub Copilot included two weeks of preparation and four weeks of active
testing. The study evaluated participant sentiment and the tool's impact on
productivity, code quality, and security. Initially, participants used GitHub
Copilot for proposed use-cases, with their feedback gathered through regular
surveys. In the second phase, they were divided into Control and Copilot
groups, each tackling the same Python challenges, and their experiences were
again surveyed. Results showed a notable boost in productivity and code quality
with GitHub Copilot, though its impact on code security remained inconclusive.
Participant responses were overall positive, confirming GitHub Copilot's
effectiveness in large-scale software engineering environments. Early data from
1000 engineers also indicated a significant increase in productivity and job
satisfaction."
"Generative artificial intelligence (GenAI) has ushered in a new era for
storytellers, providing a powerful tool to ignite creativity and explore
uncharted narrative territories. As technology continues to advance, the
synergy between human creativity and AI-generated content holds the potential
to redefine the landscape of storytelling. In this work, we propose SARD, a
drag-and-drop visual interface for generating a multi-chapter story using large
language models. Our evaluation of the usability of SARD and its creativity
support shows that while node-based visualization of the narrative may help
writers build a mental model, it exerts unnecessary mental overhead to the
writer and becomes a source of distraction as the story becomes more
elaborated. We also found that AI generates stories that are less lexically
diverse, irrespective of the complexity of the story. We identified some
patterns and limitations of our tool that can guide the development of future
human-AI co-writing tools."
"Accurate and timely prediction of crop growth is of great significance to
ensure crop yields and researchers have developed several crop models for the
prediction of crop growth. However, there are large difference between the
simulation results obtained by the crop models and the actual results, thus in
this paper, we proposed to combine the simulation results with the collected
crop data for data assimilation so that the accuracy of prediction will be
improved. In this paper, an EnKF-LSTM data assimilation method for various
crops is proposed by combining ensemble Kalman filter and LSTM neural network,
which effectively avoids the overfitting problem of existing data assimilation
methods and eliminates the uncertainty of the measured data. The verification
of the proposed EnKF-LSTM method and the comparison of the proposed method with
other data assimilation methods were performed using datasets collected by
sensor equipment deployed on a farm."
"We present a new approach for Neural Optimal Transport (NOT) training
procedure, capable of accurately and efficiently estimating optimal
transportation plan via specific regularization on dual Kantorovich potentials.
The main bottleneck of existing NOT solvers is associated with the procedure of
finding a near-exact approximation of the conjugate operator (i.e., the
c-transform), which is done either by optimizing over non-convex max-min
objectives or by the computationally intensive fine-tuning of the initial
approximated prediction. We resolve both issues by proposing a new,
theoretically justified loss in the form of expectile regularisation which
enforces binding conditions on the learning process of dual potentials. Such a
regularization provides the upper bound estimation over the distribution of
possible conjugate potentials and makes the learning stable, completely
eliminating the need for additional extensive fine-tuning. Proposed method,
called Expectile-Regularised Neural Optimal Transport (ENOT), outperforms
previous state-of-the-art approaches on the established Wasserstein-2 benchmark
tasks by a large margin (up to a 3-fold improvement in quality and up to a
10-fold improvement in runtime). Moreover, we showcase performance of ENOT for
varying cost functions on different tasks such as image generation, showing
robustness of proposed algorithm. OTT-JAX library includes our implementation
of ENOT algorithm https://ott-jax.readthedocs.io/en/latest/tutorials/ENOT.html"
"Multi-Source cross-lingual transfer learning deals with the transfer of task
knowledge from multiple labelled source languages to an unlabeled target
language under the language shift. Existing methods typically focus on
weighting the predictions produced by language-specific classifiers of
different sources that follow a shared encoder. However, all source languages
share the same encoder, which is updated by all these languages. The extracted
representations inevitably contain different source languages' information,
which may disturb the learning of the language-specific classifiers.
Additionally, due to the language gap, language-specific classifiers trained
with source labels are unable to make accurate predictions for the target
language. Both facts impair the model's performance. To address these
challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly,
we devise a feedback-guided collaborative disentanglement method that seeks to
purify input representations of classifiers, thereby mitigating mutual
interference from multiple sources. Secondly, we propose a class-aware parallel
adaptation method that aligns class-level distributions for each source-target
language pair, thereby alleviating the language pairs' language gap.
Experimental results on three different tasks involving 38 languages validate
the effectiveness of our approach."
"Common law courts need to refer to similar precedents' judgments to inform
their current decisions. Generating high-quality summaries of court judgment
documents can facilitate legal practitioners to efficiently review previous
cases and assist the general public in accessing how the courts operate and how
the law is applied. Previous court judgment summarization research focuses on
civil law or a particular jurisdiction's judgments. However, judges can refer
to the judgments from all common law jurisdictions. Current summarization
datasets are insufficient to satisfy the demands of summarizing precedents
across multiple jurisdictions, especially when labeled data are scarce for many
jurisdictions. To address the lack of datasets, we present CLSum, the first
dataset for summarizing multi-jurisdictional common law court judgment
documents. Besides, this is the first court judgment summarization work
adopting large language models (LLMs) in data augmentation, summary generation,
and evaluation. Specifically, we design an LLM-based data augmentation method
incorporating legal knowledge. We also propose a legal knowledge enhanced
evaluation metric based on LLM to assess the quality of generated judgment
summaries. Our experimental results verify that the LLM-based summarization
methods can perform well in the few-shot and zero-shot settings. Our LLM-based
data augmentation method can mitigate the impact of low data resources.
Furthermore, we carry out comprehensive comparative experiments to find
essential model components and settings that are capable of enhancing
summarization performance."
"Existing multigraph convolution methods either ignore the cross-view
interaction among multiple graphs, or induce extremely high computational cost
due to standard cross-view polynomial operators. To alleviate this problem,
this paper proposes a Simple MultiGraph Convolution Networks (SMGCN) which
first extracts consistent cross-view topology from multigraphs including
edge-level and subgraph-level topology, then performs polynomial expansion
based on raw multigraphs and consistent topologies. In theory, SMGCN utilizes
the consistent topologies in polynomial expansion rather than standard
cross-view polynomial expansion, which performs credible cross-view spatial
message-passing, follows the spectral convolution paradigm, and effectively
reduces the complexity of standard polynomial expansion. In the simulations,
experimental results demonstrate that SMGCN achieves state-of-the-art
performance on ACM and DBLP multigraph benchmark datasets. Our codes are
available at https://github.com/frinkleko/SMGCN."
"We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the
pervasive issue of reward over-optimization in Reinforcement Learning from
Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization
occurs when a reward model serves as an imperfect proxy for human preference,
and RL-driven policy optimization erroneously exploits reward inaccuracies. In
this paper, we begin by introducing a lightweight way to quantify uncertainties
in rewards, relying solely on the last layer embeddings of the reward model,
without the need for computationally expensive reward ensembles. AdvPO then
addresses a distributionally robust optimization problem centred around the
confidence interval of the reward model's predictions for policy improvement.
Through comprehensive experiments on the Anthropic HH and TL;DR summarization
datasets, we illustrate the efficacy of AdvPO in mitigating the
overoptimization issue, consequently resulting in enhanced performance as
evaluated through human-assisted evaluation."
"Domain generalization models aim to learn cross-domain knowledge from source
domain data, to improve performance on unknown target domains. Recent research
has demonstrated that diverse and rich source domain samples can enhance domain
generalization capability. This paper argues that the impact of each sample on
the model's generalization ability varies. Despite its small scale, a
high-quality dataset can still attain a certain level of generalization
ability. Motivated by this, we propose a domain-adversarial active learning
(DAAL) algorithm for classification tasks in domain generalization. First, we
analyze that the objective of tasks is to maximize the inter-class distance
within the same domain and minimize the intra-class distance across different
domains. To achieve this objective, we design a domain adversarial selection
method that prioritizes challenging samples. Second, we posit that even in a
converged model, there are subsets of features that lack discriminatory power
within each domain. We attempt to identify these feature subsets and optimize
them by a constraint loss. We validate and analyze our DAAL algorithm on
multiple domain generalization datasets, comparing it with various domain
generalization algorithms and active learning algorithms. Our results
demonstrate that the DAAL algorithm can achieve strong generalization ability
with fewer data resources, thereby reducing data annotation costs in domain
generalization tasks."
"Large-scale multi-task robotic manipulation systems often rely on text to
specify the task. In this work, we explore whether a robot can learn by
observing humans. To do so, the robot must understand a person's intent and
perform the inferred task despite differences in the embodiments and
environments. We introduce Vid2Robot, an end-to-end video-conditioned policy
that takes human videos demonstrating manipulation tasks as input and produces
robot actions. Our model is trained with a large dataset of prompt video-robot
trajectory pairs to learn unified representations of human and robot actions
from videos. Vid2Robot uses cross-attention transformer layers between video
features and the current robot state to produce the actions and perform the
same task as shown in the video. We use auxiliary contrastive losses to align
the prompt and robot video representations for better policies. We evaluate
Vid2Robot on real-world robots and observe over 20% improvement over BC-Z when
using human prompt videos. Further, we also show cross-object motion transfer
ability that enables video-conditioned policies to transfer a motion observed
on one object in the prompt video to another object in the robot's own
environment. Videos available at https://vid2robot.github.io"
"Portrait stylization is a challenging task involving the transformation of an
input portrait image into a specific style while preserving its inherent
characteristics. The recent introduction of Stable Diffusion (SD) has
significantly improved the quality of outcomes in this field. However, a
practical stylization framework that can effectively filter harmful input
content and preserve the distinct characteristics of an input, such as
skin-tone, while maintaining the quality of stylization remains lacking. These
challenges have hindered the wide deployment of such a framework. To address
these issues, this study proposes a portrait stylization framework that
incorporates a nudity content identification module (NCIM) and a
skin-tone-aware portrait stylization module (STAPSM). In experiments, NCIM
showed good performance in enhancing explicit content filtering, and STAPSM
accurately represented a diverse range of skin tones. Our proposed framework
has been successfully deployed in practice, and it has effectively satisfied
critical requirements of real-world applications."
"Recent advancements in large language models (LLMs) have highlighted the
potential for vulnerability detection, a crucial component of software quality
assurance. Despite this progress, most studies have been limited to the
perspective of a single role, usually testers, lacking diverse viewpoints from
different roles in a typical software development life-cycle, including both
developers and testers. To this end, this paper introduces a multi-role
approach to employ LLMs to act as different roles simulating a real-life code
review process and engaging in discussions toward a consensus on the existence
and classification of vulnerabilities in the code. Preliminary evaluation of
this approach indicates a 13.48% increase in the precision rate, an 18.25%
increase in the recall rate, and a 16.13% increase in the F1 score."
"The AI-SPRINT project, launched in 2021 and funded by the European
Commission, focuses on the development and implementation of AI applications
across the computing continuum. This continuum ensures the coherent integration
of computational resources and services from centralized data centers to edge
devices, facilitating efficient and adaptive computation and application
delivery. AI-SPRINT has achieved significant scientific advances, including
streamlined processes, improved efficiency, and the ability to operate in real
time, as evidenced by three practical use cases. This paper provides an
in-depth examination of these applications -- Personalized Healthcare,
Maintenance and Inspection, and Farming 4.0 -- highlighting their practical
implementation and the objectives achieved with the integration of AI-SPRINT
technologies. We analyze how the proposed toolchain effectively addresses a
range of challenges and refines processes, discussing its relevance and impact
in multiple domains. After a comprehensive overview of the main AI-SPRINT tools
used in these scenarios, the paper summarizes of the findings and key lessons
learned."
"An agent assisting humans in daily living activities can collaborate more
effectively by anticipating upcoming tasks. Data-driven methods represent the
state of the art in task anticipation, planning, and related problems, but
these methods are resource-hungry and opaque. Our prior work introduced a proof
of concept framework that used an LLM to anticipate 3 high-level tasks that
served as goals for a classical planning system that computed a sequence of
low-level actions for the agent to achieve these goals. This paper describes
DaTAPlan, our framework that significantly extends our prior work toward
human-robot collaboration. Specifically, DaTAPlan planner computes actions for
an agent and a human to collaboratively and jointly achieve the tasks
anticipated by the LLM, and the agent automatically adapts to unexpected
changes in human action outcomes and preferences. We evaluate DaTAPlan
capabilities in a realistic simulation environment, demonstrating accurate task
anticipation, effective human-robot collaboration, and the ability to adapt to
unexpected changes. Project website: https://dataplan-hrc.github.io"
"The article aims at identifying what, from a structural point of view, AI
based automatic translators cannot fully capture. It focuses on the machine's
mistakes, in order to try to explain its causes. The biblical story of Ca\""in
and Abel has been chosen because of its rich interpretive and critical
tradition, but also because of its semantic difficulty. The investigation
begins with the observation, for the translation of this text, of the language
pairs and interfaces offered by the best known machine translation services
(Google Translate, DeepL). A typology of the most frequent translation errors
is then established. Finally, contemporary translations are compared, in order
to underline the unique contribution of each. In conclusion, the article
suggests a revision of translation theory and, corArtificial Intelligence,
Translation, Limitations, Interpretation, Comparison, Unicityelatively, a
reformulation of its technology concerning cultural texts."
"Non-Local Attention (NLA) is a powerful technique for capturing long-range
feature correlations in deep single image super-resolution (SR). However, NLA
suffers from high computational complexity and memory consumption, as it
requires aggregating all non-local feature information for each query response
and recalculating the similarity weight distribution for different abstraction
levels of features. To address these challenges, we propose a novel Learnable
Collaborative Attention (LCoA) that introduces inductive bias into non-local
modeling. Our LCoA consists of two components: Learnable Sparse Pattern (LSP)
and Collaborative Attention (CoA). LSP uses the k-means clustering algorithm to
dynamically adjust the sparse attention pattern of deep features, which reduces
the number of non-local modeling rounds compared with existing sparse
solutions. CoA leverages the sparse attention pattern and weights learned by
LSP, and co-optimizes the similarity matrix across different abstraction
levels, which avoids redundant similarity matrix calculations. The experimental
results show that our LCoA can reduce the non-local modeling time by about 83%
in the inference stage. In addition, we integrate our LCoA into a deep
Learnable Collaborative Attention Network (LCoAN), which achieves competitive
performance in terms of inference time, memory consumption, and reconstruction
quality compared with other state-of-the-art SR methods."
"Racial diversity has become increasingly discussed within the AI and
algorithmic fairness literature, yet little attention is focused on justifying
the choices of racial categories and understanding how people are racialized
into these chosen racial categories. Even less attention is given to how racial
categories shift and how the racialization process changes depending on the
context of a dataset or model. An unclear understanding of \textit{who}
comprises the racial categories chosen and \textit{how} people are racialized
into these categories can lead to varying interpretations of these categories.
These varying interpretations can lead to harm when the understanding of racial
categories and the racialization process is misaligned from the actual
racialization process and racial categories used. Harm can also arise if the
racialization process and racial categories used are irrelevant or do not exist
in the context they are applied.
  In this paper, we make two contributions. First, we demonstrate how racial
categories with unclear assumptions and little justification can lead to
varying datasets that poorly represent groups obfuscated or unrepresented by
the given racial categories and models that perform poorly on these groups.
Second, we develop a framework, CIRCSheets, for documenting the choices and
assumptions in choosing racial categories and the process of racialization into
these categories to facilitate transparency in understanding the processes and
assumptions made by dataset or model developers when selecting or using these
racial categories."
"In this study, we investigate whether attention-based information flow inside
large language models (LLMs) is aggregated through noticeable patterns for long
context processing. Our observations reveal that LLMs aggregate information
through Pyramidal Information Funneling where attention is scattering widely in
lower layers, progressively consolidating within specific contexts, and
ultimately focusing on critical tokens (a.k.a massive activation or attention
sink) in higher layers. Motivated by these insights, we developed PyramidKV, a
novel and effective KV cache compression method. This approach dynamically
adjusts the KV cache size across different layers, allocating more cache in
lower layers and less in higher ones, diverging from traditional methods that
maintain a uniform KV cache size. Our experimental evaluations, utilizing the
LongBench benchmark, show that PyramidKV matches the performance of models with
a full KV cache while retaining only 12% of the KV cache, thus significantly
reducing memory usage. In scenarios emphasizing memory efficiency, where only
0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache
compression techniques, achieving up to a 20.5 absolute accuracy improvement on
TREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms
competing methods in maintaining long-context comprehension in LLMs; notably,
retaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve
100% Acc. performance, matching that of a full KV cache."
"This study investigates the loss of generalization ability in neural
networks, revisiting warm-starting experiments from Ash & Adams. Our empirical
analysis reveals that common methods designed to enhance plasticity by
maintaining trainability provide limited benefits to generalization. While
reinitializing the network can be effective, it also risks losing valuable
prior knowledge. To this end, we introduce the Hare & Tortoise, inspired by the
brain's complementary learning system. Hare & Tortoise consists of two
components: the Hare network, which rapidly adapts to new information
analogously to the hippocampus, and the Tortoise network, which gradually
integrates knowledge akin to the neocortex. By periodically reinitializing the
Hare network to the Tortoise's weights, our method preserves plasticity while
retaining general knowledge. Hare & Tortoise can effectively maintain the
network's ability to generalize, which improves advanced reinforcement learning
algorithms on the Atari-100k benchmark. The code is available at
https://github.com/dojeon-ai/hare-tortoise."
"Building world models that accurately and comprehensively represent the real
world is the utmost aspiration for conditional image generative models as it
would enable their use as world simulators. For these models to be successful
world models, they should not only excel at image quality and prompt-image
consistency but also ensure high representation diversity. However, current
research in generative models mostly focuses on creative applications that are
predominantly concerned with human preferences of image quality and aesthetics.
We note that generative models have inference time mechanisms - or knobs - that
allow the control of generation consistency, quality, and diversity. In this
paper, we use state-of-the-art text-to-image and image-and-text-to-image models
and their knobs to draw consistency-diversity-realism Pareto fronts that
provide a holistic view on consistency-diversity-realism multi-objective. Our
experiments suggest that realism and consistency can both be improved
simultaneously; however there exists a clear tradeoff between
realism/consistency and diversity. By looking at Pareto optimal points, we note
that earlier models are better at representation diversity and worse in
consistency/realism, and more recent models excel in consistency/realism while
decreasing significantly the representation diversity. By computing Pareto
fronts on a geodiverse dataset, we find that the first version of latent
diffusion models tends to perform better than more recent models in all axes of
evaluation, and there exist pronounced consistency-diversity-realism
disparities between geographical regions. Overall, our analysis clearly shows
that there is no best model and the choice of model should be determined by the
downstream application. With this analysis, we invite the research community to
consider Pareto fronts as an analytical tool to measure progress towards world
models."
"The paper presents the performance results of Reactor Mk.1, ARCs flagship
large language model, through a benchmarking process analysis. The model
utilizes the Lychee AI engine and possesses less than 100 billion parameters,
resulting in a combination of efficiency and potency. The Reactor Mk.1
outperformed models such as GPT-4o, Claude Opus, and Llama 3, with achieved
scores of 92% on the MMLU dataset, 91% on HumanEval dataset, and 88% on BBH
dataset. It excels in both managing difficult jobs and reasoning, establishing
as a prominent AI solution in the present cutting-edge AI technology."
"Existing frameworks for assessing robustness of large language models (LLMs)
overly depend on specific benchmarks, increasing costs and failing to evaluate
performance of LLMs in professional domains due to dataset limitations. This
paper proposes a framework that systematically evaluates the robustness of LLMs
under adversarial attack scenarios by leveraging knowledge graphs (KGs). Our
framework generates original prompts from the triplets of knowledge graphs and
creates adversarial prompts by poisoning, assessing the robustness of LLMs
through the results of these adversarial attacks. We systematically evaluate
the effectiveness of this framework and its modules. Experiments show that
adversarial robustness of the ChatGPT family ranks as GPT-4-turbo > GPT-4o >
GPT-3.5-turbo, and the robustness of large language models is influenced by the
professional domains in which they operate."
"This study presents a novel framework for 3D gaze tracking tailored for
mixed-reality settings, aimed at enhancing joint attention and collaborative
efforts in team-based scenarios. Conventional gaze tracking, often limited by
monocular cameras and traditional eye-tracking apparatus, struggles with
simultaneous data synchronization and analysis from multiple participants in
group contexts. Our proposed framework leverages state-of-the-art computer
vision and machine learning techniques to overcome these obstacles, enabling
precise 3D gaze estimation without dependence on specialized hardware or
complex data fusion. Utilizing facial recognition and deep learning, the
framework achieves real-time, tracking of gaze patterns across several
individuals, addressing common depth estimation errors, and ensuring spatial
and identity consistency within the dataset. Empirical results demonstrate the
accuracy and reliability of our method in group environments. This provides
mechanisms for significant advances in behavior and interaction analysis in
educational and professional training applications in dynamic and unstructured
environments."
"In aligning large language models (LLMs), utilizing feedback from existing
advanced AI rather than humans is an important method to scale supervisory
signals. However, it is highly challenging for AI to understand human
intentions and societal values, and provide accurate preference feedback based
on these. Current AI feedback methods rely on powerful LLMs, carefully designed
specific principles to describe human intentions, and are easily influenced by
position bias. To address these issues, we propose a self-reference-based AI
feedback framework that enables a 13B Llama2-Chat to provide high-quality
feedback under simple and general principles such as ``best for humanity``.
Specifically, we allow the AI to first respond to the user's instructions, then
generate criticism of other answers based on its own response as a reference,
and finally determine which answer better fits human preferences according to
the criticism. Additionally, we use a self-consistency method to further reduce
the impact of position bias, and employ semantic perplexity to calculate the
preference strength differences between different answers. Experimental results
show that our method enables 13B and 70B Llama2-Chat annotators to provide
high-quality preference feedback, and the policy models trained based on these
preference data achieve significant advantages in benchmark datasets through
reinforcement learning."
"As Large Language Models (LLMs) become an important way of information
seeking, there have been increasing concerns about the unethical content LLMs
may generate. In this paper, we conduct a rigorous evaluation of LLMs' implicit
bias towards certain groups by attacking them with carefully crafted
instructions to elicit biased responses. Our attack methodology is inspired by
psychometric principles in cognitive and social psychology. We propose three
attack approaches, i.e., Disguise, Deception, and Teaching, based on which we
built evaluation datasets for four common bias types. Each prompt attack has
bilingual versions. Extensive evaluation of representative LLMs shows that 1)
all three attack methods work effectively, especially the Deception attacks; 2)
GLM-3 performs the best in defending our attacks, compared to GPT-3.5 and
GPT-4; 3) LLMs could output content of other bias types when being taught with
one type of bias. Our methodology provides a rigorous and effective way of
evaluating LLMs' implicit bias and will benefit the assessments of LLMs'
potential ethical risks."
"Robotic assistance for experimental manipulation in the life sciences is
expected to enable favorable outcomes, regardless of the skill of the
scientist. Experimental specimens in the life sciences are subject to
individual variability hence require intricate algorithms for successful
autonomous robotic control. As a use case, we are studying the creation of
cranial windows in mice. This operation requires the removal of an
8-mm-circular patch of the skull, which is approximately 300 um thick, but the
shape and thickness of the mouse skull significantly varies depending on the
strain of mouse, sex, and age. In this work, we propose an autonomous robotic
drilling method with no offline planning, consisting of a trajectory planning
block with execution-time feedback with completion level recognition based on
image and force information. The force information allows for completion-level
resolution to increase 10 fold. We evaluate the proposed method in two ways.
First, in an eggshell drilling task and achieved a success rate of 95% and
average drilling time of 7.1 min out of 20 trials. Second, in postmortem mice
and with a success rate of 70% and average drilling time of 9.3 min out of 20
trials."
"Requirement traceability is the process of identifying the inter-dependencies
between requirements. It poses a significant challenge when conducted manually,
especially when dealing with requirements at various levels of abstraction. In
this work, we propose a novel approach to automate the task of linking
high-level business requirements with more technical system requirements. The
proposed approach begins by representing each requirement using a Bag of-Words
(BOW) model combined with the Term Frequency-Inverse Document Frequency
(TF-IDF) scoring function. Then, we suggested an enhanced cosine similarity
that uses recent advances in word embedding representation to correct
traditional cosine similarity function limitations. To evaluate the
effectiveness of our approach, we conducted experiments on three well-known
datasets: COEST, WARC(NFR), and WARC(FRS). The results demonstrate that our
approach significantly improves efficiency compared to existing methods. We
achieved better results with an increase of approximately 18.4% in one of the
datasets, as measured by the F2 score."
"With the widespread applications of neural networks (NNs) trained on personal
data, machine unlearning has become increasingly important for enabling
individuals to exercise their personal data ownership, particularly the ""right
to be forgotten"" from trained NNs. Since retraining is computationally
expensive, we seek approximate unlearning algorithms for NNs that return
identical models to the retrained oracle. While Newton's method has been
successfully used to approximately unlearn linear models, we observe that
adapting it for NN is challenging due to degenerate Hessians that make
computing Newton's update impossible. Additionally, we show that when coupled
with popular techniques to resolve the degeneracy, Newton's method often incurs
offensively large norm updates and empirically degrades model performance
post-unlearning. To address these challenges, we propose CureNewton's method, a
principle approach that leverages cubic regularization to handle the Hessian
degeneracy effectively. The added regularizer eliminates the need for manual
finetuning and affords a natural interpretation within the unlearning context.
Experiments across different models and datasets show that our method can
achieve competitive unlearning performance to the state-of-the-art algorithm in
practical unlearning settings, while being theoretically justified and
efficient in running time."
"Personality is a fundamental construct in psychology, reflecting an
individual's behavior, thinking, and emotional patterns. Previous researches
have made some progress in personality detection, primarily by utilizing the
whole text to predict personality. However, these studies generally tend to
overlook psychological knowledge: they rarely apply the well-established
correlations between emotion regulation and personality. Based on this, we
propose a new personality detection method called EERPD. This method introduces
the use of emotion regulation, a psychological concept highly correlated with
personality, for personality prediction. By combining this feature with emotion
features, it retrieves few-shot examples and provides process CoTs for
inferring labels from text. This approach enhances the understanding of LLM for
personality within text and improves the performance in personality detection.
Experimental results demonstrate that EERPD significantly enhances the accuracy
and robustness of personality detection, outperforming previous SOTA by
15.05/4.29 in average F1 on the two benchmark datasets."
"Human activity recognition (HAR) is a crucial area of research that involves
understanding human movements using computer and machine vision technology.
Deep learning has emerged as a powerful tool for this task, with models such as
Convolutional Neural Networks (CNNs) and Transformers being employed to capture
various aspects of human motion. One of the key contributions of this work is
the demonstration of the effectiveness of feature fusion in improving HAR
accuracy by capturing spatial and temporal features, which has important
implications for the development of more accurate and robust activity
recognition systems. The study uses sensory data from HuGaDB, PKU-MMD, LARa,
and TUG datasets. Two model, the PO-MS-GCN and a Transformer were trained and
evaluated, with PO-MS-GCN outperforming state-of-the-art models. HuGaDB and TUG
achieved high accuracies and f1-scores, while LARa and PKU-MMD had lower
scores. Feature fusion improved results across datasets."
"Mixture of experts is a prediction aggregation method in machine learning
that aggregates the predictions of specialized experts. This method often
outperforms Bayesian methods despite the Bayesian having stronger inductive
guarantees. We argue that this is due to the greater functional capacity of
mixture of experts. We prove that in a limiting case of mixture of experts will
have greater capacity than equivalent Bayesian methods, which we vouchsafe
through experiments on non-limiting cases. Finally, we conclude that mixture of
experts is a type of abductive reasoning in the Peircian sense of hypothesis
construction."
"Visual attention modeling, important for interpreting and prioritizing visual
stimuli, plays a significant role in applications such as marketing,
multimedia, and robotics. Traditional saliency prediction models, especially
those based on Convolutional Neural Networks (CNNs) or Transformers, achieve
notable success by leveraging large-scale annotated datasets. However, the
current state-of-the-art (SOTA) models that use Transformers are
computationally expensive. Additionally, separate models are often required for
each image type, lacking a unified approach. In this paper, we propose Saliency
Unification through Mamba (SUM), a novel approach that integrates the efficient
long-range dependency modeling of Mamba with U-Net to provide a unified model
for diverse image types. Using a novel Conditional Visual State Space (C-VSS)
block, SUM dynamically adapts to various image types, including natural scenes,
web pages, and commercial imagery, ensuring universal applicability across
different data types. Our comprehensive evaluations across five benchmarks
demonstrate that SUM seamlessly adapts to different visual characteristics and
consistently outperforms existing models. These results position SUM as a
versatile and powerful tool for advancing visual attention modeling, offering a
robust solution universally applicable across different types of visual
content."
"Given a reference computer, Kolmogorov complexity is a well defined function
on all binary strings. In the standard approach, however, only the asymptotic
properties of such functions are considered because they do not depend on the
reference computer. We argue that this approach can be more useful if it is
refined to include an important practical case of simple binary strings.
Kolmogorov complexity calculus may be developed for this case if we restrict
the class of available reference computers. The interesting problem is to
define a class of computers which is restricted in a {\it natural} way modeling
the real-life situation where only a limited class of computers is physically
available to us. We give an example of what such a natural restriction might
look like mathematically, and show that under such restrictions some error
terms, even logarithmic in complexity, can disappear from the standard
complexity calculus.
  Keywords: Kolmogorov complexity; Algorithmic information theory."
"A main problem of ""Follow the Perturbed Leader"" strategies for online
decision problems is that regret bounds are typically proven against oblivious
adversary. In partial observation cases, it was not clear how to obtain
performance guarantees against adaptive adversary, without worsening the
bounds. We propose a conceptually simple argument to resolve this problem.
Using this, a regret bound of O(t^(2/3)) for FPL in the adversarial multi-armed
bandit problem is shown. This bound holds for the common FPL variant using only
the observations from designated exploration rounds. Using all observations
allows for the stronger bound of O(t^(1/2)), matching the best bound known so
far (and essentially the known lower bound) for adversarial bandits.
Surprisingly, this variant does not even need explicit exploration, it is
self-stabilizing. However the sampling probabilities have to be either
externally provided or approximated to sufficient accuracy, using O(t^2 log t)
samples in each step."
"Bandit based methods for tree search have recently gained popularity when
applied to huge trees, e.g. in the game of go (Gelly et al., 2006). The UCT
algorithm (Kocsis and Szepesvari, 2006), a tree search method based on Upper
Confidence Bounds (UCB) (Auer et al., 2002), is believed to adapt locally to
the effective smoothness of the tree. However, we show that UCT is too
``optimistic'' in some cases, leading to a regret O(exp(exp(D))) where D is the
depth of the tree. We propose alternative bandit algorithms for tree search.
First, a modification of UCT using a confidence sequence that scales
exponentially with the horizon depth is proven to have a regret O(2^D
\sqrt{n}), but does not adapt to possible smoothness in the tree. We then
analyze Flat-UCB performed on the leaves and provide a finite regret bound with
high probability. Then, we introduce a UCB-based Bandit Algorithm for Smooth
Trees which takes into account actual smoothness of the rewards for performing
efficient ``cuts'' of sub-optimal branches with high confidence. Finally, we
present an incremental tree search version which applies when the full tree is
too big (possibly infinite) to be entirely represented and show that with high
probability, essentially only the optimal branches is indefinitely developed.
We illustrate these methods on a global optimization problem of a Lipschitz
function, given noisy data."
"This paper introduces an approach to Reinforcement Learning Algorithm by
comparing their immediate rewards using a variation of Q-Learning algorithm.
Unlike the conventional Q-Learning, the proposed algorithm compares current
reward with immediate reward of past move and work accordingly. Relative reward
based Q-learning is an approach towards interactive learning. Q-Learning is a
model free reinforcement learning method that used to learn the agents. It is
observed that under normal circumstances algorithm take more episodes to reach
optimal Q-value due to its normal reward or sometime negative reward. In this
new form of algorithm agents select only those actions which have a higher
immediate reward signal in comparison to previous one. The contribution of this
article is the presentation of new Q-Learning Algorithm in order to maximize
the performance of algorithm and reduce the number of episode required to reach
optimal Q-value. Effectiveness of proposed algorithm is simulated in a 20 x20
Grid world deterministic environment and the result for the two forms of
Q-Learning Algorithms is given."
"In this paper, we consider a queue-aware distributive resource control
algorithm for two-hop MIMO cooperative systems. We shall illustrate that relay
buffering is an effective way to reduce the intrinsic half-duplex penalty in
cooperative systems. The complex interactions of the queues at the source node
and the relays are modeled as an average-cost infinite horizon Markov Decision
Process (MDP). The traditional approach solving this MDP problem involves
centralized control with huge complexity. To obtain a distributive and low
complexity solution, we introduce a linear structure which approximates the
value function of the associated Bellman equation by the sum of per-node value
functions. We derive a distributive two-stage two-winner auction-based control
policy which is a function of the local CSI and local QSI only. Furthermore, to
estimate the best fit approximation parameter, we propose a distributive online
stochastic learning algorithm using stochastic approximation theory. Finally,
we establish technical conditions for almost-sure convergence and show that
under heavy traffic, the proposed low complexity distributive control is global
optimal."
"The classical perceptron rule provides a varying upper bound on the maximum
margin, namely the length of the current weight vector divided by the total
number of updates up to that time. Requiring that the perceptron updates its
internal state whenever the normalized margin of a pattern is found not to
exceed a certain fraction of this dynamic upper bound we construct a new
approximate maximum margin classifier called the perceptron with dynamic margin
(PDM). We demonstrate that PDM converges in a finite number of steps and derive
an upper bound on them. We also compare experimentally PDM with other
perceptron-like algorithms and support vector machines on hard margin tasks
involving linear kernels which are equivalent to 2-norm soft margin."
"For large scale learning problems, it is desirable if we can obtain the
optimal model parameters by going through the data in only one pass. Polyak and
Juditsky (1992) showed that asymptotically the test performance of the simple
average of the parameters obtained by stochastic gradient descent (SGD) is as
good as that of the parameters which minimize the empirical cost. However, to
our knowledge, despite its optimal asymptotic convergence rate, averaged SGD
(ASGD) received little attention in recent research on large scale learning.
One possible reason is that it may take a prohibitively large number of
training samples for ASGD to reach its asymptotic region for most real
problems. In this paper, we present a finite sample analysis for the method of
Polyak and Juditsky (1992). Our analysis shows that it indeed usually takes a
huge number of samples for ASGD to reach its asymptotic region for improperly
chosen learning rate. More importantly, based on our analysis, we propose a
simple way to properly set learning rate so that it takes a reasonable amount
of data for ASGD to reach its asymptotic region. We compare ASGD using our
proposed learning rate with other well known algorithms for training large
scale linear classifiers. The experiments clearly show the superiority of ASGD."
"Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model
for probabilistic topic modeling, which attracts worldwide interests and
touches on many important applications in text mining, computer vision and
computational biology. This paper represents LDA as a factor graph within the
Markov random field (MRF) framework, which enables the classic loopy belief
propagation (BP) algorithm for approximate inference and parameter estimation.
Although two commonly-used approximate inference methods, such as variational
Bayes (VB) and collapsed Gibbs sampling (GS), have gained great successes in
learning LDA, the proposed BP is competitive in both speed and accuracy as
validated by encouraging experimental results on four large-scale document data
sets. Furthermore, the BP algorithm has the potential to become a generic
learning scheme for variants of LDA-based topic models. To this end, we show
how to learn two typical variants of LDA-based topic models, such as
author-topic models (ATM) and relational topic models (RTM), using BP based on
the factor graph representation."
"Inverse reinforcement learning (IRL) addresses the problem of recovering a
task description given a demonstration of the optimal policy used to solve such
a task. The optimal policy is usually provided by an expert or teacher, making
IRL specially suitable for the problem of apprenticeship learning. The task
description is encoded in the form of a reward function of a Markov decision
process (MDP). Several algorithms have been proposed to find the reward
function corresponding to a set of demonstrations. One of the algorithms that
has provided best results in different applications is a gradient method to
optimize a policy squared error criterion. On a parallel line of research,
other authors have presented recently a gradient approximation of the maximum
likelihood estimate of the reward signal. In general, both approaches
approximate the gradient estimate and the criteria at different stages to make
the algorithm tractable and efficient. In this work, we provide a detailed
description of the different methods to highlight differences in terms of
reward estimation, policy similarity and computational costs. We also provide
experimental results to evaluate the differences in performance of the methods."
"Unsupervised models can provide supplementary soft constraints to help
classify new, ""target"" data since similar instances in the target set are more
likely to share the same class label. Such models can also help detect possible
differences between training and target distributions, which is useful in
applications where concept drift may take place, as in transfer learning
settings. This paper describes a general optimization framework that takes as
input class membership estimates from existing classifiers learnt on previously
encountered ""source"" data, as well as a similarity matrix from a cluster
ensemble operating solely on the target data to be classified, and yields a
consensus labeling of the target data. This framework admits a wide range of
loss functions and classification/clustering methods. It exploits properties of
Bregman divergences in conjunction with Legendre duality to yield a principled
and scalable approach. A variety of experiments show that the proposed
framework can yield results substantially superior to those provided by popular
transductive learning techniques or by naively applying classifiers learnt on
the original task to the target data."
"Predicting the nodes of a given graph is a fascinating theoretical problem
with applications in several domains. Since graph sparsification via spanning
trees retains enough information while making the task much easier, trees are
an important special case of this problem. Although it is known how to predict
the nodes of an unweighted tree in a nearly optimal way, in the weighted case a
fully satisfactory algorithm is not available yet. We fill this hole and
introduce an efficient node predictor, Shazoo, which is nearly optimal on any
weighted tree. Moreover, we show that Shazoo can be viewed as a common
nontrivial generalization of both previous approaches for unweighted trees and
weighted lines. Experiments on real-world datasets confirm that Shazoo performs
well in that it fully exploits the structure of the input tree, and gets very
close to (and sometimes better than) less scalable energy minimization methods."
"Traditional algorithms for stochastic optimization require projecting the
solution at each iteration into a given domain to ensure its feasibility. When
facing complex domains, such as positive semi-definite cones, the projection
operation can be expensive, leading to a high computational cost per iteration.
In this paper, we present a novel algorithm that aims to reduce the number of
projections for stochastic optimization. The proposed algorithm combines the
strength of several recent developments in stochastic optimization, including
mini-batch, extra-gradient, and epoch gradient descent, in order to effectively
explore the smoothness and strong convexity. We show, both in expectation and
with a high probability, that when the objective function is both smooth and
strongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate of
convergence with only $O(\log T)$ projections. Our empirical study verifies the
theoretical result."
"Fractals are self-similar recursive structures that have been used in
modeling several real world processes. In this work we study how ""fractal-like""
processes arise in a prediction game where an adversary is generating a
sequence of bits and an algorithm is trying to predict them. We will see that
under a certain formalization of the predictive payoff for the algorithm it is
most optimal for the adversary to produce a fractal-like sequence to minimize
the algorithm's ability to predict. Indeed it has been suggested before that
financial markets exhibit a fractal-like behavior. We prove that a fractal-like
distribution arises naturally out of an optimization from the adversary's
perspective.
  In addition, we give optimal trade-offs between predictability and expected
deviation (i.e. sum of bits) for our formalization of predictive payoff. This
result is motivated by the observation that several time series data exhibit
higher deviations than expected for a completely random walk."
"The multi-label classification problem has generated significant interest in
recent years. However, existing approaches do not adequately address two key
challenges: (a) the ability to tackle problems with a large number (say
millions) of labels, and (b) the ability to handle data with missing labels. In
this paper, we directly address both these problems by studying the multi-label
problem in a generic empirical risk minimization (ERM) framework. Our
framework, despite being simple, is surprisingly able to encompass several
recent label-compression based methods which can be derived as special cases of
our method. To optimize the ERM problem, we develop techniques that exploit the
structure of specific loss functions - such as the squared loss function - to
offer efficient algorithms. We further show that our learning framework admits
formal excess risk bounds even in the presence of missing labels. Our risk
bounds are tight and demonstrate better generalization performance for low-rank
promoting trace-norm regularization when compared to (rank insensitive)
Frobenius norm regularization. Finally, we present extensive empirical results
on a variety of benchmark datasets and show that our methods perform
significantly better than existing label compression based methods and can
scale up to very large datasets such as the Wikipedia dataset."
"We study the problem of predicting a set or list of options under knapsack
constraint. The quality of such lists are evaluated by a submodular reward
function that measures both quality and diversity. Similar to DAgger (Ross et
al., 2010), by a reduction to online learning, we show how to adapt two
sequence prediction models to imitate greedy maximization under knapsack
constraint problems: CONSEQOPT (Dey et al., 2012) and SCP (Ross et al., 2013).
Experiments on extractive multi-document summarization show that our approach
outperforms existing state-of-the-art methods."
"In this paper we propose a multi-task linear classifier learning problem
called D-SVM (Dictionary SVM). D-SVM uses a dictionary of parameter covariance
shared by all tasks to do multi-task knowledge transfer among different tasks.
We formally define the learning problem of D-SVM and show two interpretations
of this problem, from both the probabilistic and kernel perspectives. From the
probabilistic perspective, we show that our learning formulation is actually a
MAP estimation on all optimization variables. We also show its equivalence to a
multiple kernel learning problem in which one is trying to find a re-weighting
kernel for features from a dictionary of basis (despite the fact that only
linear classifiers are learned). Finally, we describe an alternative
optimization scheme to minimize the objective function and present empirical
studies to valid our algorithm."
"Sophisticated automatic incident detection (AID) technology plays a key role
in contemporary transportation systems. Though many papers were devoted to
study incident classification algorithms, few study investigated how to enhance
feature representation of incidents to improve AID performance. In this paper,
we propose to use an unsupervised feature learning algorithm to generate higher
level features to represent incidents. We used real incident data in the
experiments and found that effective feature mapping function can be learnt
from the data crosses the test sites. With the enhanced features, detection
rate (DR), false alarm rate (FAR) and mean time to detect (MTTD) are
significantly improved in all of the three representative cases. This approach
also provides an alternative way to reduce the amount of labeled data, which is
expensive to obtain, required in training better incident classifiers since the
feature learning is unsupervised."
"We define a general framework for a large class of combinatorial multi-armed
bandit (CMAB) problems, where subsets of base arms with unknown distributions
form super arms. In each round, a super arm is played and the base arms
contained in the super arm are played and their outcomes are observed. We
further consider the extension in which more based arms could be
probabilistically triggered based on the outcomes of already triggered arms.
The reward of the super arm depends on the outcomes of all played arms, and it
only needs to satisfy two mild assumptions, which allow a large class of
nonlinear reward instances. We assume the availability of an offline
(\alpha,\beta)-approximation oracle that takes the means of the outcome
distributions of arms and outputs a super arm that with probability {\beta}
generates an {\alpha} fraction of the optimal expected reward. The objective of
an online learning algorithm for CMAB is to minimize
(\alpha,\beta)-approximation regret, which is the difference between the
\alpha{\beta} fraction of the expected reward when always playing the optimal
super arm, and the expected reward of playing super arms according to the
algorithm. We provide CUCB algorithm that achieves O(log n)
distribution-dependent regret, where n is the number of rounds played, and we
further provide distribution-independent bounds for a large class of reward
functions. Our regret analysis is tight in that it matches the bound of UCB1
algorithm (up to a constant factor) for the classical MAB problem, and it
significantly improves the regret bound in a earlier paper on combinatorial
bandits with linear rewards. We apply our CMAB framework to two new
applications, probabilistic maximum coverage and social influence maximization,
both having nonlinear reward structures. In particular, application to social
influence maximization requires our extension on probabilistically triggered
arms."
"Kernel-based approaches for sequence classification have been successfully
applied to a variety of domains, including the text categorization, image
classification, speech analysis, biological sequence analysis, time series and
music classification, where they show some of the most accurate results.
  Typical kernel functions for sequences in these domains (e.g., bag-of-words,
mismatch, or subsequence kernels) are restricted to {\em discrete univariate}
(i.e. one-dimensional) string data, such as sequences of words in the text
analysis, codeword sequences in the image analysis, or nucleotide or amino acid
sequences in the DNA and protein sequence analysis. However, original sequence
data are often of real-valued multivariate nature, i.e. are not univariate and
discrete as required by typical $k$-mer based sequence kernel functions.
  In this work, we consider the problem of the {\em multivariate} sequence
classification such as classification of multivariate music sequences, or
multidimensional protein sequence representations. To this end, we extend {\em
univariate} kernel functions typically used in sequence analysis and propose
efficient {\em multivariate} similarity kernel method (MVDFQ-SK) based on (1) a
direct feature quantization (DFQ) of each sequence dimension in the original
{\em real-valued} multivariate sequences and (2) applying novel multivariate
discrete kernel measures on these multivariate discrete DFQ sequence
representations to more accurately capture similarity relationships among
sequences and improve classification performance.
  Experiments using the proposed MVDFQ-SK kernel method show excellent
classification performance on three challenging music classification tasks as
well as protein sequence classification with significant 25-40% improvements
over univariate kernel methods and existing state-of-the-art sequence
classification methods."
"CUR matrix decomposition computes the low rank approximation of a given
matrix by using the actual rows and columns of the matrix. It has been a very
useful tool for handling large matrices. One limitation with the existing
algorithms for CUR matrix decomposition is that they need an access to the {\it
full} matrix, a requirement that can be difficult to fulfill in many real world
applications. In this work, we alleviate this limitation by developing a CUR
decomposition algorithm for partially observed matrices. In particular, the
proposed algorithm computes the low rank approximation of the target matrix
based on (i) the randomly sampled rows and columns, and (ii) a subset of
observed entries that are randomly sampled from the matrix. Our analysis shows
the relative error bound, measured by spectral norm, for the proposed algorithm
when the target matrix is of full rank. We also show that only $O(n r\ln r)$
observed entries are needed by the proposed algorithm to perfectly recover a
rank $r$ matrix of size $n\times n$, which improves the sample complexity of
the existing algorithms for matrix completion. Empirical studies on both
synthetic and real-world datasets verify our theoretical claims and demonstrate
the effectiveness of the proposed algorithm."
"This report discusses two new indices for comparing clusterings of a set of
points. The motivation for looking at new ways for comparing clusterings stems
from the fact that the existing clustering indices are based on set cardinality
alone and do not consider the positions of data points. The new indices,
namely, the Random Walk index (RWI) and Variation of Information with Neighbors
(VIN), are both inspired by the clustering metric Variation of Information
(VI). VI possesses some interesting theoretical properties which are also
desirable in a metric for comparing clusterings. We define our indices and
discuss some of their explored properties which appear relevant for a
clustering index. We also include the results of these indices on clusterings
of some example data sets."
"A classic tension exists between exact inference in a simple model and
approximate inference in a complex model. The latter offers expressivity and
thus accuracy, but the former provides coverage of the space, an important
property for confidence estimation and learning with indirect supervision. In
this work, we introduce a new approach, reified context models, to reconcile
this tension. Specifically, we let the amount of context (the arity of the
factors in a graphical model) be chosen ""at run-time"" by reifying it---that is,
letting this choice itself be a random variable inside the model. Empirically,
we show that our approach obtains expressivity and coverage on three natural
language tasks."
"We provide a simple and efficient algorithm for computing the Euclidean
projection of a point onto the capped simplex---a simplex with an additional
uniform bound on each coordinate---together with an elementary proof. Both the
MATLAB and C++ implementations of the proposed algorithm can be downloaded at
https://eng.ucmerced.edu/people/wwang5."
"This paper presents an unsupervised learning approach for simultaneous sample
and feature selection, which is in contrast to existing works which mainly
tackle these two problems separately. In fact the two tasks are often
interleaved with each other: noisy and high-dimensional features will bring
adverse effect on sample selection, while informative or representative samples
will be beneficial to feature selection. Specifically, we propose a framework
to jointly conduct active learning and feature selection based on the CUR
matrix decomposition. From the data reconstruction perspective, both the
selected samples and features can best approximate the original dataset
respectively, such that the selected samples characterized by the features are
highly representative. In particular, our method runs in one-shot without the
procedure of iterative sample selection for progressive labeling. Thus, our
model is especially suitable when there are few labeled samples or even in the
absence of supervision, which is a particular challenge for existing methods.
As the joint learning problem is NP-hard, the proposed formulation involves a
convex but non-smooth optimization problem. We solve it efficiently by an
iterative algorithm, and prove its global convergence. Experimental results on
publicly available datasets corroborate the efficacy of our method compared
with the state-of-the-art."
"We present a Discriminative Switching Linear Dynamical System (DSLDS) applied
to patient monitoring in Intensive Care Units (ICUs). Our approach is based on
identifying the state-of-health of a patient given their observed vital signs
using a discriminative classifier, and then inferring their underlying
physiological values conditioned on this status. The work builds on the
Factorial Switching Linear Dynamical System (FSLDS) (Quinn et al., 2009) which
has been previously used in a similar setting. The FSLDS is a generative model,
whereas the DSLDS is a discriminative model. We demonstrate on two real-world
datasets that the DSLDS is able to outperform the FSLDS in most cases of
interest, and that an $\alpha$-mixture of the two models achieves higher
performance than either of the two models separately."
"Several real-world classification problems are example-dependent
cost-sensitive in nature, where the costs due to misclassification vary between
examples and not only within classes. However, standard classification methods
do not take these costs into account, and assume a constant cost of
misclassification errors. In previous works, some methods that take into
account the financial costs into the training of different algorithms have been
proposed, with the example-dependent cost-sensitive decision tree algorithm
being the one that gives the highest savings. In this paper we propose a new
framework of ensembles of example-dependent cost-sensitive decision-trees. The
framework consists in creating different example-dependent cost-sensitive
decision trees on random subsamples of the training set, and then combining
them using three different combination approaches. Moreover, we propose two new
cost-sensitive combination approaches; cost-sensitive weighted voting and
cost-sensitive stacking, the latter being based on the cost-sensitive logistic
regression method. Finally, using five different databases, from four
real-world applications: credit card fraud detection, churn modeling, credit
scoring and direct marketing, we evaluate the proposed method against
state-of-the-art example-dependent cost-sensitive techniques, namely,
cost-proportionate sampling, Bayes minimum risk and cost-sensitive decision
trees. The results show that the proposed algorithms have better results for
all databases, in the sense of higher savings."
"Unsupervised feature selection has been always attracting research attention
in the communities of machine learning and data mining for decades. In this
paper, we propose an unsupervised feature selection method seeking a feature
coefficient matrix to select the most distinctive features. Specifically, our
proposed algorithm integrates the Maximum Margin Criterion with a
sparsity-based model into a joint framework, where the class margin and feature
correlation are taken into account at the same time. To maximize the total data
separability while preserving minimized within-class scatter simultaneously, we
propose to embed Kmeans into the framework generating pseudo class label
information in a scenario of unsupervised feature selection. Meanwhile, a
sparsity-based model, ` 2 ,p-norm, is imposed to the regularization term to
effectively discover the sparse structures of the feature coefficient matrix.
In this way, noisy and irrelevant features are removed by ruling out those
features whose corresponding coefficients are zeros. To alleviate the local
optimum problem that is caused by random initializations of K-means, a
convergence guaranteed algorithm with an updating strategy for the clustering
indicator matrix, is proposed to iteractively chase the optimal solution.
Performance evaluation is extensively conducted over six benchmark data sets.
From plenty of experimental results, it is demonstrated that our method has
superior performance against all other compared approaches."
"The amount of data in our society has been exploding in the era of big data
today. In this paper, we address several open challenges of big data stream
classification, including high volume, high velocity, high dimensionality, high
sparsity, and high class-imbalance. Many existing studies in data mining
literature solve data stream classification tasks in a batch learning setting,
which suffers from poor efficiency and scalability when dealing with big data.
To overcome the limitations, this paper investigates an online learning
framework for big data stream classification tasks. Unlike some existing online
data stream classification techniques that are often based on first-order
online learning, we propose a framework of Sparse Online Classification (SOC)
for data stream classification, which includes some state-of-the-art
first-order sparse online learning algorithms as special cases and allows us to
derive a new effective second-order online learning algorithm for data stream
classification. In addition, we also propose a new cost-sensitive sparse online
learning algorithm by extending the framework with application to tackle online
anomaly detection tasks where class distribution of data could be very
imbalanced. We also analyze the theoretical bounds of the proposed method, and
finally conduct an extensive set of experiments, in which encouraging results
validate the efficacy of the proposed algorithms in comparison to a family of
state-of-the-art techniques on a variety of data stream classification tasks."
"We apply column generation to approximating complex structured objects via a
set of primitive structured objects under either the cross entropy or L2 loss.
We use L1 regularization to encourage the use of few structured primitive
objects. We attack approximation using convex optimization over an infinite
number of variables each corresponding to a primitive structured object that
are generated on demand by easy inference in the Lagrangian dual. We apply our
approach to producing low rank approximations to large 3-way tensors."
"Learning efficient representations for concepts has been proven to be an
important basis for many applications such as machine translation or document
classification. Proper representations of medical concepts such as diagnosis,
medication, procedure codes and visits will have broad applications in
healthcare analytics. However, in Electronic Health Records (EHR) the visit
sequences of patients include multiple concepts (diagnosis, procedure, and
medication codes) per visit. This structure provides two types of relational
information, namely sequential order of visits and co-occurrence of the codes
within each visit. In this work, we propose Med2Vec, which not only learns
distributed representations for both medical codes and visits from a large EHR
dataset with over 3 million visits, but also allows us to interpret the learned
representations confirmed positively by clinical experts. In the experiments,
Med2Vec displays significant improvement in key medical applications compared
to popular baselines such as Skip-gram, GloVe and stacked autoencoder, while
providing clinically meaningful interpretation."
"We explore the use of deep learning hierarchical models for problems in
financial prediction and classification. Financial prediction problems -- such
as those presented in designing and pricing securities, constructing
portfolios, and risk management -- often involve large data sets with complex
data interactions that currently are difficult or impossible to specify in a
full economic model. Applying deep learning methods to these problems can
produce more useful results than standard methods in finance. In particular,
deep learning can detect and exploit interactions in the data that are, at
least currently, invisible to any existing financial economic theory."
"We introduce a batched lazy algorithm for supervised classification using
decision trees. It avoids unnecessary visits to irrelevant nodes when it is
used to make predictions with either eagerly or lazily trained decision trees.
A set of experiments demonstrate that the proposed algorithm can outperform
both the conventional and lazy decision tree algorithms in terms of computation
time as well as memory consumption, without compromising accuracy."
"Most contemporary multi-task learning methods assume linear models. This
setting is considered shallow in the era of deep learning. In this paper, we
present a new deep multi-task representation learning framework that learns
cross-task sharing structure at every layer in a deep network. Our approach is
based on generalising the matrix factorisation techniques explicitly or
implicitly used by many conventional MTL algorithms to tensor factorisation, to
realise automatic learning of end-to-end knowledge sharing in deep networks.
This is in contrast to existing deep learning approaches that need a
user-defined multi-task sharing strategy. Our approach applies to both
homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our
deep multi-task representation learning in terms of both higher accuracy and
fewer design choices."
"In this paper, we bridge the gap between hyperparameter optimization and
ensemble learning by performing Bayesian optimization of an ensemble with
regards to its hyperparameters. Our method consists in building a fixed-size
ensemble, optimizing the configuration of one classifier of the ensemble at
each iteration of the hyperparameter optimization algorithm, taking into
consideration the interaction with the other models when evaluating potential
performances. We also consider the case where the ensemble is to be
reconstructed at the end of the hyperparameter optimization phase, through a
greedy selection over the pool of models generated during the optimization. We
study the performance of our proposed method on three different hyperparameter
spaces, showing that our approach is better than both the best single model and
a greedy ensemble construction over the models produced by a standard Bayesian
optimization."
"With the rise of big data sets, the popularity of kernel methods declined and
neural networks took over again. The main problem with kernel methods is that
the kernel matrix grows quadratically with the number of data points. Most
attempts to scale up kernel methods solve this problem by discarding data
points or basis functions of some approximation of the kernel map. Here we
present a simple yet effective alternative for scaling up kernel methods that
takes into account the entire data set via doubly stochastic optimization of
the emprical kernel map. The algorithm is straightforward to implement, in
particular in parallel execution settings; it leverages the full power and
versatility of classical kernel functions without the need to explicitly
formulate a kernel map approximation. We provide empirical evidence that the
algorithm works on large data sets."
"The engineering of machine learning systems is still a nascent field; relying
on a seemingly daunting collection of quickly evolving tools and best
practices. It is our hope that this guidebook will serve as a useful resource
for machine learning practitioners looking to take advantage of Bayesian
optimization techniques. We outline four example machine learning problems that
can be solved using open source machine learning libraries, and highlight the
benefits of using Bayesian optimization in the context of these common machine
learning applications."
"The recent successful deep neural networks are largely trained in a
supervised manner. It {\it associates} complex patterns of input samples with
neurons in the last layer, which form representations of {\it concepts}. In
spite of their successes, the properties of complex patterns associated a
learned concept remain elusive. In this work, by analyzing how neurons are
associated with concepts in supervised networks, we hypothesize that with
proper priors to regulate learning, neural networks can automatically associate
neurons in the intermediate layers with concepts that are aligned with real
world concepts, when trained only with labels that associate concepts with top
level neurons, which is a plausible way for unsupervised learning. We develop a
prior to verify the hypothesis and experimentally find the proposed prior help
neural networks automatically learn both basic physical concepts at the lower
layers, e.g., rotation of filters, and highly semantic concepts at the higher
layers, e.g., fine-grained categories of an entry-level category."
"In this paper, we discuss a different type of semi-supervised setting: a
coarse level of labeling is available for all observations but the model has to
learn a fine level of latent annotation for each one of them. Problems in this
setting are likely to be encountered in many domains such as text
categorization, protein function prediction, image classification as well as in
exploratory scientific studies such as medical and genomics research. We
consider this setting as simultaneously performed supervised classification
(per the available coarse labels) and unsupervised clustering (within each one
of the coarse labels) and propose a novel output layer modification called
auto-clustering output layer (ACOL) that allows concurrent classification and
clustering based on Graph-based Activity Regularization (GAR) technique. As the
proposed output layer modification duplicates the softmax nodes at the output
layer for each class, GAR allows for competitive learning between these
duplicates on a traditional error-correction learning framework to ultimately
enable a neural network to learn the latent annotations in this partially
supervised setup. We demonstrate how the coarse label supervision impacts
performance and helps propagate useful clustering information between
sub-classes. Comparative tests on three of the most popular image datasets
MNIST, SVHN and CIFAR-100 rigorously demonstrate the effectiveness and
competitiveness of the proposed approach."
"Today, detection of anomalous events in civil infrastructures (e.g. water
pipe breaks and leaks) is time consuming and often takes hours or days. Pipe
breakage as one of the most frequent types of failure of water networks often
causes community disruptions ranging from temporary interruptions in services
to extended loss of business and relocation of residents. In this project, we
design and implement a two-phase approach for leak event identification, which
leverages dynamic data from multiple information sources including IoT sensing
data (pressure values and/or flow rates), geophysical data (water systems), and
human inputs (tweets posted on Twitter). In the approach, a high order
Conditional Random Field (CRF) is constructed that enforces predictions based
on IoT observations consistent with human inputs to improve the performance of
event identifications.
  Considering the physical water network as a graph, a CRF model is built and
learned by the Structured Support Vector Machine (SSVM) using node features
such as water pressure and flow rate. After that, we built the high order CRF
system by enforcing twitter leakage detection information. An optimal inference
algorithm is proposed for the adapted high order CRF model. Experimental
results show the effectiveness of our system."
"Ensembles of neural networks are known to be much more robust and accurate
than individual networks. However, training multiple deep networks for model
averaging is computationally expensive. In this paper, we propose a method to
obtain the seemingly contradictory goal of ensembling multiple neural networks
at no additional training cost. We achieve this goal by training a single
neural network, converging to several local minima along its optimization path
and saving the model parameters. To obtain repeated rapid convergence, we
leverage recent work on cyclic learning rate schedules. The resulting
technique, which we refer to as Snapshot Ensembling, is simple, yet
surprisingly effective. We show in a series of experiments that our approach is
compatible with diverse network architectures and learning tasks. It
consistently yields lower error rates than state-of-the-art single models at no
additional training cost, and compares favorably with traditional network
ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain
error rates of 3.4% and 17.4% respectively."
"Multi-Label Classification toolbox is a MATLAB/OCTAVE library for Multi-Label
Classification (MLC). There exists a few Java libraries for MLC, but no
MATLAB/OCTAVE library that covers various methods. This toolbox offers an
environment for evaluation, comparison and visualization of the MLC results.
One attraction of this toolbox is that it enables us to try many combinations
of feature space dimension reduction, sample clustering, label space dimension
reduction and ensemble, etc."
"We propose proximal backpropagation (ProxProp) as a novel algorithm that
takes implicit instead of explicit gradient steps to update the network
parameters during neural network training. Our algorithm is motivated by the
step size limitation of explicit gradient descent, which poses an impediment
for optimization. ProxProp is developed from a general point of view on the
backpropagation algorithm, currently the most common technique to train neural
networks via stochastic gradient descent and variants thereof. Specifically, we
show that backpropagation of a prediction error is equivalent to sequential
gradient descent steps on a quadratic penalty energy, which comprises the
network activations as variables of the optimization. We further analyze
theoretical properties of ProxProp and in particular prove that the algorithm
yields a descent direction in parameter space and can therefore be combined
with a wide variety of convergent algorithms. Finally, we devise an efficient
numerical implementation that integrates well with popular deep learning
frameworks. We conclude by demonstrating promising numerical results and show
that ProxProp can be effectively combined with common first order optimizers
such as Adam."
"In the context of data-mining competitions (e.g., Kaggle, KDDCup, ILSVRC
Challenge), we show how access to an oracle that reports a contestant's
log-loss score on the test set can be exploited to deduce the ground-truth of
some of the test examples. By applying this technique iteratively to batches of
$m$ examples (for small $m$), all of the test labels can eventually be
inferred. In this paper, (1) We demonstrate this attack on the first stage of a
recent Kaggle competition (Intel & MobileODT Cancer Screening) and use it to
achieve a log-loss of $0.00000$ (and thus attain a rank of #4 out of 848
contestants), without ever training a classifier to solve the actual task. (2)
We prove an upper bound on the batch size $m$ as a function of the
floating-point resolution of the probability estimates that the contestant
submits for the labels. (3) We derive, and demonstrate in simulation, a more
flexible attack that can be used even when the oracle reports the accuracy on
an unknown (but fixed) subset of the test set's labels. These results underline
the importance of evaluating contestants based only on test data that the
oracle does not examine."
"Autonomous unpowered flight is a challenge for control and guidance systems:
all the energy the aircraft might use during flight has to be harvested
directly from the atmosphere. We investigate the design of an algorithm that
optimizes the closed-loop control of a glider's bank and sideslip angles, while
flying in the lower convective layer of the atmosphere in order to increase its
mission endurance. Using a Reinforcement Learning approach, we demonstrate the
possibility for real-time adaptation of the glider's behaviour to the
time-varying and noisy conditions associated with thermal soaring flight. Our
approach is online, data-based and model-free, hence avoids the pitfalls of
aerological and aircraft modelling and allow us to deal with uncertainties and
non-stationarity. Additionally, we put a particular emphasis on keeping low
computational requirements in order to make on-board execution feasible. This
article presents the stochastic, time-dependent aerological model used for
simulation, together with a standard aircraft model. Then we introduce an
adaptation of a Q-learning algorithm and demonstrate its ability to control the
aircraft and improve its endurance by exploiting updrafts in non-stationary
scenarios."
"In this paper, we study the problem of optimizing a two-layer artificial
neural network that best fits a training dataset. We look at this problem in
the setting where the number of parameters is greater than the number of
sampled points. We show that for a wide class of differentiable activation
functions (this class involves ""almost"" all functions which are not piecewise
linear), we have that first-order optimal solutions satisfy global optimality
provided the hidden layer is non-singular. Our results are easily extended to
hidden layers given by a flat matrix from that of a square matrix. Results are
applicable even if network has more than one hidden layer provided all hidden
layers satisfy non-singularity, all activations are from the given ""good"" class
of differentiable functions and optimization is only with respect to the last
hidden layer. We also study the smoothness properties of the objective function
and show that it is actually Lipschitz smooth, i.e., its gradients do not
change sharply. We use smoothness properties to guarantee asymptotic
convergence of O(1/number of iterations) to a first-order optimal solution. We
also show that our algorithm will maintain non-singularity of hidden layer for
any finite number of iterations."
"Coronary Artery Disease (CAD) is one of the leading causes of death
worldwide, and so it is very important to correctly diagnose patients with the
disease. For medical diagnosis, machine learning is a useful tool, however
features and algorithms must be carefully selected to get accurate
classification. To this effect, three feature selection methods have been used
on 13 input features from the Cleveland dataset with 297 entries, and 7 were
selected. The selected features were used to train three different classifiers,
which are SVM, Na\""ive Bayes and KNN using 10-fold cross-validation. The
resulting models evaluated using Accuracy, Recall, Specificity and Precision.
It is found that the Na\""ive Bayes classifier performs the best on this dataset
and features, outperforming or matching SVM and KNN in all the four evaluation
parameters used and achieving an accuracy of 84%."
"Partial Label Learning (PLL) aims to train a classifier when each training
instance is associated with a set of candidate labels, among which only one is
correct but is not accessible during the training phase. The common strategy
dealing with such ambiguous labeling information is to disambiguate the
candidate label sets. Nonetheless, existing methods ignore the disambiguation
difficulty of instances and adopt the single-trend training mechanism. The
former would lead to the vulnerability of models to the false positive labels
and the latter may arouse error accumulation problem. To remedy these two
drawbacks, this paper proposes a novel approach termed ""Network Cooperation
with Progressive Disambiguation"" (NCPD) for PLL. Specifically, we devise a
progressive disambiguation strategy of which the disambiguation operations are
performed on simple instances firstly and then gradually on more complicated
ones. Therefore, the negative impacts brought by the false positive labels of
complicated instances can be effectively mitigated as the disambiguation
ability of the model has been strengthened via learning from the simple
instances. Moreover, by employing artificial neural networks as the backbone,
we utilize a network cooperation mechanism which trains two networks
collaboratively by letting them interact with each other. As two networks have
different disambiguation ability, such interaction is beneficial for both
networks to reduce their respective disambiguation errors, and thus is much
better than the existing algorithms with single-trend training process.
Extensive experimental results on various benchmark and practical datasets
demonstrate the superiority of our NCPD to other state-of-the-art PLL methods."
"Ecological Momentary Assessment (EMA) data is organized in multiple levels
(per-subject, per-day, etc.) and this particular structure should be taken into
account in machine learning algorithms used in EMA like decision trees and its
variants. We propose a new algorithm called BBT (standing for Bagged Boosted
Trees) that is enhanced by a over/under sampling method and can provide better
estimates for the conditional class probability function. Experimental results
on a real-world dataset show that BBT can benefit EMA data classification and
performance."
"Handling imbalanced datasets is a challenging problem that if not treated
correctly results in reduced classification performance. Imbalanced datasets
are commonly handled using minority oversampling, whereas the SMOTE algorithm
is a successful oversampling algorithm with numerous extensions. SMOTE
extensions do not have a theoretical guarantee during training to work better
than SMOTE and in many instances their performance is data dependent. In this
paper we propose a novel extension to the SMOTE algorithm with a theoretical
guarantee for improved classification performance. The proposed approach
considers the classification performance of both the majority and minority
classes. In the proposed approach CGMOS (Certainty Guided Minority
OverSampling) new data points are added by considering certainty changes in the
dataset. The paper provides a proof that the proposed algorithm is guaranteed
to work better than SMOTE for training data. Further experimental results on 30
real-world datasets show that CGMOS works better than existing algorithms when
using 6 different classifiers."
"We present a method for using previously-trained 'teacher' agents to
kickstart the training of a new 'student' agent. To this end, we leverage ideas
from policy distillation and population based training. Our method places no
constraints on the architecture of the teacher or student agents, and it
regulates itself to allow the students to surpass their teachers in
performance. We show that, on a challenging and computationally-intensive
multi-task benchmark (DMLab-30), kickstarted training improves the data
efficiency of new agents, making it significantly easier to iterate on their
design. We also show that the same kickstarting pipeline can allow a single
student agent to leverage multiple 'expert' teachers which specialize on
individual tasks. In this setting kickstarting yields surprisingly large gains,
with the kickstarted agent matching the performance of an agent trained from
scratch in almost 10x fewer steps, and surpassing its final performance by 42
percent. Kickstarting is conceptually simple and can easily be incorporated
into reinforcement learning experiments."
"When dealing with multi-class classification problems, it is common practice
to build a model consisting of a series of binary classifiers using a learning
paradigm which dictates how the classifiers are built and combined to
discriminate between the individual classes. As new data enters the system and
the model needs updating, these models would often need to be retrained from
scratch. This work proposes three learning paradigms which allow trained models
to be updated without the need of retraining from scratch. A comparative
analysis is performed to evaluate them against a baseline. Results show that
the proposed paradigms are faster than the baseline at updating, with two of
them being faster at training from scratch as well, especially on larger
datasets, while retaining a comparable classification performance."
"We propose two policy gradient algorithms for solving the problem of control
in an off-policy reinforcement learning (RL) context. Both algorithms
incorporate a smoothed functional (SF) based gradient estimation scheme. The
first algorithm is a straightforward combination of importance sampling-based
off-policy evaluation with SF-based gradient estimation. The second algorithm,
inspired by the stochastic variance-reduced gradient (SVRG) algorithm,
incorporates variance reduction in the update iteration. For both algorithms,
we derive non-asymptotic bounds that establish convergence to an approximate
stationary point. From these results, we infer that the first algorithm
converges at a rate that is comparable to the well-known REINFORCE algorithm in
an off-policy RL context, while the second algorithm exhibits an improved rate
of convergence."
"A large fraction of major waterways have dams influencing streamflow, which
must be accounted for in large-scale hydrologic modeling. However, daily
streamflow prediction for basins with dams is challenging for various modeling
approaches, especially at large scales. Here we examined which types of dammed
basins could be well represented by long short-term memory (LSTM) models using
readily-available information, and delineated the remaining challenges. We
analyzed data from 3557 basins (83% dammed) over the contiguous United States
and noted strong impacts of reservoir purposes, degree of regulation (dor), and
diversion on streamflow modeling. While a model trained on a widely-used
reference-basin dataset performed poorly for non-reference basins, the model
trained on the whole dataset presented a median Nash-Sutcliffe efficiency
coefficient (NSE) of 0.74. The zero-dor, small-dor (with storage of
approximately a month of average streamflow or less), and large-dor basins were
found to have distinct behaviors, so migrating models between categories
yielded catastrophic results, which means we must not treat small-dor basins as
reference ones. However, training with pooled data from different sets yielded
optimal median NSEs of 0.72, 0.79, and 0.64 for these respective groups,
noticeably stronger than existing models. These results support a coherent
modeling strategy where smaller dams (storing about a month of average
streamflow or less) are modeled implicitly as part of basin rainfall-runoff
processes; then, large-dor reservoirs of certain types can be represented
explicitly. However, dammed basins must be present in the training dataset.
Future work should examine separate modeling of large reservoirs for fire
protection and irrigation, hydroelectric power generation, and flood control."
"Functional near-infrared spectroscopy (fNIRS) is a non-invasive, low-cost
method used to study the brain's blood flow pattern. Such patterns can enable
us to classify performed by a subject. In recent research, most classification
systems use traditional machine learning algorithms for the classification of
tasks. These methods, which are easier to implement, usually suffer from low
accuracy. Further, a complex pre-processing phase is required for data
preparation before implementing traditional machine learning methods. The
proposed system uses a Bi-Directional LSTM based deep learning architecture for
task classification, including mental arithmetic, motor imagery, and idle state
using fNIRS data. Further, this system will require less pre-processing than
the traditional approach, saving time and computational resources while
obtaining an accuracy of 81.48\%, which is considerably higher than the
accuracy obtained using conventional machine learning algorithms for the same
data set."
"Leading up to August 2020, COVID-19 has spread to almost every country in the
world, causing millions of infected and hundreds of thousands of deaths. In
this paper, we first verify the assumption that clinical variables could have
time-varying effects on COVID-19 outcomes. Then, we develop a temporal
stratification approach to make daily predictions on patients' outcome at the
end of hospital stay. Training data is segmented by the remaining length of
stay, which is a proxy for the patient's overall condition. Based on this, a
sequence of predictive models are built, one for each time segment. Thanks to
the publicly shared data, we were able to build and evaluate prototype models.
Preliminary experiments show 0.98 AUROC, 0.91 F1 score and 0.97 AUPR on
continuous deterioration prediction, encouraging further development of the
model as well as validations on different datasets. We also verify the key
assumption which motivates our method. Clinical variables could have
time-varying effects on COVID-19 outcomes. That is to say, the feature
importance of a variable in the predictive model varies at different disease
stages."
"The deadly triad refers to the instability of a reinforcement learning
algorithm when it employs off-policy learning, function approximation, and
bootstrapping simultaneously. In this paper, we investigate the target network
as a tool for breaking the deadly triad, providing theoretical support for the
conventional wisdom that a target network stabilizes training. We first propose
and analyze a novel target network update rule which augments the commonly used
Polyak-averaging style update with two projections. We then apply the target
network and ridge regularization in several divergent algorithms and show their
convergence to regularized TD fixed points. Those algorithms are off-policy
with linear function approximation and bootstrapping, spanning both policy
evaluation and control, as well as both discounted and average-reward settings.
In particular, we provide the first convergent linear $Q$-learning algorithms
under nonrestrictive and changing behavior policies without bi-level
optimization."
"Following [21, 23], the present work investigates a new relative
entropy-regularized algorithm for solving the optimal transport on a graph
problem within the randomized shortest paths formalism. More precisely, a unit
flow is injected into a set of input nodes and collected from a set of output
nodes while minimizing the expected transportation cost together with a paths
relative entropy regularization term, providing a randomized routing policy.
The main advantage of this new formulation is the fact that it can easily
accommodate edge flow capacity constraints which commonly occur in real-world
problems. The resulting optimal routing policy, i.e., the probability
distribution of following an edge in each node, is Markovian and is computed by
constraining the input and output flows to the prescribed marginal
probabilities thanks to a variant of the algorithm developed in [8]. In
addition, experimental comparisons with other recently developed techniques
show that the distance measure between nodes derived from the introduced model
provides competitive results on semi-supervised classification tasks."
"Long-tailed learning has attracted much attention recently, with the goal of
improving generalisation for tail classes. Most existing works use supervised
learning without considering the prevailing noise in the training dataset. To
move long-tailed learning towards more realistic scenarios, this work
investigates the label noise problem under long-tailed label distribution. We
first observe the negative impact of noisy labels on the performance of
existing methods, revealing the intrinsic challenges of this problem. As the
most commonly used approach to cope with noisy labels in previous literature,
we then find that the small-loss trick fails under long-tailed label
distribution. The reason is that deep neural networks cannot distinguish
correctly-labeled and mislabeled examples on tail classes. To overcome this
limitation, we establish a new prototypical noise detection method by designing
a distance-based metric that is resistant to label noise. Based on the above
findings, we propose a robust framework,~\algo, that realizes noise detection
for long-tailed learning, followed by soft pseudo-labeling via both label
smoothing and diverse label guessing. Moreover, our framework can naturally
leverage semi-supervised learning algorithms to further improve the
generalisation. Extensive experiments on benchmark and real-world datasets
demonstrate the superiority of our methods over existing baselines. In
particular, our method outperforms DivideMix by 3\% in test accuracy. Source
code will be released soon."
"In the information explosion era, recommender systems (RSs) are widely
studied and applied to discover user-preferred information. A RS performs
poorly when suffering from the cold-start issue, which can be alleviated if
incorporating Knowledge Graphs (KGs) as side information. However, most
existing works neglect the facts that node degrees in KGs are skewed and
massive amount of interactions in KGs are recommendation-irrelevant. To address
these problems, in this paper, we propose Differentiable Sampling on Knowledge
Graph for Recommendation with Relational GNN (DSKReG) that learns the relevance
distribution of connected items from KGs and samples suitable items for
recommendation following this distribution. We devise a differentiable sampling
strategy, which enables the selection of relevant items to be jointly optimized
with the model training procedure. The experimental results demonstrate that
our model outperforms state-of-the-art KG-based recommender systems. The code
is available online at https://github.com/YuWang-1024/DSKReG."
"The capacity of a learning machine is measured by its Vapnik-Chervonenkis
dimension, and learning machines with a low VC dimension generalize better. It
is well known that the VC dimension of SVMs can be very large or unbounded,
even though they generally yield state-of-the-art learning performance. In this
paper, we show how to learn a hyperplane regressor by minimizing an exact, or
\boldmath{$\Theta$} bound on its VC dimension. The proposed approach, termed as
the Minimal Complexity Machine (MCM) Regressor, involves solving a simple
linear programming problem. Experimental results show, that on a number of
benchmark datasets, the proposed approach yields regressors with error rates
much less than those obtained with conventional SVM regresssors, while often
using fewer support vectors. On some benchmark datasets, the number of support
vectors is less than one tenth the number used by SVMs, indicating that the MCM
does indeed learn simpler representations."
"Real-time prediction of clinical interventions remains a challenge within
intensive care units (ICUs). This task is complicated by data sources that are
noisy, sparse, heterogeneous and outcomes that are imbalanced. In this paper,
we integrate data from all available ICU sources (vitals, labs, notes,
demographics) and focus on learning rich representations of this data to
predict onset and weaning of multiple invasive interventions. In particular, we
compare both long short-term memory networks (LSTM) and convolutional neural
networks (CNN) for prediction of five intervention tasks: invasive ventilation,
non-invasive ventilation, vasopressors, colloid boluses, and crystalloid
boluses. Our predictions are done in a forward-facing manner to enable
""real-time"" performance, and predictions are made with a six hour gap time to
support clinically actionable planning. We achieve state-of-the-art results on
our predictive tasks using deep architectures. We explore the use of feature
occlusion to interpret LSTM models, and compare this to the interpretability
gained from examining inputs that maximally activate CNN outputs. We show that
our models are able to significantly outperform baselines in intervention
prediction, and provide insight into model learning, which is crucial for the
adoption of such models in practice."
"Our work addresses two important issues with recurrent neural networks: (1)
they are over-parameterized, and (2) the recurrence matrix is ill-conditioned.
The former increases the sample complexity of learning and the training time.
The latter causes the vanishing and exploding gradient problem. We present a
flexible recurrent neural network model called Kronecker Recurrent Units (KRU).
KRU achieves parameter efficiency in RNNs through a Kronecker factored
recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by
enforcing soft unitary constraints on the factors. Thanks to the small
dimensionality of the factors, maintaining these constraints is computationally
efficient. Our experimental results on seven standard data-sets reveal that KRU
can reduce the number of parameters by three orders of magnitude in the
recurrent weight matrix compared to the existing recurrent models, without
trading the statistical performance. These results in particular show that
while there are advantages in having a high dimensional recurrent space, the
capacity of the recurrent part of the model can be dramatically reduced."
"We study a variation of the classical multi-armed bandits problem. In this
problem, the learner has to make a sequence of decisions, picking from a fixed
set of choices. In each round, she receives as feedback only the loss incurred
from the chosen action. Conventionally, this problem has been studied when
losses of the actions are drawn from an unknown distribution or when they are
adversarial. In this paper, we study this problem when the losses of the
actions also satisfy certain structural properties, and especially, do show a
trend structure. When this is true, we show that using \textit{trend
detection}, we can achieve regret of order $\tilde{O} (N \sqrt{TK})$ with
respect to a switching strategy for the version of the problem where a single
action is chosen in each round and $\tilde{O} (Nm \sqrt{TK})$ when $m$ actions
are chosen each round. This guarantee is a significant improvement over the
conventional benchmark. Our approach can, as a framework, be applied in
combination with various well-known bandit algorithms, like Exp3. For both
versions of the problem, we give regret guarantees also for the
\textit{anytime} setting, i.e. when the length of the choice-sequence is not
known in advance. Finally, we pinpoint the advantages of our method by
comparing it to some well-known other strategies."
"The OpenAI Gym provides researchers and enthusiasts with simple to use
environments for reinforcement learning. Even the simplest environment have a
level of complexity that can obfuscate the inner workings of RL approaches and
make debugging difficult. This whitepaper describes a Python framework that
makes it very easy to create simple Markov-Decision-Process environments
programmatically by specifying state transitions and rewards of deterministic
and non-deterministic MDPs in a domain-specific language in Python. It then
presents results and visualizations created with this MDP framework."
"Importance sampling (IS) as an elegant and efficient variance reduction (VR)
technique for the acceleration of stochastic optimization problems has
attracted many researches recently. Unlike commonly adopted stochastic uniform
sampling in stochastic optimizations, IS-integrated algorithms sample training
data at each iteration with respect to a weighted sampling probability
distribution $P$, which is constructed according to the precomputed importance
factors. Previous experimental results show that IS has achieved remarkable
progresses in the acceleration of training convergence. Unfortunately, the
calculation of the sampling probability distribution $P$ causes a major
limitation of IS: it requires the input data to be well-structured, i.e., the
feature vector is properly defined. Consequently, recurrent neural networks
(RNN) as a popular learning algorithm is not able to enjoy the benefits of IS
due to the fact that its raw input data, i.e., the training sequences, are
often unstructured which makes calculation of $P$ impossible. In considering of
the the popularity of RNN-based learning applications and their relative long
training time, we are interested in accelerating them through IS. This paper
propose a novel Fast-Importance-Mining algorithm to calculate the importance
factor for unstructured data which makes the application of IS in RNN-based
applications possible. Our experimental evaluation on popular open-source
RNN-based learning applications validate the effectiveness of IS in improving
the convergence rate of RNNs."
"Data-driven algorithm design, that is, choosing the best algorithm for a
specific application, is a crucial problem in modern data science.
Practitioners often optimize over a parameterized algorithm family, tuning
parameters based on problems from their domain. These procedures have
historically come with no guarantees, though a recent line of work studies
algorithm selection from a theoretical perspective. We advance the foundations
of this field in several directions: we analyze online algorithm selection,
where problems arrive one-by-one and the goal is to minimize regret, and
private algorithm selection, where the goal is to find good parameters over a
set of problems without revealing sensitive information contained therein. We
study important algorithm families, including SDP-rounding schemes for problems
formulated as integer quadratic programs, and greedy techniques for canonical
subset selection problems. In these cases, the algorithm's performance is a
volatile and piecewise Lipschitz function of its parameters, since tweaking the
parameters can completely change the algorithm's behavior. We give a sufficient
and general condition, dispersion, defining a family of piecewise Lipschitz
functions that can be optimized online and privately, which includes the
functions measuring the performance of the algorithms we study. Intuitively, a
set of piecewise Lipschitz functions is dispersed if no small region contains
many of the functions' discontinuities. We present general techniques for
online and private optimization of the sum of dispersed piecewise Lipschitz
functions. We improve over the best-known regret bounds for a variety of
problems, prove regret bounds for problems not previously studied, and give
matching lower bounds. We also give matching upper and lower bounds on the
utility loss due to privacy. Moreover, we uncover dispersion in auction design
and pricing problems."
"This project develops and trains a Recurrent Neural Network (RNN) that
monitors sleeping infants from an auxiliary microphone for cases of Sudden
Infant Death Syndrome (SIDS), manifested in sudden or gradual respiratory
arrest. To minimize invasiveness and maximize economic viability, an electret
microphone, and parabolic concentrator, paired with a specially designed and
tuned amplifier circuit, was used as a very sensitive audio monitoring device,
which fed data to the RNN model. This RNN was trained and operated in the
frequency domain, where the respiratory activity is most unique from noise. In
both training and operation, a Fourier transform and an autoencoder compression
were applied to the raw audio, and this transformed audio data was fed into the
model in 1/8 second time steps. In operation, this model flagged each perceived
breath, and the time between breaths was analyzed through a statistical T-test
for slope, which detected dangerous trends. The entire model achieved 92.5%
accuracy on continuous data and had an 11.25-second response rate on data that
emulated total respiratory arrest. Because of the compatibility of the trained
model with many off-the-shelf devices like Android phones and Raspberry Pi's,
free-standing processing hardware deployment is a very feasible future goal."
"Solving partial differential equations (PDE) is an indispensable part of many
branches of science as many processes can be modelled in terms of PDEs.
However, recent numerical solvers require manual discretization of the
underlying equation as well as sophisticated, tailored code for distributed
computing. Scanning the parameters of the underlying model significantly
increases the runtime as the simulations have to be cold-started for each
parameter configuration. Machine Learning based surrogate models denote
promising ways for learning complex relationship among input, parameter and
solution. However, recent generative neural networks require lots of training
data, i.e. full simulation runs making them costly. In contrast, we examine the
applicability of continuous, mesh-free neural solvers for partial differential
equations, physics-informed neural networks (PINNs) solely requiring
initial/boundary values and validation points for training but no simulation
data. The induced curse of dimensionality is approached by learning a domain
decomposition that steers the number of neurons per unit volume and
significantly improves runtime. Distributed training on large-scale cluster
systems also promises great utilization of large quantities of GPUs which we
assess by a comprehensive evaluation study. Finally, we discuss the accuracy of
GatedPINN with respect to analytical solutions -- as well as state-of-the-art
numerical solvers, such as spectral solvers."
"Score matching (SM) provides a compelling approach to learn energy-based
models (EBMs) by avoiding the calculation of partition function. However, it
remains largely open to learn energy-based latent variable models (EBLVMs),
except some special cases. This paper presents a bi-level score matching (BiSM)
method to learn EBLVMs with general structures by reformulating SM as a
bi-level optimization problem. The higher level introduces a variational
posterior of the latent variables and optimizes a modified SM objective, and
the lower level optimizes the variational posterior to fit the true posterior.
To solve BiSM efficiently, we develop a stochastic optimization algorithm with
gradient unrolling. Theoretically, we analyze the consistency of BiSM and the
convergence of the stochastic algorithm. Empirically, we show the promise of
BiSM in Gaussian restricted Boltzmann machines and highly nonstructural EBLVMs
parameterized by deep convolutional neural networks. BiSM is comparable to the
widely adopted contrastive divergence and SM methods when they are applicable;
and can learn complex EBLVMs with intractable posteriors to generate natural
images."
"In many RL applications, once training ends, it is vital to detect any
deterioration in the agent performance as soon as possible. Furthermore, it
often has to be done without modifying the policy and under minimal assumptions
regarding the environment. In this paper, we address this problem by focusing
directly on the rewards and testing for degradation. We consider an episodic
framework, where the rewards within each episode are not independent, nor
identically-distributed, nor Markov. We present this problem as a multivariate
mean-shift detection problem with possibly partial observations. We define the
mean-shift in a way corresponding to deterioration of a temporal signal (such
as the rewards), and derive a test for this problem with optimal statistical
power. Empirically, on deteriorated rewards in control problems (generated
using various environment modifications), the test is demonstrated to be more
powerful than standard tests - often by orders of magnitude. We also suggest a
novel Bootstrap mechanism for False Alarm Rate control (BFAR), applicable to
episodic (non-i.i.d) signal and allowing our test to run sequentially in an
online manner. Our method does not rely on a learned model of the environment,
is entirely external to the agent, and in fact can be applied to detect changes
or drifts in any episodic signal."
"Given a set of sequences comprised of time-ordered events, sequential pattern
mining is useful to identify frequent subsequences from different sequences or
within the same sequence. However, in sport, these techniques cannot determine
the importance of particular patterns of play to good or bad outcomes, which is
often of greater interest to coaches and performance analysts. In this study,
we apply a recently proposed supervised sequential pattern mining algorithm
called safe pattern pruning (SPP) to 490 labelled event sequences representing
passages of play from one rugby team's matches from the 2018 Japan Top League.
We compare the SPP-obtained patterns that are the most discriminative between
scoring and non-scoring outcomes from both the team's and opposition teams'
perspectives, with the most frequent patterns obtained with well-known
unsupervised sequential pattern mining algorithms when applied to subsets of
the original dataset, split on the label. Our obtained results found that
linebreaks, successful lineouts, regained kicks in play, repeated
phase-breakdown play, and failed exit plays by the opposition team were
identified as as the patterns that discriminated most between the team scoring
and not scoring. Opposition team linebreaks, errors made by the team,
opposition team lineouts, and repeated phase-breakdown play by the opposition
team were identified as the patterns that discriminated most between the
opposition team scoring and not scoring. It was also found that, by virtue of
its supervised nature as well as its pruning and safe-screening properties, SPP
obtained a greater variety of generally more sophisticated patterns than the
unsupervised models, which are likely to be of more utility to coaches and
performance analysts."
"In many applications, a dataset can be considered as a set of observed
signals that live on an unknown underlying graph structure. Some of these
signals may be seen as white noise that has been filtered on the graph topology
by a graph filter. Hence, the knowledge of the filter and the graph provides
valuable information about the underlying data generation process and the
complex interactions that arise in the dataset. We hence introduce a novel
graph signal processing framework for jointly learning the graph and its
generating filter from signal observations. We cast a new optimisation problem
that minimises the Wasserstein distance between the distribution of the signal
observations and the filtered signal distribution model. Our proposed method
outperforms state-of-the-art graph learning frameworks on synthetic data. We
then apply our method to a temperature anomaly dataset, and further show how
this framework can be used to infer missing values if only very little
information is available."
"Sufficient exploration is paramount for the success of a reinforcement
learning agent. Yet, exploration is rarely assessed in an algorithm-independent
way. We compare the behavior of three data-based, offline exploration metrics
described in the literature on intuitive simple distributions and highlight
problems to be aware of when using them. We propose a fourth metric,uniform
relative entropy, and implement it using either a k-nearest-neighbor or a
nearest-neighbor-ratio estimator, highlighting that the implementation choices
have a profound impact on these measures."
"We describe lessons learned from developing and deploying machine learning
models at scale across the enterprise in a range of financial analytics
applications. These lessons are presented in the form of antipatterns. Just as
design patterns codify best software engineering practices, antipatterns
provide a vocabulary to describe defective practices and methodologies. Here we
catalog and document numerous antipatterns in financial ML operations (MLOps).
Some antipatterns are due to technical errors, while others are due to not
having sufficient knowledge of the surrounding context in which ML results are
used. By providing a common vocabulary to discuss these situations, our intent
is that antipatterns will support better documentation of issues, rapid
communication between stakeholders, and faster resolution of problems. In
addition to cataloging antipatterns, we describe solutions, best practices, and
future directions toward MLOps maturity."
"Motor disturbances can affect the interaction with dynamic objects, such as
catching a ball. A classification of clinical catching trials might give
insight into the existence of pathological alterations in the relation of arm
and ball movements. Accurate, but also early decisions are required to classify
a catching attempt before the catcher's first ball contact. To obtain
clinically valuable results, a significant decision confidence of at least 75%
is required. Hence, three competing objectives have to be optimized at the same
time: accuracy, earliness and decision-making confidence. Here we propose a
coupled classification and prediction approach for early time series
classification: a predictive, generative recurrent neural network (RNN)
forecasts the next data points of ball trajectories based on already available
observations; a discriminative RNN continuously generates classification
guesses based on the available data points and the unrolled sequence
predictions. We compare our approach, which we refer to as predictive
sequential classification (PSC), to state-of-the-art sequence learners,
including various RNN and temporal convolutional network (TCN) architectures.
On this hard real-world task we can consistently demonstrate the superiority of
PSC over all other models in terms of accuracy and confidence with respect to
earliness of recognition. Specifically, PSC is able to confidently classify the
success of catching trials as early as 123 milliseconds before the first ball
contact. We conclude that PSC is a promising approach for early time series
classification, when accurate and confident decisions are required."
"Concept learning approaches based on refinement operators explore partially
ordered solution spaces to compute concepts, which are used as binary
classification models for individuals. However, the number of concepts explored
by these approaches can grow to the millions for complex learning problems.
This often leads to impractical runtimes. We propose to alleviate this problem
by predicting the length of target concepts before the exploration of the
solution space. By these means, we can prune the search space during concept
learning. To achieve this goal, we compare four neural architectures and
evaluate them on four benchmarks. Our evaluation results suggest that recurrent
neural network architectures perform best at concept length prediction with a
macro F-measure ranging from 38% to 92%. We then extend the CELOE algorithm,
which learns ALC concepts, with our concept length predictor. Our extension
yields the algorithm CLIP. In our experiments, CLIP is at least 7.5 times
faster than other state-of-the-art concept learning algorithms for ALC --
including CELOE -- and achieves significant improvements in the F-measure of
the concepts learned on 3 out of 4 datasets. For reproducibility, we provide
our implementation in the public GitHub repository at
https://github.com/dice-group/LearnALCLengths."
"The research internship at UiT - The Arctic University of Norway was offered
for our team being the winner of the 'Smart Roads - Winter Road Maintenance
2021' Hackathon. The internship commenced on 3 May 2021 and ended on 21 May
2021 with meetings happening twice each week. In spite of having different
nationalities and educational backgrounds, we both interns tried to collaborate
as a team as much as possible. The most alluring part was working on this
project made us realize the critical conditions faced by the arctic people,
where it was hard to gain such a unique experience from our residence. We
developed and implemented several deep learning models to classify the states
(dry, moist, wet, icy, snowy, slushy). Depending upon the best model, the
weather forecast app will predict the state taking the Ta, Tsurf, Height,
Speed, Water, etc. into consideration. The crucial part was to define a safety
metric which is the product of the accident rates based on friction and the
accident rates based on states. We developed a regressor that will predict the
safety metric depending upon the state obtained from the classifier and the
friction obtained from the sensor data. A pathfinding algorithm has been
designed using the sensor data, open street map data, weather data."
"Deep learning techniques have been paramount in the last years, mainly due to
their outstanding results in a number of applications, that range from speech
recognition to face-based user identification. Despite other techniques
employed for such purposes, Deep Boltzmann Machines are among the most used
ones, which are composed of layers of Restricted Boltzmann Machines (RBMs)
stacked on top of each other. In this work, we evaluate the concept of
temperature in DBMs, which play a key role in Boltzmann-related distributions,
but it has never been considered in this context up to date. Therefore, the
main contribution of this paper is to take into account this information and to
evaluate its influence in DBMs considering the task of binary image
reconstruction. We expect this work can foster future research considering the
usage of different temperatures during learning in DBMs."
"In this document we shows a first implementation and some preliminary results
of a new theory, facing Machine Learning problems in the frameworks of
Classical Mechanics and Variational Calculus. We give a general formulation of
the problem and then we studies basic behaviors of the model on simple
practical implementations."
"This work details CipherGAN, an architecture inspired by CycleGAN used for
inferring the underlying cipher mapping given banks of unpaired ciphertext and
plaintext. We demonstrate that CipherGAN is capable of cracking language data
enciphered using shift and Vigenere ciphers to a high degree of fidelity and
for vocabularies much larger than previously achieved. We present how CycleGAN
can be made compatible with discrete data and train in a stable way. We then
prove that the technique used in CipherGAN avoids the common problem of
uninformative discrimination associated with GANs applied to discrete data."
"Semi-supervised classification on graphs aims at assigning labels to all
nodes of a graph based on the labels known for a few nodes, called the seeds.
The most popular algorithm relies on the principle of heat diffusion, where the
labels of the seeds are spread by thermo-conductance and the temperature of
each node is used as a score function for each label. Using a simple block
model, we prove that this algorithm is not consistent unless the temperatures
of the nodes are centered before classification. We show that this simple
modification of the algorithm is enough to get significant performance gains on
real data."
"Measuring the generalization capacity of Deep Generative Models (DGMs) is
difficult because of the curse of dimensionality. Evaluation metrics for DGMs
such as Inception Score, Fr\'echet Inception Distance, Precision-Recall, and
Neural Net Divergence try to estimate the distance between the generated
distribution and the target distribution using a polynomial number of samples.
These metrics are the target of researchers when designing new models. Despite
the claims, it is still unclear how well can they measure the generalization
capacity of a generative model. In this paper, we investigate the capacity of
these metrics in measuring the generalization capacity. We introduce a
framework for comparing the robustness of evaluation metrics. We show that
better scores in these metrics do not imply better generalization. They can be
fooled easily by a generator that memorizes a small subset of the training set.
We propose a fix to the NND metric to make it more robust to noise in the
generated data. Toward building a robust metric for generalization, we propose
to apply the Minimum Description Length principle to the problem of evaluating
DGMs. We develop an efficient method for estimating the complexity of
Generative Latent Variable Models (GLVMs). Experimental results show that our
metric can effectively detect training set memorization and distinguish GLVMs
of different generalization capacities. Source code is available at
https://github.com/htt210/GeneralizationMetricGAN."
"Mutual information (MI) is an information-theoretic measure of dependency
between two random variables. Several methods to estimate MI, from samples of
two random variables with unknown underlying probability distributions have
been proposed in the literature. Recent methods realize parametric probability
distributions or critic as a neural network to approximate unknown density
ratios. The approximated density ratios are used to estimate different
variational lower bounds of MI. While these methods provide reliable estimation
when the true MI is low, they produce high variance estimates in cases of high
MI. We argue that the high variance characteristic is due to the uncontrolled
complexity of the critic's hypothesis space. In support of this argument, we
use the data-driven Rademacher complexity of the hypothesis space associated
with the critic's architecture to analyse generalization error bound of
variational lower bound estimates of MI. In the proposed work, we show that it
is possible to negate the high variance characteristics of these estimators by
constraining the critic's hypothesis space to Reproducing Hilbert Kernel Space
(RKHS), which corresponds to a kernel learned using Automated Spectral Kernel
Learning (ASKL). By analysing the aforementioned generalization error bounds,
we augment the overall optimisation objective with effective regularisation
term. We empirically demonstrate the efficacy of this regularization in
enforcing proper bias variance tradeoff on four variational lower bounds,
namely NWJ, MINE, JS and SMILE."
"Production software oftentimes suffers from the issue of performance
inefficiencies caused by inappropriate use of data structures, programming
abstractions, and conservative compiler optimizations. It is desirable to avoid
unnecessary memory operations. However, existing works often use a
whole-program fine-grained monitoring method with incredibly high overhead. To
this end, we propose a learning-aided approach to identify unnecessary memory
operations intelligently with low overhead. By applying several prevalent graph
neural network models to extract program semantics with respect to program
structure, execution order and dynamic states, we present a novel, hybrid
program embedding approach so that to derive unnecessary memory operations
through the embedding. We train our model with tens of thousands of samples
acquired from a set of real-world benchmarks. Results show that our model
achieves 90% of accuracy and incurs only around a half of time overhead of the
state-of-art tool."
"Uncertainty quantification is crucial for building reliable and trustable
machine learning systems. We propose to estimate uncertainty in recurrent
neural networks (RNNs) via stochastic discrete state transitions over recurrent
timesteps. The uncertainty of the model can be quantified by running a
prediction several times, each time sampling from the recurrent state
transition distribution, leading to potentially different results if the model
is uncertain. Alongside uncertainty quantification, our proposed method offers
several advantages in different settings. The proposed method can (1) learn
deterministic and probabilistic automata from data, (2) learn well-calibrated
models on real-world classification tasks, (3) improve the performance of
out-of-distribution detection, and (4) control the exploration-exploitation
trade-off in reinforcement learning."
"Is multiplication really necessary for deep neural networks? Here we propose
just adding two IEEE754 floating-point numbers with an integer-add instruction
in place of a floating-point multiplication instruction. We show that ResNet
can be trained using this operation with competitive classification accuracy.
Our proposal did not require any methods to solve instability and decrease in
accuracy, which is common in low-precision training. In some settings, we may
obtain equal accuracy to the baseline FP32 result. This method will enable
eliminating the multiplications in deep neural-network training and inference."
"A novel design optimization approach (ActivO) that employs an ensemble of
machine learning algorithms is presented. The proposed approach is a
surrogate-based scheme, where the predictions of a weak leaner and a strong
learner are utilized within an active learning loop. The weak learner is used
to identify promising regions within the design space to explore, while the
strong learner is used to determine the exact location of the optimum within
promising regions. For each design iteration, exploration is done by randomly
selecting evaluation points within regions where the weak learner-predicted
fitness is high. The global optimum obtained by using the strong learner as a
surrogate is also evaluated to enable rapid convergence once the most promising
region has been identified. First, the performance of ActivO was compared
against five other optimizers on a cosine mixture function with 25 local optima
and one global optimum. In the second problem, the objective was to minimize
indicated specific fuel consumption of a compression-ignition internal
combustion (IC) engine while adhering to desired constraints associated with
in-cylinder pressure and emissions. Here, the efficacy of the proposed approach
is compared to that of a genetic algorithm, which is widely used within the
internal combustion engine community for engine optimization, showing that
ActivO reduces the number of function evaluations needed to reach the global
optimum, and thereby time-to-design by 80%. Furthermore, the optimization of
engine design parameters leads to savings of around 1.9% in energy consumption,
while maintaining operability and acceptable pollutant emissions."
"We study an Open-World Class Discovery problem in which, given labeled
training samples from old classes, we need to discover new classes from
unlabeled test samples. There are two critical challenges to addressing this
paradigm: (a) transferring knowledge from old to new classes, and (b)
incorporating knowledge learned from new classes back to the original model. We
propose Class Discovery Kernel Network with Expansion (CD-KNet-Exp), a deep
learning framework, which utilizes the Hilbert Schmidt Independence Criterion
to bridge supervised and unsupervised information together in a systematic way,
such that the learned knowledge from old classes is distilled appropriately for
discovering new classes. Compared to competing methods, CD-KNet-Exp shows
superior performance on three publicly available benchmark datasets and a
challenging real-world radio frequency fingerprinting dataset."
"The size of Transformer models is growing at an unprecedented pace. It has
only taken less than one year to reach trillion-level parameters after the
release of GPT-3 (175B). Training such models requires both substantial
engineering efforts and enormous computing resources, which are luxuries most
research teams cannot afford. In this paper, we propose PipeTransformer, which
leverages automated and elastic pipelining and data parallelism for efficient
distributed training of Transformer models. PipeTransformer automatically
adjusts the pipelining and data parallelism by identifying and freezing some
layers during the training, and instead allocates resources for training of the
remaining active layers. More specifically, PipeTransformer dynamically
excludes converged layers from the pipeline, packs active layers into fewer
GPUs, and forks more replicas to increase data-parallel width. We evaluate
PipeTransformer using Vision Transformer (ViT) on ImageNet and BERT on GLUE and
SQuAD datasets. Our results show that PipeTransformer attains a 2.4 fold
speedup compared to the state-of-the-art baseline. We also provide various
performance analyses for a more comprehensive understanding of our algorithmic
and system-wise design. We also develop open-sourced flexible APIs for
PipeTransformer, which offer a clean separation among the freeze algorithm,
model definitions, and training accelerations, hence allowing it to be applied
to other algorithms that require similar freezing strategies."
"While insects are the largest and most diverse group of terrestrial animals,
constituting ca. 80% of all known species, they are difficult to study due to
their small size and similarity between species. Conventional monitoring
techniques depend on time consuming trapping methods and tedious
microscope-based work by skilled experts in order to identify the caught insect
specimen at species, or even family level. Researchers and policy makers are in
urgent need of a scalable monitoring tool in order to conserve biodiversity and
secure human food production due to the rapid decline in insect numbers.
  In order to improve upon existing insect clustering methods, we propose an
adaptive variant of the variational autoencoder (VAE) which is capable of
clustering data by phylogenetic groups. The proposed dynamic beta-VAE
dynamically adapts the scaling of the reconstruction and regularization loss
terms (beta value) yielding useful latent representations of the input data. We
demonstrate the usefulness of the dynamic beta-VAE on optically recorded insect
signals from regions of southern Scandinavia to cluster unlabelled targets into
possible species. We also demonstrate improved clustering performance in a
semi-supervised setting using a small subset of labelled data. These
experimental results, in both unsupervised- and semi-supervised settings, with
the dynamic beta-VAE are promising and, in the near future, can be deployed to
monitor insects and conserve the rapidly declining insect biodiversity."
"This article proposes two different approaches to automatically create a map
for valid on-street car parking spaces. For this, we use car sharing park-out
events data. The first one uses spatial aggregation and the second a machine
learning algorithm. For the former, we chose rasterization and road sectioning;
for the latter we chose decision trees. We compare the results of these
approaches and discuss their advantages and disadvantages. Furthermore, we show
our results for a neighborhood in the city of Berlin and report a
classification accuracy of 91.6\% on the original imbalanced data. Finally, we
discuss further work; from gathering more data over a longer period of time to
fitting spatial Gaussian densities to the data and the usage of apps for manual
validation and annotation of parking spaces to improve ground truth data."
"We consider offline reinforcement learning (RL) with heterogeneous agents
under severe data scarcity, i.e., we only observe a single historical
trajectory for every agent under an unknown, potentially sub-optimal policy. We
find that the performance of state-of-the-art offline and model-based RL
methods degrade significantly given such limited data availability, even for
commonly perceived ""solved"" benchmark settings such as ""MountainCar"" and
""CartPole"". To address this challenge, we propose PerSim, a model-based offline
RL approach which first learns a personalized simulator for each agent by
collectively using the historical trajectories across all agents, prior to
learning a policy. We do so by positing that the transition dynamics across
agents can be represented as a latent function of latent factors associated
with agents, states, and actions; subsequently, we theoretically establish that
this function is well-approximated by a ""low-rank"" decomposition of separable
agent, state, and action latent functions. This representation suggests a
simple, regularized neural network architecture to effectively learn the
transition dynamics per agent, even with scarce, offline data. We perform
extensive experiments across several benchmark environments and RL methods. The
consistent improvement of our approach, measured in terms of both state
dynamics prediction and eventual reward, confirms the efficacy of our framework
in leveraging limited historical data to simultaneously learn personalized
policies across agents."
"In this work, we propose a deep reinforcement learning (DRL) model for
finding a feasible solution for (mixed) integer programming (MIP) problems.
Finding a feasible solution for MIP problems is critical because many
successful heuristics rely on a known initial feasible solution. However, it is
in general NP-hard. Inspired by the feasibility pump (FP), a well-known
heuristic for searching feasible MIP solutions, we develop a smart feasibility
pump (SFP) method using DRL. In addition to multi-layer perception (MLP), we
propose a novel convolution neural network (CNN) structure for the policy
network to capture the hidden information of the constraint matrix of the MIP
problem. Numerical experiments on various problem instances show that SFP
significantly outperforms the classic FP in terms of the number of steps
required to reach the first feasible solution. Moreover, the CNN structure
works without the projection of the current solution as the input, which saves
the computational effort at each step of the FP algorithms to find projections.
This highlights the representational power of the CNN structure."
"Recently, various auxiliary tasks have been proposed to accelerate
representation learning and improve sample efficiency in deep reinforcement
learning (RL). However, existing auxiliary tasks do not take the
characteristics of RL problems into consideration and are unsupervised. By
leveraging returns, the most important feedback signals in RL, we propose a
novel auxiliary task that forces the learnt representations to discriminate
state-action pairs with different returns. Our auxiliary loss is theoretically
justified to learn representations that capture the structure of a new form of
state-action abstraction, under which state-action pairs with similar return
distributions are aggregated together. In low data regime, our algorithm
outperforms strong baselines on complex tasks in Atari games and DeepMind
Control suite, and achieves even better performance when combined with existing
auxiliary tasks."
"Big data methods are becoming an important tool for tax fraud detection
around the world. Unsupervised learning approach is the dominant framework due
to the lack of label and ground truth in corresponding data sets although these
methods suffer from low interpretability. HUNOD, a novel hybrid unsupervised
outlier detection method for tax evasion risk management, is presented in this
paper. In contrast to previous methods proposed in the literature, the HUNOD
method combines two outlier detection approaches based on two different machine
learning designs (i.e, clustering and representational learning) to detect and
internally validate outliers in a given tax dataset. The HUNOD method allows
its users to incorporate relevant domain knowledge into both constituent
outlier detection approaches in order to detect outliers relevant for a given
economic context. The interpretability of obtained outliers is achieved by
training explainable-by-design surrogate models over results of unsupervised
outlier detection methods. The experimental evaluation of the HUNOD method is
conducted on two datasets derived from the database on individual personal
income tax declarations collected by the Tax Administration of Serbia. The
obtained results show that the method indicates between 90% and 98% internally
validated outliers depending on the clustering configuration and employed
regularization mechanisms for representational learning."
"This paper proposes a multi-label classification algorithm capable of
continual learning by applying an Adaptive Resonance Theory (ART)-based
clustering algorithm and the Bayesian approach for label probability
computation. The ART-based clustering algorithm adaptively and continually
generates prototype nodes corresponding to given data, and the generated nodes
are used as classifiers. The label probability computation independently counts
the number of label appearances for each class and calculates the Bayesian
probabilities. Thus, the label probability computation can cope with an
increase in the number of labels. Experimental results with synthetic and
real-world multi-label datasets show that the proposed algorithm has
competitive classification performance to other well-known algorithms while
realizing continual learning."
"Demand forecasting is a crucial component of demand management. While
shortening the forecasting horizon allows for more recent data and less
uncertainty, this frequently means lower data aggregation levels and a more
significant data sparsity. Sparse demand data usually results in lumpy or
intermittent demand patterns, which have sparse and irregular demand intervals.
Usual statistical and machine learning models fail to provide good forecasts in
such scenarios. Our research shows that competitive demand forecasts can be
obtained through two models: predicting the demand occurrence and estimating
the demand size. We analyze the usage of local and global machine learning
models for both cases and compare results against baseline methods. Finally, we
propose a novel evaluation criterion of lumpy and intermittent demand
forecasting models' performance. Our research shows that global classification
models are the best choice when predicting demand event occurrence. When
predicting demand sizes, we achieved the best results using Simple Exponential
Smoothing forecast. We tested our approach on real-world data consisting of 516
three-year-long time series corresponding to European automotive original
equipment manufacturers' daily demand."
"The diagnosis of cyber-physical systems aims to detect faulty behaviour, its
root cause and a mitigation or even prevention policy. Therefore, diagnosis
relies on a representation of the system's functional and faulty behaviour
combined with observations of the system taken at runtime. The main challenges
are the time-intensive building of a model, possible state-explosion while
searching for the root cause and interpretability of the results. In this paper
we propose a scalable algorithm tackling these challenges. We use a Bayesian
network to learn a structured model automatically and optimise the model by a
genetic algorithm. Our approach differs from existing work in two aspects:
instead of selecting features prior to the analysis we learn a global
representation using all available information which is then transformed to a
smaller, label-specific one and we focus on interpretability to facilitate
repairs. The evaluation shows that our approach is able to learn a model with
equal performance to state-of-the-art algorithms while giving better
interpretability and having a reduced size."
"Optimal Mass Transport (OMT) is a well studied problem with a variety of
applications in a diverse set of fields ranging from Physics to Computer Vision
and in particular Statistics and Data Science. Since the original formulation
of Monge in 1781 significant theoretical progress been made on the existence,
uniqueness and properties of the optimal transport maps. The actual numerical
computation of the transport maps, particularly in high dimensions, remains a
challenging problem. By Brenier's theorem, the continuous OMT problem can be
reduced to that of solving a non-linear PDE of Monge-Ampere type whose solution
is a convex function. In this paper, building on recent developments of input
convex neural networks and physics informed neural networks for solving PDE's,
we propose a Deep Learning approach to solve the continuous OMT problem.
  To demonstrate the versatility of our framework we focus on the ubiquitous
density estimation and generative modeling tasks in statistics and machine
learning. Finally as an example we show how our framework can be incorporated
with an autoencoder to estimate an effective probabilistic generative model."
"The Hierarchical Vote Collective of Transformation-based Ensembles
(HIVE-COTE) is a heterogeneous meta ensemble for time series classification.
HIVE-COTE forms its ensemble from classifiers of multiple domains, including
phase-independent shapelets, bag-of-words based dictionaries and
phase-dependent intervals. Since it was first proposed in 2016, the algorithm
has remained state of the art for accuracy on the UCR time series
classification archive. Over time it has been incrementally updated,
culminating in its current state, HIVE-COTE 1.0. During this time a number of
algorithms have been proposed which match the accuracy of HIVE-COTE. We propose
comprehensive changes to the HIVE-COTE algorithm which significantly improve
its accuracy and usability, presenting this upgrade as HIVE-COTE 2.0. We
introduce two novel classifiers, the Temporal Dictionary Ensemble (TDE) and
Diverse Representation Canonical Interval Forest (DrCIF), which replace
existing ensemble members. Additionally, we introduce the Arsenal, an ensemble
of ROCKET classifiers as a new HIVE-COTE 2.0 constituent. We demonstrate that
HIVE-COTE 2.0 is significantly more accurate than the current state of the art
on 112 univariate UCR archive datasets and 26 multivariate UEA archive
datasets."
"Multi-view clustering is an important yet challenging task in machine
learning and data mining community. One popular strategy for multi-view
clustering is matrix factorization which could explore useful feature
representations at lower-dimensional space and therefore alleviate dimension
curse. However, there are two major drawbacks in the existing work: i) most
matrix factorization methods are limited to shadow depth, which leads to the
inability to fully discover the rich hidden information of original data. Few
deep matrix factorization methods provide a basis for the selection of the new
representation's dimensions of different layers. ii) the majority of current
approaches only concentrate on the view-shared information and ignore the
specific local features in different views. To tackle the above issues, we
propose a novel Multi-View Clustering method with Deep semi-NMF and Global
Graph Refinement (MVC-DMF-GGR) in this paper. Firstly, we capture new
representation matrices for each view by hierarchical decomposition, then learn
a common graph by approximating a combination of graphs which are reconstructed
from these new representations to refine the new representations in return. An
alternate algorithm with proved convergence is then developed to solve the
optimization problem and the results on six multi-view benchmarks demonstrate
the effectiveness and superiority of our proposed algorithm."
"Bike sharing demand is increasing in large cities worldwide. The proper
functioning of bike-sharing systems is, nevertheless, dependent on a balanced
geographical distribution of bicycles throughout a day. In this context,
understanding the spatiotemporal distribution of check-ins and check-outs is
key for station balancing and bike relocation initiatives. Still, recent
contributions from deep learning and distance-based predictors show limited
success on forecasting bike sharing demand. This consistent observation is
hypothesized to be driven by: i) the strong dependence between demand and the
meteorological and situational context of stations; and ii) the absence of
spatial awareness as most predictors are unable to model the effects of
high-low station load on nearby stations.
  This work proposes a comprehensive set of new principles to incorporate both
historical and prospective sources of spatial, meteorological, situational and
calendrical context in predictive models of station demand. To this end, a new
recurrent neural network layering composed by serial long-short term memory
(LSTM) components is proposed with two major contributions: i) the feeding of
multivariate time series masks produced from historical context data at the
input layer, and ii) the time-dependent regularization of the forecasted time
series using prospective context data. This work further assesses the impact of
incorporating different sources of context, showing the relevance of the
proposed principles for the community even though not all improvements from the
context-aware predictors yield statistical significance."
"Sequential fundraising in two sided online platforms enable peer to peer
lending by sequentially bringing potential contributors, each of whose
decisions impact other contributors in the market. However, understanding the
dynamics of sequential contributions in online platforms for peer lending has
been an open ended research question. The centralized investment mechanism in
these platforms makes it difficult to understand the implicit competition that
borrowers face from a single lender at any point in time. Matching markets are
a model of pairing agents where the preferences of agents from both sides in
terms of their preferred pairing for transactions can allow to decentralize the
market. We study investment designs in two sided platforms using matching
markets when the investors or lenders also face restrictions on the investments
based on borrower preferences. This situation creates an implicit competition
among the lenders in addition to the existing borrower competition, especially
when the lenders are uncertain about their standing in the market and thereby
the probability of their investments being accepted or the borrower loan
requests for projects reaching the reserve price. We devise a technique based
on sequential decision making that allows the lenders to adjust their choices
based on the dynamics of uncertainty from competition over time. We simulate
two sided market matchings in a sequential decision framework and show the
dynamics of the lender regret amassed compared to the optimal borrower-lender
matching and find that the lender regret depends on the initial preferences set
by the lenders which could affect their learning over decision making steps."
"Accurate protein structure prediction from amino-acid sequences is critical
to better understanding the protein function. Recent advances in this area
largely benefit from more precise inter-residue distance and orientation
predictions, powered by deep neural networks. However, the structure
optimization procedure is still dominated by traditional tools, e.g. Rosetta,
where the structure is solved via minimizing a pre-defined statistical energy
function (with optional prediction-based restraints). Such energy function may
not be optimal in formulating the whole conformation space of proteins. In this
paper, we propose a fully-differentiable approach for protein structure
optimization, guided by a data-driven generative network. This network is
trained in a denoising manner, attempting to predict the correction signal from
corrupted distance matrices between Ca atoms. Once the network is well trained,
Langevin dynamics based sampling is adopted to gradually optimize structures
from random initialization. Extensive experiments demonstrate that our EBM-Fold
approach can efficiently produce high-quality decoys, compared against
traditional Rosetta-based structure optimization routines."
"Deep learning (DL) has achieved unprecedented success in a variety of tasks.
However, DL systems are notoriously difficult to test and debug due to the lack
of explainability of DL models and the huge test input space to cover.
Generally speaking, it is relatively easy to collect a massive amount of test
data, but the labeling cost can be quite high. Consequently, it is essential to
conduct test selection and label only those selected ""high quality""
bug-revealing test inputs for test cost reduction.
  In this paper, we propose a novel test prioritization technique that brings
order into the unlabeled test instances according to their bug-revealing
capabilities, namely TestRank. Different from existing solutions, TestRank
leverages both intrinsic attributes and contextual attributes of test instances
when prioritizing them. To be specific, we first build a similarity graph on
test instances and training samples, and we conduct graph-based semi-supervised
learning to extract contextual features. Then, for a particular test instance,
the contextual features extracted from the graph neural network (GNN) and the
intrinsic features obtained with the DL model itself are combined to predict
its bug-revealing probability. Finally, TestRank prioritizes unlabeled test
instances in descending order of the above probability value. We evaluate the
performance of TestRank on a variety of image classification datasets.
Experimental results show that the debugging efficiency of our method
significantly outperforms existing test prioritization techniques."
"How to obtain good value estimation is one of the key problems in
Reinforcement Learning (RL). Current value estimation methods, such as DDPG and
TD3, suffer from unnecessary over- or underestimation bias. In this paper, we
explore the potential of double actors, which has been neglected for a long
time, for better value function estimation in continuous setting. First, we
uncover and demonstrate the bias alleviation property of double actors by
building double actors upon single critic and double critics to handle
overestimation bias in DDPG and underestimation bias in TD3 respectively. Next,
we interestingly find that double actors help improve the exploration ability
of the agent. Finally, to mitigate the uncertainty of value estimate from
double critics, we further propose to regularize the critic networks under
double actors architecture, which gives rise to Double Actors Regularized
Critics (DARC) algorithm. Extensive experimental results on challenging
continuous control tasks show that DARC significantly outperforms
state-of-the-art methods with higher sample efficiency."
"To increase the ubiquity of machine learning it needs to be automated.
Automation is cost-effective as it allows experts to spend less time tuning the
approach, which leads to shorter development times. However, while this
automation produces highly accurate architectures, they can be uninterpretable,
acting as `black-boxes' which produce low conventional errors but fail to model
the underlying input-output relationships -- the ground truth. This paper
explores the use of the Fit to Median Error measure in machine learning
regression automation, using evolutionary computation in order to improve the
approximation of the ground truth. When used alongside conventional error
measures it improves interpretability by regularising learnt input-output
relationships to the conditional median. It is compared to traditional
regularisers to illustrate that the use of the Fit to Median Error produces
regression neural networks which model more consistent input-output
relationships. The problem considered is ship power prediction using a
fuel-saving air lubrication system, which is highly stochastic in nature. The
networks optimised for their Fit to Median Error are shown to approximate the
ground truth more consistently, without sacrificing conventional Minkowski-r
error values."
"Cardiovascular diseases (CVDs) are one of the most common chronic illnesses
that affect peoples health. Early detection of CVDs can reduce mortality rates
by preventing or reducing the severity of the disease. Machine learning
algorithms are a promising method for identifying risk factors. This paper
proposes a proposed recursive feature elimination-based gradient boosting
(RFE-GB) algorithm in order to obtain accurate heart disease prediction. The
patients health record with important CVD features has been analyzed for the
evaluation of the results. Several other machine learning methods were also
used to build the prediction model, and the results were compared with the
proposed model. The results of this proposed model infer that the combined
recursive feature elimination and gradient boosting algorithm achieves the
highest accuracy (89.7 %). Further, with an area under the curve of 0.84, the
proposed RFE-GB algorithm was found superior and had obtained a substantial
gain over other techniques. Thus, the proposed RFE-GB algorithm will serve as a
prominent model for CVD estimation and treatment."
"An aircraft conflict occurs when two or more aircraft cross at a certain
distance at the same time. Specific air traffic controllers are assigned to
solve such conflicts. A controller needs to consider various types of
information in order to solve a conflict. The most common and preliminary
information is the coordinate position of the involved aircraft. Additionally,
a controller has to take into account more information such as flight planning,
weather, restricted territory, etc. The most important challenges a controller
has to face are: to think about the issues involved and make a decision in a
very short time. Due to the increased number of aircraft, it is crucial to
reduce the workload of the controllers and help them make quick decisions. A
conflict can be solved in many ways, therefore, we consider this problem as a
multi-label classification problem. In doing so, we are proposing a multi-label
classification model which provides multiple heading advisories for a given
conflict. This model we named CRMLnet is based on a novel application of a
multi-layer neural network and helps the controllers in their decisions. When
compared to other machine learning models, our CRMLnet has achieved the best
results with an accuracy of 98.72% and ROC of 0.999. The simulated data set
that we have developed and used in our experiments will be delivered to the
research community."
"Major League Baseball (MLB) has a storied history of using statistics to
better understand and discuss the game of baseball, with an entire discipline
of statistics dedicated to the craft, known as sabermetrics. At their core, all
sabermetrics seek to quantify some aspect of the game, often a specific aspect
of a player's skill set - such as a batter's ability to drive in runs (RBI) or
a pitcher's ability to keep batters from reaching base (WHIP). While useful,
such statistics are fundamentally limited by the fact that they are derived
from an account of what happened on the field, not how it happened. As a first
step towards alleviating this shortcoming, we present a novel, contrastive
learning-based framework for describing player form in the MLB. We use form to
refer to the way in which a player has impacted the course of play in their
recent appearances. Concretely, a player's form is described by a
72-dimensional vector. By comparing clusters of players resulting from our form
representations and those resulting from traditional abermetrics, we
demonstrate that our form representations contain information about how players
impact the course of play, not present in traditional, publicly available
statistics. We believe these embeddings could be utilized to predict both
in-game and game-level events, such as the result of an at-bat or the winner of
a game."
"Many real-world graphs involve different types of nodes and relations between
nodes, being heterogeneous by nature. The representation learning of
heterogeneous graphs (HGs) embeds the rich structure and semantics of such
graphs into a low-dimensional space and facilitates various data mining tasks,
such as node classification, node clustering, and link prediction. In this
paper, we propose a self-supervised method that learns HG representations by
relying on knowledge exchange and discovery among different HG structural
semantics (meta-paths). Specifically, by maximizing the mutual information of
meta-path representations, we promote meta-path information fusion and
consensus, and ensure that globally shared semantics are encoded. By extensive
experiments on node classification, node clustering, and link prediction tasks,
we show that the proposed self-supervision both outperforms and improves
competing methods by 1% and up to 10% for all tasks."
"The vulnerability of machine learning models to adversarial perturbations has
motivated a significant amount of research under the broad umbrella of
adversarial machine learning. Sophisticated attacks may cause learning
algorithms to learn decision functions or make decisions with poor predictive
performance. In this context, there is a growing body of literature that uses
local intrinsic dimensionality (LID), a local metric that describes the minimum
number of latent variables required to describe each data point, for detecting
adversarial samples and subsequently mitigating their effects. The research to
date has tended to focus on using LID as a practical defence method often
without fully explaining why LID can detect adversarial samples. In this paper,
we derive a lower-bound and an upper-bound for the LID value of a perturbed
data point and demonstrate that the bounds, in particular the lower-bound, has
a positive correlation with the magnitude of the perturbation. Hence, we
demonstrate that data points that are perturbed by a large amount would have
large LID values compared to unperturbed samples, thus justifying its use in
the prior literature. Furthermore, our empirical validation demonstrates the
validity of the bounds on benchmark datasets."
"Given that cloud servers are usually remotely located from the devices of
mobile apps, the end-users of the apps can face delays. The Fog has been
introduced to augment the apps with machines located at the network edge close
to the end-users. However, edge machines are usually resource constrained.
Thus, the execution of online data-analytics on edge machines may not be
feasible if the time complexity of the data-analytics algorithm is high. To
overcome this, multiple instances of the back-end should be deployed on edge
and remote machines. In this case, the research question is how the switching
of the app among the instances of the back-end can be dynamically decided based
on the response time of the service instances. To answer this, we contribute an
AI approach that trains machine-learning models of the response time of service
instances. Our approach extends a back-end as a service into an AI
self-back-end as a service that self-decides at runtime the right edge/remote
instance that achieves the lowest response-time. We evaluate the accuracy and
the efficiency of our approach by using real-word machine-learning datasets on
an existing auction app."
"Federated learning (FL) is a privacy-preserving machine learning setting that
enables many devices to jointly train a shared global model without the need to
reveal their data to a central server. However, FL involves a frequent exchange
of the parameters between all the clients and the server that coordinates the
training. This introduces extensive communication overhead, which can be a
major bottleneck in FL with limited communication links. In this paper, we
consider training the binary neural networks (BNN) in the FL setting instead of
the typical real-valued neural networks to fulfill the stringent delay and
efficiency requirement in wireless edge networks. We introduce a novel FL
framework of training BNN, where the clients only upload the binary parameters
to the server. We also propose a novel parameter updating scheme based on the
Maximum Likelihood (ML) estimation that preserves the performance of the BNN
even without the availability of aggregated real-valued auxiliary parameters
that are usually needed during the training of the BNN. Moreover, for the first
time in the literature, we theoretically derive the conditions under which the
training of BNN is converging. { Numerical results show that the proposed FL
framework significantly reduces the communication cost compared to the
conventional neural networks with typical real-valued parameters, and the
performance loss incurred by the binarization can be further compensated by a
hybrid method."
"Counterfactual examples are one of the most commonly-cited methods for
explaining the predictions of machine learning models in key areas such as
finance and medical diagnosis. Counterfactuals are often discussed under the
assumption that the model on which they will be used is static, but in
deployment models may be periodically retrained or fine-tuned. This paper
studies the consistency of model prediction on counterfactual examples in deep
networks under small changes to initial training conditions, such as weight
initialization and leave-one-out variations in data, as often occurs during
model deployment. We demonstrate experimentally that counterfactual examples
for deep models are often inconsistent across such small changes, and that
increasing the cost of the counterfactual, a stability-enhancing mitigation
suggested by prior work in the context of simpler models, is not a reliable
heuristic in deep networks. Rather, our analysis shows that a model's local
Lipschitz continuity around the counterfactual is key to its consistency across
related models. To this end, we propose Stable Neighbor Search as a way to
generate more consistent counterfactual explanations, and illustrate the
effectiveness of this approach on several benchmark datasets."
"We consider a stochastic multi-armed bandit setting where reward must be
actively queried for it to be observed. We provide tight lower and upper
problem-dependent guarantees on both the regret and the number of queries.
Interestingly, we prove that there is a fundamental difference between problems
with a unique and multiple optimal arms, unlike in the standard multi-armed
bandit problem. We also present a new, simple, UCB-style sampling concept, and
show that it naturally adapts to the number of optimal arms and achieves tight
regret and querying bounds."
"Increasingly complex learning methods such as boosting, bagging and deep
learning have made ML models more accurate, but harder to understand and
interpret. A tradeoff between performance and intelligibility is often to be
faced, especially in high-stakes applications like medicine. In the present
article we propose a novel methodological approach for generating explanations
of the predictions of a generic ML model, given a specific instance for which
the prediction has been made, that can tackle both classification and
regression tasks. Advantages of the proposed XAI approach include improved
fidelity to the original model, the ability to deal with non-linear decision
boundaries, and native support to both classification and regression problems"
"Many complex time series can be effectively subdivided into distinct regimes
that exhibit persistent dynamics. Discovering the switching behavior and the
statistical patterns in these regimes is important for understanding the
underlying dynamical system. We propose the Recurrent Explicit Duration
Switching Dynamical System (RED-SDS), a flexible model that is capable of
identifying both state- and time-dependent switching dynamics. State-dependent
switching is enabled by a recurrent state-to-switch connection and an explicit
duration count variable is used to improve the time-dependent switching
behavior. We demonstrate how to perform efficient inference using a hybrid
algorithm that approximates the posterior of the continuous states via an
inference network and performs exact inference for the discrete switches and
counts. The model is trained by maximizing a Monte Carlo lower bound of the
marginal log-likelihood that can be computed efficiently as a byproduct of the
inference routine. Empirical results on multiple datasets demonstrate that
RED-SDS achieves considerable improvement in time series segmentation and
competitive forecasting performance against the state of the art."
"Learning to execute algorithms is a fundamental problem that has been widely
studied. Prior work~\cite{veli19neural} has shown that to enable systematic
generalisation on graph algorithms it is critical to have access to the
intermediate steps of the program/algorithm. In many reasoning tasks, where
algorithmic-style reasoning is important, we only have access to the input and
output examples. Thus, inspired by the success of pre-training on similar tasks
or data in Natural Language Processing (NLP) and Computer Vision, we set out to
study how we can transfer algorithmic reasoning knowledge. Specifically, we
investigate how we can use algorithms for which we have access to the execution
trace to learn to solve similar tasks for which we do not. We investigate two
major classes of graph algorithms, parallel algorithms such as breadth-first
search and Bellman-Ford and sequential greedy algorithms such as Prim and
Dijkstra. Due to the fundamental differences between algorithmic reasoning
knowledge and feature extractors such as used in Computer Vision or NLP, we
hypothesise that standard transfer techniques will not be sufficient to achieve
systematic generalisation. To investigate this empirically we create a dataset
including 9 algorithms and 3 different graph types. We validate this
empirically and show how instead multi-task learning can be used to achieve the
transfer of algorithmic reasoning knowledge."
"Federated learning (FL) is a distributed learning technique that trains a
shared model over distributed data in a privacy-preserving manner.
Unfortunately, FL's performance degrades when there is (i) variability in
client characteristics in terms of computational and memory resources (system
heterogeneity) and (ii) non-IID data distribution across clients (statistical
heterogeneity). For example, slow clients get dropped in FL schemes, such as
Federated Averaging (FedAvg), which not only limits overall learning but also
biases results towards fast clients. We propose FedPrune; a system that tackles
this challenge by pruning the global model for slow clients based on their
device characteristics. By doing so, slow clients can train a small model
quickly and participate in FL which increases test accuracy as well as
fairness. By using insights from Central Limit Theorem, FedPrune incorporates a
new aggregation technique that achieves robust performance over non-IID data.
Experimental evaluation shows that Fed- Prune provides robust convergence and
better fairness compared to Federated Averaging."
"Various graph contrastive learning models have been proposed to improve the
performance of learning tasks on graph datasets in recent years. While
effective and prevalent, these models are usually carefully customized. In
particular, although all recent researches create two contrastive views, they
differ greatly in view augmentations, architectures, and objectives. It remains
an open question how to build your graph contrastive learning model from
scratch for particular graph learning tasks and datasets. In this work, we aim
to fill this gap by studying how graph information is transformed and
transferred during the contrastive learning process and proposing an
information-aware graph contrastive learning framework called InfoGCL. The key
point of this framework is to follow the Information Bottleneck principle to
reduce the mutual information between contrastive parts while keeping
task-relevant information intact at both the levels of the individual module
and the entire framework so that the information loss during graph
representation learning can be minimized. We show for the first time that all
recent graph contrastive learning methods can be unified by our framework. We
empirically validate our theoretical analysis on both node and graph
classification benchmark datasets, and demonstrate that our algorithm
significantly outperforms the state-of-the-arts."
"Anomaly detection is a well-known task that involves the identification of
abnormal events that occur relatively infrequently. Methods for improving
anomaly detection performance have been widely studied. However, no studies
utilizing test-time augmentation (TTA) for anomaly detection in tabular data
have been performed. TTA involves aggregating the predictions of several
synthetic versions of a given test sample; TTA produces different points of
view for a specific test instance and might decrease its prediction bias. We
propose the Test-Time Augmentation for anomaly Detection (TTAD) technique, a
TTA-based method aimed at improving anomaly detection performance. TTAD
augments a test instance based on its nearest neighbors; various methods,
including the k-Means centroid and SMOTE methods, are used to produce the
augmentations. Our technique utilizes a Siamese network to learn an advanced
distance metric when retrieving a test instance's neighbors. Our experiments
show that the anomaly detector that uses our TTA technique achieved
significantly higher AUC results on all datasets evaluated."
"AutoML systems build machine learning models automatically by performing a
search over valid data transformations and learners, along with hyper-parameter
optimization for each learner. Many AutoML systems use meta-learning to guide
search for optimal pipelines. In this work, we present a novel meta-learning
system called KGpip which, (1) builds a database of datasets and corresponding
pipelines by mining thousands of scripts with program analysis, (2) uses
dataset embeddings to find similar datasets in the database based on its
content instead of metadata-based features, (3) models AutoML pipeline creation
as a graph generation problem, to succinctly characterize the diverse pipelines
seen for a single dataset. KGpip's meta-learning is a sub-component for AutoML
systems. We demonstrate this by integrating KGpip with two AutoML systems. Our
comprehensive evaluation using 126 datasets, including those used by the
state-of-the-art systems, shows that KGpip significantly outperforms these
systems."
"In this paper, we establish the global optimality and convergence rate of an
off-policy actor critic algorithm in the tabular setting without using density
ratio to correct the discrepancy between the state distribution of the behavior
policy and that of the target policy. Our work goes beyond existing works on
the optimality of policy gradient methods in that existing works use the exact
policy gradient for updating the policy parameters while we use an approximate
and stochastic update step. Our update step is not a gradient update because we
do not use a density ratio to correct the state distribution, which aligns well
with what practitioners do. Our update is approximate because we use a learned
critic instead of the true value function. Our update is stochastic because at
each step the update is done for only the current state action pair. Moreover,
we remove several restrictive assumptions from existing works in our analysis.
Central to our work is the finite sample analysis of a generic stochastic
approximation algorithm with time-inhomogeneous update operators on
time-inhomogeneous Markov chains, based on its uniform contraction properties."
"Not only can discovering patterns and insights from atmospheric data enable
more accurate weather predictions, but it may also provide valuable information
to help tackle climate change. Weather4cast is an open competition that aims to
evaluate machine learning algorithms' capability to predict future atmospheric
states. Here, we describe our third-place solution to Weather4cast. We present
a novel Variational U-Net that combines a Variational Autoencoder's ability to
consider the probabilistic nature of data with a U-Net's ability to recover
fine-grained details. This solution is an evolution from our fourth-place
solution to Traffic4cast 2020 with many commonalities, suggesting its
applicability to vastly different domains, such as weather and traffic."
"Feature selection methods are widely used to address the high computational
overheads and curse of dimensionality in classifying high-dimensional data.
Most conventional feature selection methods focus on handling homogeneous
features, while real-world datasets usually have a mixture of continuous and
discrete features. Some recent mixed-type feature selection studies only select
features with high relevance to class labels and ignore the redundancy among
features. The determination of an appropriate feature subset is also a
challenge. In this paper, a supervised feature selection method using
density-based feature clustering (SFSDFC) is proposed to obtain an appropriate
final feature subset for mixed-type data. SFSDFC decomposes the feature space
into a set of disjoint feature clusters using a novel density-based clustering
method. Then, an effective feature selection strategy is employed to obtain a
subset of important features with minimal redundancy from those feature
clusters. Extensive experiments as well as comparison studies with five
state-of-the-art methods are conducted on SFSDFC using thirteen real-world
benchmark datasets and results justify the efficacy of the SFSDFC method."
"This manuscript is focused on features' definition for the outcome prediction
of matches of NBA basketball championship. It is shown how models based on one
a single feature (Elo rating or the relative victory frequency) have a quality
of fit better than models using box-score predictors (e.g. the Four Factors).
Features have been ex ante calculated for a dataset containing data of 16 NBA
regular seasons, paying particular attention to home court factor. Models have
been produced via Deep Learning, using cross validation."
"Federated Learning (FL) enables a group of clients to jointly train a machine
learning model with the help of a centralized server. Clients do not need to
submit their local data to the server during training, and hence the local
training data of clients is protected. In FL, distributed clients collect their
local data independently, so the dataset of each client may naturally form a
distinct source domain. In practice, the model trained over multiple source
domains may have poor generalization performance on unseen target domains. To
address this issue, we propose FedADG to equip federated learning with domain
generalization capability. FedADG employs the federated adversarial learning
approach to measure and align the distributions among different source domains
via matching each distribution to a reference distribution. The reference
distribution is adaptively generated (by accommodating all source domains) to
minimize the domain shift distance during alignment. In FedADG, the alignment
is fine-grained since each class is aligned independently. In this way, the
learned feature representation is supposed to be universal, so it can
generalize well on the unseen domains. Intensive experiments on various
datasets demonstrate that FedADG has comparable performance with the
state-of-the-art."
"Providing front-line health workers in low- and middle- income countries with
recommendations and predictions to improve health outcomes can have a
tremendous impact on reducing healthcare inequalities, for instance by helping
to prevent the thousands of maternal and newborn deaths that occur every day.
To that end, we are developing a data-centric machine learning platform that
leverages the behavioral logs from a wide range of mobile health applications
running in those countries. Here we describe the platform architecture,
focusing on the details that help us to maximize the quality and organization
of the data throughout the whole process, from the data ingestion with a
data-science purposed software development kit to the data pipelines, feature
engineering and model management."
"We devise a coreset selection method based on the idea of gradient matching:
The gradients induced by the coreset should match, as closely as possible,
those induced by the original training dataset. We evaluate the method in the
context of continual learning, where it can be used to curate a rehearsal
memory. Our method performs strong competitors such as reservoir sampling
across a range of memory sizes."
"In this work we propose the use of quantile regression and dilated recurrent
neural networks with temporal scaling (MQ-DRNN-s) and apply it to the inventory
management task. This model showed a better performance of up to 3.2\% over a
statistical benchmark (the quantile autoregressive model with exogenous
variables, QAR-X), being better than the MQ-DRNN without temporal scaling by
6\%. The above on a set of 10,000 time series of sales of El Globo over a
53-week horizon using rolling windows of 7-day ahead each week."
"Temperature field inversion of heat-source systems (TFI-HSS) with limited
observations is essential to monitor the system health. Although some methods
such as interpolation have been proposed to solve TFI-HSS, those existing
methods ignore correlations between data constraints and physics constraints,
causing the low precision. In this work, we develop a physics-informed neural
network-based temperature field inversion (PINN-TFI) method to solve the
TFI-HSS task and a coefficient matrix condition number based position selection
of observations (CMCN-PSO) method to select optima positions of noise
observations. For the TFI-HSS task, the PINN-TFI method encodes constrain terms
into the loss function, thus the task is transformed into an optimization
problem of minimizing the loss function. In addition, we have found that noise
observations significantly affect reconstruction performances of the PINN-TFI
method. To alleviate the effect of noise observations, the CMCN-PSO method is
proposed to find optimal positions, where the condition number of observations
is used to evaluate positions. The results demonstrate that the PINN-TFI method
can significantly improve prediction precisions and the CMCN-PSO method can
find good positions to acquire a more robust temperature field."
"We develop a category-theoretic criterion for determining the equivalence of
causal models having different but homomorphic directed acyclic graphs over
discrete variables. Following Jacobs et al. (2019), we define a causal model as
a probabilistic interpretation of a causal string diagram, i.e., a functor from
the ``syntactic'' category $\textsf{Syn}_G$ of graph $G$ to the category
$\textsf{Stoch}$ of finite sets and stochastic matrices. The equivalence of
causal models is then defined in terms of a natural transformation or
isomorphism between two such functors, which we call a $\Phi$-abstraction and
$\Phi$-equivalence, respectively. It is shown that when one model is a
$\Phi$-abstraction of another, the intervention calculus of the former can be
consistently translated into that of the latter. We also identify the condition
under which a model accommodates a $\Phi$-abstraction, when transformations are
deterministic."
"We investigate the role of noise in optimization algorithms for learning
over-parameterized models. Specifically, we consider the recovery of a rank one
matrix $Y^*\in R^{d\times d}$ from a noisy observation $Y$ using an
over-parameterization model. We parameterize the rank one matrix $Y^*$ by
$XX^\top$, where $X\in R^{d\times d}$. We then show that under mild conditions,
the estimator, obtained by the randomly perturbed gradient descent algorithm
using the square loss function, attains a mean square error of $O(\sigma^2/d)$,
where $\sigma^2$ is the variance of the observational noise. In contrast, the
estimator obtained by gradient descent without random perturbation only attains
a mean square error of $O(\sigma^2)$. Our result partially justifies the
implicit regularization effect of noise when learning over-parameterized
models, and provides new understanding of training over-parameterized neural
networks."
"Fault diagnosis plays an essential role in reducing the maintenance costs of
rotating machinery manufacturing systems. In many real applications of fault
detection and diagnosis, data tend to be imbalanced, meaning that the number of
samples for some fault classes is much less than the normal data samples. At
the same time, in an industrial condition, accelerometers encounter high levels
of disruptive signals and the collected samples turn out to be heavily noisy.
As a consequence, many traditional Fault Detection and Diagnosis (FDD)
frameworks get poor classification performances when dealing with real-world
circumstances. Three main solutions have been proposed in the literature to
cope with this problem: (1) the implementation of generative algorithms to
increase the amount of under-represented input samples, (2) the employment of a
classifier being powerful to learn from imbalanced and noisy data, (3) the
development of an efficient data pre-processing including feature extraction
and data augmentation. This paper proposes a hybrid framework which uses the
three aforementioned components to achieve an effective signal-based FDD system
for imbalanced conditions. Specifically, it first extracts the fault features,
using Fourier and wavelet transforms to make full use of the signals. Then, it
employs Wasserstein Generative Adversarial Networks (WGAN) to generate
synthetic samples to populate the rare fault class and enhance the training
set. Moreover, to achieve a higher performance a novel combination of
Convolutional Long Short-term Memory (CLSTM) and Weighted Extreme Learning
Machine (WELM) is proposed. To verify the effectiveness of the developed
framework, different datasets settings on different imbalance severities and
noise degrees were used. The comparative results demonstrate that in different
scenarios GAN-CLSTM-ELM outperforms the other state-of-the-art FDD frameworks."
"Although deep neural networks have been immensely successful, there is no
comprehensive theoretical understanding of how they work or are structured. As
a result, deep networks are often seen as black boxes with unclear
interpretations and reliability. Understanding the performance of deep neural
networks is one of the greatest scientific challenges. This work aims to apply
principles and techniques from information theory to deep learning models to
increase our theoretical understanding and design better algorithms. We first
describe our information-theoretic approach to deep learning. Then, we propose
using the Information Bottleneck (IB) theory to explain deep learning systems.
The novel paradigm for analyzing networks sheds light on their layered
structure, generalization abilities, and learning dynamics. We later discuss
one of the most challenging problems of applying the IB to deep neural networks
- estimating mutual information. Recent theoretical developments, such as the
neural tangent kernel (NTK) framework, are used to investigate generalization
signals. In our study, we obtained tractable computations of many
information-theoretic quantities and their bounds for infinite ensembles of
infinitely wide neural networks. With these derivations, we can determine how
compression, generalization, and sample size pertain to the network and how
they are related. At the end, we present the dual Information Bottleneck
(dualIB). This new information-theoretic framework resolves some of the IB's
shortcomings by merely switching terms in the distortion function. The dualIB
can account for known data features and use them to make better predictions
over unseen examples. An analytical framework reveals the underlying structure
and optimal representations, and a variational framework using deep neural
network optimization validates the results."
"In this paper, we fully answer the above question through a key algebraic
condition on graph functions, called \textit{permutation compatibility}, that
relates permutations of weights and features of the graph to functional
constraints. We prove that: (i) a GNN, as a graph function, is necessarily
permutation compatible; (ii) conversely, any permutation compatible function,
when restricted on input graphs with distinct node features, can be generated
by a GNN; (iii) for arbitrary node features (not necessarily distinct), a
simple feature augmentation scheme suffices to generate a permutation
compatible function by a GNN; (iv) permutation compatibility can be verified by
checking only quadratically many functional constraints, rather than an
exhaustive search over all the permutations; (v) GNNs can generate \textit{any}
graph function once we augment the node features with node identities, thus
going beyond graph isomorphism and permutation compatibility. The above
characterizations pave the path to formally study the intricate connection
between GNNs and other algorithmic procedures on graphs. For instance, our
characterization implies that many natural graph problems, such as min-cut
value, max-flow value, max-clique size, and shortest path can be generated by a
GNN using a simple feature augmentation. In contrast, the celebrated
Weisfeiler-Lehman graph-isomorphism test fails whenever a permutation
compatible function with identical features cannot be generated by a GNN. At
the heart of our analysis lies a novel representation theorem that identifies
basis functions for GNNs. This enables us to translate the properties of the
target graph function into properties of the GNN's aggregation function."
"Federated learning is gaining popularity as a distributed machine learning
method that can be used to deploy AI-dependent IoT applications while
protecting client data privacy and security. Due to the differences of clients,
a single global model may not perform well on all clients, so the personalized
federated learning method, which trains a personalized model for each client
that better suits its individual needs, becomes a research hotspot. Most
personalized federated learning research, however, focuses on data
heterogeneity while ignoring the need for model architecture heterogeneity.
Most existing federated learning methods uniformly set the model architecture
of all clients participating in federated learning, which is inconvenient for
each client's individual model and local data distribution requirements, and
also increases the risk of client model leakage. This paper proposes a
federated learning method based on co-training and generative adversarial
networks(GANs) that allows each client to design its own model to participate
in federated learning training independently without sharing any model
architecture or parameter information with other clients or a center. In our
experiments, the proposed method outperforms the existing methods in mean test
accuracy by 42% when the client's model architecture and data distribution vary
significantly."
"This note introduces a novel clustering preserving transformation of cluster
sets obtained from $k$-means algorithm. This transformation may be used to
generate new labeled data{}sets from existent ones. It is more flexible that
Kleinberg axiom based consistency transformation because data points in a
cluster can be moved away and datapoints between clusters may come closer
together."
"Diabetes is one of the chronic diseases that has been discovered for decades.
However, several cases are diagnosed in their late stages. Every one in eleven
of the world's adult population has diabetes. Forty-six percent of people with
diabetes have not been diagnosed. Diabetes can develop several other severe
diseases that can lead to patient death. Developing and rural areas suffer the
most due to the limited medical providers and financial situations. This paper
proposed a novel approach based on an extreme learning machine for diabetes
prediction based on a data questionnaire that can early alert the users to seek
medical assistance and prevent late diagnoses and severe illness development."
"Constructing accurate and generalizable approximators for complex
physico-chemical processes exhibiting highly non-smooth dynamics is
challenging. In this work, we propose new developments and perform comparisons
for two promising approaches: manifold-based polynomial chaos expansion (m-PCE)
and the deep neural operator (DeepONet), and we examine the effect of
over-parameterization on generalization. We demonstrate the performance of
these methods in terms of generalization accuracy by solving the 2D
time-dependent Brusselator reaction-diffusion system with uncertainty sources,
modeling an autocatalytic chemical reaction between two species. We first
propose an extension of the m-PCE by constructing a mapping between latent
spaces formed by two separate embeddings of input functions and output QoIs. To
enhance the accuracy of the DeepONet, we introduce weight self-adaptivity in
the loss function. We demonstrate that the performance of m-PCE and DeepONet is
comparable for cases of relatively smooth input-output mappings. However, when
highly non-smooth dynamics is considered, DeepONet shows higher accuracy. We
also find that for m-PCE, modest over-parameterization leads to better
generalization, both within and outside of distribution, whereas aggressive
over-parameterization leads to over-fitting. In contrast, an even highly
over-parameterized DeepONet leads to better generalization for both smooth and
non-smooth dynamics. Furthermore, we compare the performance of the above
models with another operator learning model, the Fourier Neural Operator, and
show that its over-parameterization also leads to better generalization. Our
studies show that m-PCE can provide very good accuracy at very low training
cost, whereas a highly over-parameterized DeepONet can provide better accuracy
and robustness to noise but at higher training cost. In both methods, the
inference cost is negligible."
"State estimation is required whenever we deal with high-dimensional dynamical
systems, as the complete measurement is often unavailable. It is key to gaining
insight, performing control or optimizing design tasks. Most deep
learning-based approaches require high-resolution labels and work with fixed
sensor locations, thus being restrictive in their scope. Also, doing Proper
orthogonal decomposition (POD) on sparse data is nontrivial. To tackle these
problems, we propose a technique with an implicit optimization layer and a
physics-based loss function that can learn from sparse labels. It works by
minimizing the energy of the neural network prediction, enabling it to work
with a varying number of sensors at different locations. Based on this
technique we present two models for discrete and continuous prediction in
space. We demonstrate the performance using two high-dimensional fluid problems
of Burgers' equation and Flow Past Cylinder for discrete model and using Allen
Cahn equation and Convection-diffusion equations for continuous model. We show
the models are also robust to noise in measurements."
"In this work, we study two self-play training schemes, Chainer and Pool, and
show they lead to improved agent performance in Atari Pong compared to a
standard DQN agent -- trained against the built-in Atari opponent. To measure
agent performance, we define a robustness metric that captures how difficult it
is to learn a strategy that beats the agent's learned policy. Through playing
past versions of themselves, Chainer and Pool are able to target weaknesses in
their policies and improve their resistance to attack. Agents trained using
these methods score well on our robustness metric and can easily defeat the
standard DQN agent. We conclude by using linear probing to illuminate what
internal structures the different agents develop to play the game. We show that
training agents with Chainer or Pool leads to richer network activations with
greater predictive power to estimate critical game-state features compared to
the standard DQN agent."
"Measuring model performance is a key issue for deep learning practitioners.
However, we often lack the ability to explain why a specific architecture
attains superior predictive accuracy for a given data set. Often, validation
accuracy is used as a performance heuristic quantifying how well a network
generalizes to unseen data, but it does not capture anything about the
information flow in the model. Mutual information can be used as a measure of
the quality of internal representations in deep learning models, and the
information plane may provide insights into whether the model exploits the
available information in the data. The information plane has previously been
explored for fully connected neural networks and convolutional architectures.
We present an architecture-agnostic method for tracking a network's internal
representations during training, which are then used to create the mutual
information plane. The method is exemplified for graph-based neural networks
fitted on citation data. We compare how the inductive bias introduced in
graph-based architectures changes the mutual information plane relative to a
fully connected neural network."
"Machine unlearning has become an important area of research due to an
increasing need for machine learning (ML) applications to comply with the
emerging data privacy regulations. It facilitates the provision for removal of
certain set or class of data from an already trained ML model without requiring
retraining from scratch. Recently, several efforts have been put in to make
unlearning to be effective and efficient. We propose a novel machine unlearning
method by exploring the utility of competent and incompetent teachers in a
student-teacher framework to induce forgetfulness. The knowledge from the
competent and incompetent teachers is selectively transferred to the student to
obtain a model that doesn't contain any information about the forget data. We
experimentally show that this method generalizes well, is fast and effective.
Furthermore, we introduce the zero retrain forgetting (ZRF) metric to evaluate
any unlearning method. Unlike the existing unlearning metrics, the ZRF score
does not depend on the availability of the expensive retrained model. This
makes it useful for analysis of the unlearned model after deployment as well.
We present results of experiments conducted for random subset forgetting and
class forgetting on various deep networks and across different application
domains.~Source code is at:
https://github.com/vikram2000b/bad-teaching-unlearning"
"Transformers achieve state-of-the-art performance for natural language
processing tasks by pre-training on large-scale text corpora. They are
extremely compute-intensive and have very high sample complexity. Memory replay
is a mechanism that remembers and reuses past examples by saving to and
replaying from a memory buffer. It has been successfully used in reinforcement
learning and GANs due to better sample efficiency. In this paper, we propose
\emph{Transformer with Memory Replay} (TMR), which integrates memory replay
with transformer, making transformer more sample-efficient. Experiments on GLUE
and SQuAD benchmark datasets show that Transformer with Memory Replay achieves
at least $1\%$ point increase compared to the baseline transformer model when
pretrained with the same number of examples. Further, by adopting a careful
design that reduces the wall-clock time overhead of memory replay, we also
empirically achieve a better runtime efficiency."
"Efficient design and discovery of target-driven molecules is a critical step
in facilitating lead optimization in drug discovery. Current approaches to
develop molecules for a target protein are intuition-driven, hampered by slow
iterative design-test cycles due to computational challenges in utilizing 3D
structural data, and ultimately limited by the expertise of the chemist -
leading to bottlenecks in molecular design. In this contribution, we propose a
novel framework, called 3D-MolGNN$_{RL}$, coupling reinforcement learning (RL)
to a deep generative model based on 3D-Scaffold to generate target candidates
specific to a protein building up atom by atom from the starting core scaffold.
3D-MolGNN$_{RL}$ provides an efficient way to optimize key features by
multi-objective reward function within a protein pocket using parallel graph
neural network models. The agent learns to build molecules in 3D space while
optimizing the activity, binding affinity, potency, and synthetic accessibility
of the candidates generated for infectious disease protein targets. Our
approach can serve as an interpretable artificial intelligence (AI) tool for
lead optimization with optimized activity, potency, and biophysical properties."
"Temporal domain generalization is a promising yet extremely challenging area
where the goal is to learn models under temporally changing data distributions
and generalize to unseen data distributions following the trends of the change.
The advancement of this area is challenged by: 1) characterizing data
distribution drift and its impacts on models, 2) expressiveness in tracking the
model dynamics, and 3) theoretical guarantee on the performance. To address
them, we propose a Temporal Domain Generalization with Drift-Aware Dynamic
Neural Network (DRAIN) framework. Specifically, we formulate the problem into a
Bayesian framework that jointly models the relation between data and model
dynamics. We then build a recurrent graph generation scenario to characterize
the dynamic graph-structured neural networks learned across different time
points. It captures the temporal drift of model parameters and data
distributions and can predict models in the future without the presence of
future data. In addition, we explore theoretical guarantees of the model
performance under the challenging temporal DG setting and provide theoretical
analysis, including uncertainty and generalization error. Finally, extensive
experiments on several real-world benchmarks with temporal drift demonstrate
the effectiveness and efficiency of the proposed method."
"Deep neural networks (DNNs), the agents of deep learning (DL), require a
massive number of parallel/sequential operations, which makes it difficult to
comprehend them and impedes proper diagnosis. Without better knowledge of DNNs'
internal process, deploying DNNs in high-stakes domains may lead to
catastrophic failures. Therefore, to build more reliable DNNs/DL, it is
imperative that we gain insights into their underlying decision-making process.
Here, we use the self-organizing map (SOM) to analyze DL models' internal codes
associated with DNNs' decision-making. Our analyses suggest that shallow layers
close to the input layer map onto homogeneous codes and that deep layers close
to the output layer transform these homogeneous codes in shallow layers to
diverse codes. We also found evidence indicating that homogeneous codes may
underlie DNNs' vulnerabilities to adversarial perturbations."
"Trust evaluation is critical for many applications such as cyber security,
social communication and recommender systems. Users and trust relationships
among them can be seen as a graph. Graph neural networks (GNNs) show their
powerful ability for analyzing graph-structural data. Very recently, existing
work attempted to introduce the attributes and asymmetry of edges into GNNs for
trust evaluation, while failed to capture some essential properties (e.g., the
propagative and composable nature) of trust graphs. In this work, we propose a
new GNN based trust evaluation method named TrustGNN, which integrates smartly
the propagative and composable nature of trust graphs into a GNN framework for
better trust evaluation. Specifically, TrustGNN designs specific propagative
patterns for different propagative processes of trust, and distinguishes the
contribution of different propagative processes to create new trust. Thus,
TrustGNN can learn comprehensive node embeddings and predict trust
relationships based on these embeddings. Experiments on some widely-used
real-world datasets indicate that TrustGNN significantly outperforms the
state-of-the-art methods. We further perform analytical experiments to
demonstrate the effectiveness of the key designs in TrustGNN."
"Generalization to out of distribution tasks in reinforcement learning is a
challenging problem. One successful approach improves generalization by
conditioning policies on task or environment descriptions that provide
information about the current transition or reward functions. Previously, these
descriptions were often expressed as generated or crowd sourced text. In this
work, we begin to tackle the problem of extracting useful information from
natural language found in the wild (e.g. internet forums, documentation, and
wikis). These natural, pre-existing sources are especially challenging, noisy,
and large and present novel challenges compared to previous approaches. We
propose to address these challenges by training reinforcement learning agents
to learn to query these sources as a human would, and we experiment with how
and when an agent should query. To address the \textit{how}, we demonstrate
that pretrained QA models perform well at executing zero-shot queries in our
target domain. Using information retrieved by a QA model, we train an agent to
learn \textit{when} it should execute queries. We show that our method
correctly learns to execute queries to maximize reward in a reinforcement
learning setting."
"Observational data in medicine arise as a result of the complex interaction
between patients and the healthcare system. The sampling process is often
highly irregular and itself constitutes an informative process. When using such
data to develop prediction models, this phenomenon is often ignored, leading to
sub-optimal performance and generalisability of models when practices evolve.
We propose a multi-task recurrent neural network which models three clinical
presence dimensions -- namely the longitudinal, the inter-observation and the
missingness processes -- in parallel to the survival outcome. On a prediction
task using MIMIC III laboratory tests, explicit modelling of these three
processes showed improved performance in comparison to state-of-the-art
predictive models (C-index at 1 day horizon: 0.878). More importantly, the
proposed approach was more robust to change in the clinical presence setting,
demonstrated by performance comparison between patients admitted on weekdays
and weekends. This analysis demonstrates the importance of studying and
leveraging clinical presence to improve performance and create more
transportable clinical models."
"Recent studies have shown that the training samples can be recovered from
gradients, which are called Gradient Inversion (GradInv) attacks. However,
there remains a lack of extensive surveys covering recent advances and thorough
analysis of this issue. In this paper, we present a comprehensive survey on
GradInv, aiming to summarize the cutting-edge research and broaden the horizons
for different domains. Firstly, we propose a taxonomy of GradInv attacks by
characterizing existing attacks into two paradigms: iteration- and
recursion-based attacks. In particular, we dig out some critical ingredients
from the iteration-based attacks, including data initialization, model training
and gradient matching. Second, we summarize emerging defense strategies against
GradInv attacks. We find these approaches focus on three perspectives covering
data obscuration, model improvement and gradient protection. Finally, we
discuss some promising directions and open problems for further research."
"Domain generalization methods aim to learn models robust to domain shift with
data from a limited number of source domains and without access to target
domain samples during training. Popular domain alignment methods for domain
generalization seek to extract domain-invariant features by minimizing the
discrepancy between feature distributions across all domains, disregarding
inter-domain relationships. In this paper, we instead propose a novel
representation learning methodology that selectively enforces prediction
consistency between source domains estimated to be closely-related.
Specifically, we hypothesize that domains share different class-informative
representations, so instead of aligning all domains which can cause negative
transfer, we only regularize the discrepancy between closely-related domains.
We apply our method to time-series classification tasks and conduct
comprehensive experiments on three public real-world datasets. Our method
significantly improves over the baseline and achieves better or competitive
performance in comparison with state-of-the-art methods in terms of both
accuracy and model calibration."
"Computational simulations of wildfire spread typically employ empirical
rate-of-spread calculations under various conditions (such as terrain, fuel
type, weather). Small perturbations in conditions can often lead to significant
changes in fire spread (such as speed and direction), necessitating a
computationally expensive large set of simulations to quantify uncertainty.
Model emulation seeks alternative representations of physical models using
machine learning, aiming to provide more efficient and/or simplified surrogate
models. We propose a dedicated spatio-temporal neural network based framework
for model emulation, able to capture the complex behaviour of fire spread
models. The proposed approach can approximate forecasts at fine spatial and
temporal resolutions that are often challenging for neural network based
approaches. Furthermore, the proposed approach is robust even with small
training sets, due to novel data augmentation methods. Empirical experiments
show good agreement between simulated and emulated firefronts, with an average
Jaccard score of 0.76."
"As a typical application of deep learning, physics-informed neural network
(PINN) {has been} successfully used to find numerical solutions of partial
differential equations (PDEs), but how to improve the limited accuracy is still
a great challenge for PINN. In this work, we introduce a new method,
symmetry-enhanced physics informed neural network (SPINN) where the invariant
surface conditions induced by the Lie symmetries or non-classical symmetries of
PDEs are embedded into the loss function in PINN, to improve the accuracy of
PINN for solving the forward and inverse problems of PDEs. We test the
effectiveness of SPINN for the forward problem via two groups of ten
independent numerical experiments using different numbers of collocation points
and neurons for the heat equation, Korteweg-de Vries (KdV) equation and
potential Burgers {equations} respectively, and for the inverse problem by
considering different layers and neurons as well as different training points
for the Burgers equation in potential form. The numerical results show that
SPINN performs better than PINN with fewer training points and simpler
architecture of neural network. Furthermore, we discuss the computational
overhead of SPINN in terms of the relative computational cost to PINN and show
that the training time of SPINN has no obvious increases, even less than PINN
for certain cases."
"Nowadays, so as to improve services and urban areas livability, multiple
smart city initiatives are being carried out throughout the world.
SmartSantander is a smart city project in Santander, Spain, which has relied on
wireless sensor network technologies to deploy heterogeneous sensors within the
city to measure multiple parameters, including outdoor parking information. In
this paper, we study the prediction of parking lot availability using
historical data from more than 300 outdoor parking sensors with SmartSantander.
We design a graph-to-sequence model to capture the periodical fluctuation and
geographical proximity of parking lots. For developing and evaluating our
model, we use a 3-year dataset of parking lot availability in the city of
Santander. Our model achieves a high accuracy compared with existing
sequence-to-sequence models, which is accurate enough to provide a parking
information service in the city. We apply our model to a smartphone application
to be widely used by citizens and tourists."
"With the rapid development of eXplainable Artificial Intelligence (XAI), a
long line of past work has shown concerns about the Out-of-Distribution (OOD)
problem in perturbation-based post-hoc XAI models and explanations are socially
misaligned. We explore the limitations of post-hoc explanation methods that use
approximators to mimic the behavior of black-box models. Then we propose
eXplanation-based Counterfactual Retraining (XCR), which extracts feature
importance fastly. XCR applies the explanations generated by the XAI model as
counterfactual input to retrain the black-box model to address OOD and social
misalignment problems. Evaluation of popular image datasets shows that XCR can
improve model performance when only retaining 12.5% of the most crucial
features without changing the black-box model structure. Furthermore, the
evaluation of the benchmark of corruption datasets shows that the XCR is very
helpful for improving model robustness and positively impacts the calibration
of OOD problems. Even though not calibrated in the validation set like some OOD
calibration methods, the corrupted data metric outperforms existing methods.
Our method also beats current OOD calibration methods on the OOD calibration
metric if calibration on the validation set is applied."
"Machine learning recently proved efficient in learning differential equations
and dynamical systems from data. However, the data is commonly assumed to
originate from a single never-changing system. In contrast, when modeling
real-world dynamical processes, the data distribution often shifts due to
changes in the underlying system dynamics. Continual learning of these
processes aims to rapidly adapt to abrupt system changes without forgetting
previous dynamical regimes. This work proposes an approach to continual
learning based on reservoir computing, a state-of-the-art method for training
recurrent neural networks on complex spatiotemporal dynamical systems.
Reservoir computing fixes the recurrent network weights - hence these cannot be
forgotten - and only updates linear projection heads to the output. We propose
to train multiple competitive prediction heads concurrently. Inspired by
neuroscience's predictive coding, only the most predictive heads activate,
laterally inhibiting and thus protecting the inactive heads from forgetting
induced by interfering parameter updates. We show that this multi-head
reservoir minimizes interference and catastrophic forgetting on several
dynamical systems, including the Van-der-Pol oscillator, the chaotic Lorenz
attractor, and the high-dimensional Lorenz-96 weather model. Our results
suggest that reservoir computing is a promising candidate framework for the
continual learning of dynamical systems. We provide our code for data
generation, method, and comparisons at
\url{https://github.com/leonardbereska/multiheadreservoir}."
"Deep neural networks present impressive performance, yet they cannot reliably
estimate their predictive confidence, limiting their applicability in high-risk
domains. We show that applying a multi-label one-vs-all loss reveals
classification ambiguity and reduces model overconfidence. The introduced SLOVA
(Single Label One-Vs-All) model redefines typical one-vs-all predictive
probabilities to a single label situation, where only one class is the correct
answer. The proposed classifier is confident only if a single class has a high
probability and other probabilities are negligible. Unlike the typical softmax
function, SLOVA naturally detects out-of-distribution samples if the
probabilities of all other classes are small. The model is additionally
fine-tuned with exponential calibration, which allows us to precisely align the
confidence score with model accuracy. We verify our approach on three tasks.
First, we demonstrate that SLOVA is competitive with the state-of-the-art on
in-distribution calibration. Second, the performance of SLOVA is robust under
dataset shifts. Finally, our approach performs extremely well in the detection
of out-of-distribution samples. Consequently, SLOVA is a tool that can be used
in various applications where uncertainty modeling is required."
"The ability to separate signal from noise, and reason with clean
abstractions, is critical to intelligence. With this ability, humans can
efficiently perform real world tasks without considering all possible nuisance
factors.How can artificial agents do the same? What kind of information can
agents safely discard as noises?
  In this work, we categorize information out in the wild into four types based
on controllability and relation with reward, and formulate useful information
as that which is both controllable and reward-relevant. This framework
clarifies the kinds information removed by various prior work on representation
learning in reinforcement learning (RL), and leads to our proposed approach of
learning a Denoised MDP that explicitly factors out certain noise distractors.
Extensive experiments on variants of DeepMind Control Suite and RoboDesk
demonstrate superior performance of our denoised world model over using raw
observations alone, and over prior works, across policy optimization control
tasks as well as the non-control task of joint position regression."
"Multi-fidelity modeling and learning are important in physical
simulation-related applications. It can leverage both low-fidelity and
high-fidelity examples for training so as to reduce the cost of data generation
while still achieving good performance. While existing approaches only model
finite, discrete fidelities, in practice, the fidelity choice is often
continuous and infinite, which can correspond to a continuous mesh spacing or
finite element length. In this paper, we propose Infinite Fidelity
Coregionalization (IFC). Given the data, our method can extract and exploit
rich information within continuous, infinite fidelities to bolster the
prediction accuracy. Our model can interpolate and/or extrapolate the
predictions to novel fidelities, which can be even higher than the fidelities
of training data. Specifically, we introduce a low-dimensional latent output as
a continuous function of the fidelity and input, and multiple it with a basis
matrix to predict high-dimensional solution outputs. We model the latent output
as a neural Ordinary Differential Equation (ODE) to capture the complex
relationships within and integrate information throughout the continuous
fidelities. We then use Gaussian processes or another ODE to estimate the
fidelity-varying bases. For efficient inference, we reorganize the bases as a
tensor, and use a tensor-Gaussian variational posterior to develop a scalable
inference algorithm for massive outputs. We show the advantage of our method in
several benchmark tasks in computational physics."
"Using transfer learning to adapt a pre-trained ""source model"" to a downstream
""target task"" can dramatically increase performance with seemingly no downside.
In this work, we demonstrate that there can exist a downside after all: bias
transfer, or the tendency for biases of the source model to persist even after
adapting the model to the target class. Through a combination of synthetic and
natural experiments, we show that bias transfer both (a) arises in realistic
settings (such as when pre-training on ImageNet or other standard datasets) and
(b) can occur even when the target dataset is explicitly de-biased. As
transfer-learned models are increasingly deployed in the real world, our work
highlights the importance of understanding the limitations of pre-trained
source models. Code is available at https://github.com/MadryLab/bias-transfer"
"The ensemble random forest filter (ERFF) is presented as an alternative to
the ensemble Kalman filter (EnKF) for the purpose of inverse modeling. The EnKF
is a data assimilation approach that forecasts and updates parameter estimates
sequentially in time as observations are being collected. The updating step is
based on the experimental covariances computed from an ensemble of realizations
and the updates are given as linear combinations of the differences between
observations and forecasted system state values. The ERFF replaces the linear
combination in the update step with a non-linear function represented by a
random forest. In this way, the non-linear relationships between the parameters
to be updated and the observations can be captured and a better update
produced. The ERFF is demonstrated for the purpose of log-conductivity
identification from piezometric head observations in a number of scenarios with
varying degrees of heterogeneity (log-conductivity variances going from 1 up to
6.25 (ln m/d)2), number of realizations in the ensemble (50 or 100), and number
of piezometric head observations (18 or 36). In all scenarios, the ERFF works
well, being able to reconstruct the log-conductivity spatial heterogeneity
while matching the observed piezometric heads at selected control points. For
benchmarking purposes the ERFF is compared to the restart EnKF to find that the
ERFF is superior to the EnKF for the number of ensemble realizations used
(small in typical EnKF applications). Only when the number of realizations
grows to 500, the restart EnKF is able to match the performance of the ERFF,
albeit at triple the computational cost."
"The growing adoption of IoT devices for healthcare has enabled researchers to
build intelligence using all the data produced by these devices. Monitoring and
diagnosing health have been the two most common scenarios where such devices
have proven beneficial. Achieving high prediction accuracy was a top priority
initially, but the focus has slowly shifted to efficiency and higher
throughput, and processing the data from these devices in a distributed manner
has proven to help achieve both. Since the field of machine learning is vast
with numerous state-of-the-art algorithms in play, it has been a challenge to
identify the algorithms that perform best in different scenarios. In this
literature review, we explored the distributed machine learning algorithms
tested by the authors of the selected studies and identified the ones that
achieved the best prediction accuracy in each healthcare scenario. While no
algorithm performed consistently, Random Forest performed the best in a few
studies. This could serve as a good starting point for future studies on
collaborative machine learning on IoMT data."
"Accurate and efficient detection of ovarian cancer at early stages is
critical to ensure proper treatments for patients. Among the first-line
modalities investigated in studies of early diagnosis are features distilled
from protein mass spectra. This method, however, considers only a specific
subset of spectral responses and ignores the interplay among protein expression
levels, which can also contain diagnostic information. We propose a new
modality that automatically searches protein mass spectra for discriminatory
features by considering the self-similar nature of the spectra. Self-similarity
is assessed by taking a wavelet decomposition of protein mass spectra and
estimating the rate of level-wise decay in the energies of the resulting
wavelet coefficients. Level-wise energies are estimated in a robust manner
using distance variance, and rates are estimated locally via a rolling window
approach. This results in a collection of rates that can be used to
characterize the interplay among proteins, which can be indicative of cancer
presence. Discriminatory descriptors are then selected from these evolutionary
rates and used as classifying features. The proposed wavelet-based features are
used in conjunction with features proposed in the existing literature for early
stage diagnosis of ovarian cancer using two datasets published by the American
National Cancer Institute. Including the wavelet-based features from the new
modality results in improvements in diagnostic performance for early-stage
ovarian cancer detection. This demonstrates the ability of the proposed
modality to characterize new ovarian cancer diagnostic information."
"Nearest prototype classifiers (NPCs) assign to each input point the label of
the nearest prototype with respect to a chosen distance metric. A direct
advantage of NPCs is that the decisions are interpretable. Previous work could
provide lower bounds on the minimal adversarial perturbation in the
$\ell_p$-threat model when using the same $\ell_p$-distance for the NPCs. In
this paper we provide a complete discussion on the complexity when using
$\ell_p$-distances for decision and $\ell_q$-threat models for certification
for $p,q \in \{1,2,\infty\}$. In particular we provide scalable algorithms for
the \emph{exact} computation of the minimal adversarial perturbation when using
$\ell_2$-distance and improved lower bounds in other cases. Using efficient
improved lower bounds we train our Provably adversarially robust NPC (PNPC),
for MNIST which have better $\ell_2$-robustness guarantees than neural
networks. Additionally, we show up to our knowledge the first certification
results w.r.t. to the LPIPS perceptual metric which has been argued to be a
more realistic threat model for image classification than $\ell_p$-balls. Our
PNPC has on CIFAR10 higher certified robust accuracy than the empirical robust
accuracy reported in (Laidlaw et al., 2021). The code is available in our
repository."
"Federated Learning (FL) has become a practical and widely adopted distributed
learning paradigm. However, the lack of a comprehensive and standardized
solution covering diverse use cases makes it challenging to use in practice. In
addition, selecting an appropriate FL framework for a specific use case can be
a daunting task. In this work, we present UniFed, the first unified platform
for standardizing existing open-source FL frameworks. The platform streamlines
the end-to-end workflow for distributed experimentation and deployment,
encompassing 11 popular open-source FL frameworks. In particular, to address
the substantial variations in workflows and data formats, UniFed introduces a
configuration-based schema-enforced task specification, offering 20 editable
fields. UniFed also provides functionalities such as distributed execution
management, logging, and data analysis.
  With UniFed, we evaluate and compare 11 popular FL frameworks from the
perspectives of functionality, privacy protection, and performance, through
conducting developer surveys and code-level investigation. We collect 15
diverse FL scenario setups (e.g., horizontal and vertical settings) for FL
framework evaluation. This comprehensive evaluation allows us to analyze both
model and system performance, providing detailed comparisons and offering
recommendations for framework selection. UniFed simplifies the process of
selecting and utilizing the appropriate FL framework for specific use cases,
while enabling standardized distributed experimentation and deployment. Our
results and analysis based on experiments with up to 178 distributed nodes
provide valuable system design and deployment insights, aiming to empower
practitioners in their pursuit of effective FL solutions."
"Heterogeneous graphs have multiple node and edge types and are semantically
richer than homogeneous graphs. To learn such complex semantics, many graph
neural network approaches for heterogeneous graphs use metapaths to capture
multi-hop interactions between nodes. Typically, features from non-target nodes
are not incorporated into the learning procedure. However, there can be
nonlinear, high-order interactions involving multiple nodes or edges. In this
paper, we present Simplicial Graph Attention Network (SGAT), a simplicial
complex approach to represent such high-order interactions by placing features
from non-target nodes on the simplices. We then use attention mechanisms and
upper adjacencies to generate representations. We empirically demonstrate the
efficacy of our approach with node classification tasks on heterogeneous graph
datasets and further show SGAT's ability in extracting structural information
by employing random node features. Numerical experiments indicate that SGAT
performs better than other current state-of-the-art heterogeneous graph
learning methods."
"Graph Neural Network (GNNs) based methods have recently become a popular tool
to deal with graph data because of their ability to incorporate structural
information. The only hurdle in the performance of GNNs is the lack of labeled
data. Data Augmentation techniques for images and text data can not be used for
graph data because of the complex and non-euclidean structure of graph data.
This gap has forced researchers to shift their focus towards the development of
data augmentation techniques for graph data. Most of the proposed Graph Data
Augmentation (GDA) techniques are task-specific. In this paper, we survey the
existing GDA techniques based on different graph tasks. This survey not only
provides a reference to the research community of GDA but also provides the
necessary information to the researchers of other domains."
"Predicting the traffic incident duration is a hard problem to solve due to
the stochastic nature of incident occurrence in space and time, a lack of
information at the beginning of a reported traffic disruption, and lack of
advanced methods in transport engineering to derive insights from past
accidents. This paper proposes a new fusion framework for predicting the
incident duration from limited information by using an integration of machine
learning with traffic flow/speed and incident description as features, encoded
via several Deep Learning methods (ANN autoencoder and character-level LSTM-ANN
sentiment classifier). The paper constructs a cross-disciplinary modelling
approach in transport and data science. The approach improves the incident
duration prediction accuracy over the top-performing ML models applied to
baseline incident reports. Results show that our proposed method can improve
the accuracy by $60\%$ when compared to standard linear or support vector
regression models, and a further $7\%$ improvement with respect to the hybrid
deep learning auto-encoded GBDT model which seems to outperform all other
models. The application area is the city of San Francisco, rich in both traffic
incident logs (Countrywide Traffic Accident Data set) and past historical
traffic congestion information (5-minute precision measurements from Caltrans
Performance Measurement System)."
"Clinical records frequently include assessments of the characteristics of
patients, which may include the completion of various questionnaires. These
questionnaires provide a variety of perspectives on a patient's current state
of well-being. Not only is it critical to capture the heterogeneity given by
these perspectives, but there is also a growing demand for developing
cost-effective technologies for clinical phenotyping. Filling out many
questionnaires may be a strain for the patients and therefore costly. In this
work, we propose COBALT -- a cost-based layer selector model for detecting
phenotypes using a community detection approach. Our goal is to minimize the
number of features used to build these phenotypes while preserving its quality.
We test our model using questionnaire data from chronic tinnitus patients and
represent the data in a multi-layer network structure. The model is then
evaluated by predicting post-treatment data using baseline features (age,
gender, and pre-treatment data) as well as the identified phenotypes as a
feature. For some post-treatment variables, predictors using phenotypes from
COBALT as features outperformed those using phenotypes detected by traditional
clustering methods. Moreover, using phenotype data to predict post-treatment
data proved beneficial in comparison with predictors that were solely trained
with baseline features."
"Sports, due to their global reach and impact-rich prediction tasks, are an
exciting domain to deploy machine learning models. However, data from
conventional sports is often unsuitable for research use due to its size,
veracity, and accessibility. To address these issues, we turn to esports, a
growing domain that encompasses video games played in a capacity similar to
conventional sports. Since esports data is acquired through server logs rather
than peripheral sensors, esports provides a unique opportunity to obtain a
massive collection of clean and detailed spatiotemporal data, similar to those
collected in conventional sports. To parse esports data, we develop awpy, an
open-source esports game log parsing library that can extract player
trajectories and actions from game logs. Using awpy, we parse 8.6m actions,
7.9m game frames, and 417k trajectories from 1,558 game logs from professional
Counter-Strike tournaments to create the Esports Trajectory and Actions (ESTA)
dataset. ESTA is one of the largest and most granular publicly available sports
data sets to date. We use ESTA to develop benchmarks for win prediction using
player-specific information. The ESTA data is available at
https://github.com/pnxenopoulos/esta and awpy is made public through PyPI."
"We summarize the model and results of PirouNet, a semi-supervised recurrent
variational autoencoder. Given a small amount of dance sequences labeled with
qualitative choreographic annotations, PirouNet conditionally generates dance
sequences in the style of the choreographer."
"Specializing Directed Acyclic Graph Federated Learning(SDAGFL) is a new
federated learning framework which updates model from the devices with similar
data distribution through Directed Acyclic Graph Distributed Ledger Technology
(DAG-DLT). SDAGFL has the advantage of personalization, resisting single point
of failure and poisoning attack in fully decentralized federated learning.
Because of these advantages, the SDAGFL is suitable for the federated learning
in IoT scenario where the device is usually battery-powered. To promote the
application of SDAGFL in IoT, we propose an energy optimized SDAGFL based
event-triggered communication mechanism, called ESDAGFL. In ESDAGFL, the new
model is broadcasted only when it is significantly changed. We evaluate the
ESDAGFL on a clustered synthetically FEMNIST dataset and a dataset from texts
by Shakespeare and Goethe's works. The experiment results show that our
approach can reduce energy consumption by 33\% compared with SDAGFL, and
realize the same balance between training accuracy and specialization as
SDAGFL."
"Accurate traffic forecasting is vital to an intelligent transportation
system. Although many deep learning models have achieved state-of-art
performance for short-term traffic forecasting of up to 1 hour, long-term
traffic forecasting that spans multiple hours remains a major challenge.
Moreover, most of the existing deep learning traffic forecasting models are
black box, presenting additional challenges related to explainability and
interpretability. We develop Graph Pyramid Autoformer (X-GPA), an explainable
attention-based spatial-temporal graph neural network that uses a novel pyramid
autocorrelation attention mechanism. It enables learning from long temporal
sequences on graphs and improves long-term traffic forecasting accuracy. Our
model can achieve up to 35 % better long-term traffic forecast accuracy than
that of several state-of-the-art methods. The attention-based scores from the
X-GPA model provide spatial and temporal explanations based on the traffic
dynamics, which change for normal vs. peak-hour traffic and weekday vs. weekend
traffic."
"Semantic search is an important task which objective is to find the relevant
index from a database for query. It requires a retrieval model that can
properly learn the semantics of sentences. Transformer-based models are widely
used as retrieval models due to their excellent ability to learn semantic
representations. in the meantime, many regularization methods suitable for them
have also been proposed. In this paper, we propose a new regularization method:
Regularized Contrastive Learning, which can help transformer-based models to
learn a better representation of sentences. It firstly augments several
different semantic representations for every sentence, then take them into the
contrastive objective as regulators. These contrastive regulators can overcome
overfitting issues and alleviate the anisotropic problem. We firstly evaluate
our approach on 7 semantic search benchmarks with the outperforming pre-trained
model SRoBERTA. The results show that our method is more effective for learning
a superior sentence representation. Then we evaluate our approach on 2
challenging FAQ datasets, Cough and Faqir, which have long query and index. The
results of our experiments demonstrate that our method outperforms baseline
methods."
"Training recurrent neural networks is predominantly achieved via
backpropagation through time (BPTT). However, this algorithm is not an optimal
solution from both a biological and computational perspective. A more efficient
and biologically plausible alternative for BPTT is e-prop. We investigate the
applicability of e-prop to long short-term memorys (LSTMs), for both supervised
and reinforcement learning (RL) tasks. We show that e-prop is a suitable
optimization algorithm for LSTMs by comparing it to BPTT on two benchmarks for
supervised learning. This proves that e-prop can achieve learning even for
problems with long sequences of several hundred timesteps. We introduce
extensions that improve the performance of e-prop, which can partially be
applied to other network architectures. With the help of these extensions we
show that, under certain conditions, e-prop can outperform BPTT for one of the
two benchmarks for supervised learning. Finally, we deliver a proof of concept
for the integration of e-prop to RL in the domain of deep recurrent Q-learning."
"We present a dynamic model in which the weights are conditioned on an input
sample x and are learned to match those that would be obtained by finetuning a
base model on x and its label y. This mapping between an input sample and
network weights is approximated by a denoising diffusion model. The diffusion
model we employ focuses on modifying a single layer of the base model and is
conditioned on the input, activations, and output of this layer. Since the
diffusion model is stochastic in nature, multiple initializations generate
different networks, forming an ensemble, which leads to further improvements.
Our experiments demonstrate the wide applicability of the method for image
classification, 3D reconstruction, tabular data, speech separation, and natural
language processing. Our code is available at
https://github.com/ShaharLutatiPersonal/OCD"
"Given the wide and ever growing range of different efficient Transformer
attention mechanisms, it is important to identify which attention is most
effective when given a task. In this work, we are also interested in combining
different attention types to build heterogeneous Transformers. We first propose
a DARTS-like Neural Architecture Search (NAS) method to find the best attention
for a given task, in this setup, all heads use the same attention (homogeneous
models). Our results suggest that NAS is highly effective on this task, and it
identifies the best attention mechanisms for IMDb byte level text
classification and Listops. We then extend our framework to search for and
build Transformers with multiple different attention types, and call them
heterogeneous Transformers. We show that whilst these heterogeneous
Transformers are better than the average homogeneous models, they cannot
outperform the best. We explore the reasons why heterogeneous attention makes
sense, and why it ultimately fails."
"Interpretable machine learning plays a key role in healthcare because it is
challenging in understanding feature importance in deep learning model
predictions. We propose a novel framework that uses deep learning to study
feature sensitivity for model predictions. This work combines sensitivity
analysis with heterogeneous time-series deep learning model prediction, which
corresponds to the interpretations of spatio-temporal features. We forecast
county-level COVID-19 infection using the Temporal Fusion Transformer. We then
use the sensitivity analysis extending Morris Method to see how sensitive the
outputs are with respect to perturbation to our static and dynamic input
features. The significance of the work is grounded in a real-world COVID-19
infection prediction with highly non-stationary, finely granular, and
heterogeneous data. 1) Our model can capture the detailed daily changes of
temporal and spatial model behaviors and achieves high prediction performance
compared to a PyTorch baseline. 2) By analyzing the Morris sensitivity indices
and attention patterns, we decipher the meaning of feature importance with
observational population and dynamic model changes. 3) We have collected 2.5
years of socioeconomic and health features over 3142 US counties, such as
observed cases and deaths, and a number of static (age distribution, health
disparity, and industry) and dynamic features (vaccination, disease spread,
transmissible cases, and social distancing). Using the proposed framework, we
conduct extensive experiments and show our model can learn complex interactions
and perform predictions for daily infection at the county level. Being able to
model the disease infection with a hybrid prediction and description accuracy
measurement with Morris index at the county level is a central idea that sheds
light on individual feature interpretation via sensitivity analysis."
"Training dynamic models, such as neural ODEs, on long trajectories is a hard
problem that requires using various tricks, such as trajectory splitting, to
make model training work in practice. These methods are often heuristics with
poor theoretical justifications, and require iterative manual tuning. We
propose a principled multiple shooting technique for neural ODEs that splits
the trajectories into manageable short segments, which are optimised in
parallel, while ensuring probabilistic control on continuity over consecutive
segments. We derive variational inference for our shooting-based latent neural
ODE models and propose amortized encodings of irregularly sampled trajectories
with a transformer-based recognition network with temporal attention and
relative positional encoding. We demonstrate efficient and stable training, and
state-of-the-art performance on multiple large-scale benchmark datasets."
"Quantizing a Deep Neural Network (DNN) model to be used on a custom
accelerator with efficient fixed-point hardware implementations, requires
satisfying many stringent hardware-friendly quantization constraints to train
the model. We evaluate the two main classes of hardware-friendly quantization
methods in the context of weight quantization: the traditional Mean Squared
Quantization Error (MSQE)-based methods and the more recent gradient-based
methods. We study the two methods on MobileNetV1 and MobileNetV2 using multiple
empirical metrics to identify the sources of performance differences between
the two classes, namely, sensitivity to outliers and convergence instability of
the quantizer scaling factor. Using those insights, we propose various
techniques to improve the performance of both quantization methods - they fix
the optimization instability issues present in the MSQE-based methods during
quantization of MobileNet models and allow us to improve validation performance
of the gradient-based methods by 4.0% and 3.3% for MobileNetV1 and MobileNetV2
on ImageNet respectively."
"Federated learning (FL) has increasingly been deployed, in its vertical form,
among organizations to facilitate secure collaborative training over siloed
data. In vertical FL (VFL), participants hold disjoint features of the same set
of sample instances. Among them, only one has labels. This participant, known
as the active party, initiates the training and interacts with the other
participants, known as the passive parties. Despite the increasing adoption of
VFL, it remains largely unknown if and how the active party can extract feature
data from the passive party, especially when training deep neural network (DNN)
models.
  This paper makes the first attempt to study the feature security problem of
DNN training in VFL. We consider a DNN model partitioned between active and
passive parties, where the latter only holds a subset of the input layer and
exhibits some categorical features of binary values. Using a reduction from the
Exact Cover problem, we prove that reconstructing those binary features is
NP-hard. Through analysis, we demonstrate that, unless the feature dimension is
exceedingly large, it remains feasible, both theoretically and practically, to
launch a reconstruction attack with an efficient search-based algorithm that
prevails over current feature protection techniques. To address this problem,
we develop a novel feature protection scheme against the reconstruction attack
that effectively misleads the search to some pre-specified random values. With
an extensive set of experiments, we show that our protection scheme sustains
the feature reconstruction attack in various VFL applications at no expense of
accuracy loss."
"The current success of Graph Neural Networks (GNNs) usually relies on loading
the entire attributed graph for processing, which may not be satisfied with
limited memory resources, especially when the attributed graph is large. This
paper pioneers to propose a Binary Graph Convolutional Network (Bi-GCN), which
binarizes both the network parameters and input node attributes and exploits
binary operations instead of floating-point matrix multiplications for network
compression and acceleration. Meanwhile, we also propose a new gradient
approximation based back-propagation method to properly train our Bi-GCN.
According to the theoretical analysis, our Bi-GCN can reduce the memory
consumption by an average of ~31x for both the network parameters and input
data, and accelerate the inference speed by an average of ~51x, on three
citation networks, i.e., Cora, PubMed, and CiteSeer. Besides, we introduce a
general approach to generalize our binarization method to other variants of
GNNs, and achieve similar efficiencies. Although the proposed Bi-GCN and
Bi-GNNs are simple yet efficient, these compressed networks may also possess a
potential capacity problem, i.e., they may not have enough storage capacity to
learn adequate representations for specific tasks. To tackle this capacity
problem, an Entropy Cover Hypothesis is proposed to predict the lower bound of
the width of Bi-GNN hidden layers. Extensive experiments have demonstrated that
our Bi-GCN and Bi-GNNs can give comparable performances to the corresponding
full-precision baselines on seven node classification datasets and verified the
effectiveness of our Entropy Cover Hypothesis for solving the capacity problem."
"Graph Neural Networks (GNNs) have received significant attention recently,
but training them at a large scale remains a challenge. Mini-batch training
coupled with sampling is used to alleviate this challenge. However, existing
approaches either suffer from the neighborhood explosion phenomenon or have
poor performance. To address these issues, we propose a new sampling algorithm
called LAyer-neighBOR sampling (LABOR). It is designed to be a direct
replacement for Neighbor Sampling (NS) with the same fanout hyperparameter
while sampling up to 7 times fewer vertices, without sacrificing quality. By
design, the variance of the estimator of each vertex matches NS from the point
of view of a single vertex. Moreover, under the same vertex sampling budget
constraints, LABOR converges faster than existing layer sampling approaches and
can use up to 112 times larger batch sizes compared to NS."
"Time series forecasting is crucial for many fields, such as disaster warning,
weather prediction, and energy consumption. The Transformer-based models are
considered to have revolutionized the field of sequence modeling. However, the
complex temporal patterns of the time series hinder the model from mining
reliable temporal dependencies. Furthermore, the autoregressive form of the
Transformer introduces cumulative errors in the inference step. In this paper,
we propose the probabilistic decomposition Transformer model that combines the
Transformer with a conditional generative model, which provides hierarchical
and interpretable probabilistic forecasts for intricate time series. The
Transformer is employed to learn temporal patterns and implement primary
probabilistic forecasts, while the conditional generative model is used to
achieve non-autoregressive hierarchical probabilistic forecasts by introducing
latent space feature representations. In addition, the conditional generative
model reconstructs typical features of the series, such as seasonality and
trend terms, from probability distributions in the latent space to enable
complex pattern separation and provide interpretable forecasts. Extensive
experiments on several datasets demonstrate the effectiveness and robustness of
the proposed model, indicating that it compares favorably with the state of the
art."
"We propose a novel neural algorithm for the fundamental problem of computing
the entropic optimal transport (EOT) plan between continuous probability
distributions which are accessible by samples. Our algorithm is based on the
saddle point reformulation of the dynamic version of EOT which is known as the
Schr\""odinger Bridge problem. In contrast to the prior methods for large-scale
EOT, our algorithm is end-to-end and consists of a single learning step, has
fast inference procedure, and allows handling small values of the entropy
regularization coefficient which is of particular importance in some applied
problems. Empirically, we show the performance of the method on several
large-scale EOT tasks.
https://github.com/ngushchin/EntropicNeuralOptimalTransport"
"In fighting games, individual players of the same skill level often exhibit
distinct strategies from one another through their gameplay. Despite this, the
majority of AI agents for fighting games have only a single strategy for each
""level"" of difficulty. To make AI opponents more human-like, we'd ideally like
to see multiple different strategies at each level of difficulty, a concept we
refer to as ""multidimensional"" difficulty. In this paper, we introduce a
diversity-based deep reinforcement learning approach for generating a set of
agents of similar difficulty that utilize diverse strategies. We find this
approach outperforms a baseline trained with specialized, human-authored reward
functions in both diversity and performance."
"Hyper-parameters (HPs) are an important part of machine learning (ML) model
development and can greatly influence performance. This paper studies their
behavior for three algorithms: Extreme Gradient Boosting (XGB), Random Forest
(RF), and Feedforward Neural Network (FFNN) with structured data. Our empirical
investigation examines the qualitative behavior of model performance as the HPs
vary, quantifies the importance of each HP for different ML algorithms, and
stability of the performance near the optimal region. Based on the findings, we
propose a set of guidelines for efficient HP tuning by reducing the search
space."
"The message-passing scheme is the core of graph representation learning.
While most existing message-passing graph neural networks (MPNNs) are
permutation-invariant in graph-level representation learning and
permutation-equivariant in node- and edge-level representation learning, their
expressive power is commonly limited by the 1-Weisfeiler-Lehman (1-WL) graph
isomorphism test. Recently proposed expressive graph neural networks (GNNs)
with specially designed complex message-passing mechanisms are not practical.
To bridge the gap, we propose a plug-in Equivariant Distance ENcoding (EDEN)
for MPNNs. EDEN is derived from a series of interpretable transformations on
the graph's distance matrix. We theoretically prove that EDEN is
permutation-equivariant for all level graph representation learning, and we
empirically illustrate that EDEN's expressive power can reach up to the 3-WL
test. Extensive experiments on real-world datasets show that combining EDEN
with conventional GNNs surpasses recent advanced GNNs."
"Many common methods for learning a world model for pixel-based environments
use generative architectures trained with pixel-level reconstruction
objectives. Recently proposed Joint Embedding Predictive Architectures (JEPA)
offer a reconstruction-free alternative. In this work, we analyze performance
of JEPA trained with VICReg and SimCLR objectives in the fully offline setting
without access to rewards, and compare the results to the performance of the
generative architecture. We test the methods in a simple environment with a
moving dot with various background distractors, and probe learned
representations for the dot's location. We find that JEPA methods perform on
par or better than reconstruction when distractor noise changes every time
step, but fail when the noise is fixed. Furthermore, we provide a theoretical
explanation for the poor performance of JEPA-based methods with fixed noise,
highlighting an important limitation."
"Transfer of recent advances in deep reinforcement learning to real-world
applications is hindered by high data demands and thus low efficiency and
scalability. Through independent improvements of components such as replay
buffers or more stable learning algorithms, and through massively distributed
systems, training time could be reduced from several days to several hours for
standard benchmark tasks. However, while rewards in simulated environments are
well-defined and easy to compute, reward evaluation becomes the bottleneck in
many real-world environments, e.g., in molecular optimization tasks, where
computationally demanding simulations or even experiments are required to
evaluate states and to quantify rewards. Therefore, training might become
prohibitively expensive without an extensive amount of computational resources
and time. We propose to alleviate this problem by replacing costly ground-truth
rewards with rewards modeled by neural networks, counteracting non-stationarity
of state and reward distributions during training with an active learning
component. We demonstrate that using our proposed ACRL method (Actively
learning Costly rewards for Reinforcement Learning), it is possible to train
agents in complex real-world environments orders of magnitudes faster. By
enabling the application of reinforcement learning methods to new domains, we
show that we can find interesting and non-trivial solutions to real-world
optimization problems in chemistry, materials science and engineering."
"Unsupervised learning methods are well established in the area of anomaly
detection and achieve state of the art performances on outlier datasets.
Outliers play a significant role, since they bear the potential to distort the
predictions of a machine learning algorithm on a given dataset. Especially
among PCA-based methods, outliers have an additional destructive potential
regarding the result: they may not only distort the orientation and translation
of the principal components, they also make it more complicated to detect
outliers. To address this problem, we propose the robust outlier detection
algorithm CoMadOut, which satisfies two required properties: (1) being robust
towards outliers and (2) detecting them. Our CoMadOut outlier detection
variants using comedian PCA define, dependent on its variant, an inlier region
with a robust noise margin by measures of in-distribution (variant CMO) and
optimized scores by measures of out-of-distribution (variants CMO*), e.g.
kurtosis-weighting by CMO+k. These measures allow distribution based outlier
scoring for each principal component, and thus, an appropriate alignment of the
degree of outlierness between normal and abnormal instances. Experiments
comparing CoMadOut with traditional, deep and other comparable robust outlier
detection methods showed that the performance of the introduced CoMadOut
approach is competitive to well established methods related to average
precision (AP), area under the precision recall curve (AUPRC) and area under
the receiver operating characteristic (AUROC) curve. In summary our approach
can be seen as a robust alternative for outlier detection tasks."
"Neural networks with sinusoidal activations have been proposed as an
alternative to networks with traditional activation functions. Despite their
promise, particularly for learning implicit models, their training behavior is
not yet fully understood, leading to a number of empirical design choices that
are not well justified. In this work, we first propose a simplified version of
such sinusoidal neural networks, which allows both for easier practical
implementation and simpler theoretical analysis. We then analyze the behavior
of these networks from the neural tangent kernel perspective and demonstrate
that their kernel approximates a low-pass filter with an adjustable bandwidth.
Finally, we utilize these insights to inform the sinusoidal network
initialization, optimizing their performance for each of a series of tasks,
including learning implicit models and solving differential equations."
"This paper revisits building machine learning algorithms that involve
interactions between entities, such as those between financial assets in an
actively managed portfolio, or interactions between users in a social network.
Our goal is to forecast the future evolution of ensembles of multivariate time
series in such applications (e.g., the future return of a financial asset or
the future popularity of a Twitter account). Designing ML algorithms for such
systems requires addressing the challenges of high-dimensional interactions and
non-linearity. Existing approaches usually adopt an ad-hoc approach to
integrating high-dimensional techniques into non-linear models and recent
studies have shown these approaches have questionable efficacy in time-evolving
interacting systems.
  To this end, we propose a novel framework, which we dub as the additive
influence model. Under our modeling assumption, we show that it is possible to
decouple the learning of high-dimensional interactions from the learning of
non-linear feature interactions. To learn the high-dimensional interactions, we
leverage kernel-based techniques, with provable guarantees, to embed the
entities in a low-dimensional latent space. To learn the non-linear
feature-response interactions, we generalize prominent machine learning
techniques, including designing a new statistically sound non-parametric method
and an ensemble learning algorithm optimized for vector regressions. Extensive
experiments on two common applications demonstrate that our new algorithms
deliver significantly stronger forecasting power compared to standard and
recently proposed methods."
"Fatigue strength estimation is a costly manual material characterization
process in which state-of-the-art approaches follow a standardized experiment
and analysis procedure. In this paper, we examine a modular, Machine
Learning-based approach for fatigue strength estimation that is likely to
reduce the number of experiments and, thus, the overall experimental costs.
Despite its high potential, deployment of a new approach in a real-life lab
requires more than the theoretical definition and simulation. Therefore, we
study the robustness of the approach against misspecification of the prior and
discretization of the specified loads. We identify its applicability and its
advantageous behavior over the state-of-the-art methods, potentially reducing
the number of costly experiments."
"In time series forecasting, decomposition-based algorithms break aggregate
data into meaningful components and are therefore appreciated for their
particular advantages in interpretability. Recent algorithms often combine
machine learning (hereafter ML) methodology with decomposition to improve
prediction accuracy. However, incorporating ML is generally considered to
sacrifice interpretability inevitably. In addition, existing hybrid algorithms
usually rely on theoretical models with statistical assumptions and focus only
on the accuracy of aggregate predictions, and thus suffer from accuracy
problems, especially in component estimates. In response to the above issues,
this research explores the possibility of improving accuracy without losing
interpretability in time series forecasting. We first quantitatively define
interpretability for data-driven forecasts and systematically review the
existing forecasting algorithms from the perspective of interpretability.
Accordingly, we propose the W-R algorithm, a hybrid algorithm that combines
decomposition and ML from a novel perspective. Specifically, the W-R algorithm
replaces the standard additive combination function with a weighted variant and
uses ML to modify the estimates of all components simultaneously. We
mathematically analyze the theoretical basis of the algorithm and validate its
performance through extensive numerical experiments. In general, the W-R
algorithm outperforms all decomposition-based and ML benchmarks. Based on
P50_QL, the algorithm relatively improves by 8.76% in accuracy on the practical
sales forecasts of JD.com and 77.99% on a public dataset of electricity loads.
This research offers an innovative perspective to combine the statistical and
ML algorithms, and JD.com has implemented the W-R algorithm to make accurate
sales predictions and guide its marketing activities."
"Various graph neural networks (GNNs) have been proposed to solve node
classification tasks in machine learning for graph data. GNNs use the
structural information of graph data by aggregating the features of neighboring
nodes. However, they fail to directly characterize and leverage the structural
information. In this paper, we propose multi-duplicated characterization of
graph structures using information gain ratio (IGR) for GNNs (MSI-GNN), which
enhances the performance of node classification by using an i-hop adjacency
matrix as the structural information of the graph data. In MSI-GNN, the i-hop
adjacency matrix is adaptively adjusted by two methods: (i) structural features
in the matrix are selected based on the IGR, and (ii) the selected features in
(i) for each node are duplicated and combined flexibly. In an experiment, we
show that our MSI-GNN outperforms GCN, H2GCN, and GCNII in terms of average
accuracies in benchmark graph datasets."
"Early detection of many life-threatening diseases (e.g., prostate and breast
cancer) within at-risk population can improve clinical outcomes and reduce cost
of care. While numerous disease-specific ""screening"" tests that are closer to
Point-of-Care (POC) are in use for this task, their low specificity results in
unnecessary biopsies, leading to avoidable patient trauma and wasteful
healthcare spending. On the other hand, despite the high accuracy of Magnetic
Resonance (MR) imaging in disease diagnosis, it is not used as a POC disease
identification tool because of poor accessibility. The root cause of poor
accessibility of MR stems from the requirement to reconstruct high-fidelity
images, as it necessitates a lengthy and complex process of acquiring large
quantities of high-quality k-space measurements. In this study we explore the
feasibility of an ML-augmented MR pipeline that directly infers the disease
sidestepping the image reconstruction process. We hypothesise that the disease
classification task can be solved using a very small tailored subset of k-space
data, compared to image reconstruction. Towards that end, we propose a method
that performs two tasks: 1) identifies a subset of the k-space that maximizes
disease identification accuracy, and 2) infers the disease directly using the
identified k-space subset, bypassing the image reconstruction step. We validate
our hypothesis by measuring the performance of the proposed system across
multiple diseases and anatomies. We show that comparable performance to
image-based classifiers, trained on images reconstructed with full k-space
data, can be achieved using small quantities of data: 8% of the data for
detecting multiple abnormalities in prostate and brain scans, and 5% of the
data for knee abnormalities. To better understand the proposed approach and
instigate future research, we provide an extensive analysis and release code."
"Neural networks achieved high performance over different tasks, i.e. image
identification, voice recognition and other applications. Despite their
success, these models are still vulnerable regarding small perturbations, which
can be used to craft the so-called adversarial examples. Different approaches
have been proposed to circumvent their vulnerability, including formal
verification systems, which employ a variety of techniques, including
reachability, optimization and search procedures, to verify that the model
satisfies some property. In this paper we propose three novel reachability
algorithms for verifying deep neural networks with ReLU activations. The first
and third algorithms compute an over-approximation for the reachable set,
whereas the second one computes the exact reachable set. Differently from
previously proposed approaches, our algorithms take as input a V-polytope. Our
experiments on the ACAS Xu problem show that the Exact Polytope Network Mapping
(EPNM) reachability algorithm proposed in this work surpass the
state-of-the-art results from the literature, specially in relation to other
reachability methods."
"While acute stress has been shown to have both positive and negative effects
on performance, not much is known about the impacts of stress on students
grades during examinations. To answer this question, we examined whether a
correlation could be found between physiological stress signals and exam
performance. We conducted this study using multiple physiological signals of
ten undergraduate students over three different exams. The study focused on
three signals, i.e., skin temperature, heart rate, and electrodermal activity.
We extracted statistics as features and fed them into a variety of binary
classifiers to predict relatively higher or lower grades. Experimental results
showed up to 0.81 ROC-AUC with k-nearest neighbor algorithm among various
machine learning algorithms."
"Causal Bayesian optimisation (CaBO) combines causality with Bayesian
optimisation (BO) and shows that there are situations where the optimal reward
is not achievable if causal knowledge is ignored. While CaBO exploits causal
relations to determine the set of controllable variables to intervene on, it
does not exploit purely observational variables and marginalises them. We show
that, in general, utilising a subset of observational variables as a context to
choose the values of interventional variables leads to lower cumulative
regrets. We propose a general framework of contextual causal Bayesian
optimisation that efficiently searches through combinations of controlled and
contextual variables, known as policy scopes, and identifies the one yielding
the optimum. We highlight the difficulties arising from the application of the
causal acquisition function currently used in CaBO to select the policy scope
in contextual settings and propose a multi-armed bandits based selection
mechanism. We analytically show that well-established methods, such as
contextual BO (CoBO) or CaBO, are not able to achieve the optimum in some
cases, and empirically show that the proposed method achieves sub-linear regret
in various environments and under different configurations."
"In recent years numerous methods have been developed to formally verify the
robustness of deep neural networks (DNNs). Though the proposed techniques are
effective in providing mathematical guarantees about the DNNs behavior, it is
not clear whether the proofs generated by these methods are
human-interpretable. In this paper, we bridge this gap by developing new
concepts, algorithms, and representations to generate human understandable
interpretations of the proofs. Leveraging the proposed method, we show that the
robustness proofs of standard DNNs rely on spurious input features, while the
proofs of DNNs trained to be provably robust filter out even the semantically
meaningful features. The proofs for the DNNs combining adversarial and provably
robust training are the most effective at selectively filtering out spurious
features as well as relying on human-understandable input features."
"As graph data size increases, the vast latency and memory consumption during
inference pose a significant challenge to the real-world deployment of Graph
Neural Networks (GNNs). While quantization is a powerful approach to reducing
GNNs complexity, most previous works on GNNs quantization fail to exploit the
unique characteristics of GNNs, suffering from severe accuracy degradation.
Through an in-depth analysis of the topology of GNNs, we observe that the
topology of the graph leads to significant differences between nodes, and most
of the nodes in a graph appear to have a small aggregation value. Motivated by
this, in this paper, we propose the Aggregation-Aware mixed-precision
Quantization ($\rm A^2Q$) for GNNs, where an appropriate bitwidth is
automatically learned and assigned to each node in the graph. To mitigate the
vanishing gradient problem caused by sparse connections between nodes, we
propose a Local Gradient method to serve the quantization error of the node
features as the supervision during training. We also develop a Nearest Neighbor
Strategy to deal with the generalization on unseen graphs. Extensive
experiments on eight public node-level and graph-level datasets demonstrate the
generality and robustness of our proposed method. Compared to the FP32 models,
our method can achieve up to a 18.6x (i.e., 1.70bit) compression ratio with
negligible accuracy degradation. Morever, compared to the state-of-the-art
quantization method, our method can achieve up to 11.4\% and 9.5\% accuracy
improvements on the node-level and graph-level tasks, respectively, and up to
2x speedup on a dedicated hardware accelerator."
"By generating prediction intervals (PIs) to quantify the uncertainty of each
prediction in deep learning regression, the risk of wrong predictions can be
effectively controlled. High-quality PIs need to be as narrow as possible,
whilst covering a preset proportion of real labels. At present, many approaches
to improve the quality of PIs can effectively reduce the width of PIs, but they
do not ensure that enough real labels are captured. Inductive Conformal
Predictor (ICP) is an algorithm that can generate effective PIs which is
theoretically guaranteed to cover a preset proportion of data. However,
typically ICP is not directly optimized to yield minimal PI width. However, in
this study, we use Directly Optimized Inductive Conformal Regression (DOICR)
that takes only the average width of PIs as the loss function and increases the
quality of PIs through an optimized scheme under the validity condition that
sufficient real labels are captured in the PIs. Benchmark experiments show that
DOICR outperforms current state-of-the-art algorithms for regression problems
using underlying Deep Neural Network structures for both tabular and image
data."
"Communication-reduction techniques are a popular way to improve scalability
in data-parallel training of deep neural networks (DNNs). The recent emergence
of large language models such as GPT has created the need for new approaches to
exploit data-parallelism. Among these, fully-sharded data parallel (FSDP)
training is highly popular, yet it still encounters scalability bottlenecks.
One reason is that applying compression techniques to FSDP is challenging: as
the vast majority of the communication involves the model's weights, direct
compression alters convergence and leads to accuracy loss. We present QSDP, a
variant of FSDP which supports both gradient and weight quantization with
theoretical guarantees, is simple to implement and has essentially no
overheads. To derive QSDP we prove that a natural modification of SGD achieves
convergence even when we only maintain quantized weights, and thus the domain
over which we train consists of quantized points and is, therefore, highly
non-convex. We validate this approach by training GPT-family models with up to
1.3 billion parameters on a multi-node cluster. Experiments show that QSDP
preserves model accuracy, while completely removing the communication
bottlenecks of FSDP, providing end-to-end speedups of up to 2.2x."
"Rectified Linear Units (ReLU) are the default choice for activation functions
in deep neural networks. While they demonstrate excellent empirical
performance, ReLU activations can fall victim to the dead neuron problem. In
these cases, the weights feeding into a neuron end up being pushed into a state
where the neuron outputs zero for all inputs. Consequently, the gradient is
also zero for all inputs, which means that the weights which feed into the
neuron cannot update. The neuron is not able to recover from direct back
propagation and model capacity is reduced as those parameters can no longer be
further optimized. Inspired by a neurological process of the same name, we
introduce Synaptic Stripping as a means to combat this dead neuron problem. By
automatically removing problematic connections during training, we can
regenerate dead neurons and significantly improve model capacity and parametric
utilization. Synaptic Stripping is easy to implement and results in sparse
networks that are more efficient than the dense networks they are derived from.
We conduct several ablation studies to investigate these dynamics as a function
of network width and depth and we conduct an exploration of Synaptic Stripping
with Vision Transformers on a variety of benchmark datasets."
"Contemporary graph learning algorithms are not well-defined for large
molecules since they do not consider the hierarchical interactions among the
atoms, which are essential to determine the molecular properties of
macromolecules. In this work, we propose Multiresolution Graph Transformers
(MGT), the first graph transformer architecture that can learn to represent
large molecules at multiple scales. MGT can learn to produce representations
for the atoms and group them into meaningful functional groups or repeating
units. We also introduce Wavelet Positional Encoding (WavePE), a new positional
encoding method that can guarantee localization in both spectral and spatial
domains. Our proposed model achieves competitive results on two macromolecule
datasets consisting of polymers and peptides, and one drug-like molecule
dataset. Importantly, our model outperforms other state-of-the-art methods and
achieves chemical accuracy in estimating molecular properties (e.g., GAP, HOMO
and LUMO) calculated by Density Functional Theory (DFT) in the polymers
dataset. Furthermore, the visualizations, including clustering results on
macromolecules and low-dimensional spaces of their representations, demonstrate
the capability of our methodology in learning to represent long-range and
hierarchical structures. Our PyTorch implementation is publicly available at
https://github.com/HySonLab/Multires-Graph-Transformer"
"Traffic analysis is crucial for urban operations and planning, while the
availability of dense urban traffic data beyond loop detectors is still scarce.
We present a large-scale floating vehicle dataset of per-street segment traffic
information, Metropolitan Segment Traffic Speeds from Massive Floating Car Data
in 10 Cities (MeTS-10), available for 10 global cities with a 15-minute
resolution for collection periods ranging between 108 and 361 days in 2019-2021
and covering more than 1500 square kilometers per metropolitan area. MeTS-10
features traffic speed information at all street levels from main arterials to
local streets for Antwerp, Bangkok, Barcelona, Berlin, Chicago, Istanbul,
London, Madrid, Melbourne and Moscow. The dataset leverages the
industrial-scale floating vehicle Traffic4cast data with speeds and vehicle
counts provided in a privacy-preserving spatio-temporal aggregation. We detail
the efficient matching approach mapping the data to the OpenStreetMap road
graph. We evaluate the dataset by comparing it with publicly available
stationary vehicle detector data (for Berlin, London, and Madrid) and the Uber
traffic speed dataset (for Barcelona, Berlin, and London). The comparison
highlights the differences across datasets in spatio-temporal coverage and
variations in the reported traffic caused by the binning method. MeTS-10
enables novel, city-wide analysis of mobility and traffic patterns for ten
major world cities, overcoming current limitations of spatially sparse vehicle
detector data. The large spatial and temporal coverage offers an opportunity
for joining the MeTS-10 with other datasets, such as traffic surveys in traffic
planning studies or vehicle detector data in traffic control settings."
"Text-based games are a popular testbed for language-based reinforcement
learning (RL). In previous work, deep Q-learning is commonly used as the
learning agent. Q-learning algorithms are challenging to apply to complex
real-world domains due to, for example, their instability in training.
Therefore, in this paper, we adapt the soft-actor-critic (SAC) algorithm to the
text-based environment. To deal with sparse extrinsic rewards from the
environment, we combine it with a potential-based reward shaping technique to
provide more informative (dense) reward signals to the RL agent. We apply our
method to play difficult text-based games. The SAC method achieves higher
scores than the Q-learning methods on many games with only half the number of
training steps. This shows that it is well-suited for text-based games.
Moreover, we show that the reward shaping technique helps the agent to learn
the policy faster and achieve higher scores. In particular, we consider a
dynamically learned value function as a potential function for shaping the
learner's original sparse reward signals."
"Personalized federated learning, as a variant of federated learning, trains
customized models for clients using their heterogeneously distributed data.
However, it is still inconclusive about how to design personalized models with
better representation of shared global knowledge and personalized pattern. To
bridge the gap, we in this paper explore personalized models with low-rank and
sparse decomposition. Specifically, we employ proper regularization to extract
a low-rank global knowledge representation (GKR), so as to distill global
knowledge into a compact representation. Subsequently, we employ a sparse
component over the obtained GKR to fuse the personalized pattern into the
global knowledge. As a solution, we propose a two-stage proximal-based
algorithm named \textbf{Fed}erated learning with mixed \textbf{S}parse and
\textbf{L}ow-\textbf{R}ank representation (FedSLR) to efficiently search for
the mixed models. Theoretically, under proper assumptions, we show that the GKR
trained by FedSLR can at least sub-linearly converge to a stationary point of
the regularized problem, and that the sparse component being fused can converge
to its stationary point under proper settings. Extensive experiments also
demonstrate the superior empirical performance of FedSLR. Moreover, FedSLR
reduces the number of parameters, and lowers the down-link communication
complexity, which are all desirable for federated learning algorithms. Source
code is available in \url{https://github.com/huangtiansheng/fedslr}."
"In recent years, deep neural networks have significantly impacted the seismic
interpretation process. Due to the simple implementation and low interpretation
costs, deep neural networks are an attractive component for the common
interpretation pipeline. However, neural networks are frequently met with
distrust due to their property of producing semantically incorrect outputs when
exposed to sections the model was not trained on. We address this issue by
explaining model behaviour and improving generalization properties through
example forgetting: First, we introduce a method that effectively relates
semantically malfunctioned predictions to their respectful positions within the
neural network representation manifold. More concrete, our method tracks how
models ""forget"" seismic reflections during training and establishes a
connection to the decision boundary proximity of the target class. Second, we
use our analysis technique to identify frequently forgotten regions within the
training volume and augment the training set with state-of-the-art style
transfer techniques from computer vision. We show that our method improves the
segmentation performance on underrepresented classes while significantly
reducing the forgotten regions in the F3 volume in the Netherlands."
"In many high-impact applications, it is important to ensure the quality of
output of a machine learning algorithm as well as its reliability in comparison
with the complexity of the algorithm used. In this paper, we have initiated a
mathematically rigorous theory to decide which models (algorithms applied on
data sets) are close to each other in terms of certain metrics, such as
performance and the complexity level of the algorithm. This involves creating a
grid on the hypothetical spaces of data sets and algorithms so as to identify a
finite set of probability distributions from which the data sets are sampled
and a finite set of algorithms. A given threshold metric acting on this grid
will express the nearness (or statistical distance) from each algorithm and
data set of interest to any given application. A technically difficult part of
this project is to estimate the so-called metric entropy of a compact subset of
functions of \textbf{infinitely many variables} that arise in the definition of
these spaces."
"The problem of model selection with a limited number of experimental trials
has received considerable attention in cognitive science, where the role of
experiments is to discriminate between theories expressed as computational
models. Research on this subject has mostly been restricted to optimal
experiment design with analytically tractable models. However, cognitive models
of increasing complexity, with intractable likelihoods, are becoming more
commonplace. In this paper, we propose BOSMOS: an approach to experimental
design that can select between computational models without tractable
likelihoods. It does so in a data-efficient manner, by sequentially and
adaptively generating informative experiments. In contrast to previous
approaches, we introduce a novel simulator-based utility objective for design
selection, and a new approximation of the model likelihood for model selection.
In simulated experiments, we demonstrate that the proposed BOSMOS technique can
accurately select models in up to 2 orders of magnitude less time than existing
LFI alternatives for three cognitive science tasks: memory retention,
sequential signal detection and risky choice."
"Multi-modal high throughput biological data presents a great scientific
opportunity and a significant computational challenge. In multi-modal
measurements, every sample is observed simultaneously by two or more sets of
sensors. In such settings, many observed variables in both modalities are often
nuisance and do not carry information about the phenomenon of interest. Here,
we propose a multi-modal unsupervised feature selection framework: identifying
informative variables based on coupled high-dimensional measurements. Our
method is designed to identify features associated with two types of latent
low-dimensional structures: (i) shared structures that govern the observations
in both modalities and (ii) differential structures that appear in only one
modality. To that end, we propose two Laplacian-based scoring operators. We
incorporate the scores with differentiable gates that mask nuisance features
and enhance the accuracy of the structure captured by the graph Laplacian. The
performance of the new scheme is illustrated using synthetic and real datasets,
including an extended biological application to single-cell multi-omics."
"Imitation learning uses data for training policies to solve complex tasks.
However, when the training data is collected from human demonstrators, it often
leads to multimodal distributions because of the variability in human actions.
Most imitation learning methods rely on a maximum likelihood (ML) objective to
learn a parameterized policy, but this can result in suboptimal or unsafe
behavior due to the mode-averaging property of the ML objective. In this work,
we propose Information Maximizing Curriculum, a curriculum-based approach that
assigns a weight to each data point and encourages the model to specialize in
the data it can represent, effectively mitigating the mode-averaging problem by
allowing the model to ignore data from modes it cannot represent. To cover all
modes and thus, enable diverse behavior, we extend our approach to a mixture of
experts (MoE) policy, where each mixture component selects its own subset of
the training data for learning. A novel, maximum entropy-based objective is
proposed to achieve full coverage of the dataset, thereby enabling the policy
to encompass all modes within the data distribution. We demonstrate the
effectiveness of our approach on complex simulated control tasks using diverse
human demonstrations, achieving superior performance compared to
state-of-the-art methods."
"In this paper, a modified version of conservative Physics-informed Neural
Networks (cPINN for short) is provided to construct the weak solutions of
Riemann problem for the hyperbolic scalar conservation laws in non-conservative
form. To demonstrate the results, we use the model of generalized
Buckley-Leverett equation (GBL equation for short) with discontinuous porosity
in porous media. By inventing a new unknown, the GBL equation is transformed
into a two-by-two resonant hyperbolic conservation laws in conservative form.
The modified method of cPINN is invented to overcome the difficulties due to
the discontinuity of the porosity and the appearance of the critical states
(near vacuum) in the Riemann data. We experiment with our idea by using a deep
learning algorithm to solve the GBL equation in both conservative and
non-conservative forms, as well as the cases of critical and non-critical
states. This method provides a combination of two different neural networks and
corresponding loss functions, one is for the two-by-two resonant hyperbolic
system, and the other is for the scalar conservation law with a discontinuous
perturbation term in the non-convex flux. The technique of re-scaling to the
unknowns is adopted to avoid the oscillation of the Riemann solutions in the
cases of critical Riemann data. The solutions constructed by the modified cPINN
match the exact solutions constructed by the theoretical analysis for
hyperbolic conservation laws. In addition, the solutions are identical in both
conservative and non-conservative cases. Finally, we compare the performance of
the modified cPINN with numerical method called WENO5. Whereas WENO5 struggles
with the highly oscillation of approximate solutions for the Riemann problems
of GBL equation in non-conservative form, cPINN works admirably."
"This paper studies multiparty learning, aiming to learn a model using the
private data of different participants. Model reuse is a promising solution for
multiparty learning, assuming that a local model has been trained for each
party. Considering the potential sample selection bias among different parties,
some heterogeneous model reuse approaches have been developed. However,
although pre-trained local classifiers are utilized in these approaches, the
characteristics of the local data are not well exploited. This motivates us to
estimate the density of local data and design an auxiliary model together with
the local classifiers for reuse. To address the scenarios where some local
models are not well pre-trained, we further design a multiparty cross-entropy
loss for calibration. Upon existing works, we address a challenging problem of
heterogeneous model reuse from a decision theory perspective and take advantage
of recent advances in density estimation. Experimental results on both
synthetic and benchmark data demonstrate the superiority of the proposed
method."
"The shift between the training and testing distributions is commonly due to
sample selection bias, a type of bias caused by non-random sampling of examples
to be included in the training set. Although there are many approaches proposed
to learn a classifier under sample selection bias, few address the case where a
subset of labels in the training set are missing-not-at-random (MNAR) as a
result of the selection process. In statistics, Greene's method formulates this
type of sample selection with logistic regression as the prediction model.
However, we find that simply integrating this method into a robust
classification framework is not effective for this bias setting. In this paper,
we propose BiasCorr, an algorithm that improves on Greene's method by modifying
the original training set in order for a classifier to learn under MNAR sample
selection bias. We provide theoretical guarantee for the improvement of
BiasCorr over Greene's method by analyzing its bias. Experimental results on
real-world datasets demonstrate that BiasCorr produces robust classifiers and
can be extended to outperform state-of-the-art classifiers that have been
proposed to train under sample selection bias."
"Given samples from two joint distributions, we consider the problem of
Optimal Transportation (OT) between them when conditioned on a common variable.
We focus on the general setting where the conditioned variable may be
continuous, and the marginals of this variable in the two joint distributions
may not be the same. In such settings, standard OT variants cannot be employed,
and novel estimation techniques are necessary. Since the main challenge is that
the conditional distributions are not explicitly available, the key idea in our
OT formulation is to employ kernelized-least-squares terms computed over the
joint samples, which implicitly match the transport plan's marginals with the
empirical conditionals. Under mild conditions, we prove that our estimated
transport plans, as a function of the conditioned variable, are asymptotically
optimal. For finite samples, we show that the deviation in terms of our
regularized objective is bounded by $O(1/m^{1/4})$, where $m$ is the number of
samples. We also discuss how the conditional transport plan could be modelled
using explicit probabilistic models as well as using implicit generative ones.
We empirically verify the consistency of our estimator on synthetic datasets,
where the optimal plan is analytically known. When employed in applications
like prompt learning for few-shot classification and conditional-generation in
the context of predicting cell responses to treatment, our methodology improves
upon state-of-the-art methods."
"Co-pyrolysis of biomass feedstocks with polymeric wastes is a promising
strategy for improving the quantity and quality parameters of the resulting
liquid fuel. Numerous experimental measurements are typically conducted to find
the optimal operating conditions. However, performing co-pyrolysis experiments
is highly challenging due to the need for costly and lengthy procedures.
Machine learning (ML) provides capabilities to cope with such issues by
leveraging on existing data. This work aims to introduce an evolutionary ML
approach to quantify the (by)products of the biomass-polymer co-pyrolysis
process. A comprehensive dataset covering various biomass-polymer mixtures
under a broad range of process conditions is compiled from the qualified
literature. The database was subjected to statistical analysis and mechanistic
discussion. The input features are constructed using an innovative approach to
reflect the physics of the process. The constructed features are subjected to
principal component analysis to reduce their dimensionality. The obtained
scores are introduced into six ML models. Gaussian process regression model
tuned by particle swarm optimization algorithm presents better prediction
performance (R2 > 0.9, MAE < 0.03, and RMSE < 0.06) than other developed
models. The multi-objective particle swarm optimization algorithm successfully
finds optimal independent parameters."
"Electronic health records (EHR) often contain different rates of
representation of certain subpopulations (SP). Factors like patient
demographics, clinical condition prevalence, and medical center type contribute
to this underrepresentation. Consequently, when training machine learning
models on such datasets, the models struggle to generalize well and perform
poorly on underrepresented SPs. To address this issue, we propose a novel
ensemble framework that utilizes generative models. Specifically, we train a
GAN-based synthetic data generator for each SP and incorporate synthetic
samples into each SP training set. Ultimately, we train SP-specific prediction
models. To properly evaluate this method, we design an evaluation pipeline with
2 real-world use case datasets, queried from the MIMIC database. Our approach
shows increased model performance over underrepresented SPs. Our code and
models are given as supplementary and will be made available on a public
repository."
"We consider a collaborative learning setting where the goal of each agent is
to improve their own model by leveraging the expertise of collaborators, in
addition to their own training data. To facilitate the exchange of expertise
among agents, we propose a distillation-based method leveraging shared
unlabeled auxiliary data, which is pseudo-labeled by the collective. Central to
our method is a trust weighting scheme that serves to adaptively weigh the
influence of each collaborator on the pseudo-labels until a consensus on how to
label the auxiliary data is reached. We demonstrate empirically that our
collaboration scheme is able to significantly boost the performance of
individual models in the target domain from which the auxiliary data is
sampled. By design, our method adeptly accommodates heterogeneity in model
architectures and substantially reduces communication overhead compared to
typical collaborative learning methods. At the same time, it can provably
mitigate the negative impact of bad models on the collective."
"Autonomous or self-driving networks are expected to provide a solution to the
myriad of extremely demanding new applications with minimal human supervision.
For this purpose, the community relies on the development of new Machine
Learning (ML) models and techniques. %, like the celebrated Deep Learning (DL).
However, ML can only be as good as the data it is fitted with, and data quality
is an elusive concept difficult to assess. In this paper, we show that
relatively minor modifications on a benchmark dataset (UGR'16, a flow-based
real-traffic dataset for anomaly detection) cause significantly more impact on
model performance than the specific ML technique considered. We also show that
the measured model performance is uncertain, as a result of labelling
inaccuracies. Our findings illustrate that the widely adopted approach of
comparing a set of models in terms of performance results (e.g., in terms of
accuracy or ROC curves) may lead to incorrect conclusions when done without a
proper understanding of dataset biases and sensitivity. We contribute a
methodology to interpret a model response that can be useful for this
understanding."
"We introduce a novel class of sample-based explanations we term
high-dimensional representers, that can be used to explain the predictions of a
regularized high-dimensional model in terms of importance weights for each of
the training samples. Our workhorse is a novel representer theorem for general
regularized high-dimensional models, which decomposes the model prediction in
terms of contributions from each of the training samples: with positive
(negative) values corresponding to positive (negative) impact training samples
to the model's prediction. We derive consequences for the canonical instances
of $\ell_1$ regularized sparse models, and nuclear norm regularized low-rank
models. As a case study, we further investigate the application of low-rank
models in the context of collaborative filtering, where we instantiate
high-dimensional representers for specific popular classes of models. Finally,
we study the empirical performance of our proposed methods on three real-world
binary classification datasets and two recommender system datasets. We also
showcase the utility of high-dimensional representers in explaining model
recommendations."
"Recent advances in large language models have shown that autoregressive
modeling can generate complex and novel sequences that have many real-world
applications. However, these models must generate outputs autoregressively,
which becomes time-consuming when dealing with long sequences. Hierarchical
autoregressive approaches that compress data have been proposed as a solution,
but these methods still generate outputs at the original data frequency,
resulting in slow and memory-intensive models. In this paper, we propose a
model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture.
This model independently encodes input sub-sequences without global context,
processes these sequences using a lower-frequency model, and decodes outputs at
the original data frequency. By interpreting the encoder as an implicitly
defined embedding matrix and using sampled softmax estimation, we develop a
training algorithm that can train the entire model without a high-frequency
decoder, which is the most memory and compute-intensive part of hierarchical
approaches. In a final, brief phase, we train the decoder to generate data at
the original granularity. Our algorithm significantly reduces memory
requirements for training autoregressive models and it also improves the total
training wall-clock time."
"Traditional convolutional neural networks are limited to handling Euclidean
space data, overlooking the vast realm of real-life scenarios represented as
graph data, including transportation networks, social networks, and reference
networks. The pivotal step in transferring convolutional neural networks to
graph data analysis and processing lies in the construction of graph
convolutional operators and graph pooling operators. This comprehensive review
article delves into the world of graph convolutional neural networks. Firstly,
it elaborates on the fundamentals of graph convolutional neural networks.
Subsequently, it elucidates the graph neural network models based on attention
mechanisms and autoencoders, summarizing their application in node
classification, graph classification, and link prediction along with the
associated datasets."
"Heterogeneous graph neural networks (GNNs) have been successful in handling
heterogeneous graphs. In existing heterogeneous GNNs, meta-path plays an
essential role. However, recent work pointed out that simple homogeneous graph
model without meta-path can also achieve comparable results, which calls into
question the necessity of meta-path. In this paper, we first present the
intrinsic difference about meta-path-based and meta-path-free models, i.e., how
to select neighbors for node aggregation. Then, we propose a novel framework to
utilize the rich type semantic information in heterogeneous graphs
comprehensively, namely HAGNN (Hybrid Aggregation for Heterogeneous GNNs). The
core of HAGNN is to leverage the meta-path neighbors and the directly connected
neighbors simultaneously for node aggregations. HAGNN divides the overall
aggregation process into two phases: meta-path-based intra-type aggregation and
meta-path-free inter-type aggregation. During the intra-type aggregation phase,
we propose a new data structure called fused meta-path graph and perform
structural semantic aware aggregation on it. Finally, we combine the embeddings
generated by each phase. Compared with existing heterogeneous GNN models, HAGNN
can take full advantage of the heterogeneity in heterogeneous graphs. Extensive
experimental results on node classification, node clustering, and link
prediction tasks show that HAGNN outperforms the existing modes, demonstrating
the effectiveness of HAGNN."
"We explore the methodology and theory of reward-directed generation via
conditional diffusion models. Directed generation aims to generate samples with
desired properties as measured by a reward function, which has broad
applications in generative AI, reinforcement learning, and computational
biology. We consider the common learning scenario where the data set consists
of unlabeled data along with a smaller set of data with noisy reward labels.
Our approach leverages a learned reward function on the smaller data set as a
pseudolabeler. From a theoretical standpoint, we show that this directed
generator can effectively learn and sample from the reward-conditioned data
distribution. Additionally, our model is capable of recovering the latent
subspace representation of data. Moreover, we establish that the model
generates a new population that moves closer to a user-specified target reward
value, where the optimality gap aligns with the off-policy bandit regret in the
feature subspace. The improvement in rewards obtained is influenced by the
interplay between the strength of the reward signal, the distribution shift,
and the cost of off-support extrapolation. We provide empirical results to
validate our theory and highlight the relationship between the strength of
extrapolation and the quality of generated samples."
"We view variational autoencoders (VAE) as decoder-encoder pairs, which map
distributions in the data space to distributions in the latent space and vice
versa. The standard learning approach for VAEs is the maximisation of the
evidence lower bound (ELBO). It is asymmetric in that it aims at learning a
latent variable model while using the encoder as an auxiliary means only.
Moreover, it requires a closed form a-priori latent distribution. This limits
its applicability in more complex scenarios, such as general semi-supervised
learning and employing complex generative models as priors. We propose a Nash
equilibrium learning approach, which is symmetric with respect to the encoder
and decoder and allows learning VAEs in situations where both the data and the
latent distributions are accessible only by sampling. The flexibility and
simplicity of this approach allows its application to a wide range of learning
scenarios and downstream tasks."
"As machine learning (ML) systems increasingly permeate high-stakes settings
such as healthcare, transportation, military, and national security, concerns
regarding their reliability have emerged. Despite notable progress, the
performance of these systems can significantly diminish due to adversarial
attacks or environmental changes, leading to overconfident predictions,
failures to detect input faults, and an inability to generalize in unexpected
scenarios. This paper proposes a holistic assessment methodology for the
reliability of ML systems. Our framework evaluates five key properties:
in-distribution accuracy, distribution-shift robustness, adversarial
robustness, calibration, and out-of-distribution detection. A reliability score
is also introduced and used to assess the overall system reliability. To
provide insights into the performance of different algorithmic approaches, we
identify and categorize state-of-the-art techniques, then evaluate a selection
on real-world tasks using our proposed reliability metrics and reliability
score. Our analysis of over 500 models reveals that designing for one metric
does not necessarily constrain others but certain algorithmic techniques can
improve reliability across multiple metrics simultaneously. This study
contributes to a more comprehensive understanding of ML reliability and
provides a roadmap for future research and development."
"We present a new category of physics-informed neural networks called physics
informed variational embedding generative adversarial network (PI-VEGAN), that
effectively tackles the forward, inverse, and mixed problems of stochastic
differential equations. In these scenarios, the governing equations are known,
but only a limited number of sensor measurements of the system parameters are
available. We integrate the governing physical laws into PI-VEGAN with
automatic differentiation, while introducing a variational encoder for
approximating the latent variables of the actual distribution of the
measurements. These latent variables are integrated into the generator to
facilitate accurate learning of the characteristics of the stochastic partial
equations. Our model consists of three components, namely the encoder,
generator, and discriminator, each of which is updated alternatively employing
the stochastic gradient descent algorithm. We evaluate the effectiveness of
PI-VEGAN in addressing forward, inverse, and mixed problems that require the
concurrent calculation of system parameters and solutions. Numerical results
demonstrate that the proposed method achieves satisfactory stability and
accuracy in comparison with the previous physics-informed generative
adversarial network (PI-WGAN)."
"Counterfactual Explanations (CEs) are an important tool in Algorithmic
Recourse for addressing two questions: 1. What are the crucial factors that led
to an automated prediction/decision? 2. How can these factors be changed to
achieve a more favorable outcome from a user's perspective? Thus, guiding the
user's interaction with AI systems by proposing easy-to-understand explanations
and easy-to-attain feasible changes is essential for the trustworthy adoption
and long-term acceptance of AI systems. In the literature, various methods have
been proposed to generate CEs, and different quality measures have been
suggested to evaluate these methods. However, the generation of CEs is usually
computationally expensive, and the resulting suggestions are unrealistic and
thus non-actionable. In this paper, we introduce a new method to generate CEs
for a pre-trained binary classifier by first shaping the latent space of an
autoencoder to be a mixture of Gaussian distributions. CEs are then generated
in latent space by linear interpolation between the query sample and the
centroid of the target class. We show that our method maintains the
characteristics of the input sample during the counterfactual search. In
various experiments, we show that the proposed method is competitive based on
different quality measures on image and tabular datasets -- efficiently returns
results that are closer to the original data manifold compared to three
state-of-the-art methods, which are essential for realistic high-dimensional
machine learning applications."
"Graph and hypergraph representation learning has attracted increasing
attention from various research fields. Despite the decent performance and
fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural
Networks (HGNNs), and their well-designed variants, on some commonly used
benchmark graphs and hypergraphs, they are outperformed by even a simple
Multi-Layer Perceptron. This observation motivates a reexamination of the
design paradigm of the current GNNs and HGNNs and poses challenges of
extracting graph features effectively. In this work, a universal feature
encoder for both graph and hypergraph representation learning is designed,
called UniG-Encoder. The architecture starts with a forward transformation of
the topological relationships of connected nodes into edge or hyperedge
features via a normalized projection matrix. The resulting edge/hyperedge
features, together with the original node features, are fed into a neural
network. The encoded node embeddings are then derived from the reversed
transformation, described by the transpose of the projection matrix, of the
network's output, which can be further used for tasks such as node
classification. The proposed architecture, in contrast to the traditional
spectral-based and/or message passing approaches, simultaneously and
comprehensively exploits the node features and graph/hypergraph topologies in
an efficient and unified manner, covering both heterophilic and homophilic
graphs. The designed projection matrix, encoding the graph features, is
intuitive and interpretable. Extensive experiments are conducted and
demonstrate the superior performance of the proposed framework on twelve
representative hypergraph datasets and six real-world graph datasets, compared
to the state-of-the-art methods. Our implementation is available online at
https://github.com/MinhZou/UniG-Encoder."
"We present a novel unified bilevel optimization-based framework,
\textsf{PARL}, formulated to address the recently highlighted critical issue of
policy alignment in reinforcement learning using utility or preference-based
feedback. We identify a major gap within current algorithmic designs for
solving policy alignment due to a lack of precise characterization of the
dependence of the alignment objective on the data generated by policy
trajectories. This shortfall contributes to the sub-optimal performance
observed in contemporary algorithms. Our framework addressed these concerns by
explicitly parameterizing the distribution of the upper alignment objective
(reward design) by the lower optimal variable (optimal policy for the designed
reward). Interestingly, from an optimization perspective, our formulation leads
to a new class of stochastic bilevel problems where the stochasticity at the
upper objective depends upon the lower-level variable. {True to our best
knowledge, this work presents the first formulation of the RLHF as a bilevel
optimization problem which generalizes the existing RLHF formulations and
addresses the existing distribution shift issues in RLHF formulations.} To
demonstrate the efficacy of our formulation in resolving alignment issues in
RL, we devised an algorithm named \textsf{A-PARL} to solve PARL problem,
establishing sample complexity bounds of order $\mathcal{O}(1/T)$. Our
empirical results substantiate that the proposed \textsf{PARL} can address the
alignment concerns in RL by showing significant improvements (up to 63\% in
terms of required samples) for policy alignment in large-scale environments of
the Deepmind control suite and Meta world tasks."
"Non-communicable disease is the leading cause of death, emphasizing the need
for accurate prediction of disease progression and informed clinical
decision-making. Machine learning (ML) models have shown promise in this domain
by capturing non-linear patterns within patient features. However, existing
ML-based models cannot provide causal interpretable predictions and estimate
treatment effects, limiting their decision-making perspective. In this study,
we propose a novel model called causal trajectory prediction (CTP) to tackle
the limitation. The CTP model combines trajectory prediction and causal
discovery to enable accurate prediction of disease progression trajectories and
uncover causal relationships between features. By incorporating a causal graph
into the prediction process, CTP ensures that ancestor features are not
influenced by the treatment of descendant features, thereby enhancing the
interpretability of the model. By estimating the bounds of treatment effects,
even in the presence of unmeasured confounders, the CTP provides valuable
insights for clinical decision-making. We evaluate the performance of the CTP
using simulated and real medical datasets. Experimental results demonstrate
that our model achieves satisfactory performance, highlighting its potential to
assist clinical decisions. Source code is in
\href{https://github.com/DanielSun94/CFPA}{here}."
"This research aims to improve the accuracy of complex volleyball predictions
and provide more meaningful insights to coaches and players. We introduce a
specialized graph encoding technique to add additional contact-by-contact
volleyball context to an already available volleyball dataset without any
additional data gathering. We demonstrate the potential benefits of using graph
neural networks (GNNs) on this enriched dataset for three different volleyball
prediction tasks: rally outcome prediction, set location prediction, and hit
type prediction. We compare the performance of our graph-based models to
baseline models and analyze the results to better understand the underlying
relationships in a volleyball rally. Our results show that the use of GNNs with
our graph encoding yields a much more advanced analysis of the data, which
noticeably improves prediction results overall. We also show that these
baseline tasks can be significantly improved with simple adjustments, such as
removing blocked hits. Lastly, we demonstrate the importance of choosing a
model architecture that will better extract the important information for a
certain task. Overall, our study showcases the potential strengths and
weaknesses of using graph encodings in sports data analytics and hopefully will
inspire future improvements in machine learning strategies across sports and
applications by using graphbased encodings."
"Addressing missing data in complex datasets including electronic health
records (EHR) is critical for ensuring accurate analysis and decision-making in
healthcare. This paper proposes dynamically adaptable structural equation
modeling (SEM) using a self-attention method (SESA), an approach to data
imputation in EHR. SESA innovates beyond traditional SEM-based methods by
incorporating self-attention mechanisms, thereby enhancing model adaptability
and accuracy across diverse EHR datasets. Such enhancement allows SESA to
dynamically adjust and optimize imputation and overcome the limitations of
static SEM frameworks. Our experimental analyses demonstrate the achievement of
robust predictive SESA performance for effectively handling missing data in
EHR. Moreover, the SESA architecture not only rectifies potential
mis-specifications in SEM but also synergizes with causal discovery algorithms
to refine its imputation logic based on underlying data structures. Such
features highlight its capabilities and broadening applicational potential in
EHR data analysis and beyond, marking a reasonable leap forward in the field of
data imputation."
"The safe linear bandit problem is a version of the classical stochastic
linear bandit problem where the learner's actions must satisfy an uncertain
constraint at all rounds. Due its applicability to many real-world settings,
this problem has received considerable attention in recent years. By leveraging
a novel approach that we call directional optimism, we find that it is possible
to achieve improved regret guarantees for both well-separated problem instances
and action sets that are finite star convex sets. Furthermore, we propose a
novel algorithm for this setting that improves on existing algorithms in terms
of empirical performance, while enjoying matching regret guarantees. Lastly, we
introduce a generalization of the safe linear bandit setting where the
constraints are convex and adapt our algorithms and analyses to this setting by
leveraging a novel convex-analysis based approach."
"Deep reinforcement learning agents for continuous control are known to
exhibit significant instability in their performance over time. In this work,
we provide a fresh perspective on these behaviors by studying the return
landscape: the mapping between a policy and a return. We find that popular
algorithms traverse noisy neighborhoods of this landscape, in which a single
update to the policy parameters leads to a wide range of returns. By taking a
distributional view of these returns, we map the landscape, characterizing
failure-prone regions of policy space and revealing a hidden dimension of
policy quality. We show that the landscape exhibits surprising structure by
finding simple paths in parameter space which improve the stability of a
policy. To conclude, we develop a distribution-aware procedure which finds such
paths, navigating away from noisy neighborhoods in order to improve the
robustness of a policy. Taken together, our results provide new insight into
the optimization, evaluation, and design of agents."
"Most knowledge graph completion (KGC) methods learn latent representations of
entities and relations of a given graph by mapping them into a vector space.
Although the majority of these methods focus on static knowledge graphs, a
large number of publicly available KGs contain temporal information stating the
time instant/period over which a certain fact has been true. Such graphs are
often known as temporal knowledge graphs. Furthermore, knowledge graphs may
also contain textual descriptions of entities and relations. Both temporal
information and textual descriptions are not taken into account during
representation learning by static KGC methods, and only structural information
of the graph is leveraged. Recently, some studies have used temporal
information to improve link prediction, yet they do not exploit textual
descriptions and do not support inductive inference (prediction on entities
that have not been seen in training).
  We propose a novel framework called TEMT that exploits the power of
pre-trained language models (PLMs) for text-enhanced temporal knowledge graph
completion. The knowledge stored in the parameters of a PLM allows TEMT to
produce rich semantic representations of facts and to generalize on previously
unseen entities. TEMT leverages textual and temporal information available in a
KG, treats them separately, and fuses them to get plausibility scores of facts.
Unlike previous approaches, TEMT effectively captures dependencies across
different time points and enables predictions on unseen entities. To assess the
performance of TEMT, we carried out several experiments including time interval
prediction, both in transductive and inductive settings, and triple
classification. The experimental results show that TEMT is competitive with the
state-of-the-art."
"Self-supervised learning (SSL) is a popular paradigm for representation
learning. Recent multiview methods can be classified as sample-contrastive,
dimension-contrastive, or asymmetric network-based, with each family having its
own approach to avoiding informational collapse. While these families converge
to solutions of similar quality, it can be empirically shown that some methods
are epoch-inefficient and require longer training to reach a target
performance. Two main approaches to improving efficiency are covariance
eigenvalue regularization and using more views. However, these two approaches
are difficult to combine due to the computational complexity of computing
eigenvalues. We present the objective function FroSSL which reconciles both
approaches while avoiding eigendecomposition entirely. FroSSL works by
minimizing covariance Frobenius norms to avoid collapse and minimizing
mean-squared error to achieve augmentation invariance. We show that FroSSL
reaches competitive accuracies more quickly than any other SSL method and
provide theoretical and empirical support that this faster convergence is due
to how FroSSL affects the eigenvalues of the embedding covariance matrices. We
also show that FroSSL learns competitive representations on linear probe
evaluation when used to train a ResNet-18 on several datasets, including
STL-10, Tiny ImageNet, and ImageNet-100."
"Keystroke dynamics is a behavioural biometric utilised for user
identification and authentication. We propose a new set of features based on
the distance between keys on the keyboard, a concept that has not been
considered before in keystroke dynamics. We combine flight times, a popular
metric, with the distance between keys on the keyboard and call them as
Distance Enhanced Flight Time features (DEFT). This novel approach provides
comprehensive insights into a person's typing behaviour, surpassing typing
velocity alone. We build a DEFT model by combining DEFT features with other
previously used keystroke dynamic features. The DEFT model is designed to be
device-agnostic, allowing us to evaluate its effectiveness across three
commonly used devices: desktop, mobile, and tablet. The DEFT model outperforms
the existing state-of-the-art methods when we evaluate its effectiveness across
two datasets. We obtain accuracy rates exceeding 99% and equal error rates
below 10% on all three devices."
"Sea surface height observations provided by satellite altimetry since 1993
show a rising rate (3.4 mm/year) for global mean sea level. While on average,
sea level has risen 10 cm over the last 30 years, there is considerable
regional variation in the sea level change. Through this work, we predict sea
level trends 30 years into the future at a 2-degree spatial resolution and
investigate the future patterns of the sea level change. We show the potential
of machine learning (ML) in this challenging application of long-term sea level
forecasting over the global ocean. Our approach incorporates sea level data
from both altimeter observations and climate model simulations. We develop a
supervised learning framework using fully connected neural networks (FCNNs)
that can predict the sea level trend based on climate model projections.
Alongside this, our method provides uncertainty estimates associated with the
ML prediction. We also show the effectiveness of partitioning our spatial
dataset and learning a dedicated ML model for each segmented region. We compare
two partitioning strategies: one achieved using domain knowledge, and the other
employing spectral clustering. Our results demonstrate that segmenting the
spatial dataset with spectral clustering improves the ML predictions."
"The inability of DNNs to explain their black-box behavior has led to a recent
surge of explainability methods. However, there are growing concerns that these
explainability methods are not robust and trustworthy. In this work, we perform
the first robustness analysis of Neuron Explanation Methods under a unified
pipeline and show that these explanations can be significantly corrupted by
random noises and well-designed perturbations added to their probing data. We
find that even adding small random noise with a standard deviation of 0.02 can
already change the assigned concepts of up to 28% neurons in the deeper layers.
Furthermore, we devise a novel corruption algorithm and show that our algorithm
can manipulate the explanation of more than 80% neurons by poisoning less than
10% of probing data. This raises the concern of trusting Neuron Explanation
Methods in real-life safety and fairness critical applications."
"Graph Neural Networks (GNNs) are powerful in learning semantics of graph
data. Recently, a new paradigm ""pre-train and prompt"" has shown promising
results in adapting GNNs to various tasks with less supervised data. The
success of such paradigm can be attributed to the more consistent objectives of
pre-training and task-oriented prompt tuning, where the pre-trained knowledge
can be effectively transferred to downstream tasks. Most existing methods are
based on the class prototype vector framework. However, in the few-shot
scenarios, given few labeled data, class prototype vectors are difficult to be
accurately constructed or learned. Meanwhile, the structure information of
graph is usually exploited during pre-training for learning node
representations, while neglected in the prompt tuning stage for learning more
accurate prototype vectors. In addition, they generally ignore the impact of
heterophilous neighborhoods on node representation and are not suitable for
heterophilous graphs. To bridge these gaps, we propose a novel pre-training and
structure prompt tuning framework for GNNs, namely PSP, which consistently
exploits structure information in both pre-training and prompt tuning stages.
In particular, PSP 1) employs a dual-view contrastive learning to align the
latent semantic spaces of node attributes and graph structure, and 2)
incorporates structure information in prompted graph to construct more accurate
prototype vectors and elicit more pre-trained knowledge in prompt tuning. We
conduct extensive experiments on node classification and graph classification
tasks to evaluate the effectiveness of PSP. We show that PSP can lead to
superior performance in few-shot scenarios on both homophilous and
heterophilous graphs. The implemented code is available at
https://github.com/gqq1210/PSP."
"Collaborative Vehicle Routing is where delivery companies cooperate by
sharing their delivery information and performing delivery requests on behalf
of each other. This achieves economies of scale and thus reduces cost,
greenhouse gas emissions, and road congestion. But which company should partner
with whom, and how much should each company be compensated? Traditional game
theoretic solution concepts, such as the Shapley value or nucleolus, are
difficult to calculate for the real-world problem of Collaborative Vehicle
Routing due to the characteristic function scaling exponentially with the
number of agents. This would require solving the Vehicle Routing Problem (an
NP-Hard problem) an exponential number of times. We therefore propose to model
this problem as a coalitional bargaining game where - crucially - agents are
not given access to the characteristic function. Instead, we implicitly reason
about the characteristic function, and thus eliminate the need to evaluate the
VRP an exponential number of times - we only need to evaluate it once. Our
contribution is that our decentralised approach is both scalable and considers
the self-interested nature of companies. The agents learn using a modified
Independent Proximal Policy Optimisation. Our RL agents outperform a strong
heuristic bot. The agents correctly identify the optimal coalitions 79% of the
time with an average optimality gap of 4.2% and reduction in run-time of 62%."
"Empirical risk minimization (ERM) is sensitive to spurious correlations in
the training data, which poses a significant risk when deploying systems
trained under this paradigm in high-stake applications. While the existing
literature focuses on maximizing group-balanced or worst-group accuracy,
estimating these accuracies is hindered by costly bias annotations. This study
contends that current bias-unsupervised approaches to group robustness continue
to rely on group information to achieve optimal performance. Firstly, these
methods implicitly assume that all group combinations are represented during
training. To illustrate this, we introduce a systematic generalization task on
the MPI3D dataset and discover that current algorithms fail to improve the ERM
baseline when combinations of observed attribute values are missing. Secondly,
bias labels are still crucial for effective model selection, restricting the
practicality of these methods in real-world scenarios. To address these
limitations, we propose a revised methodology for training and validating
debiased models in an entirely bias-unsupervised manner. We achieve this by
employing pretrained self-supervised models to reliably extract bias
information, which enables the integration of a logit adjustment training loss
with our validation criterion. Our empirical analysis on synthetic and
real-world tasks provides evidence that our approach overcomes the identified
challenges and consistently enhances robust accuracy, attaining performance
which is competitive with or outperforms that of state-of-the-art methods,
which, conversely, rely on bias labels for validation."
"Learning optimal behavior policy for each agent in multi-agent systems is an
essential yet difficult problem. Despite fruitful progress in multi-agent
reinforcement learning, the challenge of addressing the dynamics of whether two
agents should exhibit consistent behaviors is still under-explored. In this
paper, we propose a new approach that enables agents to learn whether their
behaviors should be consistent with that of other agents by utilizing intrinsic
rewards to learn the optimal policy for each agent. We begin by defining
behavior consistency as the divergence in output actions between two agents
when provided with the same observation. Subsequently, we introduce dynamic
consistency intrinsic reward (DCIR) to stimulate agents to be aware of others'
behaviors and determine whether to be consistent with them. Lastly, we devise a
dynamic scale network (DSN) that provides learnable scale factors for the agent
at every time step to dynamically ascertain whether to award consistent
behavior and the magnitude of rewards. We evaluate DCIR in multiple
environments including Multi-agent Particle, Google Research Football and
StarCraft II Micromanagement, demonstrating its efficacy."
"Despite groundbreaking success in image and text learning, deep learning has
not achieved significant improvements against traditional machine learning (ML)
when it comes to tabular data. This performance gap underscores the need for
data-centric treatment and benchmarking of learning algorithms. Recently,
attention and contrastive learning breakthroughs have shifted computer vision
and natural language processing paradigms. However, the effectiveness of these
advanced deep models on tabular data is sparsely studied using a few data sets
with very large sample sizes, reporting mixed findings after benchmarking
against a limited number of baselines. We argue that the heterogeneity of
tabular data sets and selective baselines in the literature can bias the
benchmarking outcomes. This article extensively evaluates state-of-the-art
attention and contrastive learning methods on a wide selection of 28 tabular
data sets (14 easy and 14 hard-to-classify) against traditional deep and
machine learning. Our data-centric benchmarking demonstrates when traditional
ML is preferred over deep learning and vice versa because no best learning
method exists for all tabular data sets. Combining between-sample and
between-feature attentions conquers the invincible traditional ML on tabular
data sets by a significant margin but fails on high dimensional data, where
contrastive learning takes a robust lead. While a hybrid attention-contrastive
learning strategy mostly wins on hard-to-classify data sets, traditional
methods are frequently superior on easy-to-classify data sets with presumably
simpler decision boundaries. To the best of our knowledge, this is the first
benchmarking paper with statistical analyses of attention and contrastive
learning performances on a diverse selection of tabular data sets against
traditional deep and machine learning baselines to facilitate further advances
in this field."
"Elliptic partial differential equations (PDEs) are a major class of
time-independent PDEs that play a key role in many scientific and engineering
domains such as fluid dynamics, plasma physics, and solid mechanics. Recently,
neural operators have emerged as a promising technique to solve elliptic PDEs
more efficiently by directly mapping the input to solutions. However, existing
networks typically cannot handle complex geometries and inhomogeneous boundary
values present in the real world. Here we introduce Boundary-Embedded Neural
Operators (BENO), a novel neural operator architecture that embeds the complex
geometries and inhomogeneous boundary values into the solving of elliptic PDEs.
Inspired by classical Green's function, BENO consists of two branches of Graph
Neural Networks (GNNs) for interior source term and boundary values,
respectively. Furthermore, a Transformer encoder maps the global boundary
geometry into a latent vector which influences each message passing layer of
the GNNs. We test our model extensively in elliptic PDEs with various boundary
conditions. We show that all existing baseline methods fail to learn the
solution operator. In contrast, our model, endowed with boundary-embedded
architecture, outperforms state-of-the-art neural operators and strong
baselines by an average of 60.96\%. Our source code can be found
https://github.com/AI4Science-WestlakeU/beno.git."
"Long-term time series forecasting (LTSF) represents a critical frontier in
time series analysis, characterized by extensive input sequences, as opposed to
the shorter spans typical of traditional approaches. While longer sequences
inherently offer richer information for enhanced predictive precision,
prevailing studies often respond by escalating model complexity. These
intricate models can inflate into millions of parameters, resulting in
prohibitive parameter scales. Our study demonstrates, through both analytical
and empirical evidence, that decomposition is key to containing excessive model
inflation while achieving uniformly superior and robust results across various
datasets. Remarkably, by tailoring decomposition to the intrinsic dynamics of
time series data, our proposed model outperforms existing benchmarks, using
over 99 \% fewer parameters than the majority of competing methods. Through
this work, we aim to unleash the power of a restricted set of parameters by
capitalizing on domain characteristics--a timely reminder that in the realm of
LTSF, bigger is not invariably better."
"Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)
contain a diverse set of data modalities. While prior works have successfully
leveraged multiple modalities in supervised settings, we apply advanced
self-supervised multi-modal contrastive learning techniques to ICU data,
specifically focusing on clinical notes and time-series for clinically relevant
online prediction tasks. We introduce a loss function Multi-Modal Neighborhood
Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the
excellent linear probe and zero-shot performance of our approach."
"Bone non-union is among the most severe complications associated with trauma
surgery, occurring in 10-30% of cases after long bone fractures. Treating
non-unions requires a high level of surgical expertise and often involves
multiple revision surgeries, sometimes even leading to amputation. Thus, more
accurate prognosis is crucial for patient well-being. Recent advances in
machine learning (ML) hold promise for developing models to predict non-union
healing, even when working with smaller datasets, a commonly encountered
challenge in clinical domains. To demonstrate the effectiveness of ML in
identifying candidates at risk of failed non-union healing, we applied three ML
models (logistic regression, support vector machine, and XGBoost) to the
clinical dataset TRUFFLE, which includes 797 patients with long bone non-union.
The models provided prediction results with 70% sensitivity, and the
specificities of 66% (XGBoost), 49% (support vector machine), and 43% (logistic
regression). These findings offer valuable clinical insights because they
enable early identification of patients at risk of failed non-union healing
after the initial surgical revision treatment protocol."
"Conditional Neural Processes (CNPs) constitute a family of probabilistic
models that harness the flexibility of neural networks to parameterize
stochastic processes. Their capability to furnish well-calibrated predictions,
combined with simple maximum-likelihood training, has established them as
appealing solutions for addressing various learning problems, with a particular
emphasis on meta-learning. A prominent member of this family, Convolutional
Conditional Neural Processes (ConvCNPs), utilizes convolution to explicitly
introduce translation equivariance as an inductive bias. However, ConvCNP's
reliance on local discrete kernels in its convolution layers can pose
challenges in capturing long-range dependencies and complex patterns within the
data, especially when dealing with limited and irregularly sampled observations
from a new task. Building on the successes of Fourier neural operators (FNOs)
for approximating the solution operators of parametric partial differential
equations (PDEs), we propose Spectral Convolutional Conditional Neural
Processes (SConvCNPs), a new addition to the NPs family that allows for more
efficient representation of functions in the frequency domain."
"Federated learning is an emerging machine learning approach that allows the
construction of a model between several participants who hold their own private
data. This method is secure and privacy-preserving, suitable for training a
machine learning model using sensitive data from different sources, such as
hospitals. In this paper, the authors propose two innovative methodologies for
Particle Swarm Optimisation-based federated learning of Fuzzy Cognitive Maps in
a privacy-preserving way. In addition, one relevant contribution this research
includes is the lack of an initial model in the federated learning process,
making it effectively blind. This proposal is tested with several open
datasets, improving both accuracy and precision."
"With the increasing computational costs associated with deep learning,
automated hyperparameter optimization methods, strongly relying on black-box
Bayesian optimization (BO), face limitations. Freeze-thaw BO offers a promising
grey-box alternative, strategically allocating scarce resources incrementally
to different configurations. However, the frequent surrogate model updates
inherent to this approach pose challenges for existing methods, requiring
retraining or fine-tuning their neural network surrogates online, introducing
overhead, instability, and hyper-hyperparameters. In this work, we propose
FT-PFN, a novel surrogate for Freeze-thaw style BO. FT-PFN is a prior-data
fitted network (PFN) that leverages the transformers' in-context learning
ability to efficiently and reliably do Bayesian learning curve extrapolation in
a single forward pass. Our empirical analysis across three benchmark suites
shows that the predictions made by FT-PFN are more accurate and 10-100 times
faster than those of the deep Gaussian process and deep ensemble surrogates
used in previous work. Furthermore, we show that, when combined with our novel
acquisition mechanism (MFPI-random), the resulting in-context freeze-thaw BO
method (ifBO), yields new state-of-the-art performance in the same three
families of deep learning HPO benchmarks considered in prior work."
"Neural networks have been extensively applied to a variety of tasks,
achieving astounding results. Applying neural networks in the scientific field
is an important research direction that is gaining increasing attention. In
scientific applications, the scale of neural networks is generally
moderate-size, mainly to ensure the speed of inference during application.
Additionally, comparing neural networks to traditional algorithms in scientific
applications is inevitable. These applications often require rapid
computations, making the reduction of neural network sizes increasingly
important. Existing work has found that the powerful capabilities of neural
networks are primarily due to their non-linearity. Theoretical work has
discovered that under strong non-linearity, neurons in the same layer tend to
behave similarly, a phenomenon known as condensation. Condensation offers an
opportunity to reduce the scale of neural networks to a smaller subnetwork with
similar performance. In this article, we propose a condensation reduction
algorithm to verify the feasibility of this idea in practical problems. Our
reduction method can currently be applied to both fully connected networks and
convolutional networks, achieving positive results. In complex combustion
acceleration tasks, we reduced the size of the neural network to 41.7% of its
original scale while maintaining prediction accuracy. In the CIFAR10 image
classification task, we reduced the network size to 11.5% of the original
scale, still maintaining a satisfactory validation accuracy. Our method can be
applied to most trained neural networks, reducing computational pressure and
improving inference speed."
"The recent breakthrough in large language models (LLMs) such as ChatGPT has
revolutionized production processes at an unprecedented pace. Alongside this
progress also comes mounting concerns about LLMs' susceptibility to
jailbreaking attacks, which leads to the generation of harmful or unsafe
content. While safety alignment measures have been implemented in LLMs to
mitigate existing jailbreak attempts and force them to become increasingly
complicated, it is still far from perfect. In this paper, we analyze the common
pattern of the current safety alignment and show that it is possible to exploit
such patterns for jailbreaking attacks by simultaneous obfuscation in queries
and responses. Specifically, we propose WordGame attack, which replaces
malicious words with word games to break down the adversarial intent of a query
and encourage benign content regarding the games to precede the anticipated
harmful content in the response, creating a context that is hardly covered by
any corpus used for safety alignment. Extensive experiments demonstrate that
WordGame attack can break the guardrails of the current leading proprietary and
open-source LLMs, including the latest Claude-3, GPT-4, and Llama-3 models.
Further ablation studies on such simultaneous obfuscation in query and response
provide evidence of the merits of the attack strategy beyond an individual
attack."
"Active learning (AL) for multiple target models aims to reduce labeled data
querying while effectively training multiple models concurrently. Existing AL
algorithms often rely on iterative model training, which can be computationally
expensive, particularly for deep models. In this paper, we propose a one-shot
AL method to address this challenge, which performs all label queries without
repeated model training.
  Specifically, we extract different representations of the same dataset using
distinct network backbones, and actively learn the linear prediction layer on
each representation via an $\ell_p$-regression formulation. The regression
problems are solved approximately by sampling and reweighting the unlabeled
instances based on their maximum Lewis weights across the representations. An
upper bound on the number of samples needed is provided with a rigorous
analysis for $p\in [1, +\infty)$.
  Experimental results on 11 benchmarks show that our one-shot approach
achieves competitive performances with the state-of-the-art AL methods for
multiple target models."
"In this paper, we introduce the Dependent Noise-based Inaccurate Label
Distribution Learning (DN-ILDL) framework to tackle the challenges posed by
noise in label distribution learning, which arise from dependencies on
instances and labels. We start by modeling the inaccurate label distribution
matrix as a combination of the true label distribution and a noise matrix
influenced by specific instances and labels. To address this, we develop a
linear mapping from instances to their true label distributions, incorporating
label correlations, and decompose the noise matrix using feature and label
representations, applying group sparsity constraints to accurately capture the
noise. Furthermore, we employ graph regularization to align the topological
structures of the input and output spaces, ensuring accurate reconstruction of
the true label distribution matrix. Utilizing the Alternating Direction Method
of Multipliers (ADMM) for efficient optimization, we validate our method's
capability to recover true labels accurately and establish a generalization
error bound. Extensive experiments demonstrate that DN-ILDL effectively
addresses the ILDL problem and outperforms existing LDL methods."
"Desirable random graph models (RGMs) should (i) generate realistic structures
such as high clustering (i.e., high subgraph densities), (ii) generate variable
(i.e., not overly similar) graphs, and (iii) remain tractable to compute and
control graph statistics. A common class of RGMs (e.g., Erd\H{o}s-R'{e}nyi and
stochastic Kronecker) outputs edge probabilities, and we need to realize (i.e.,
sample from) the edge probabilities to generate graphs. Typically, each edge's
existence is assumed to be determined independently for simplicity and
tractability. However, with edge independency, RGMs theoretically cannot
produce high subgraph densities and high output variability simultaneously. In
this work, we explore realization beyond edge independence that can produce
more realistic structures while maintaining high traceability and variability.
Theoretically, we propose an edge-dependent realization framework called
binding that provably preserves output variability, and derive closed-form
tractability results on subgraph (e.g., triangle) densities in generated
graphs. Practically, we propose algorithms for graph generation with binding
and parameter fitting of binding. Our empirical results demonstrate that
binding exhibits high tractability and generates realistic graphs with high
clustering, significantly improving upon existing RGMs assuming edge
independency."
"The burgeoning volume of graph data presents significant computational
challenges in training graph neural networks (GNNs), critically impeding their
efficiency in various applications. To tackle this challenge, graph
condensation (GC) has emerged as a promising acceleration solution, focusing on
the synthesis of a compact yet representative graph for efficiently training
GNNs while retaining performance. Despite the potential to promote scalable use
of GNNs, existing GC methods are limited to aligning the condensed graph with
merely the observed static graph distribution. This limitation significantly
restricts the generalization capacity of condensed graphs, particularly in
adapting to dynamic distribution changes. In real-world scenarios, however,
graphs are dynamic and constantly evolving, with new nodes and edges being
continually integrated. Consequently, due to the limited generalization
capacity of condensed graphs, applications that employ GC for efficient GNN
training end up with sub-optimal GNNs when confronted with evolving graph
structures and distributions in dynamic real-world situations. To overcome this
issue, we propose open-world graph condensation (OpenGC), a robust GC framework
that integrates structure-aware distribution shift to simulate evolving graph
patterns and exploit the temporal environments for invariance condensation.
This approach is designed to extract temporal invariant patterns from the
original graph, thereby enhancing the generalization capabilities of the
condensed graph and, subsequently, the GNNs trained on it. Extensive
experiments on both real-world and synthetic evolving graphs demonstrate that
OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic
changes in open-world graph environments."
"The purpose of offline multi-task reinforcement learning (MTRL) is to develop
a unified policy applicable to diverse tasks without the need for online
environmental interaction. Recent advancements approach this through sequence
modeling, leveraging the Transformer architecture's scalability and the
benefits of parameter sharing to exploit task similarities. However, variations
in task content and complexity pose significant challenges in policy
formulation, necessitating judicious parameter sharing and management of
conflicting gradients for optimal policy performance. In this work, we
introduce the Harmony Multi-Task Decision Transformer (HarmoDT), a novel
solution designed to identify an optimal harmony subspace of parameters for
each task. We approach this as a bi-level optimization problem, employing a
meta-learning framework that leverages gradient-based techniques. The upper
level of this framework is dedicated to learning a task-specific mask that
delineates the harmony subspace, while the inner level focuses on updating
parameters to enhance the overall performance of the unified policy. Empirical
evaluations on a series of benchmarks demonstrate the superiority of HarmoDT,
verifying the effectiveness of our approach."
"Graph Attention Networks (GATs) are designed to provide flexible neighborhood
aggregation that assigns weights to neighbors according to their importance. In
practice, however, GATs are often unable to switch off task-irrelevant
neighborhood aggregation, as we show experimentally and analytically. To
address this challenge, we propose GATE, a GAT extension that holds three major
advantages: i) It alleviates over-smoothing by addressing its root cause of
unnecessary neighborhood aggregation. ii) Similarly to perceptrons, it benefits
from higher depth as it can still utilize additional layers for (non-)linear
feature transformations in case of (nearly) switched-off neighborhood
aggregation. iii) By down-weighting connections to unrelated neighbors, it
often outperforms GATs on real-world heterophilic datasets. To further validate
our claims, we construct a synthetic test bed to analyze a model's ability to
utilize the appropriate amount of neighborhood aggregation, which could be of
independent interest."
"Deep Neural Networks (DNNs) are extensively employed in safety-critical
applications where ensuring hardware reliability is a primary concern. To
enhance the reliability of DNNs against hardware faults, activation restriction
techniques significantly mitigate the fault effects at the DNN structure level,
irrespective of accelerator architectures. State-of-the-art methods offer
either neuron-wise or layer-wise clipping activation functions. They attempt to
determine optimal clipping thresholds using heuristic and learning-based
approaches. Layer-wise clipped activation functions cannot preserve DNNs
resilience at high bit error rates. On the other hand, neuron-wise clipping
activation functions introduce considerable memory overhead due to the addition
of parameters, which increases their vulnerability to faults. Moreover, the
heuristic-based optimization approach demands numerous fault injections during
the search process, resulting in time-consuming threshold identification. On
the other hand, learning-based techniques that train thresholds for entire
layers concurrently often yield sub-optimal results. In this work, first, we
demonstrate that it is not essential to incorporate neuron-wise activation
functions throughout all layers in DNNs. Then, we propose a hybrid clipped
activation function that integrates neuron-wise and layer-wise methods that
apply neuron-wise clipping only in the last layer of DNNs. Additionally, to
attain optimal thresholds in the clipping activation function, we introduce
ProAct, a progressive training methodology. This approach iteratively trains
the thresholds on a layer-by-layer basis, aiming to obtain optimal threshold
values in each layer separately."
"The rapid advancement of Artificial Intelligence (AI) has introduced Deep
Neural Network (DNN)-based tasks to the ecosystem of vehicular networks. These
tasks are often computation-intensive, requiring substantial computation
resources, which are beyond the capability of a single vehicle. To address this
challenge, Vehicular Edge Computing (VEC) has emerged as a solution, offering
computing services for DNN-based tasks through resource pooling via
Vehicle-to-Vehicle/Infrastructure (V2V/V2I) communications. In this paper, we
formulate the problem of joint DNN partitioning, task offloading, and resource
allocation in VEC as a dynamic long-term optimization. Our objective is to
minimize the DNN-based task completion time while guaranteeing the system
stability over time. To this end, we first leverage a Lyapunov optimization
technique to decouple the original long-term optimization with stability
constraints into a per-slot deterministic problem. Afterwards, we propose a
Multi-Agent Diffusion-based Deep Reinforcement Learning (MAD2RL) algorithm,
incorporating the innovative use of diffusion models to determine the optimal
DNN partitioning and task offloading decisions. Furthermore, we integrate
convex optimization techniques into MAD2RL as a subroutine to allocate
computation resources, enhancing the learning efficiency. Through simulations
under real-world movement traces of vehicles, we demonstrate the superior
performance of our proposed algorithm compared to existing benchmark solutions."
"Tabular data is ubiquitous in many real-life systems. In particular,
time-dependent tabular data, where rows are chronologically related, is
typically used for recording historical events, e.g., financial transactions,
healthcare records, or stock history. Recently, hierarchical variants of the
attention mechanism of transformer architectures have been used to model
tabular time-series data. At first, rows (or columns) are encoded separately by
computing attention between their fields. Subsequently, encoded rows (or
columns) are attended to one another to model the entire tabular time-series.
While efficient, this approach constrains the attention granularity and limits
its ability to learn patterns at the field-level across separate rows, or
columns. We take a first step to address this gap by proposing Fieldy, a
fine-grained hierarchical model that contextualizes fields at both the row and
column levels. We compare our proposal against state of the art models on
regression and classification tasks using public tabular time-series datasets.
Our results show that combining row-wise and column-wise attention improves
performance without increasing model size. Code and data are available at
https://github.com/raphaaal/fieldy."
"We present ""DistML.js"", a library designed for training and inference of
machine learning models within web browsers. Not only does DistML.js facilitate
model training on local devices, but it also supports distributed learning
through communication with servers. Its design and define-by-run API for deep
learning model construction resemble PyTorch, thereby reducing the learning
curve for prototyping. Matrix computations involved in model training and
inference are executed on the backend utilizing WebGL, enabling high-speed
calculations. We provide a comprehensive explanation of DistML.js's design,
API, and implementation, alongside practical applications including data
parallelism in learning. The source code is publicly available at
https://github.com/mil-tokyo/distmljs."
"In real-world applications, input data distributions are rarely static over a
period of time, a phenomenon known as concept drift. Such concept drifts
degrade the model's prediction performance, and therefore we require methods to
overcome these issues. The initial step is to identify concept drifts and have
a training method in place to recover the model's performance. Most concept
drift detection methods work on detecting concept drifts and signalling the
requirement to retrain the model. However, in real-world cases, there could be
concept drifts that recur over a period of time. In this paper, we present an
unsupervised method based on Generative Adversarial Networks(GAN) to detect
concept drifts and identify whether a specific concept drift occurred in the
past. Our method reduces the time and data the model requires to get up to
speed for recurring drifts. Our key results indicate that our proposed model
can outperform the current state-of-the-art models in most datasets. We also
test our method on a real-world use case from astrophysics, where we detect the
bow shock and magnetopause crossings with better results than the existing
methods in the domain."
"In today's fast-paced world, accurately monitoring stress levels is crucial.
Sensor-based stress monitoring systems often need large datasets for training
effective models. However, individual-specific models are necessary for
personalized and interactive scenarios. Traditional methods like Ecological
Momentary Assessments (EMAs) assess stress but struggle with efficient data
collection without burdening users. The challenge is to timely send EMAs,
especially during stress, balancing monitoring efficiency and user convenience.
This paper introduces a novel context-aware active reinforcement learning (RL)
algorithm for enhanced stress detection using Photoplethysmography (PPG) data
from smartwatches and contextual data from smartphones. Our approach
dynamically selects optimal times for deploying EMAs, utilizing the user's
immediate context to maximize label accuracy and minimize intrusiveness.
Initially, the study was executed in an offline environment to refine the label
collection process, aiming to increase accuracy while reducing user burden.
Later, we integrated a real-time label collection mechanism, transitioning to
an online methodology. This shift resulted in an 11% improvement in stress
detection efficiency. Incorporating contextual data improved model accuracy by
4%. Personalization studies indicated a 10% enhancement in AUC-ROC scores,
demonstrating better stress level differentiation. This research marks a
significant move towards personalized, context-driven real-time stress
monitoring methods."
"As language models become more general purpose, increased attention needs to
be paid to detecting out-of-distribution (OOD) instances, i.e., those not
belonging to any of the distributions seen during training. Existing methods
for detecting OOD data are computationally complex and storage-intensive. We
propose a novel soft clustering approach for OOD detection based on
non-negative kernel regression. Our approach greatly reduces computational and
space complexities (up to 11x improvement in inference time and 87% reduction
in storage requirements) and outperforms existing approaches by up to 4 AUROC
points on four different benchmarks. We also introduce an entropy-constrained
version of our algorithm, which leads to further reductions in storage
requirements (up to 97% lower than comparable approaches) while retaining
competitive performance. Our soft clustering approach for OOD detection
highlights its potential for detecting tail-end phenomena in extreme-scale data
settings."
"We propose a foundation model for soccer, which is able to predict subsequent
actions in a soccer match from a given input sequence of actions. As a proof of
concept, we train a transformer architecture on three seasons of data from a
professional soccer league. We quantitatively and qualitatively compare the
performance of this transformer architecture to two baseline models: a Markov
model and a multi-layer perceptron. Additionally, we discuss potential
applications of our model. We provide an open-source implementation of our
methods at https://github.com/danielhocevar/Foundation-Model-for-Soccer."
"The Maximal Update Parametrization ($\mu$P) aims to make the optimal
hyperparameters (HPs) of a model independent of its size, allowing them to be
swept using a cheap proxy model rather than the full-size target model. We
present a new scheme, u-$\mu$P, which improves upon $\mu$P by combining it with
Unit Scaling, a method for designing models that makes them easy to train in
low-precision. The two techniques have a natural affinity: $\mu$P ensures that
the scale of activations is independent of model size, and Unit Scaling ensures
that activations, weights and gradients begin training with a scale of one.
This synthesis opens the door to a simpler scheme, whose default values are
near-optimal. This in turn facilitates a more efficient sweeping strategy, with
u-$\mu$P models reaching a loss that is equal to or lower than comparable
$\mu$P models and working out-of-the-box in FP8."
"Machine unlearning, an emerging research topic focusing on compliance with
data privacy regulations, enables trained models to remove the information
learned from specific data. While many existing methods indirectly address this
issue by intentionally injecting incorrect supervisions, they can drastically
and unpredictably alter the decision boundaries and feature spaces, leading to
training instability and undesired side effects. To fundamentally approach this
task, we first analyze the changes in latent feature spaces between original
and retrained models, and observe that the feature representations of samples
not involved in training are closely aligned with the feature manifolds of
previously seen samples in training. Based on these findings, we introduce a
novel evaluation metric for machine unlearning, coined dimensional alignment,
which measures the alignment between the eigenspaces of the forget and retain
set samples. We employ this metric as a regularizer loss to build a robust and
stable unlearning framework, which is further enhanced by integrating a
self-distillation loss and an alternating training scheme. Our framework
effectively eliminates information from the forget set and preserves knowledge
from the retain set. Lastly, we identify critical flaws in established
evaluation metrics for machine unlearning, and introduce new evaluation tools
that more accurately reflect the fundamental goals of machine unlearning."
"The integration of Artificial Intelligence (AI) into education has
transformative potential, providing tailored learning experiences and creative
instructional approaches. However, the inherent biases in AI algorithms hinder
this improvement by unintentionally perpetuating prejudice against specific
demographics, especially in human-centered applications like education. This
survey delves deeply into the developing topic of algorithmic fairness in
educational contexts, providing a comprehensive evaluation of the diverse
literature on fairness, bias, and ethics in AI-driven educational applications.
It identifies the common forms of biases, such as data-related, algorithmic,
and user-interaction, that fundamentally undermine the accomplishment of
fairness in AI teaching aids. By outlining existing techniques for mitigating
these biases, ranging from varied data gathering to algorithmic fairness
interventions, the survey emphasizes the critical role of ethical
considerations and legal frameworks in shaping a more equitable educational
environment. Furthermore, it guides readers through the complexities of
fairness measurements, methods, and datasets, shedding light on the way to bias
reduction. Despite these gains, this survey highlights long-standing issues,
such as achieving a balance between fairness and accuracy, as well as the need
for diverse datasets. Overcoming these challenges and ensuring the ethical and
fair use of AI's promise in education call for a collaborative,
interdisciplinary approach."
"The pipeline of a fair ML practitioner is generally divided into three
phases: 1) Selecting a fairness measure. 2) Choosing a model that minimizes
this measure. 3) Maximizing the model's performance on the data. In the context
of group fairness, this approach often obscures implicit assumptions about how
bias is introduced into the data. For instance, in binary classification, it is
often assumed that the best model, with equal fairness, is the one with better
performance. However, this belief already imposes specific properties on the
process that introduced bias. More precisely, we are already assuming that the
biasing process is a monotonic function of the fair scores, dependent solely on
the sensitive attribute. We formally prove this claim regarding several
implicit fairness assumptions. This leads, in our view, to two possible
conclusions: either the behavior of the biasing process is more complex than
mere monotonicity, which means we need to identify and reject our implicit
assumptions in order to develop models capable of tackling more complex
situations; or the bias introduced in the data behaves predictably, implying
that many of the developed models are superfluous."
"Generalized eigenvalue proximal support vector machine (GEPSVM) has attracted
widespread attention due to its simple architecture, rapid execution, and
commendable performance. GEPSVM gives equal significance to all samples,
thereby diminishing its robustness and efficacy when confronted with real-world
datasets containing noise and outliers. In order to reduce the impact of noises
and outliers, we propose a novel intuitionistic fuzzy generalized eigenvalue
proximal support vector machine (IF-GEPSVM). The proposed IF-GEPSVM assigns the
intuitionistic fuzzy score to each training sample based on its location and
surroundings in the high-dimensional feature space by using a kernel function.
The solution of the IF-GEPSVM optimization problem is obtained by solving a
generalized eigenvalue problem. Further, we propose an intuitionistic fuzzy
improved GEPSVM (IF-IGEPSVM) by solving the standard eigenvalue decomposition
resulting in simpler optimization problems with less computation cost which
leads to an efficient intuitionistic fuzzy-based model. We conduct a
comprehensive evaluation of the proposed IF-GEPSVM and IF-IGEPSVM models on UCI
and KEEL datasets. Moreover, to evaluate the robustness of the proposed
IF-GEPSVM and IF-IGEPSVM models, label noise is introduced into some UCI and
KEEL datasets. The experimental findings showcase the superior generalization
performance of the proposed models when compared to the existing baseline
models, both with and without label noise. Our experimental results, supported
by rigorous statistical analyses, confirm the superior generalization abilities
of the proposed IF-GEPSVM and IF-IGEPSVM models over the baseline models.
Furthermore, we implement the proposed IF-GEPSVM and IF-IGEPSVM models on the
USPS recognition dataset, yielding promising results that underscore the
models' effectiveness in practical and real-world applications."
"Detecting flying animals (e.g., birds, bats, and insects) using weather radar
helps gain insights into animal movement and migration patterns, aids in
management efforts (such as biosecurity) and enhances our understanding of the
ecosystem.The conventional approach to detecting animals in weather radar
involves thresholding: defining and applying thresholds for the radar
variables, based on expert opinion. More recently, Deep Learning approaches
have been shown to provide improved performance in detection. However,
obtaining sufficient labelled weather radar data for flying animals to build
learning-based models is time-consuming and labor-intensive. To address the
challenge of data labelling, we propose a self-supervised learning method for
detecting animal movement. In our proposed method, we pre-train our model on a
large dataset with noisy labels produced by a threshold approach. The key
advantage is that the pre-trained dataset size is limited only by the number of
radar images available. We then fine-tune the model on a small human-labelled
dataset. Our experiments on Australian weather radar data for waterbird
segmentation show that the proposed method outperforms the current state-of-the
art approach by 43.53% in the dice co-efficient statistic."
"Graph Neural Networks (GNNs) have emerged as powerful tools for analyzing and
learning representations from graph-structured data. A crucial prerequisite for
the outstanding performance of GNNs is the availability of complete graph
information, i.e., node features and graph structure, which is frequently unmet
in real-world scenarios since graphs are often incomplete due to various
uncontrollable factors. Existing approaches only focus on dealing with either
incomplete features or incomplete structure, which leads to performance loss
inevitably. To address this issue, this study proposes a mutual dual-stream
graph neural network (MDS-GNN), which implements a mutual benefit learning
between features and structure. Its main ideas are as follows: a)
reconstructing the missing node features based on the initial incomplete graph
structure; b) generating an augmented global graph based on the reconstructed
node features, and propagating the incomplete node features on this global
graph; and c) utilizing contrastive learning to make the dual-stream process
mutually benefit from each other. Extensive experiments on six real-world
datasets demonstrate the effectiveness of our proposed MDS-GNN on incomplete
graphs."
"Existing local Explainable AI (XAI) methods, such as LIME, select a region of
the input space in the vicinity of a given input instance, for which they
approximate the behaviour of a model using a simpler and more interpretable
surrogate model. The size of this region is often controlled by a user-defined
locality hyperparameter. In this paper, we demonstrate the difficulties
associated with defining a suitable locality size to capture impactful model
behaviour, as well as the inadequacy of using a single locality size to explain
all predictions. We propose a novel method, MASALA, for generating
explanations, which automatically determines the appropriate local region of
impactful model behaviour for each individual instance being explained. MASALA
approximates the local behaviour used by a complex model to make a prediction
by fitting a linear surrogate model to a set of points which experience similar
model behaviour. These points are found by clustering the input space into
regions of linear behavioural trends exhibited by the model. We compare the
fidelity and consistency of explanations generated by our method with existing
local XAI methods, namely LIME and CHILLI. Experiments on the PHM08 and MIDAS
datasets show that our method produces more faithful and consistent
explanations than existing methods, without the need to define any sensitive
locality hyperparameters."
"Detecting out-of-distribution (OOD) samples is a critical task for reliable
machine learning. However, it becomes particularly challenging when the models
are trained on long-tailed datasets, as the models often struggle to
distinguish tail-class in-distribution samples from OOD samples. We examine the
main challenges in this problem by identifying the trade-offs between OOD
detection and in-distribution (ID) classification, faced by existing methods.
We then introduce our method, called \textit{Representation Norm Amplification}
(RNA), which solves this challenge by decoupling the two problems. The main
idea is to use the norm of the representation as a new dimension for OOD
detection, and to develop a training method that generates a noticeable
discrepancy in the representation norm between ID and OOD data, while not
perturbing the feature learning for ID classification. Our experiments show
that RNA achieves superior performance in both OOD detection and classification
compared to the state-of-the-art methods, by 1.70\% and 9.46\% in FPR95 and
2.43\% and 6.87\% in classification accuracy on CIFAR10-LT and ImageNet-LT,
respectively. The code for this work is available at
https://github.com/dgshin21/RNA."
"Continual learning and machine unlearning are crucial challenges in machine
learning, typically addressed separately. Continual learning focuses on
adapting to new knowledge while preserving past information, whereas unlearning
involves selectively forgetting specific subsets of data. In this paper, we
introduce a new framework that jointly tackles both tasks by leveraging
controlled knowledge distillation. Our approach enables efficient learning with
minimal forgetting and effective targeted unlearning. By incorporating a fixed
memory buffer, the system supports learning new concepts while retaining prior
knowledge. The distillation process is carefully managed to ensure a balance
between acquiring new information and forgetting specific data as needed.
Experimental results on benchmark datasets show that our method matches or
exceeds the performance of existing approaches in both continual learning and
machine unlearning. This unified framework is the first to address both
challenges simultaneously, paving the way for adaptable models capable of
dynamic learning and forgetting while maintaining strong overall performance.
Source code: \textcolor{blue}{https://respailab.github.io/CLMUL}"
"Medication nonadherence significantly reduces the effectiveness of therapies,
yet it remains prevalent among patients. Nonadherence has been linked to
adverse outcomes, including increased risks of mortality and hospitalization.
Although various methods exist to help patients track medication schedules,
such as the Intelligent Drug Administration System (IDAS) and Smart Blister,
these tools often face challenges that hinder their commercial viability.
Building on the principles of dosage measurement and information communication
in IoT, we introduce the Smart Pill Case a smart health adherence tool that
leverages RFID-based data recording and NFC-based data extraction. This system
incorporates a load cell for precise dosage measurement and features an Android
app to monitor medication intake, offer suggestions, and issue warnings. To
enhance the effectiveness and personalization of the Smart Pill Case, we
propose integrating federated learning into the system. Federated learning
allows the Smart Pill Case to learn from medication adherence patterns across
multiple users without compromising individual privacy. By training machine
learning models on decentralized data collected from various Smart Pill Cases,
the system can continuously improve its recommendations and warnings, adapting
to the diverse needs and behaviors of users. This approach not only enhances
the tools ability to support medication adherence but also ensures that
sensitive user data remains secure and private."
"Traditionally, reward models used for reinforcement learning from human
feedback (RLHF) are trained to directly predict preference scores without
leveraging the generation capabilities of the underlying large language model
(LLM). This limits the capabilities of reward models as they must reason
implicitly about the quality of a response, i.e., preference modeling must be
performed in a single forward pass through the model. To enable reward models
to reason explicitly about the quality of a response, we introduce
Critique-out-Loud (CLoud) reward models. CLoud reward models operate by first
generating a natural language critique of the assistant's response that is then
used to predict a scalar reward for the quality of the response. We demonstrate
the success of CLoud reward models for both Llama-3-8B and 70B base models:
compared to classic reward models CLoud reward models improve pairwise
preference classification accuracy on RewardBench by 4.65 and 5.84 percentage
points for the 8B and 70B base models respectively. Furthermore, CLoud reward
models lead to a Pareto improvement for win rate on ArenaHard when used as the
scoring model for Best-of-N. Finally, we explore how to exploit the dynamic
inference compute capabilities of CLoud reward models by performing
self-consistency decoding for reward prediction."
"\texttt{ml\_edm} is a Python 3 library, designed for early decision making of
any learning tasks involving temporal/sequential data. The package is also
modular, providing researchers an easy way to implement their own triggering
strategy for classification, regression or any machine learning task. As of
now, many Early Classification of Time Series (ECTS) state-of-the-art
algorithms, are efficiently implemented in the library leveraging parallel
computation. The syntax follows the one introduce in \texttt{scikit-learn},
making estimators and pipelines compatible with \texttt{ml\_edm}. This software
is distributed over the BSD-3-Clause license, source code can be found at
\url{https://github.com/ML-EDM/ml_edm}."
"Causal machine learning (ML) promises to provide powerful tools for
estimating individual treatment effects. Although causal ML methods are now
well established, they still face the significant challenge of
interpretability, which is crucial for medical applications. In this work, we
propose a new algorithm based on the Conditional Permutation Importance (CPI)
method for statistically rigorous variable importance assessment in the context
of Conditional Average Treatment Effect (CATE) estimation. Our method termed
PermuCATE is agnostic to both the meta-learner and the ML model used. Through
theoretical analysis and empirical studies, we show that this approach provides
a reliable measure of variable importance and exhibits lower variance compared
to the standard Leave-One-Covariate-Out (LOCO) method. We illustrate how this
property leads to increased statistical power, which is crucial for the
application of explainable ML in small sample sizes or high-dimensional
settings. We empirically demonstrate the benefits of our approach in various
simulation scenarios, including previously proposed benchmarks as well as more
complex settings with high-dimensional and correlated variables that require
advanced CATE estimators."
"In this survey, we provide an overview of category theory-derived machine
learning from four mainstream perspectives: gradient-based learning,
probability-based learning, invariance and equivalence-based learning, and
topos-based learning. For the first three topics, we primarily review research
in the past five years, updating and expanding on the previous survey by
Shiebler et al.. The fourth topic, which delves into higher category theory,
particularly topos theory, is surveyed for the first time in this paper. In
certain machine learning methods, the compositionality of functors plays a
vital role, prompting the development of specific categorical frameworks.
However, when considering how the global properties of a network reflect in
local structures and how geometric properties are expressed with logic, the
topos structure becomes particularly significant and profound."
"Learning from label proportions (LLP) is a kind of weakly supervised learning
that trains an instance-level classifier from label proportions of bags, which
consist of sets of instances without using instance labels. A challenge in LLP
arises when the number of instances in a bag (bag size) is numerous, making the
traditional LLP methods difficult due to GPU memory limitations. This study
aims to develop an LLP method capable of learning from bags with large sizes.
In our method, smaller bags (mini-bags) are generated by sampling instances
from large-sized bags (original bags), and these mini-bags are used in place of
the original bags. However, the proportion of a mini-bag is unknown and differs
from that of the original bag, leading to overfitting. To address this issue,
we propose a perturbation method for the proportion labels of sampled mini-bags
to mitigate overfitting to noisy label proportions. This perturbation is added
based on the multivariate hypergeometric distribution, which is statistically
modeled. Additionally, loss weighting is implemented to reduce the negative
impact of proportions sampled from the tail of the distribution. Experimental
results demonstrate that the proportion label perturbation and loss weighting
achieve classification accuracy comparable to that obtained without sampling.
Our codes are available at https://github.com/stainlessnight/LLP-LargeBags."
"Erasure-coded computing has been successfully used in cloud systems to reduce
tail latency caused by factors such as straggling servers and heterogeneous
traffic variations. A majority of cloud computing traffic now consists of
inference on neural networks on shared resources where the response time of
inference queries is also adversely affected by the same factors. However,
current erasure coding techniques are largely focused on linear computations
such as matrix-vector and matrix-matrix multiplications and hence do not work
for the highly non-linear neural network functions. In this paper, we seek to
design a method to code over neural networks, that is, given two or more neural
network models, how to construct a coded model whose output is a linear
combination of the outputs of the given neural networks. We formulate the
problem as a KL barycenter problem and propose a practical algorithm COIN that
leverages the diagonal Fisher information to create a coded model that
approximately outputs the desired linear combination of outputs. We conduct
experiments to perform erasure coding over neural networks trained on
real-world vision datasets and show that the accuracy of the decoded outputs
using COIN is significantly higher than other baselines while being extremely
compute-efficient."
"This is the 1st part of the dissertation for my master degree and compares
the power consumption using the default floating point (32bit) and Nvidia mixed
precision (16bit and 32bit) while training a classification ML model. A custom
PC with specific hardware was built to perform the experiments, and different
ML hyper-parameters, such as batch size, neurons, and epochs, were chosen to
build Deep Neural Networks (DNN). Additionally, various software was used
during the experiments to collect the power consumption data in Watts from the
Graphics Processing Unit (GPU), Central Processing Unit (CPU), Random Access
Memory (RAM) and manually from a wattmeter connected to the wall. A
benchmarking test with default hyper parameter values for the DNN was used as a
reference, while the experiments used a combination of different settings. The
results were recorded in Excel, and descriptive statistics were chosen to
calculate the mean between the groups and compare them using graphs and tables.
The outcome was positive when using mixed precision combined with specific
hyper-parameters. Compared to the benchmarking, the optimisation for the
classification reduced the power consumption between 7 and 11 Watts. Similarly,
the carbon footprint is reduced because the calculation uses the same power
consumption data. Still, a consideration is required when configuring
hyper-parameters because it can negatively affect hardware performance.
However, this research required inferential statistics, specifically ANOVA and
T-test, to compare the relationship between the means. Furthermore, tests
indicated no statistical significance of the relationship between the
benchmarking and experiments. However, a more extensive implementation with a
cluster of GPUs can increase the sample size significantly, as it is an
essential factor and can change the outcome of the statistical analysis."
"Accurate power load forecasting is crucial for improving energy efficiency
and ensuring power supply quality. Considering the power load forecasting
problem involves not only dynamic factors like historical load variations but
also static factors such as climate conditions that remain constant over
specific periods. From the model-agnostic perspective, this paper proposes a
parallel structure network to extract important information from both dynamic
and static data. Firstly, based on complexity learning theory, it is
demonstrated that models integrated through parallel structures exhibit
superior generalization abilities compared to individual base learners.
Additionally, the higher the independence between base learners, the stronger
the generalization ability of the parallel structure model. This suggests that
the structure of machine learning models inherently contains significant
information. Building on this theoretical foundation, a parallel convolutional
neural network (CNN)-gate recurrent unit (GRU) attention model (PCGA) is
employed to address the power load forecasting issue, aiming to effectively
integrate the influences of dynamic and static features. The CNN module is
responsible for capturing spatial characteristics from static data, while the
GRU module captures long-term dependencies in dynamic time series data. The
attention layer is designed to focus on key information from the
spatial-temporal features extracted by the parallel CNN-GRU. To substantiate
the advantages of the parallel structure model in extracting and integrating
multi-source information, a series of experiments are conducted."
"Data-driven simulation of physical systems has recently kindled significant
attention, where many neural models have been developed. In particular,
mesh-based graph neural networks (GNNs) have demonstrated significant potential
in predicting spatiotemporal dynamics across arbitrary geometric domains.
However, the existing node-edge message passing mechanism in GNNs limits the
model's representation learning ability. In this paper, we proposed a
cell-embedded GNN model (aka CeGNN) to learn spatiotemporal dynamics with
lifted performance. Specifically, we introduce a learnable cell attribution to
the node-edge message passing process, which better captures the spatial
dependency of regional features. Such a strategy essentially upgrades the local
aggregation scheme from the first order (e.g., from edge to node) to a higher
order (e.g., from volume to edge and then to node), which takes advantage of
volumetric information in message passing. Meanwhile, a novel feature-enhanced
block is designed to further improve the performance of CeGNN and relieve the
over-smoothness problem, via treating the latent features as basis functions.
The extensive experiments on various PDE systems and one real-world dataset
demonstrate that CeGNN achieves superior performance compared with other
baseline models, particularly reducing the prediction error with up to 1 orders
of magnitude on several PDE systems."
"Human Activity Recognition (HAR) is a challenging, multi-label classification
problem as activities may co-occur and sensor signals corresponding to the same
activity may vary in different contexts (e.g., different device placements).
This paper proposes a Deep Heterogeneous Contrastive Hyper-Graph Learning
(DHC-HGL) framework that captures heterogenous Context-Aware HAR (CA-HAR)
hypergraph properties in a message-passing and neighborhood-aggregation
fashion. Prior work only explored homogeneous or shallow-node-heterogeneous
graphs. DHC-HGL handles heterogeneous CA-HAR data by innovatively 1)
Constructing three different types of sub-hypergraphs that are each passed
through different custom HyperGraph Convolution (HGC) layers designed to handle
edge-heterogeneity and 2) Adopting a contrastive loss function to ensure
node-heterogeneity. In rigorous evaluation on two CA-HAR datasets, DHC-HGL
significantly outperformed state-of-the-art baselines by 5.8% to 16.7% on
Matthews Correlation Coefficient (MCC) and 3.0% to 8.4% on Macro F1 scores.
UMAP visualizations of learned CA-HAR node embeddings are also presented to
enhance model explainability."
"For multivariate time series (MTS) tasks, previous state space models (SSMs)
followed the modeling paradigm of Transformer-based methods. However, none of
them explicitly model the complex dependencies of MTS: the Channel Dependency
variations with Time (CDT). In view of this, we delve into the derivation of
SSM, which involves approximating continuously updated functions by orthogonal
function basis. We then develop Poly-Mamba, a novel method for MTS forecasting.
Its core concept is to expand the original orthogonal function basis space into
a multivariate orthogonal function space containing variable mixing terms, and
make a projection on this space so as to explicitly describe the CDT by
weighted coefficients. In Poly-Mamba, we propose the Multivariate Orthogonal
Polynomial Approximation (MOPA) as a simplified implementation of this concept.
For the simple linear relationship between channels, we propose Linear Channel
Mixing (LCM) and generate CDT patterns adaptively for different channels
through a proposed Order Combining method. Experiments on six real-world
datasets demonstrate that Poly-Mamba outperforms the SOTA methods, especially
when dealing with datasets having a large number of channels and complex
correlations. The codes and log files will be released at:
https://github.com/Joeland4/Poly-Mamba."
"Intracranial aneurysms (IAs) that rupture result in significant morbidity and
mortality. While traditional risk models such as the PHASES score are useful in
clinical decision making, machine learning (ML) models offer the potential to
provide more accuracy. In this study, we compared the performance of four
different machine learning algorithms Random Forest (RF), XGBoost (XGB),
Support Vector Machine (SVM), and Multi Layer Perceptron (MLP) on clinical and
radiographic features to predict rupture status of intracranial aneurysms.
Among the models, RF achieved the highest accuracy (85%) with balanced
precision and recall, while MLP had the lowest overall performance (accuracy of
63%). Fractal dimension ranked as the most important feature for model
performance across all models."
"This paper reviews the applications of Graph Neural Networks (GNNs), Graph
Convolutional Networks (GCNs), and Convolutional Neural Networks (CNNs) in
blockchain technology. As the complexity and adoption of blockchain networks
continue to grow, traditional analytical methods are proving inadequate in
capturing the intricate relationships and dynamic behaviors of decentralized
systems. To address these limitations, deep learning models such as GNNs, GCNs,
and CNNs offer robust solutions by leveraging the unique graph-based and
temporal structures inherent in blockchain architectures. GNNs and GCNs, in
particular, excel in modeling the relational data of blockchain nodes and
transactions, making them ideal for applications such as fraud detection,
transaction verification, and smart contract analysis. Meanwhile, CNNs can be
adapted to analyze blockchain data when represented as structured matrices,
revealing hidden temporal and spatial patterns in transaction flows. This paper
explores how these models enhance the efficiency, security, and scalability of
both linear blockchains and Directed Acyclic Graph (DAG)-based systems,
providing a comprehensive overview of their strengths and future research
directions. By integrating advanced neural network techniques, we aim to
demonstrate the potential of these models in revolutionizing blockchain
analytics, paving the way for more sophisticated decentralized applications and
improved network performance."
"Meta-learning has been widely used in recent years in areas such as few-shot
learning and reinforcement learning. However, the questions of why and when it
is better than other algorithms in few-shot classification remain to be
explored. In this paper, we perform pre-experiments by adjusting the proportion
of label noise and the degree of task heterogeneity in the dataset. We use the
metric of Singular Vector Canonical Correlation Analysis to quantify the
representation stability of the neural network and thus to compare the behavior
of meta-learning and classical learning algorithms. We find that benefiting
from the bi-level optimization strategy, the meta-learning algorithm has better
robustness to label noise and heterogeneous tasks. Based on the above
conclusion, we argue a promising future for meta-learning in the unsupervised
area, and thus propose DHM-UHT, a dynamic head meta-learning algorithm with
unsupervised heterogeneous task construction. The core idea of DHM-UHT is to
use DBSCAN and dynamic head to achieve heterogeneous task construction and
meta-learn the whole process of unsupervised heterogeneous task construction.
On several unsupervised zero-shot and few-shot datasets, DHM-UHT obtains
state-of-the-art performance. The code is released at
https://github.com/tuantuange/DHM-UHT."
"We develop a new, unsupervised symmetry learning method that starts with raw
data, and gives the minimal (discrete) generator of an underlying Lie group of
symmetries, together with a symmetry equivariant representation of the data.
The method is able to learn the pixel translation operator from a dataset with
only an approximate translation symmetry, and can learn quite different types
of symmetries which are not apparent to the naked eye, equally well. The method
is based on the formulation of an information-theoretic loss function that
measures both the degree to which the dataset is symmetric under a given
candidate symmetry, and also, the degree of locality of the samples in the
dataset with respect to this symmetry. We demonstrate that this coupling
between symmetry and locality, together with a special optimization technique
developed for entropy estimation, results in a highly stable system that gives
reproducible results. The symmetry actions we consider are group
representations, however, we believe the approach has the potential to be
generalized to more general, nonlinear actions of non-commutative Lie groups."
"In this paper, we propose enhanced feature based granular ball twin support
vector machine (EF-GBTSVM). EF-GBTSVM employs the coarse granularity of
granular balls (GBs) as input rather than individual data samples. The GBs are
mapped to the feature space of the hidden layer using random projection
followed by the utilization of a non-linear activation function. The
concatenation of original and hidden features derived from the centers of GBs
gives rise to an enhanced feature space, commonly referred to as the random
vector functional link (RVFL) space. This space encapsulates nuanced feature
information to GBs. Further, we employ twin support vector machine (TSVM) in
the RVFL space for classification. TSVM generates the two non-parallel
hyperplanes in the enhanced feature space, which improves the generalization
performance of the proposed EF-GBTSVM model. Moreover, the coarser granularity
of the GBs enables the proposed EF-GBTSVM model to exhibit robustness to
resampling, showcasing reduced susceptibility to the impact of noise and
outliers. We undertake a thorough evaluation of the proposed EF-GBTSVM model on
benchmark UCI and KEEL datasets. This evaluation encompasses scenarios with and
without the inclusion of label noise. Moreover, experiments using NDC datasets
further emphasize the proposed model's ability to handle large datasets.
Experimental results, supported by thorough statistical analyses, demonstrate
that the proposed EF-GBTSVM model significantly outperforms the baseline models
in terms of generalization capabilities, scalability, and robustness. The
source code for the proposed EF-GBTSVM model, along with additional results and
further details, can be accessed at https://github.com/mtanveer1/EF-GBTSVM."
"Hyperparameter optimization is an essential component in many data science
pipelines and typically entails exhaustive time and resource-consuming
computations in order to explore the combinatorial search space. Similar to
this problem, other key operations in data science pipelines exhibit the exact
same properties. Important examples are: neural architecture search, where the
goal is to identify the best design choices for a neural network, and query
cardinality estimation, where given different predicate values for a SQL query
the goal is to estimate the size of the output. In this paper, we abstract away
those essential components of data science pipelines and we model them as
instances of tensor completion, where each variable of the search space
corresponds to one mode of the tensor, and the goal is to identify all missing
entries of the tensor, corresponding to all combinations of variable values,
starting from a very small sample of observed entries. In order to do so, we
first conduct a thorough experimental evaluation of existing state-of-the-art
tensor completion techniques and introduce domain-inspired adaptations (such as
smoothness across the discretized variable space) and an ensemble technique
which is able to achieve state-of-the-art performance. We extensively evaluate
existing and proposed methods in a number of datasets generated corresponding
to (a) hyperparameter optimization for non-neural network models, (b) neural
architecture search, and (c) variants of query cardinality estimation,
demonstrating the effectiveness of tensor completion as a tool for automating
data science pipelines. Furthermore, we release our generated datasets and code
in order to provide benchmarks for future work on this topic."
"Heart failure is a leading cause of global mortality, necessitating improved
diagnostic strategies. Classical machine learning models struggle with
challenges such as high-dimensional data, class imbalances, poor feature
representations, and lack of interpretability. While quantum machine learning
holds promise, current hybrid models have not fully exploited quantum
advantages. In this paper, we propose the Kolmogorov-Arnold Classical-Quantum
Dual-Channel Neural Network (KACQ-DCNN), a novel hybrid architecture that
replaces traditional multilayer perceptrons with Kolmogorov-Arnold Networks
(KANs), enabling learnable univariate activation functions. Our KACQ-DCNN
4-qubit, 1-layer model outperforms 37 benchmark models, including 16 classical
and 12 quantum neural networks, achieving an accuracy of 92.03%, with
macro-average precision, recall, and F1 scores of 92.00%. It also achieved a
ROC-AUC of 94.77%, surpassing other models by significant margins, as validated
by paired t-tests with a significance threshold of 0.0056 (after Bonferroni
correction). Ablation studies highlight the synergistic effect of
classical-quantum integration, improving performance by about 2% over MLP
variants. Additionally, LIME and SHAP explainability techniques enhance feature
interpretability, while conformal prediction provides robust uncertainty
quantification. Our results demonstrate that KACQ-DCNN improves cardiovascular
diagnostics by combining high accuracy with interpretability and uncertainty
quantification."
"Sample-efficient online reinforcement learning often uses replay buffers to
store experience for reuse when updating the value function. However, uniform
replay is inefficient, since certain classes of transitions can be more
relevant to learning. While prioritization of more useful samples is helpful,
this strategy can also lead to overfitting, as useful samples are likely to be
more rare. In this work, we instead propose a prioritized, parametric version
of an agent's memory, using generative models to capture online experience.
This paradigm enables (1) densification of past experience, with new
generations that benefit from the generative model's generalization capacity
and (2) guidance via a family of ""relevance functions"" that push these
generations towards more useful parts of an agent's acquired history. We show
this recipe can be instantiated using conditional diffusion models and simple
relevance functions such as curiosity- or value-based metrics. Our approach
consistently improves performance and sample efficiency in both state- and
pixel-based domains. We expose the mechanisms underlying these gains, showing
how guidance promotes diversity in our generated transitions and reduces
overfitting. We also showcase how our approach can train policies with even
higher update-to-data ratios than before, opening up avenues to better scale
online RL agents."
"Approximate nearest neighbor (ANN) search is a key component in many modern
machine learning pipelines; recent use cases include retrieval-augmented
generation (RAG) and vector databases. Clustering-based ANN algorithms, that
use score computation methods based on product quantization (PQ), are often
used in industrial-scale applications due to their scalability and suitability
for distributed and disk-based implementations. However, they have slower query
times than the leading graph-based ANN algorithms. In this work, we propose a
new supervised score computation method based on the observation that inner
product approximation is a multivariate (multi-output) regression problem that
can be solved efficiently by reduced-rank regression. Our experiments show that
on modern high-dimensional data sets, the proposed reduced-rank regression
(RRR) method is superior to PQ in both query latency and memory usage. We also
introduce LoRANN, a clustering-based ANN library that leverages the proposed
score computation method. LoRANN is competitive with the leading graph-based
algorithms and outperforms the state-of-the-art GPU ANN methods on
high-dimensional data sets."
"When training neural networks with custom objectives, such as ranking losses
and shortest-path losses, a common problem is that they are, per se,
non-differentiable. A popular approach is to continuously relax the objectives
to provide gradients, enabling learning. However, such differentiable
relaxations are often non-convex and can exhibit vanishing and exploding
gradients, making them (already in isolation) hard to optimize. Here, the loss
function poses the bottleneck when training a deep neural network. We present
Newton Losses, a method for improving the performance of existing hard to
optimize losses by exploiting their second-order information via their
empirical Fisher and Hessian matrices. Instead of training the neural network
with second-order techniques, we only utilize the loss function's second-order
information to replace it by a Newton Loss, while training the network with
gradient descent. This makes our method computationally efficient. We apply
Newton Losses to eight differentiable algorithms for sorting and
shortest-paths, achieving significant improvements for less-optimized
differentiable algorithms, and consistent improvements, even for well-optimized
differentiable algorithms."
"Large language models (LLMs) are susceptible to memorizing training data,
raising concerns due to the potential extraction of sensitive information.
Current methods to measure memorization rates of LLMs, primarily discoverable
extraction (Carlini et al., 2022), rely on single-sequence greedy sampling,
potentially underestimating the true extent of memorization. This paper
introduces a probabilistic relaxation of discoverable extraction that
quantifies the probability of extracting a target sequence within a set of
generated samples, considering various sampling schemes and multiple attempts.
This approach addresses the limitations of reporting memorization rates through
discoverable extraction by accounting for the probabilistic nature of LLMs and
user interaction patterns. Our experiments demonstrate that this probabilistic
measure can reveal cases of higher memorization rates compared to rates found
through discoverable extraction. We further investigate the impact of different
sampling schemes on extractability, providing a more comprehensive and
realistic assessment of LLM memorization and its associated risks. Our
contributions include a new probabilistic memorization definition, empirical
evidence of its effectiveness, and a thorough evaluation across different
models, sizes, sampling schemes, and training data repetitions."
"High-dimensional data is commonly encountered in numerous data analysis
tasks. Feature selection techniques aim to identify the most representative
features from the original high-dimensional data. Due to the absence of class
label information, it is significantly more challenging to select appropriate
features in unsupervised learning scenarios compared to supervised ones.
Traditional unsupervised feature selection methods typically score the features
of samples based on certain criteria, treating samples indiscriminately.
However, these approaches fail to fully capture the internal structure of the
data. The importance of different samples should vary, and there is a dual
relationship between the weight of samples and features that will influence
each other. Therefore, an unsupervised feature selection algorithm based on
dual manifold re-ranking (DMRR) is proposed in this paper. Different similarity
matrices are constructed to depict the manifold structures among samples,
between samples and features, and among features themselves. Then, manifold
re-ranking is performed by combining the initial scores of samples and
features. By comparing DMRR with three original unsupervised feature selection
algorithms and two unsupervised feature selection post-processing algorithms,
experimental results confirm that the importance information of different
samples and the dual relationship between sample and feature are beneficial for
achieving better feature selection."
"We propose ODTE, a new ensemble that uses oblique decision trees as base
classifiers. Additionally, we introduce STree, the base algorithm for growing
oblique decision trees, which leverages support vector machines to define
hyperplanes within the decision nodes. We embed a multiclass strategy --
one-vs-one or one-vs-rest -- at the decision nodes, allowing the model to
directly handle non-binary classification tasks without the need to cluster
instances into two groups, as is common in other approaches from the
literature. In each decision node, only the best-performing model SVM -- the
one that minimizes an impurity measure for the n-ary classification -- is
retained, even if the learned SVM addresses a binary classification subtask. An
extensive experimental study involving 49 datasets and various state-of-the-art
algorithms for oblique decision tree ensembles has been conducted. Our results
show that ODTE ranks consistently above its competitors, achieving significant
performance gains when hyperparameters are carefully tuned. Moreover, the
oblique decision trees learned through STree are more compact than those
produced by other algorithms evaluated in our experiments."
"Many continuous control problems can be formulated as sparse-reward
reinforcement learning (RL) tasks. In principle, online RL methods can
automatically explore the state space to solve each new task. However,
discovering sequences of actions that lead to a non-zero reward becomes
exponentially more difficult as the task horizon increases. Manually shaping
rewards can accelerate learning for a fixed task, but it is an arduous process
that must be repeated for each new environment. We introduce a systematic
reward-shaping framework that distills the information contained in 1) a
task-agnostic prior data set and 2) a small number of task-specific expert
demonstrations, and then uses these priors to synthesize dense dynamics-aware
rewards for the given task. This supervision substantially accelerates learning
in our experiments, and we provide analysis demonstrating how the approach can
effectively guide online learning agents to faraway goals."
"Tabular data plays a critical role in real-world financial scenarios.
Traditionally, tree models have dominated in handling tabular data. However,
financial datasets in the industry often encounter some challenges, such as
data heterogeneity, the predominance of numerical features and the large scale
of the data, which can range from tens of millions to hundreds of millions of
records. These challenges can lead to significant memory and computational
issues when using tree-based models. Consequently, there is a growing need for
neural network-based solutions that can outperform these models. In this paper,
we introduce TKGMLP, an hybrid network for tabular data that combines shallow
Kolmogorov Arnold Networks with Gated Multilayer Perceptron. This model
leverages the strengths of both architectures to improve performance and
scalability. We validate TKGMLP on a real-world credit scoring dataset, where
it achieves state-of-the-art results and outperforms current benchmarks.
Furthermore, our findings demonstrate that the model continues to improve as
the dataset size increases, making it highly scalable. Additionally, we propose
a novel feature encoding method for numerical data, specifically designed to
address the predominance of numerical features in financial datasets. The
integration of this feature encoding method within TKGMLP significantly
improves prediction accuracy. This research not only advances table prediction
technology but also offers a practical and effective solution for handling
large-scale numerical tabular data in various industrial applications."
"Cross-device Federated Analytics (FA) is a distributed computation paradigm
designed to answer analytics queries about and derive insights from data held
locally on users' devices. On-device computations combined with other privacy
and security measures ensure that only minimal data is transmitted off-device,
achieving a high standard of data protection. Despite FA's broad relevance, the
applicability of existing FA systems is limited by compromised accuracy; lack
of flexibility for data analytics; and an inability to scale effectively. In
this paper, we describe our approach to combine privacy, scalability, and
practicality to build and deploy a system that overcomes these limitations. Our
FA system leverages trusted execution environments (TEEs) and optimizes the use
of on-device computing resources to facilitate federated data processing across
large fleets of devices, while ensuring robust, defensible, and verifiable
privacy safeguards. We focus on federated analytics (statistics and
monitoring), in contrast to systems for federated learning (ML workloads), and
we flag the key differences."
"In this work, we study the experts problem in the distributed setting where
an expert's cost needs to be aggregated across multiple servers. Our study
considers various communication models such as the message-passing model and
the broadcast model, along with multiple aggregation functions, such as summing
and taking the $\ell_p$ norm of an expert's cost across servers. We propose the
first communication-efficient protocols that achieve near-optimal regret in
these settings, even against a strong adversary who can choose the inputs
adaptively. Additionally, we give a conditional lower bound showing that the
communication of our protocols is nearly optimal. Finally, we implement our
protocols and demonstrate empirical savings on the HPO-B benchmarks."
"We show how to control the generalization error of time series models wherein
past values of the outcome are used to predict future values. The results are
based on a generalization of standard i.i.d. concentration inequalities to
dependent data without the mixing assumptions common in the time series
setting. Our proof and the result are simpler than previous analyses with
dependent data or stochastic adversaries which use sequential Rademacher
complexities rather than the expected Rademacher complexity for i.i.d.
processes. We also derive empirical Rademacher results without mixing
assumptions resulting in fully calculable upper bounds."
"This paper describes the solution method taken by LeBuSiShu team for track1
in ACM KDD CUP 2011 contest (resulting in the 5th place). We identified two
main challenges: the unique item taxonomy characteristics as well as the large
data set size.To handle the item taxonomy, we present a novel method called
Matrix Factorization Item Taxonomy Regularization (MFITR). MFITR obtained the
2nd best prediction result out of more then ten implemented algorithms. For
rapidly computing multiple solutions of various algorithms, we have implemented
an open source parallel collaborative filtering library on top of the GraphLab
machine learning framework. We report some preliminary performance results
obtained using the BlackLight supercomputer."
"Stability is a general notion that quantifies the sensitivity of a learning
algorithm's output to small change in the training dataset (e.g. deletion or
replacement of a single training sample). Such conditions have recently been
shown to be more powerful to characterize learnability in the general learning
setting under i.i.d. samples where uniform convergence is not necessary for
learnability, but where stability is both sufficient and necessary for
learnability. We here show that similar stability conditions are also
sufficient for online learnability, i.e. whether there exists a learning
algorithm such that under any sequence of examples (potentially chosen
adversarially) produces a sequence of hypotheses that has no regret in the
limit with respect to the best hypothesis in hindsight. We introduce online
stability, a stability condition related to uniform-leave-one-out stability in
the batch setting, that is sufficient for online learnability. In particular we
show that popular classes of online learners, namely algorithms that fall in
the category of Follow-the-(Regularized)-Leader, Mirror Descent, gradient-based
methods and randomized algorithms like Weighted Majority and Hedge, are
guaranteed to have no regret if they have such online stability property. We
provide examples that suggest the existence of an algorithm with such stability
condition might in fact be necessary for online learnability. For the more
restricted binary classification setting, we establish that such stability
condition is in fact both sufficient and necessary. We also show that for a
large class of online learnable problems in the general learning setting,
namely those with a notion of sub-exponential covering, no-regret online
algorithms that have such stability condition exists."
"The most basic assumption used in statistical learning theory is that
training data and test data are drawn from the same underlying distribution.
Unfortunately, in many applications, the ""in-domain"" test data is drawn from a
distribution that is related, but not identical, to the ""out-of-domain""
distribution of the training data. We consider the common case in which labeled
out-of-domain data is plentiful, but labeled in-domain data is scarce. We
introduce a statistical formulation of this problem in terms of a simple
mixture model and present an instantiation of this framework to maximum entropy
classifiers and their linear chain counterparts. We present efficient inference
algorithms for this special case based on the technique of conditional
expectation maximization. Our experimental results show that our approach leads
to improved performance on three real world tasks on four different data sets
from the natural language processing domain."
"In this paper, we consider the problem of estimating self-tuning histograms
using query workloads. To this end, we propose a general learning theoretic
formulation. Specifically, we use query feedback from a workload as training
data to estimate a histogram with a small memory footprint that minimizes the
expected error on future queries. Our formulation provides a framework in which
different approaches can be studied and developed. We first study the simple
class of equi-width histograms and present a learning algorithm, EquiHist, that
is competitive in many settings. We also provide formal guarantees for
equi-width histograms that highlight scenarios in which equi-width histograms
can be expected to succeed or fail. We then go beyond equi-width histograms and
present a novel learning algorithm, SpHist, for estimating general histograms.
Here we use Haar wavelets to reduce the problem of learning histograms to that
of learning a sparse vector. Both algorithms have multiple advantages over
existing methods: 1) simple and scalable extensions to multi-dimensional data,
2) scalability with number of histogram buckets and size of query feedback, 3)
natural extensions to incorporate new feedback and handle database updates. We
demonstrate these advantages over the current state-of-the-art, ISOMER, through
detailed experiments on real and synthetic data. In particular, we show that
SpHist obtains up to 50% less error than ISOMER on real-world multi-dimensional
datasets."
"Structured prediction tasks pose a fundamental trade-off between the need for
model complexity to increase predictive power and the limited computational
resources for inference in the exponentially-sized output spaces such models
require. We formulate and develop the Structured Prediction Cascade
architecture: a sequence of increasingly complex models that progressively
filter the space of possible outputs. The key principle of our approach is that
each model in the cascade is optimized to accurately filter and refine the
structured output state space of the next model, speeding up both learning and
inference in the next layer of the cascade. We learn cascades by optimizing a
novel convex loss function that controls the trade-off between the filtering
efficiency and the accuracy of the cascade, and provide generalization bounds
for both accuracy and efficiency. We also extend our approach to intractable
models using tree-decomposition ensembles, and provide algorithms and theory
for this setting. We evaluate our approach on several large-scale problems,
achieving state-of-the-art performance in handwriting recognition and human
pose recognition. We find that structured prediction cascades allow tremendous
speedups and the use of previously intractable features and models in both
settings."
"Clustering ensemble is one of the most recent advances in unsupervised
learning. It aims to combine the clustering results obtained using different
algorithms or from different runs of the same clustering algorithm for the same
data set, this is accomplished using on a consensus function, the efficiency
and accuracy of this method has been proven in many works in literature. In the
first part of this paper we make a comparison among current approaches to
clustering ensemble in literature. All of these approaches consist of two main
steps: the ensemble generation and consensus function. In the second part of
the paper, we suggest engaging supervision in the clustering ensemble procedure
to get more enhancements on the clustering results. Supervision can be applied
in two places: either by using semi-supervised algorithms in the clustering
ensemble generation step or in the form of a feedback used by the consensus
function stage. Also, we introduce a flexible two parameter weighting
mechanism, the first parameter describes the compatibility between the datasets
under study and the semi-supervised clustering algorithms used to generate the
base partitions, the second parameter is used to provide the user feedback on
the these partitions. The two parameters are engaged in a ""relabeling and
voting"" based consensus function to produce the final clustering."
"In order to anticipate dangerous events, like a collision, an agent needs to
make long-term predictions. However, those are challenging due to uncertainties
in internal and external variables and environment dynamics. A sensorimotor
model is acquired online by the mobile robot using a state-of-the-art method
that learns the optical flow distribution in images, both in space and time.
The learnt model is used to anticipate the optical flow up to a given time
horizon and to predict an imminent collision by using reinforcement learning.
We demonstrate that multi-modal predictions reduce to simpler distributions
once actions are taken into account."
"To ensure quality results from crowdsourced tasks, requesters often aggregate
worker responses and use one of a plethora of strategies to infer the correct
answer from the set of noisy responses. However, all current models assume
prior knowledge of all possible outcomes of the task. While not an unreasonable
assumption for tasks that can be posited as multiple-choice questions (e.g.
n-ary classification), we observe that many tasks do not naturally fit this
paradigm, but instead demand a free-response formulation where the outcome
space is of infinite size (e.g. audio transcription). We model such tasks with
a novel probabilistic graphical model, and design and implement LazySusan, a
decision-theoretic controller that dynamically requests responses as necessary
in order to infer answers to these tasks. We also design an EM algorithm to
jointly learn the parameters of our model while inferring the correct answers
to multiple tasks at a time. Live experiments on Amazon Mechanical Turk
demonstrate the superiority of LazySusan at solving SAT Math questions,
eliminating 83.2% of the error and achieving greater net utility compared to
the state-ofthe-art strategy, majority-voting. We also show in live experiments
that our EM algorithm outperforms majority-voting on a visualization task that
we design."
"Recently, Petrik et al. demonstrated that L1Regularized Approximate Linear
Programming (RALP) could produce value functions and policies which compared
favorably to established linear value function approximation techniques like
LSPI. RALP's success primarily stems from the ability to solve the feature
selection and value function approximation steps simultaneously. RALP's
performance guarantees become looser if sampled next states are used. For very
noisy domains, RALP requires an accurate model rather than samples, which can
be unrealistic in some practical scenarios. In this paper, we demonstrate this
weakness, and then introduce Locally Smoothed L1-Regularized Approximate Linear
Programming (LS-RALP). We demonstrate that LS-RALP mitigates inaccuracies
stemming from noise even without an accurate model. We show that, given some
smoothness assumptions, as the number of samples increases, error from noise
approaches zero, and provide experimental examples of LS-RALP's success on
common reinforcement learning benchmark problems."
"Like a scientist or a playing child, PowerPlay not only learns new skills to
solve given problems, but also invents new interesting problems by itself. By
design, it continually comes up with the fastest to find, initially novel, but
eventually solvable tasks. It also continually simplifies or compresses or
speeds up solutions to previous tasks. Here we describe first experiments with
PowerPlay. A self-delimiting recurrent neural network SLIM RNN is used as a
general computational problem solving architecture. Its connection weights can
encode arbitrary, self-delimiting, halting or non-halting programs affecting
both environment (through effectors) and internal states encoding abstractions
of event sequences. Our PowerPlay-driven SLIM RNN learns to become an
increasingly general solver of self-invented problems, continually adding new
problem solving procedures to its growing skill repertoire. Extending a recent
conference paper, we identify interesting, emerging, developmental stages of
our open-ended system. We also show how it automatically self-modularizes,
frequently re-using code for previously invented skills, always trying to
invent novel tasks that can be quickly validated because they do not require
too many weight changes affecting too many previous tasks."
"The mean field methods, which entail approximating intractable probability
distributions variationally with distributions from a tractable family, enjoy
high efficiency, guaranteed convergence, and provide lower bounds on the true
likelihood. But due to requirement for model-specific derivation of the
optimization equations and unclear inference quality in various models, it is
not widely used as a generic approximate inference algorithm. In this paper, we
discuss a generalized mean field theory on variational approximation to a broad
class of intractable distributions using a rich set of tractable distributions
via constrained optimization over distribution spaces. We present a class of
generalized mean field (GMF) algorithms for approximate inference in complex
exponential family models, which entails limiting the optimization over the
class of cluster-factorizable distributions. GMF is a generic method requiring
no model-specific derivations. It factors a complex model into a set of
disjoint variable clusters, and uses a set of canonical fix-point equations to
iteratively update the cluster distributions, and converge to locally optimal
cluster marginals that preserve the original dependency structure within each
cluster, hence, fully decomposed the overall inference problem. We empirically
analyzed the effect of different tractable family (clusters of different
granularity) on inference quality, and compared GMF with BP on several
canonical models. Possible extension to higher-order MF approximation is also
discussed."
"We pursue an early stopping technique that helps Gaussian Restricted
Boltzmann Machines (GRBMs) to gain good natural image representations in terms
of overcompleteness and data fitting. GRBMs are widely considered as an
unsuitable model for natural images because they gain non-overcomplete
representations which include uniform filters that do not represent useful
image features. We have recently found that GRBMs once gain and subsequently
lose useful filters during their training, contrary to this common perspective.
We attribute this phenomenon to a tradeoff between overcompleteness of GRBM
representations and data fitting. To gain GRBM representations that are
overcomplete and fit data well, we propose a measure for GRBM representation
quality, approximated mutual information, and an early stopping technique based
on this measure. The proposed method boosts performance of classifiers trained
on GRBM representations."
"In this paper, we investigate a new framework for image classification that
adaptively generates spatial representations. Our strategy is based on a
sequential process that learns to explore the different regions of any image in
order to infer its category. In particular, the choice of regions is specific
to each image, directed by the actual content of previously selected
regions.The capacity of the system to handle incomplete image information as
well as its adaptive region selection allow the system to perform well in
budgeted classification tasks by exploiting a dynamicly generated
representation of each image. We demonstrate the system's abilities in a series
of image-based exploration and classification tasks that highlight its learned
exploration and inference abilities."
"We propose a frequency domain method based on robust independent component
analysis (RICA) to address the multichannel Blind Source Separation (BSS)
problem of convolutive speech mixtures in highly reverberant environments. We
impose regularization processes to tackle the ill-conditioning problem of the
covariance matrix and to mitigate the performance degradation in the frequency
domain. We apply an algorithm to separate the source signals in adverse
conditions, i.e. high reverberation conditions when short observation signals
are available. Furthermore, we study the impact of several parameters on the
performance of separation, e.g. overlapping ratio and window type of the
frequency domain method. We also compare different techniques to solve the
frequency-domain permutation ambiguity. Through simulations and real world
experiments, we verify the superiority of the presented convolutive algorithm
among other BSS algorithms, including recursive regularized ICA (RR ICA),
independent vector analysis (IVA)."
"A key topic in classification is the accuracy loss produced when the data
distribution in the training (source) domain differs from that in the testing
(target) domain. This is being recognized as a very relevant problem for many
computer vision tasks such as image classification, object detection, and
object category recognition. In this paper, we present a novel domain
adaptation method that leverages multiple target domains (or sub-domains) in a
hierarchical adaptation tree. The core idea is to exploit the commonalities and
differences of the jointly considered target domains.
  Given the relevance of structural SVM (SSVM) classifiers, we apply our idea
to the adaptive SSVM (A-SSVM), which only requires the target domain samples
together with the existing source-domain classifier for performing the desired
adaptation. Altogether, we term our proposal as hierarchical A-SSVM (HA-SSVM).
  As proof of concept we use HA-SSVM for pedestrian detection and object
category recognition. In the former we apply HA-SSVM to the deformable
part-based model (DPM) while in the latter HA-SSVM is applied to multi-category
classifiers. In both cases, we show how HA-SSVM is effective in increasing the
detection/recognition accuracy with respect to adaptation strategies that
ignore the structure of the target data. Since, the sub-domains of the target
data are not always known a priori, we shown how HA-SSVM can incorporate
sub-domain structure discovery for object category recognition."
"Gibbs sampling is a workhorse for Bayesian inference but has several
limitations when used for parameter estimation, and is often much slower than
non-sampling inference methods. SAME (State Augmentation for Marginal
Estimation) \cite{Doucet99,Doucet02} is an approach to MAP parameter estimation
which gives improved parameter estimates over direct Gibbs sampling. SAME can
be viewed as cooling the posterior parameter distribution and allows annealed
search for the MAP parameters, often yielding very high quality (lower loss)
estimates. But it does so at the expense of additional samples per iteration
and generally slower performance. On the other hand, SAME dramatically
increases the parallelism in the sampling schedule, and is an excellent match
for modern (SIMD) hardware. In this paper we explore the application of SAME to
graphical model inference on modern hardware. We show that combining SAME with
factored sample representation (or approximation) gives throughput competitive
with the fastest symbolic methods, but with potentially better quality. We
describe experiments on Latent Dirichlet Allocation, achieving speeds similar
to the fastest reported methods (online Variational Bayes) and lower
cross-validated loss than other LDA implementations. The method is simple to
implement and should be applicable to many other models."
"Adaptivity is an important feature of data analysis---typically the choice of
questions asked about a dataset depends on previous interactions with the same
dataset. However, generalization error is typically bounded in a non-adaptive
model, where all questions are specified before the dataset is drawn. Recent
work by Dwork et al. (STOC '15) and Hardt and Ullman (FOCS '14) initiated the
formal study of this problem, and gave the first upper and lower bounds on the
achievable generalization error for adaptive data analysis.
  Specifically, suppose there is an unknown distribution $\mathcal{P}$ and a
set of $n$ independent samples $x$ is drawn from $\mathcal{P}$. We seek an
algorithm that, given $x$ as input, ""accurately"" answers a sequence of
adaptively chosen ""queries"" about the unknown distribution $\mathcal{P}$. How
many samples $n$ must we draw from the distribution, as a function of the type
of queries, the number of queries, and the desired level of accuracy?
  In this work we make two new contributions towards resolving this question:
  *We give upper bounds on the number of samples $n$ that are needed to answer
statistical queries that improve over the bounds of Dwork et al.
  *We prove the first upper bounds on the number of samples required to answer
more general families of queries. These include arbitrary low-sensitivity
queries and the important class of convex risk minimization queries.
  As in Dwork et al., our algorithms are based on a connection between
differential privacy and generalization error, but we feel that our analysis is
simpler and more modular, which may be useful for studying these questions in
the future."
"There are two major approaches for sequence labeling. One is the
probabilistic gradient-based methods such as conditional random fields (CRF)
and neural networks (e.g., RNN), which have high accuracy but drawbacks: slow
training, and no support of search-based optimization (which is important in
many cases). The other is the search-based learning methods such as structured
perceptron and margin infused relaxed algorithm (MIRA), which have fast
training but also drawbacks: low accuracy, no probabilistic information, and
non-convergence in real-world tasks. We propose a novel and ""easy"" solution, a
search-based probabilistic online learning method, to address most of those
issues. The method is ""easy"", because the optimization algorithm at the
training stage is as simple as the decoding algorithm at the test stage. This
method searches the output candidates, derives probabilities, and conducts
efficient online learning. We show that this method with fast training and
theoretical guarantee of convergence, which is easy to implement, can support
search-based optimization and obtain top accuracy. Experiments on well-known
tasks show that our method has better accuracy than CRF and BiLSTM\footnote{The
SAPO code is released at \url{https://github.com/lancopku/SAPO}.}."
"A deep neural network model is a powerful framework for learning
representations. Usually, it is used to learn the relation $x \to y$ by
exploiting the regularities in the input $x$. In structured output prediction
problems, $y$ is multi-dimensional and structural relations often exist between
the dimensions. The motivation of this work is to learn the output dependencies
that may lie in the output data in order to improve the prediction accuracy.
Unfortunately, feedforward networks are unable to exploit the relations between
the outputs. In order to overcome this issue, we propose in this paper a
regularization scheme for training neural networks for these particular tasks
using a multi-task framework. Our scheme aims at incorporating the learning of
the output representation $y$ in the training process in an unsupervised
fashion while learning the supervised mapping function $x \to y$.
  We evaluate our framework on a facial landmark detection problem which is a
typical structured output task. We show over two public challenging datasets
(LFPW and HELEN) that our regularization scheme improves the generalization of
deep neural networks and accelerates their training. The use of unlabeled data
and label-only data is also explored, showing an additional improvement of the
results. We provide an opensource implementation
(https://github.com/sbelharbi/structured-output-ae) of our framework."
"The development of sensory receptive fields has been modeled in the past by a
variety of models including normative models such as sparse coding or
independent component analysis and bottom-up models such as spike-timing
dependent plasticity or the Bienenstock-Cooper-Munro model of synaptic
plasticity. Here we show that the above variety of approaches can all be
unified into a single common principle, namely Nonlinear Hebbian Learning. When
Nonlinear Hebbian Learning is applied to natural images, receptive field shapes
were strongly constrained by the input statistics and preprocessing, but
exhibited only modest variation across different choices of nonlinearities in
neuron models or synaptic plasticity rules. Neither overcompleteness nor sparse
network activity are necessary for the development of localized receptive
fields. The analysis of alternative sensory modalities such as auditory models
or V2 development lead to the same conclusions. In all examples, receptive
fields can be predicted a priori by reformulating an abstract model as
nonlinear Hebbian learning. Thus nonlinear Hebbian learning and natural
statistics can account for many aspects of receptive field formation across
models and sensory modalities."
"A search engine recommends to the user a list of web pages. The user examines
this list, from the first page to the last, and clicks on all attractive pages
until the user is satisfied. This behavior of the user can be described by the
dependent click model (DCM). We propose DCM bandits, an online learning variant
of the DCM where the goal is to maximize the probability of recommending
satisfactory items, such as web pages. The main challenge of our learning
problem is that we do not observe which attractive item is satisfactory. We
propose a computationally-efficient learning algorithm for solving our problem,
dcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable
assumptions; and also prove a matching lower bound up to logarithmic factors.
We evaluate our algorithm on synthetic and real-world problems, and show that
it performs well even when our model is misspecified. This work presents the
first practical and regret-optimal online algorithm for learning to rank with
multiple clicks in a cascade-like click model."
"This paper proposes a universal method, Boost Picking, to train supervised
classification models mainly by un-labeled data. Boost Picking only adopts two
weak classifiers to estimate and correct the error. It is theoretically proved
that Boost Picking could train a supervised model mainly by un-labeled data as
effectively as the same model trained by 100% labeled data, only if recalls of
the two weak classifiers are all greater than zero and the sum of precisions is
greater than one. Based on Boost Picking, we present ""Test along with Training
(TawT)"" to improve the generalization of supervised models. Both Boost Picking
and TawT are successfully tested in varied little data sets."
"Recently, there has been growing interest in developing optimization methods
for solving large-scale machine learning problems. Most of these problems boil
down to the problem of minimizing an average of a finite set of smooth and
strongly convex functions where the number of functions $n$ is large. Gradient
descent method (GD) is successful in minimizing convex problems at a fast
linear rate; however, it is not applicable to the considered large-scale
optimization setting because of the high computational complexity. Incremental
methods resolve this drawback of gradient methods by replacing the required
gradient for the descent direction with an incremental gradient approximation.
They operate by evaluating one gradient per iteration and executing the average
of the $n$ available gradients as a gradient approximate. Although, incremental
methods reduce the computational cost of GD, their convergence rates do not
justify their advantage relative to GD in terms of the total number of gradient
evaluations until convergence. In this paper, we introduce a Double Incremental
Aggregated Gradient method (DIAG) that computes the gradient of only one
function at each iteration, which is chosen based on a cyclic scheme, and uses
the aggregated average gradient of all the functions to approximate the full
gradient. The iterates of the proposed DIAG method uses averages of both
iterates and gradients in oppose to classic incremental methods that utilize
gradient averages but do not utilize iterate averages. We prove that not only
the proposed DIAG method converges linearly to the optimal solution, but also
its linear convergence factor justifies the advantage of incremental methods on
GD. In particular, we prove that the worst case performance of DIAG is better
than the worst case performance of GD."
"Non-linear kernel methods can be approximated by fast linear ones using
suitable explicit feature maps allowing their application to large scale
problems. We investigate how convolution kernels for structured data are
composed from base kernels and construct corresponding feature maps. On this
basis we propose exact and approximative feature maps for widely used graph
kernels based on the kernel trick. We analyze for which kernels and graph
properties computation by explicit feature maps is feasible and actually more
efficient. In particular, we derive approximative, explicit feature maps for
state-of-the-art kernels supporting real-valued attributes including the
GraphHopper and graph invariant kernels. In extensive experiments we show that
our approaches often achieve a classification accuracy close to the exact
methods based on the kernel trick, but require only a fraction of their running
time. Moreover, we propose and analyze algorithms for computing random walk,
shortest-path and subgraph matching kernels by explicit and implicit feature
maps. Our theoretical results are confirmed experimentally by observing a phase
transition when comparing running time with respect to label diversity, walk
lengths and subgraph size, respectively."
"In this paper, an unsupervised steganalysis method that combines artificial
training setsand supervised classification is proposed. We provide a formal
framework for unsupervisedclassification of stego and cover images in the
typical situation of targeted steganalysis (i.e.,for a known algorithm and
approximate embedding bit rate). We also present a completeset of experiments
using 1) eight different image databases, 2) image features based on
RichModels, and 3) three different embedding algorithms: Least Significant Bit
(LSB) matching,Highly undetectable steganography (HUGO) and Wavelet Obtained
Weights (WOW). Weshow that the experimental results outperform previous methods
based on Rich Models inthe majority of the tested cases. At the same time, the
proposed approach bypasses theproblem of Cover Source Mismatch -when the
embedding algorithm and bit rate are known-, since it removes the need of a
training database when we have a large enough testing set.Furthermore, we
provide a generic proof of the proposed framework in the machine
learningcontext. Hence, the results of this paper could be extended to other
classification problemssimilar to steganalysis."
"Exploiting the wealth of imaging and non-imaging information for disease
prediction tasks requires models capable of representing, at the same time,
individual features as well as data associations between subjects from
potentially large populations. Graphs provide a natural framework for such
tasks, yet previous graph-based approaches focus on pairwise similarities
without modelling the subjects' individual characteristics and features. On the
other hand, relying solely on subject-specific imaging feature vectors fails to
model the interaction and similarity between subjects, which can reduce
performance. In this paper, we introduce the novel concept of Graph
Convolutional Networks (GCN) for brain analysis in populations, combining
imaging and non-imaging data. We represent populations as a sparse graph where
its vertices are associated with image-based feature vectors and the edges
encode phenotypic information. This structure was used to train a GCN model on
partially labelled graphs, aiming to infer the classes of unlabelled nodes from
the node features and pairwise associations between subjects. We demonstrate
the potential of the method on the challenging ADNI and ABIDE databases, as a
proof of concept of the benefit from integrating contextual information in
classification tasks. This has a clear impact on the quality of the
predictions, leading to 69.5% accuracy for ABIDE (outperforming the current
state of the art of 66.8%) and 77% for ADNI for prediction of MCI conversion,
significantly outperforming standard linear classifiers where only individual
features are considered."
"We introduce in this work an efficient approach for audio scene
classification using deep recurrent neural networks. An audio scene is firstly
transformed into a sequence of high-level label tree embedding feature vectors.
The vector sequence is then divided into multiple subsequences on which a deep
GRU-based recurrent neural network is trained for sequence-to-label
classification. The global predicted label for the entire sequence is finally
obtained via aggregation of subsequence classification outputs. We will show
that our approach obtains an F1-score of 97.7% on the LITIS Rouen dataset,
which is the largest dataset publicly available for the task. Compared to the
best previously reported result on the dataset, our approach is able to reduce
the relative classification error by 35.3%."
"Unsupervised segmentation and clustering of unlabelled speech are core
problems in zero-resource speech processing. Most approaches lie at
methodological extremes: some use probabilistic Bayesian models with
convergence guarantees, while others opt for more efficient heuristic
techniques. Despite competitive performance in previous work, the full Bayesian
approach is difficult to scale to large speech corpora. We introduce an
approximation to a recent Bayesian model that still has a clear objective
function but improves efficiency by using hard clustering and segmentation
rather than full Bayesian inference. Like its Bayesian counterpart, this
embedded segmental K-means model (ES-KMeans) represents arbitrary-length word
segments as fixed-dimensional acoustic word embeddings. We first compare
ES-KMeans to previous approaches on common English and Xitsonga data sets (5
and 2.5 hours of speech): ES-KMeans outperforms a leading heuristic method in
word segmentation, giving similar scores to the Bayesian model while being 5
times faster with fewer hyperparameters. However, its clusters are less pure
than those of the other models. We then show that ES-KMeans scales to larger
corpora by applying it to the 5 languages of the Zero Resource Speech Challenge
2017 (up to 45 hours), where it performs competitively compared to the
challenge baseline."
"The project aims to research on combining deep learning specifically
Long-Short Memory (LSTM) and basic statistics in multiple multistep time series
prediction. LSTM can dive into all the pages and learn the general trends of
variation in a large scope, while the well selected medians for each page can
keep the special seasonality of different pages so that the future trend will
not fluctuate too much from the reality. A recent Kaggle competition on 145K
Web Traffic Time Series Forecasting [1] is used to thoroughly illustrate and
test this idea."
"Optimal Transport has recently gained interest in machine learning for
applications ranging from domain adaptation, sentence similarities to deep
learning. Yet, its ability to capture frequently occurring structure beyond the
""ground metric"" is limited. In this work, we develop a nonlinear generalization
of (discrete) optimal transport that is able to reflect much additional
structure. We demonstrate how to leverage the geometry of this new model for
fast algorithms, and explore connections and properties. Illustrative
experiments highlight the benefit of the induced structured couplings for tasks
in domain adaptation and natural language processing."
"Objective: Predict patient-specific vitals deemed medically acceptable for
discharge from a pediatric intensive care unit (ICU). Design: The means of each
patient's hr, sbp and dbp measurements between their medical and physical
discharge from the ICU were computed as a proxy for their physiologically
acceptable state space (PASS) for successful ICU discharge. These individual
PASS values were compared via root mean squared error (rMSE) to population
age-normal vitals, a polynomial regression through the PASS values of a
Pediatric ICU (PICU) population and predictions from two recurrent neural
network models designed to predict personalized PASS within the first twelve
hours following ICU admission. Setting: PICU at Children's Hospital Los Angeles
(CHLA). Patients: 6,899 PICU episodes (5,464 patients) collected between 2009
and 2016. Interventions: None. Measurements: Each episode data contained 375
variables representing vitals, labs, interventions, and drugs. They also
included a time indicator for PICU medical discharge and physical discharge.
Main Results: The rMSEs between individual PASS values and population
age-normals (hr: 25.9 bpm, sbp: 13.4 mmHg, dbp: 13.0 mmHg) were larger than the
rMSEs corresponding to the polynomial regression (hr: 19.1 bpm, sbp: 12.3 mmHg,
dbp: 10.8 mmHg). The rMSEs from the best performing RNN model were the lowest
(hr: 16.4 bpm; sbp: 9.9 mmHg, dbp: 9.0 mmHg). Conclusion: PICU patients are a
unique subset of the general population, and general age-normal vitals may not
be suitable as target values indicating physiologic stability at discharge.
Age-normal vitals that were specifically derived from the medical-to-physical
discharge window of ICU patients may be more appropriate targets for
'acceptable' physiologic state for critical care patients. Going beyond simple
age bins, an RNN model can provide more personalized target values."
"Parameter identification and comparison of dynamical systems is a challenging
task in many fields. Bayesian approaches based on Gaussian process regression
over time-series data have been successfully applied to infer the parameters of
a dynamical system without explicitly solving it. While the benefits in
computational cost are well established, a rigorous mathematical framework has
been missing. We offer a novel interpretation which leads to a better
understanding and improvements in state-of-the-art performance in terms of
accuracy for nonlinear dynamical systems."
"Is it possible to extract malicious IP addresses reported in security forums
in an automatic way? This is the question at the heart of our work. We focus on
security forums, where security professionals and hackers share knowledge and
information, and often report misbehaving IP addresses. So far, there have only
been a few efforts to extract information from such security forums. We propose
RIPEx, a systematic approach to identify and label IP addresses in security
forums by utilizing a cross-forum learning method. In more detail, the
challenge is twofold: (a) identifying IP addresses from other numerical
entities, such as software version numbers, and (b) classifying the IP address
as benign or malicious. We propose an integrated solution that tackles both
these problems. A novelty of our approach is that it does not require training
data for each new forum. Our approach does knowledge transfer across forums: we
use a classifier from our source forums to identify seed information for
training a classifier on the target forum. We evaluate our method using data
collected from five security forums with a total of 31K users and 542K posts.
First, RIPEx can distinguish IP address from other numeric expressions with 95%
precision and above 93% recall on average. Second, RIPEx identifies malicious
IP addresses with an average precision of 88% and over 78% recall, using our
cross-forum learning. Our work is a first step towards harnessing the wealth of
useful information that can be found in security forums."
"This paper studies the convergence behaviour of dictionary learning via the
Iterative Thresholding and K-residual Means (ITKrM) algorithm. On one hand it
is proved that ITKrM is a contraction under much more relaxed conditions than
previously necessary. On the other hand it is shown that there seem to exist
stable fixed points that do not correspond to the generating dictionary, which
can be characterised as very coherent. Based on an analysis of the residuals
using these bad dictionaries, replacing coherent atoms with carefully designed
replacement candidates is proposed. In experiments on synthetic data this
outperforms random or no replacement and always leads to full dictionary
recovery. Finally the question how to learn dictionaries without knowledge of
the correct dictionary size and sparsity level is addressed. Decoupling the
replacement strategy of coherent or unused atoms into pruning and adding, and
slowly carefully increasing the sparsity level, leads to an adaptive version of
ITKrM. In several experiments this adaptive dictionary learning algorithm is
shown to recover a generating dictionary from randomly initialised dictionaries
of various sizes on synthetic data and to learn meaningful dictionaries on
image data."
"Importance-weighting is a popular and well-researched technique for dealing
with sample selection bias and covariate shift. It has desirable
characteristics such as unbiasedness, consistency and low computational
complexity. However, weighting can have a detrimental effect on an estimator as
well. In this work, we empirically show that the sampling distribution of an
importance-weighted estimator can be skewed. For sample selection bias
settings, and for small sample sizes, the importance-weighted risk estimator
produces overestimates for datasets in the body of the sampling distribution,
i.e. the majority of cases, and large underestimates for data sets in the tail
of the sampling distribution. These over- and underestimates of the risk lead
to suboptimal regularization parameters when used for importance-weighted
validation."
"It is well known that, for most datasets, the use of large-size minibatches
for Stochastic Gradient Descent (SGD) typically leads to slow convergence and
poor generalization. On the other hand, large minibatches are of great
practical interest as they allow for a better exploitation of modern GPUs.
Previous literature on the subject concentrated on how to adjust the main SGD
parameters (in particular, the learning rate) when using large minibatches. In
this work we introduce an additional feature, that we call minibatch
persistency, that consists in reusing the same minibatch for K consecutive SGD
iterations. The computational conjecture here is that a large minibatch
contains a significant sample of the training set, so one can afford to
slightly overfitting it without worsening generalization too much. The approach
is intended to speedup SGD convergence, and also has the advantage of reducing
the overhead related to data loading on the internal GPU memory. We present
computational results on CIFAR-10 with an AlexNet architecture, showing that
even small persistency values (K=2 or 5) already lead to a significantly faster
convergence and to a comparable (or even better) generalization than the
standard ""disposable minibatch"" approach (K=1), in particular when large
minibatches are used. The lesson learned is that minibatch persistency can be a
simple yet effective way to deal with large minibatches."
"Computable Stein discrepancies have been deployed for a variety of
applications, ranging from sampler selection in posterior inference to
approximate Bayesian inference to goodness-of-fit testing. Existing
convergence-determining Stein discrepancies admit strong theoretical guarantees
but suffer from a computational cost that grows quadratically in the sample
size. While linear-time Stein discrepancies have been proposed for
goodness-of-fit testing, they exhibit avoidable degradations in testing power
-- even when power is explicitly optimized. To address these shortcomings, we
introduce feature Stein discrepancies ($\Phi$SDs), a new family of quality
measures that can be cheaply approximated using importance sampling. We show
how to construct $\Phi$SDs that provably determine the convergence of a sample
to its target and develop high-accuracy approximations -- random $\Phi$SDs
(R$\Phi$SDs) -- which are computable in near-linear time. In our experiments
with sampler selection for approximate posterior inference and goodness-of-fit
testing, R$\Phi$SDs perform as well or better than quadratic-time KSDs while
being orders of magnitude faster to compute."
"Random forest and deep neural network are two schools of effective
classification methods in machine learning. While the random forest is robust
irrespective of the data domain, the deep neural network has advantages in
handling high dimensional data. In view that a differentiable neural decision
forest can be added to the neural network to fully exploit the benefits of both
models, in our work, we further combine convolutional autoencoder with neural
decision forest, where autoencoder has its advantages in finding the hidden
representations of the input data. We develop a gradient boost module and embed
it into the proposed convolutional autoencoder with neural decision forest to
improve the performance. The idea of gradient boost is to learn and use the
residual in the prediction. In addition, we design a structure to learn the
parameters of the neural decision forest and gradient boost module at
contiguous steps. The extensive experiments on several public datasets
demonstrate that our proposed model achieves good efficiency and prediction
performance compared with a series of baseline methods."
"Recent breakthroughs in Neural Architectural Search (NAS) have achieved
state-of-the-art performances in applications such as image classification and
language modeling. However, these techniques typically ignore device-related
objectives such as inference time, memory usage, and power consumption.
Optimizing neural architecture for device-related objectives is immensely
crucial for deploying deep networks on portable devices with limited computing
resources. We propose DPP-Net: Device-aware Progressive Search for
Pareto-optimal Neural Architectures, optimizing for both device-related (e.g.,
inference time and memory usage) and device-agnostic (e.g., accuracy and model
size) objectives. DPP-Net employs a compact search space inspired by current
state-of-the-art mobile CNNs, and further improves search efficiency by
adopting progressive search (Liu et al. 2017). Experimental results on CIFAR-10
are poised to demonstrate the effectiveness of Pareto-optimal networks found by
DPP-Net, for three different devices: (1) a workstation with Titan X GPU, (2)
NVIDIA Jetson TX1 embedded system, and (3) mobile phone with ARM Cortex-A53.
Compared to CondenseNet and NASNet (Mobile), DPP-Net achieves better
performances: higher accuracy and shorter inference time on various devices.
Additional experimental results show that models found by DPP-Net also achieve
considerably-good performance on ImageNet as well."
"Multi-label classification (MLC) is a supervised learning problem in which,
contrary to standard multiclass classification, an instance can be associated
with several class labels simultaneously. In this chapter, we advocate a
rule-based approach to multi-label classification. Rule learning algorithms are
often employed when one is not only interested in accurate predictions, but
also requires an interpretable theory that can be understood, analyzed, and
qualitatively evaluated by domain experts. Ideally, by revealing patterns and
regularities contained in the data, a rule-based theory yields new insights in
the application domain. Recently, several authors have started to investigate
how rule-based models can be used for modeling multi-label data. Discussing
this task in detail, we highlight some of the problems that make rule learning
considerably more challenging for MLC than for conventional classification.
While mainly focusing on our own previous work, we also provide a short
overview of related work in this area."
"A fundamental challenge in networked systems is detection and removal of
suspected malicious nodes. In reality, detection is always imperfect, and the
decision about which potentially malicious nodes to remove must trade off false
positives (erroneously removing benign nodes) and false negatives (mistakenly
failing to remove malicious nodes). However, in network settings this
conventional tradeoff must now account for node connectivity. In particular,
malicious nodes may exert malicious influence, so that mistakenly leaving some
of these in the network may cause damage to spread. On the other hand, removing
benign nodes causes direct harm to these, and indirect harm to their benign
neighbors who would wish to communicate with them.
  We formalize the problem of removing potentially malicious nodes from a
network under uncertainty through an objective that takes connectivity into
account. We show that optimally solving the resulting problem is NP-Hard. We
then propose a tractable solution approach based on a convex relaxation of the
objective. Finally, we experimentally demonstrate that our approach
significantly outperforms both a simple baseline that ignores network
structure, as well as a state-of-the-art approach for a related problem, on
both synthetic and real-world datasets."
"Time series forecasting is difficult. It is difficult even for recurrent
neural networks with their inherent ability to learn sequentiality. This
article presents a recurrent neural network based time series forecasting
framework covering feature engineering, feature importances, point and interval
predictions, and forecast evaluation. The description of the method is followed
by an empirical study using both LSTM and GRU networks."
"Linear mixture models have proven very useful in a plethora of applications,
e.g., topic modeling, clustering, and source separation. As a critical aspect
of the linear mixture models, identifiability of the model parameters is
well-studied, under frameworks such as independent component analysis and
constrained matrix factorization. Nevertheless, when the linear mixtures are
distorted by an unknown nonlinear functions -- which is well-motivated and more
realistic in many cases -- the identifiability issues are much less studied.
This work proposes an identification criterion for a nonlinear mixture model
that is well grounded in many real-world applications, and offers
identifiability guarantees. A practical implementation based on a judiciously
designed neural network is proposed to realize the criterion, and an effective
learning algorithm is proposed. Numerical results on synthetic and real-data
corroborate effectiveness of the proposed method."
"Open data plays a fundamental role in the 21th century by stimulating
economic growth and by enabling more transparent and inclusive societies.
However, it is always difficult to create new high-quality datasets with the
required privacy guarantees for many use cases. This paper aims at creating a
framework for releasing new open data while protecting the individuality of the
users through a strict definition of privacy called differential privacy.
Unlike previous work, this paper provides a framework for privacy preserving
data publishing that can be easily adapted to different use cases, from the
generation of time-series to continuous data, and discrete data; no previous
work has focused on the later class. Indeed, many use cases expose discrete
data or at least a combination between categorical and numerical values. Thanks
to the latest developments in deep learning and generative models, it is now
possible to model rich-semantic data maintaining both the original distribution
of the features and the correlations between them. The output of this framework
is a deep network, namely a generator, able to create new data on demand. We
demonstrate the efficiency of our approach on real datasets from the French
public administration and classic benchmark datasets."
"It has been noted in existing literature that over-parameterization in ReLU
networks generally improves performance. While there could be several factors
involved behind this, we prove some desirable theoretical properties at
initialization which may be enjoyed by ReLU networks. Specifically, it is known
that He initialization in deep ReLU networks asymptotically preserves variance
of activations in the forward pass and variance of gradients in the backward
pass for infinitely wide networks, thus preserving the flow of information in
both directions. Our paper goes beyond these results and shows novel properties
that hold under He initialization: i) the norm of hidden activation of each
layer is equal to the norm of the input, and, ii) the norm of weight gradient
of each layer is equal to the product of norm of the input vector and the error
at output layer. These results are derived using the PAC analysis framework,
and hold true for finitely sized datasets such that the width of the ReLU
network only needs to be larger than a certain finite lower bound. As we show,
this lower bound depends on the depth of the network and the number of samples,
and by the virtue of being a lower bound, over-parameterized ReLU networks are
endowed with these desirable properties. For the aforementioned hidden
activation norm property under He initialization, we further extend our theory
and show that this property holds for a finite width network even when the
number of data samples is infinite. Thus we overcome several limitations of
existing papers, and show new properties of deep ReLU networks at
initialization."
"Decentralized Online Learning (online learning in decentralized networks)
attracts more and more attention, since it is believed that Decentralized
Online Learning can help the data providers cooperatively better solve their
online problems without sharing their private data to a third party or other
providers. Typically, the cooperation is achieved by letting the data providers
exchange their models between neighbors, e.g., recommendation model. However,
the best regret bound for a decentralized online learning algorithm is
$\Ocal{n\sqrt{T}}$, where $n$ is the number of nodes (or users) and $T$ is the
number of iterations. This is clearly insignificant since this bound can be
achieved \emph{without} any communication in the networks. This reminds us to
ask a fundamental question: \emph{Can people really get benefit from the
decentralized online learning by exchanging information?} In this paper, we
studied when and why the communication can help the decentralized online
learning to reduce the regret. Specifically, each loss function is
characterized by two components: the adversarial component and the stochastic
component. Under this characterization, we show that decentralized online
gradient (DOG) enjoys a regret bound $\Ocal{n\sqrt{T}G + \sqrt{nT}\sigma}$,
where $G$ measures the magnitude of the adversarial component in the private
data (or equivalently the local loss function) and $\sigma$ measures the
randomness within the private data. This regret suggests that people can get
benefits from the randomness in the private data by exchanging private
information. Another important contribution of this paper is to consider the
dynamic regret -- a more practical regret to track users' interest dynamics.
Empirical studies are also conducted to validate our analysis."
"We propose to meta-learn causal structures based on how fast a learner adapts
to new distributions arising from sparse distributional changes, e.g. due to
interventions, actions of agents and other sources of non-stationarities. We
show that under this assumption, the correct causal structural choices lead to
faster adaptation to modified distributions because the changes are
concentrated in one or just a few mechanisms when the learned knowledge is
modularized appropriately. This leads to sparse expected gradients and a lower
effective number of degrees of freedom needing to be relearned while adapting
to the change. It motivates using the speed of adaptation to a modified
distribution as a meta-learning objective. We demonstrate how this can be used
to determine the cause-effect relationship between two observed variables. The
distributional changes do not need to correspond to standard interventions
(clamping a variable), and the learner has no direct knowledge of these
interventions. We show that causal structures can be parameterized via
continuous variables and learned end-to-end. We then explore how these ideas
could be used to also learn an encoder that would map low-level observed
variables to unobserved causal variables leading to faster adaptation
out-of-distribution, learning a representation space where one can satisfy the
assumptions of independent mechanisms and of small and sparse changes in these
mechanisms due to actions and non-stationarities."
"Deep neural networks are usually trained with stochastic gradient descent
(SGD), which minimizes objective function using very rough approximations of
gradient, only averaging to the real gradient. Standard approaches like
momentum or ADAM only consider a single direction, and do not try to model
distance from extremum - neglecting valuable information from calculated
sequence of gradients, often stagnating in some suboptimal plateau. Second
order methods could exploit these missed opportunities, however, beside
suffering from very large cost and numerical instabilities, many of them
attract to suboptimal points like saddles due to negligence of signs of
curvatures (as eigenvalues of Hessian).
  Saddle-free Newton method is a rare example of addressing this issue -
changes saddle attraction into repulsion, and was shown to provide essential
improvement for final value this way. However, it neglects noise while
modelling second order behavior, focuses on Krylov subspace for numerical
reasons, and requires costly eigendecomposion.
  Maintaining SFN advantages, there are proposed inexpensive ways for
exploiting these opportunities. Second order behavior is linear dependence of
first derivative - we can optimally estimate it from sequence of noisy
gradients with least square linear regression, in online setting here: with
weakening weights of old gradients. Statistically relevant subspace is
suggested by PCA of recent noisy gradients - in online setting it can be made
by slowly rotating considered directions toward new gradients, gradually
replacing old directions with recent statistically relevant. Eigendecomposition
can be also performed online: with regularly performed step of QR method to
maintain diagonal Hessian. Outside the second order modeled subspace we can
simultaneously perform gradient descent."
"How does missing data affect our ability to learn signal structures? It has
been shown that learning signal structure in terms of principal components is
dependent on the ratio of sample size and dimensionality and that a critical
number of observations is needed before learning starts (Biehl and Mietzner,
1993). Here we generalize this analysis to include missing data. Probabilistic
principal component analysis is regularly used for estimating signal structures
in datasets with missing data. Our analytic result suggests that the effect of
missing data is to effectively reduce signal-to-noise ratio rather than - as
generally believed - to reduce sample size. The theory predicts a phase
transition in the learning curves and this is indeed found both in simulation
data and in real datasets."
"Video-based person re-identification has drawn massive attention in recent
years due to its extensive applications in video surveillance. While deep
learning-based methods have led to significant progress, these methods are
limited by ineffectively using complementary information, which is blamed on
necessary data augmentation in the training process. Data augmentation has been
widely used to mitigate the over-fitting trap and improve the ability of
network representation. However, the previous methods adopt image-based data
augmentation scheme to individually process the input frames, which corrupts
the complementary information between consecutive frames and causes performance
degradation. Extensive experiments on three benchmark datasets demonstrate that
our framework outperforms the most recent state-of-the-art methods. We also
perform cross-dataset validation to prove the generality of our method."
"Score matching is a popular method for estimating unnormalized statistical
models. However, it has been so far limited to simple, shallow models or
low-dimensional data, due to the difficulty of computing the Hessian of
log-density functions. We show this difficulty can be mitigated by projecting
the scores onto random vectors before comparing them. This objective, called
sliced score matching, only involves Hessian-vector products, which can be
easily implemented using reverse-mode automatic differentiation. Therefore,
sliced score matching is amenable to more complex models and higher dimensional
data compared to score matching. Theoretically, we prove the consistency and
asymptotic normality of sliced score matching estimators. Moreover, we
demonstrate that sliced score matching can be used to learn deep score
estimators for implicit distributions. In our experiments, we show sliced score
matching can learn deep energy-based models effectively, and can produce
accurate score estimates for applications such as variational inference with
implicit distributions and training Wasserstein Auto-Encoders."
"Deep neural networks achieve state-of-the-art results on several tasks while
increasing in complexity. It has been shown that neural networks can be pruned
during training by imposing sparsity inducing regularizers. In this paper, we
investigate two techniques for group-wise pruning during training in order to
improve network efficiency. We propose a gating factor after every
convolutional layer to induce channel level sparsity, encouraging insignificant
channels to become exactly zero. Further, we introduce and analyse a bounded
variant of the L1 regularizer, which interpolates between L1 and L0-norms to
retain performance of the network at higher pruning rates. To underline
effectiveness of the proposed methods,we show that the number of parameters of
ResNet-164, DenseNet-40 and MobileNetV2 can be reduced down by 30%, 69% and 75%
on CIFAR100 respectively without a significant drop in accuracy. We achieve
state-of-the-art pruning results for ResNet-50 with higher accuracy on
ImageNet. Furthermore, we show that the light weight MobileNetV2 can further be
compressed on ImageNet without a significant drop in performance."
"We consider an adversarial variant of the classic $K$-armed linear contextual
bandit problem where the sequence of loss functions associated with each arm
are allowed to change without restriction over time. Under the assumption that
the $d$-dimensional contexts are generated i.i.d.~at random from a known
distributions, we develop computationally efficient algorithms based on the
classic Exp3 algorithm. Our first algorithm, RealLinExp3, is shown to achieve a
regret guarantee of $\widetilde{O}(\sqrt{KdT})$ over $T$ rounds, which matches
the best available bound for this problem. Our second algorithm, RobustLinExp3,
is shown to be robust to misspecification, in that it achieves a regret bound
of $\widetilde{O}((Kd)^{1/3}T^{2/3}) + \varepsilon \sqrt{d} T$ if the true
reward function is linear up to an additive nonlinear error uniformly bounded
in absolute value by $\varepsilon$. To our knowledge, our performance
guarantees constitute the very first results on this problem setting."
"Testing software is often costly due to the need of mass-producing test cases
and providing a test oracle for it. This is often referred to as the oracle
problem. One method that has been proposed in order to alleviate the oracle
problem is metamorphic testing. Metamorphic testing produces new test cases by
altering an existing test case, and uses the metamorphic relation between the
inputs and the outputs of the System Under Test (SUT) to predict the expected
outputs of the produced test cases. Metamorphic testing has often been used for
image processing software, where changes are applied to the image's attributes
to create new test cases with annotations that are the same as the original
image. We refer to this existing method as the image-based metamorphic testing.
In this research, we propose an object-based metamorphic testing and a
composite metamorphic testing which combines different metamorphic testing
approaches to relatively increase test coverage."
"Incorporating hierarchical structures like constituency trees has been shown
to be effective for various natural language processing (NLP) tasks. However,
it is evident that state-of-the-art (SOTA) sequence-based models like the
Transformer struggle to encode such structures inherently. On the other hand,
dedicated models like the Tree-LSTM, while explicitly modeling hierarchical
structures, do not perform as efficiently as the Transformer. In this paper, we
attempt to bridge this gap with ""Hierarchical Accumulation"" to encode parse
tree structures into self-attention at constant time complexity. Our approach
outperforms SOTA methods in four IWSLT translation tasks and the WMT'14
English-German translation task. It also yields improvements over Transformer
and Tree-LSTM on three text classification tasks. We further demonstrate that
using hierarchical priors can compensate for data shortage, and that our model
prefers phrase-level attentions over token-level attentions."
"Traditionally, there are two models on differential privacy: the central
model and the local model. The central model focuses on the machine learning
model and the local model focuses on the training data. In this paper, we study
the \textit{input perturbation} method in differentially private empirical risk
minimization (DP-ERM), preserving privacy of the central model. By adding noise
to the original training data and training with the `perturbed data', we
achieve ($\epsilon$,$\delta$)-differential privacy on the final model, along
with some kind of privacy on the original data. We observe that there is an
interesting connection between the local model and the central model: the
perturbation on the original data causes the perturbation on the gradient, and
finally the model parameters. This observation means that our method builds a
bridge between local and central model, protecting the data, the gradient and
the model simultaneously, which is more superior than previous central methods.
Detailed theoretical analysis and experiments show that our method achieves
almost the same (or even better) performance as some of the best previous
central methods with more protections on privacy, which is an attractive
result. Moreover, we extend our method to a more general case: the loss
function satisfies the Polyak-Lojasiewicz condition, which is more general than
strong convexity, the constraint on the loss function in most previous work."
"Kernel methods have achieved very good performance on large scale regression
and classification problems, by using the Nystr\""om method and preconditioning
techniques. The Nystr\""om approximation -- based on a subset of landmarks --
gives a low rank approximation of the kernel matrix, and is known to provide a
form of implicit regularization. We further elaborate on the impact of sampling
diverse landmarks for constructing the Nystr\""om approximation in supervised as
well as unsupervised kernel methods. By using Determinantal Point Processes for
sampling, we obtain additional theoretical results concerning the interplay
between diversity and regularization. Empirically, we demonstrate the
advantages of training kernel methods based on subsets made of diverse points.
In particular, if the dataset has a dense bulk and a sparser tail, we show that
Nystr\""om kernel regression with diverse landmarks increases the accuracy of
the regression in sparser regions of the dataset, with respect to a uniform
landmark sampling. A greedy heuristic is also proposed to select diverse
samples of significant size within large datasets when exact DPP sampling is
not practically feasible."
"A recent line of work studies overparametrized neural networks in the ""kernel
regime,"" i.e. when the network behaves during training as a kernelized linear
predictor, and thus training with gradient descent has the effect of finding
the minimum RKHS norm solution. This stands in contrast to other studies which
demonstrate how gradient descent on overparametrized multilayer networks can
induce rich implicit biases that are not RKHS norms. Building on an observation
by Chizat and Bach, we show how the scale of the initialization controls the
transition between the ""kernel"" (aka lazy) and ""rich"" (aka active) regimes and
affects generalization properties in multilayer homogeneous models. We also
highlight an interesting role for the width of a model in the case that the
predictor is not identically zero at initialization. We provide a complete and
detailed analysis for a family of simple depth-$D$ models that already exhibit
an interesting and meaningful transition between the kernel and rich regimes,
and we also demonstrate this transition empirically for more complex matrix
factorization models and multilayer non-linear networks."
"Neural networks with binary weights are computation-efficient and
hardware-friendly, but their training is challenging because it involves a
discrete optimization problem. Surprisingly, ignoring the discrete nature of
the problem and using gradient-based methods, such as the Straight-Through
Estimator, still works well in practice. This raises the question: are there
principled approaches which justify such methods? In this paper, we propose
such an approach using the Bayesian learning rule. The rule, when applied to
estimate a Bernoulli distribution over the binary weights, results in an
algorithm which justifies some of the algorithmic choices made by the previous
approaches. The algorithm not only obtains state-of-the-art performance, but
also enables uncertainty estimation for continual learning to avoid
catastrophic forgetting. Our work provides a principled approach for training
binary neural networks which justifies and extends existing approaches."
"Adversarial training based on the minimax formulation is necessary for
obtaining adversarial robustness of trained models. However, it is conservative
or even pessimistic so that it sometimes hurts the natural generalization. In
this paper, we raise a fundamental question---do we have to trade off natural
generalization for adversarial robustness? We argue that adversarial training
is to employ confident adversarial data for updating the current model. We
propose a novel approach of friendly adversarial training (FAT): rather than
employing most adversarial data maximizing the loss, we search for least
adversarial (i.e., friendly adversarial) data minimizing the loss, among the
adversarial data that are confidently misclassified. Our novel formulation is
easy to implement by just stopping the most adversarial data searching
algorithms such as PGD (projected gradient descent) early, which we call
early-stopped PGD. Theoretically, FAT is justified by an upper bound of the
adversarial risk. Empirically, early-stopped PGD allows us to answer the
earlier question negatively---adversarial robustness can indeed be achieved
without compromising the natural generalization."
"We propose a novel confidence scoring mechanism for deep neural networks
based on a two-model paradigm involving a base model and a meta-model. The
confidence score is learned by the meta-model observing the base model
succeeding/failing at its task. As features to the meta-model, we investigate
linear classifier probes inserted between the various layers of the base model.
Our experiments demonstrate that this approach outperforms various baselines in
a filtering task, i.e., task of rejecting samples with low confidence.
Experimental results are presented using CIFAR-10 and CIFAR-100 dataset with
and without added noise. We discuss the importance of confidence scoring to
bridge the gap between experimental and real-world applications."
"In this short paper, we evaluate the performance of three well-known Machine
Learning techniques for predicting the impact of a post in Facebook. Social
medias have a huge influence in the social behaviour. Therefore to develop an
automatic model for predicting the impact of posts in social medias can be
useful to the society. In this article, we analyze the efficiency for
predicting the post impact of three popular techniques: Support Vector
Regression (SVR), Echo State Network (ESN) and Adaptive Network Fuzzy Inject
System (ANFIS). The evaluation was done over a public and well-known benchmark
dataset."
"We propose a new method of estimation in topic models, that is not a
variation on the existing simplex finding algorithms, and that estimates the
number of topics K from the observed data. We derive new finite sample minimax
lower bounds for the estimation of A, as well as new upper bounds for our
proposed estimator. We describe the scenarios where our estimator is minimax
adaptive. Our finite sample analysis is valid for any number of documents (n),
individual document length (N_i), dictionary size (p) and number of topics (K),
and both p and K are allowed to increase with n, a situation not handled well
by previous analyses. We complement our theoretical results with a detailed
simulation study. We illustrate that the new algorithm is faster and more
accurate than the current ones, although we start out with a computational and
theoretical disadvantage of not knowing the correct number of topics K, while
we provide the competing methods with the correct value in our simulations."
"The success of deep learning has been due, in no small part, to the
availability of large annotated datasets. Thus, a major bottleneck in current
learning pipelines is the time-consuming human annotation of data. In scenarios
where such input-output pairs cannot be collected, simulation is often used
instead, leading to a domain-shift between synthesized and real-world data.
This work offers an unsupervised alternative that relies on the availability of
task-specific energy functions, replacing the generic supervised loss. Such
energy functions are assumed to lead to the desired label as their minimizer
given the input. The proposed approach, termed ""Deep Energy"", trains a Deep
Neural Network (DNN) to approximate this minimization for any chosen input.
Once trained, a simple and fast feed-forward computation provides the inferred
label. This approach allows us to perform unsupervised training of DNNs with
real-world inputs only, and without the need for manually-annotated labels, nor
synthetically created data. ""Deep Energy"" is demonstrated in this paper on
three different tasks -- seeded segmentation, image matting and single image
dehazing -- exposing its generality and wide applicability. Our experiments
show that the solution provided by the network is often much better in quality
than the one obtained by a direct minimization of the energy function,
suggesting an added regularization property in our scheme."
"Models like support vector machines or Gaussian process regression often
require positive semi-definite kernels. These kernels may be based on distance
functions. While definiteness is proven for common distances and kernels, a
proof for a new kernel may require too much time and effort for users who
simply aim at practical usage. Furthermore, designing definite distances or
kernels may be equally intricate. Finally, models can be enabled to use
indefinite kernels. This may deteriorate the accuracy or computational cost of
the model. Hence, an efficient method to determine definiteness is required. We
propose an empirical approach. We show that sampling as well as optimization
with an evolutionary algorithm may be employed to determine definiteness. We
provide a proof-of-concept with 16 different distance measures for
permutations. Our approach allows to disprove definiteness if a respective
counter-example is found. It can also provide an estimate of how likely it is
to obtain indefinite kernel matrices. This provides a simple, efficient tool to
decide whether additional effort should be spent on designing/selecting a more
suitable kernel or algorithm."
"One of the challenges in model-based control of stochastic dynamical systems
is that the state transition dynamics are involved, and it is not easy or
efficient to make good-quality predictions of the states. Moreover, there are
not many representational models for the majority of autonomous systems, as it
is not easy to build a compact model that captures the entire dynamical
subtleties and uncertainties. In this work, we present a hierarchical Bayesian
linear regression model with local features to learn the dynamics of a
micro-robotic system as well as two simpler examples, consisting of a
stochastic mass-spring damper and a stochastic double inverted pendulum on a
cart. The model is hierarchical since we assume non-stationary priors for the
model parameters. These non-stationary priors make the model more flexible by
imposing priors on the priors of the model. To solve the maximum likelihood
(ML) problem for this hierarchical model, we use the variational expectation
maximization (EM) algorithm, and enhance the procedure by introducing hidden
target variables. The algorithm yields parsimonious model structures, and
consistently provides fast and accurate predictions for all our examples
involving large training and test sets. This demonstrates the effectiveness of
the method in learning stochastic dynamics, which makes it suitable for future
use in a paradigm, such as model-based reinforcement learning, to compute
optimal control policies in real time."
"In this paper we study the prediction of heart rate from acceleration using a
wrist worn wearable. Although existing photoplethysmography (PPG) heart rate
sensors provide reliable measurements, they use considerably more energy than
accelerometers and have a major impact on battery life of wearable devices. By
using energy-efficient accelerometers to predict heart rate, significant energy
savings can be made. Further, we are interested in understanding patient
recovery after a heart rate intervention, where we expect a variation in heart
rate over time. Therefore, we propose an online approach to tackle the concept
as time passes. We evaluate the methods on approximately 4 weeks of free living
data from three patients over a number of months. We show that our approach can
achieve good predictive performance (e.g., 2.89 Mean Absolute Error) while
using the PPG heart rate sensor infrequently (e.g., 20.25% of the samples)."
"Generative adversarial networks (GANs) are a class of deep generative models
which aim to learn a target distribution in an unsupervised fashion. While they
were successfully applied to many problems, training a GAN is a notoriously
challenging task and requires a significant number of hyperparameter tuning,
neural architecture engineering, and a non-trivial amount of ""tricks"". The
success in many practical applications coupled with the lack of a measure to
quantify the failure modes of GANs resulted in a plethora of proposed losses,
regularization and normalization schemes, as well as neural architectures. In
this work we take a sober view of the current state of GANs from a practical
perspective. We discuss and evaluate common pitfalls and reproducibility
issues, open-source our code on Github, and provide pre-trained models on
TensorFlow Hub."
"While research on adversarial examples in machine learning for images has
been prolific, similar attacks on deep learning (DL) for radio frequency (RF)
signals and their mitigation strategies are scarcely addressed in the published
work, with only one recent publication in the RF domain [1]. RF adversarial
examples (AdExs) can cause drastic, targeted misclassification results mostly
in spectrum sensing/ survey applications (e.g. BPSK mistaken for 8-PSK) with
minimal waveform perturbation. It is not clear if the RF AdExs maintain their
effects in the physical world, i.e., when AdExs are delivered over-the-air
(OTA). Our research on deep learning AdExs and proposed defense mechanisms are
RF-centric, and incorporate physical world, OTA effects. We here present
defense mechanisms based on statistical tests. One test to detect AdExs
utilizes Peak-to- Average-Power-Ratio (PAPR) of the DL data points delivered
OTA, while another statistical test uses the Softmax outputs of the DL
classifier, which corresponds to the probabilities the classifier assigns to
each of the trained classes. The former test leverages the RF nature of the
data, and the latter is universally applicable to AdExs regardless of their
origin. Both solutions are shown as viable mitigation methods to subvert
adversarial attacks against communications and radar sensing systems."
"We synthesize the knowledge present in various scientific disciplines for the
development of semiparametric endogenous truncation-proof algorithm, correcting
for truncation bias due to endogenous self-selection. This synthesis enriches
the algorithm's accuracy, efficiency and applicability. Improving upon the
covariate shift assumption, data are intrinsically affected and largely
generated by their own behavior (cognition). Refining the concept of Vox Populi
(Wisdom of Crowd) allows data points to sort themselves out depending on their
estimated latent reference group opinion space. Monte Carlo simulations, based
on 2,000,000 different distribution functions, practically generating 100
million realizations, attest to a very high accuracy of our model."
"Catastrophic forgetting can be a significant problem for institutions that
must delete historic data for privacy reasons. For example, hospitals might not
be able to retain patient data permanently. But neural networks trained on
recent data alone will tend to forget lessons learned on old data. We present a
differentially private continual learning framework based on variational
inference. We estimate the likelihood of past data given the current model
using differentially private generative models of old datasets."
"Most of the work on interpretable machine learning has focused on designing
either inherently interpretable models, which typically trade-off accuracy for
interpretability, or post-hoc explanation systems, whose explanation quality
can be unpredictable. Our method, ExpO, is a hybridization of these approaches
that regularizes a model for explanation quality at training time. Importantly,
these regularizers are differentiable, model agnostic, and require no domain
knowledge to define. We demonstrate that post-hoc explanations for
ExpO-regularized models have better explanation quality, as measured by the
common fidelity and stability metrics. We verify that improving these metrics
leads to significantly more useful explanations with a user study on a
realistic task."
"Building agents to interact with the web would allow for significant
improvements in knowledge understanding and representation learning. However,
web navigation tasks are difficult for current deep reinforcement learning (RL)
models due to the large discrete action space and the varying number of actions
between the states. In this work, we introduce DOM-Q-NET, a novel architecture
for RL-based web navigation to address both of these problems. It parametrizes
Q functions with separate networks for different action categories: clicking a
DOM element and typing a string input. Our model utilizes a graph neural
network to represent the tree-structured HTML of a standard web page. We
demonstrate the capabilities of our model on the MiniWoB environment where we
can match or outperform existing work without the use of expert demonstrations.
Furthermore, we show 2x improvements in sample efficiency when training in the
multi-task setting, allowing our model to transfer learned behaviours across
tasks."
"We address the problem of aggregating an ensemble of predictors with known
loss bounds in a semi-supervised binary classification setting, to minimize
prediction loss incurred on the unlabeled data. We find the minimax optimal
predictions for a very general class of loss functions including all convex and
many non-convex losses, extending a recent analysis of the problem for
misclassification error. The result is a family of semi-supervised ensemble
aggregation algorithms which are as efficient as linear learning by convex
optimization, but are minimax optimal without any relaxations. Their decision
rules take a form familiar in decision theory -- applying sigmoid functions to
a notion of ensemble margin -- without the assumptions typically made in
margin-based learning."
"We propose a new algorithm that uses an auxiliary neural network to express
the potential of the optimal transport map between two data distributions. In
the sequel, we use the aforementioned map to train generative networks. Unlike
WGANs, where the Euclidean distance is ${\it implicitly}$ used, this new method
allows to ${\it explicitly}$ use ${\it any}$ transportation cost function that
can be chosen to match the problem at hand. For example, it allows to use the
squared distance as a transportation cost function, giving rise to the
Wasserstein-2 metric for probability distributions, which results in fast and
stable gradient descends. It also allows to use image centered distances, like
the structure similarity index, with notable differences in the results."
"In this paper, we present a thorough evaluation of the efficacy of knowledge
distillation and its dependence on student and teacher architectures. Starting
with the observation that more accurate teachers often don't make good
teachers, we attempt to tease apart the factors that affect knowledge
distillation performance. We find crucially that larger models do not often
make better teachers. We show that this is a consequence of mismatched
capacity, and that small students are unable to mimic large teachers. We find
typical ways of circumventing this (such as performing a sequence of knowledge
distillation steps) to be ineffective. Finally, we show that this effect can be
mitigated by stopping the teacher's training early. Our results generalize
across datasets and models."
"While attention mechanisms have been proven to be effective in many NLP
tasks, majority of them are data-driven. We propose a novel knowledge-attention
encoder which incorporates prior knowledge from external lexical resources into
deep neural networks for relation extraction task. Furthermore, we present
three effective ways of integrating knowledge-attention with self-attention to
maximize the utilization of both knowledge and data. The proposed relation
extraction system is end-to-end and fully attention-based. Experiment results
show that the proposed knowledge-attention mechanism has complementary
strengths with self-attention, and our integrated models outperform existing
CNN, RNN, and self-attention based models. State-of-the-art performance is
achieved on TACRED, a complex and large-scale relation extraction dataset."
"Similarity measures based purely on word embeddings are comfortably competing
with much more sophisticated deep learning and expert-engineered systems on
unsupervised semantic textual similarity (STS) tasks. In contrast to commonly
used geometric approaches, we treat a single word embedding as e.g. 300
observations from a scalar random variable. Using this paradigm, we first
illustrate that similarities derived from elementary pooling operations and
classic correlation coefficients yield excellent results on standard STS
benchmarks, outperforming many recently proposed methods while being much
faster and trivial to implement. Next, we demonstrate how to avoid pooling
operations altogether and compare sets of word embeddings directly via
correlation operators between reproducing kernel Hilbert spaces. Just like
cosine similarity is used to compare individual word vectors, we introduce a
novel application of the centered kernel alignment (CKA) as a natural
generalisation of squared cosine similarity for sets of word vectors. Likewise,
CKA is very easy to implement and enjoys very strong empirical results."
"International trade policies have recently garnered attention for limiting
cross-border exchange of essential goods (e.g. steel, aluminum, soybeans, and
beef). Since trade critically affects employment and wages, predicting future
patterns of trade is a high-priority for policy makers around the world. While
traditional economic models aim to be reliable predictors, we consider the
possibility that Machine Learning (ML) techniques allow for better predictions
to inform policy decisions. Open-government data provide the fuel to power the
algorithms that can explain and forecast trade flows to inform policies. Data
collected in this article describe international trade transactions and
commonly associated economic factors. Machine learning (ML) models deployed
include: ARIMA, GBoosting, XGBoosting, and LightGBM for predicting future trade
patterns, and K-Means clustering of countries according to economic factors.
Unlike short-term and subjective (straight-line) projections and medium-term
(aggre-gated) projections, ML methods provide a range of data-driven and
interpretable projections for individual commodities. Models, their results,
and policies are introduced and evaluated for prediction quality."
"Accurate models of robots' dynamics are critical for control, stability,
motion optimization, and interaction. Semi-Parametric approaches to dynamics
learning combine physics-based Parametric models with unstructured
Non-Parametric regression with the hope to achieve both accuracy and
generalizablity. In this paper we highlight the non-stationary problem created
when attempting to adapt both Parametric and Non-Parametric components
simultaneously. We present a consistency transform designed to compensate for
this non-stationary effect, such that the contributions of both models can
adapt simultaneously without adversely affecting the performance of the
platform. Thus we are able to apply the Semi-Parametric learning approach for
continuous iterative online adaptation, without relying on batch or offline
updates. We validate the transform via a perfect virtual model as well as by
applying the overall system on a Kuka LWR IV manipulator. We demonstrate
improved tracking performance during online learning and show a clear
transference of contribution between the two components with a learning bias
towards the Parametric component."
"Although most current license plate (LP) recognition applications have been
significantly advanced, they are still limited to ideal environments where
training data are carefully annotated with constrained scenes. In this paper,
we propose a novel license plate recognition method to handle unconstrained
real world traffic scenes. To overcome these difficulties, we use adversarial
super-resolution (SR), and one-stage character segmentation and recognition.
Combined with a deep convolutional network based on VGG-net, our method
provides simple but reasonable training procedure. Moreover, we introduce
GIST-LP, a challenging LP dataset where image samples are effectively collected
from unconstrained surveillance scenes. Experimental results on AOLP and
GIST-LP dataset illustrate that our method, without any scene-specific
adaptation, outperforms current LP recognition approaches in accuracy and
provides visual enhancement in our SR results that are easier to understand
than original data."
"The motion planners used in self-driving vehicles need to generate
trajectories that are safe, comfortable, and obey the traffic rules. This is
usually achieved by two modules: behavior planner, which handles high-level
decisions and produces a coarse trajectory, and trajectory planner that
generates a smooth, feasible trajectory for the duration of the planning
horizon. These planners, however, are typically developed separately, and
changes in the behavior planner might affect the trajectory planner in
unexpected ways. Furthermore, the final trajectory outputted by the trajectory
planner might differ significantly from the one generated by the behavior
planner, as they do not share the same objective. In this paper, we propose a
jointly learnable behavior and trajectory planner. Unlike most existing
learnable motion planners that address either only behavior planning, or use an
uninterpretable neural network to represent the entire logic from sensors to
driving commands, our approach features an interpretable cost function on top
of perception, prediction and vehicle dynamics, and a joint learning algorithm
that learns a shared cost function employed by our behavior and trajectory
components. Experiments on real-world self-driving data demonstrate that
jointly learned planner performs significantly better in terms of both
similarity to human driving and other safety metrics, compared to baselines
that do not adopt joint behavior and trajectory learning."
"Most methods for time series classification that attain state-of-the-art
accuracy have high computational complexity, requiring significant training
time even for smaller datasets, and are intractable for larger datasets.
Additionally, many existing methods focus on a single type of feature such as
shape or frequency. Building on the recent success of convolutional neural
networks for time series classification, we show that simple linear classifiers
using random convolutional kernels achieve state-of-the-art accuracy with a
fraction of the computational expense of existing methods."
"Recent work by Brock et al. (2018) suggests that Generative Adversarial
Networks (GANs) benefit disproportionately from large mini-batch sizes.
Unfortunately, using large batches is slow and expensive on conventional
hardware. Thus, it would be nice if we could generate batches that were
effectively large though actually small. In this work, we propose a method to
do this, inspired by the use of Coreset-selection in active learning. When
training a GAN, we draw a large batch of samples from the prior and then
compress that batch using Coreset-selection. To create effectively large
batches of 'real' images, we create a cached dataset of Inception activations
of each training image, randomly project them down to a smaller dimension, and
then use Coreset-selection on those projected activations at training time. We
conduct experiments showing that this technique substantially reduces training
time and memory usage for modern GAN variants, that it reduces the fraction of
dropped modes in a synthetic dataset, and that it allows GANs to reach a new
state of the art in anomaly detection."
"We introduce a framework for automatic differentiation with weighted
finite-state transducers (WFSTs) allowing them to be used dynamically at
training time. Through the separation of graphs from operations on graphs, this
framework enables the exploration of new structured loss functions which in
turn eases the encoding of prior knowledge into learning algorithms. We show
how the framework can combine pruning and back-off in transition models with
various sequence-level loss functions. We also show how to learn over the
latent decomposition of phrases into word pieces. Finally, to demonstrate that
WFSTs can be used in the interior of a deep neural network, we propose a
convolutional WFST layer which maps lower-level representations to higher-level
representations and can be used as a drop-in replacement for a traditional
convolution. We validate these algorithms with experiments in handwriting
recognition and speech recognition."
"In object detection with deep neural networks, the box-wise objectness score
tends to be overconfident, sometimes even indicating high confidence in
presence of inaccurate predictions. Hence, the reliability of the prediction
and therefore reliable uncertainties are of highest interest. In this work, we
present a post processing method that for any given neural network provides
predictive uncertainty estimates and quality estimates. These estimates are
learned by a post processing model that receives as input a hand-crafted set of
transparent metrics in form of a structured dataset. Therefrom, we learn two
tasks for predicted bounding boxes. We discriminate between true positives
($\mathit{IoU}\geq0.5$) and false positives ($\mathit{IoU} < 0.5$) which we
term meta classification, and we predict $\mathit{IoU}$ values directly which
we term meta regression. The probabilities of the meta classification model aim
at learning the probabilities of success and failure and therefore provide a
modelled predictive uncertainty estimate. On the other hand, meta regression
gives rise to a quality estimate. In numerical experiments, we use the publicly
available YOLOv3 network and the Faster-RCNN network and evaluate meta
classification and regression performance on the Kitti, Pascal VOC and COCO
datasets. We demonstrate that our metrics are indeed well correlated with the
$\mathit{IoU}$. For meta classification we obtain classification accuracies of
up to 98.92% and AUROCs of up to 99.93%. For meta regression we obtain an $R^2$
value of up to 91.78%. These results yield significant improvements compared to
other network's objectness score and other baseline approaches. Therefore, we
obtain more reliable uncertainty and quality estimates which is particularly
interesting in the absence of ground truth."
"A success factor for modern companies in the age of Digital Marketing is to
understand how customers think and behave based on their online shopping
patterns. While the conventional method of gathering consumer insights through
questionnaires and surveys still form the bases of descriptive analytics for
market intelligence units, we propose a machine learning framework to automate
this process. In this paper we present a modular consumer data analysis
platform that processes session level interaction records between users and
products to predict session level, user journey level and customer behavior
specific patterns leading towards purchase events. We explore the computational
framework and provide test results on two Big data sets-cosmetics and consumer
electronics of size 2GB and 15GB, respectively. The proposed system achieves
97-99% classification accuracy and recall for user-journey level purchase
predictions and categorizes buying behavior into 5 clusters with increasing
purchase ratios for both data sets. Thus, the proposed framework is extendable
to other large e-commerce data sets to obtain automated purchase predictions
and descriptive consumer insights."
"Scaling model-based inverse reinforcement learning (IRL) to real robotic
manipulation tasks with unknown dynamics remains an open problem. The key
challenges lie in learning good dynamics models, developing algorithms that
scale to high-dimensional state-spaces and being able to learn from both visual
and proprioceptive demonstrations. In this work, we present a gradient-based
inverse reinforcement learning framework that utilizes a pre-trained visual
dynamics model to learn cost functions when given only visual human
demonstrations. The learned cost functions are then used to reproduce the
demonstrated behavior via visual model predictive control. We evaluate our
framework on hardware on two basic object manipulation tasks."
"Many real-life problems are represented as a black-box, i.e., the internal
workings are inaccessible or a closed-form mathematical expression of the
likelihood function cannot be defined. For continuous random variables
likelihood-free inference problems can be solved by a group of methods under
the name of Approximate Bayesian Computation (ABC). However, a similar approach
for discrete random variables is yet to be formulated. Here, we aim to fill
this research gap. We propose to use a population-based MCMC ABC framework.
Further, we present a valid Markov kernel, and propose a new kernel that is
inspired by Differential Evolution. We assess the proposed approach on a
problem with the known likelihood function, namely, discovering the underlying
diseases based on a QMR-DT Network, and three likelihood-free inference
problems: (i) the QMR-DT Network with the unknown likelihood function, (ii)
learning binary neural network, and (iii) Neural Architecture Search. The
obtained results indicate the high potential of the proposed framework and the
superiority of the new Markov kernel."
"Among existing uncertainty estimation approaches, Dirichlet Prior Network
(DPN) distinctly models different predictive uncertainty types. However, for
in-domain examples with high data uncertainties among multiple classes, even a
DPN model often produces indistinguishable representations from the
out-of-distribution (OOD) examples, compromising their OOD detection
performance. We address this shortcoming by proposing a novel loss function for
DPN to maximize the \textit{representation gap} between in-domain and OOD
examples. Experimental results demonstrate that our proposed approach
consistently improves OOD detection performance."
"In this paper, we propose a simple yet effective method to represent point
clouds as sets of samples drawn from a cloud-specific probability distribution.
This interpretation matches intrinsic characteristics of point clouds: the
number of points and their ordering within a cloud is not important as all
points are drawn from the proximity of the object boundary. We postulate to
represent each cloud as a parameterized probability distribution defined by a
generative neural network. Once trained, such a model provides a natural
framework for point cloud manipulation operations, such as aligning a new cloud
into a default spatial orientation. To exploit similarities between same-class
objects and to improve model performance, we turn to weight sharing: networks
that model densities of points belonging to objects in the same family share
all parameters with the exception of a small, object-specific embedding vector.
We show that these embedding vectors capture semantic relationships between
objects. Our method leverages generative invertible flow networks to learn
embeddings as well as to generate point clouds. Thanks to this formulation and
contrary to similar approaches, we are able to train our model in an end-to-end
fashion. As a result, our model offers competitive or superior quantitative
results on benchmark datasets, while enabling unprecedented capabilities to
perform cloud manipulation tasks, such as point cloud registration and
regeneration, by a generative network."
"We consider off-policy evaluation (OPE) in continuous treatment settings,
such as personalized dose-finding. In OPE, one aims to estimate the mean
outcome under a new treatment decision rule using historical data generated by
a different decision rule. Most existing works on OPE focus on discrete
treatment settings. To handle continuous treatments, we develop a novel
estimation method for OPE using deep jump learning. The key ingredient of our
method lies in adaptively discretizing the treatment space using deep
discretization, by leveraging deep learning and multi-scale change point
detection. This allows us to apply existing OPE methods in discrete treatments
to handle continuous treatments. Our method is further justified by theoretical
results, simulations, and a real application to Warfarin Dosing."
"Contrastive learning (CL) has been successful as a powerful representation
learning method. In this work we propose CLIM: Contrastive Learning with mutual
Information Maximization, to explore the potential of CL on cross-domain
sentiment classification. To the best of our knowledge, CLIM is the first to
adopt contrastive learning for natural language processing (NLP) tasks across
domains. Due to scarcity of labels on the target domain, we introduce mutual
information maximization (MIM) apart from CL to exploit the features that best
support the final prediction. Furthermore, MIM is able to maintain a relatively
balanced distribution of the model's prediction, and enlarges the margin
between classes on the target domain. The larger margin increases our model's
robustness and enables the same classifier to be optimal across domains.
Consequently, we achieve new state-of-the-art results on the Amazon-review
dataset as well as the airlines dataset, showing the efficacy of our proposed
method CLIM."
"To alleviate human efforts from obtaining large-scale annotations,
Semi-Supervised Relation Extraction methods aim to leverage unlabeled data in
addition to learning from limited samples. Existing self-training methods
suffer from the gradual drift problem, where noisy pseudo labels on unlabeled
data are incorporated during training. To alleviate the noise in pseudo labels,
we propose a method called MetaSRE, where a Relation Label Generation Network
generates quality assessment on pseudo labels by (meta) learning from the
successful and failed attempts on Relation Classification Network as an
additional meta-objective. To reduce the influence of noisy pseudo labels,
MetaSRE adopts a pseudo label selection and exploitation scheme which assesses
pseudo label quality on unlabeled samples and only exploits high-quality pseudo
labels in a self-training fashion to incrementally augment labeled samples for
both robustness and accuracy. Experimental results on two public datasets
demonstrate the effectiveness of the proposed approach."
"This paper presents a new method for pre-training neural networks that can
decrease the total training time for a neural network while maintaining the
final performance, which motivates its use on deep neural networks. By
partitioning the training task in multiple training subtasks with sub-models,
which can be performed independently and in parallel, it is shown that the size
of the sub-models reduces almost quadratically with the number of subtasks
created, quickly scaling down the sub-models used for the pre-training. The
sub-models are then merged to provide a pre-trained initial set of weights for
the original model. The proposed method is independent of the other aspects of
the training, such as architecture of the neural network, training method, and
objective, making it compatible with a wide range of existing approaches. The
speedup without loss of performance is validated experimentally on MNIST and on
CIFAR10 data sets, also showing that even performing the subtasks sequentially
can decrease the training time. Moreover, we show that larger models may
present higher speedups and conjecture about the benefits of the method in
distributed learning systems."
"We propose a soft attention based model for the task of action recognition in
videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long
Short-Term Memory (LSTM) units which are deep both spatially and temporally.
Our model learns to focus selectively on parts of the video frames and
classifies videos after taking a few glimpses. The model essentially learns
which parts in the frames are relevant for the task at hand and attaches higher
importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51
and Hollywood2 datasets and analyze how the model focuses its attention
depending on the scene and the action being performed."
"The benefits of automating design cycles for Bayesian inference-based
algorithms are becoming increasingly recognized by the machine learning
community. As a result, interest in probabilistic programming frameworks has
much increased over the past few years. This paper explores a specific
probabilistic programming paradigm, namely message passing in Forney-style
factor graphs (FFGs), in the context of automated design of efficient Bayesian
signal processing algorithms. To this end, we developed ""ForneyLab""
(https://github.com/biaslab/ForneyLab.jl) as a Julia toolbox for message
passing-based inference in FFGs. We show by example how ForneyLab enables
automatic derivation of Bayesian signal processing algorithms, including
algorithms for parameter estimation and model comparison. Crucially, due to the
modular makeup of the FFG framework, both the model specification and inference
methods are readily extensible in ForneyLab. In order to test this framework,
we compared variational message passing as implemented by ForneyLab with
automatic differentiation variational inference (ADVI) and Monte Carlo methods
as implemented by state-of-the-art tools ""Edward"" and ""Stan"". In terms of
performance, extensibility and stability issues, ForneyLab appears to enjoy an
edge relative to its competitors for automated inference in state-space models."
"Plug-and-play priors (PnP) is a popular framework for regularized signal
reconstruction by using advanced denoisers within an iterative algorithm. In
this paper, we discuss our recent online variant of PnP that uses only a subset
of measurements at every iteration, which makes it scalable to very large
datasets. We additionally present novel convergence results for both batch and
online PnP algorithms."
"We introduce a novel framework for the estimation of the posterior
distribution over the weights of a neural network, based on a new probabilistic
interpretation of adaptive optimisation algorithms such as AdaGrad and Adam. We
demonstrate the effectiveness of our Bayesian Adam method, Badam, by
experimentally showing that the learnt uncertainties correctly relate to the
weights' predictive capabilities by weight pruning. We also demonstrate the
quality of the derived uncertainty measures by comparing the performance of
Badam to standard methods in a Thompson sampling setting for multi-armed
bandits, where good uncertainty measures are required for an agent to balance
exploration and exploitation."
"Deep Learning is moving to edge devices, ushering in a new age of distributed
Artificial Intelligence (AI). The high demand of computational resources
required by deep neural networks may be alleviated by approximate computing
techniques, and most notably reduced-precision arithmetic with coarsely
quantized numerical representations. In this context, Bonseyes comes in as an
initiative to enable stakeholders to bring AI to low-power and autonomous
environments such as: Automotive, Medical Healthcare and Consumer Electronics.
To achieve this, we introduce LPDNN, a framework for optimized deployment of
Deep Neural Networks on heterogeneous embedded devices. In this work, we detail
the quantization engine that is integrated in LPDNN. The engine depends on a
fine-grained workflow which enables a Neural Network Design Exploration and a
sensitivity analysis of each layer for quantization. We demonstrate the engine
with a case study on Alexnet and VGG16 for three different techniques for
direct quantization: standard fixed-point, dynamic fixed-point and k-means
clustering, and demonstrate the potential of the latter. We argue that using a
Gaussian quantizer with k-means clustering can achieve better performance than
linear quantizers. Without retraining, we achieve over 55.64\% saving for
weights' storage and 69.17\% for run-time memory accesses with less than 1\%
drop in top5 accuracy in Imagenet."
"The verification problem for neural networks is verifying whether a neural
network will suffer from adversarial samples, or approximating the maximal
allowed scale of adversarial perturbation that can be endured. While most prior
work contributes to verifying feed-forward networks, little has been explored
for verifying recurrent networks. This is due to the existence of a more
rigorous constraint on the perturbation space for sequential data, and the lack
of a proper metric for measuring the perturbation. In this work, we address
these challenges by proposing a metric which measures the distance between
strings, and use deterministic finite automata (DFA) to represent a rigorous
oracle which examines if the generated adversarial samples violate certain
constraints on a perturbation. More specifically, we empirically show that
certain recurrent networks allow relatively stable DFA extraction. As such,
DFAs extracted from these recurrent networks can serve as a surrogate oracle
for when the ground truth DFA is unknown. We apply our verification mechanism
to several widely used recurrent networks on a set of the Tomita grammars. The
results demonstrate that only a few models remain robust against adversarial
samples. In addition, we show that for grammars with different levels of
complexity, there is also a difference in the difficulty of robust learning of
these grammars."
"Deep learning involves a difficult non-convex optimization problem, which is
often solved by stochastic gradient (SG) methods. While SG is usually
effective, it may not be robust in some situations. Recently, Newton methods
have been investigated as an alternative optimization technique, but nearly all
existing studies consider only fully-connected feedforward neural networks.
They do not investigate other types of networks such as Convolutional Neural
Networks (CNN), which are more commonly used in deep-learning applications. One
reason is that Newton methods for CNN involve complicated operations, and so
far no works have conducted a thorough investigation. In this work, we give
details of all building blocks including function, gradient, and Jacobian
evaluation, and Gauss-Newton matrix-vector products. These basic components are
very important because with them further developments of Newton methods for CNN
become possible. We show that an efficient MATLAB implementation can be done in
just several hundred lines of code and demonstrate that the Newton method gives
competitive test accuracy."
"We propose a modification that corrects for split-improvement variable
importance measures in Random Forests and other tree-based methods. These
methods have been shown to be biased towards increasing the importance of
features with more potential splits. We show that by appropriately
incorporating split-improvement as measured on out of sample data, this bias
can be corrected yielding better summaries and screening tools."
"How to obtain a model with good interpretability and performance has always
been an important research topic. In this paper, we propose rectified decision
trees (ReDT), a knowledge distillation based decision trees rectification with
high interpretability, small model size, and empirical soundness. Specifically,
we extend the impurity calculation and the pure ending condition of the
classical decision tree to propose a decision tree extension that allows the
use of soft labels generated by a well-trained teacher model in training and
prediction process. It is worth noting that for the acquisition of soft labels,
we propose a new multiple cross-validation based method to reduce the effects
of randomness and overfitting. These approaches ensure that ReDT retains
excellent interpretability and even achieves fewer nodes than the decision tree
in the aspect of compression while having relatively good performance. Besides,
in contrast to traditional knowledge distillation, back propagation of the
student model is not necessarily required in ReDT, which is an attempt of a new
knowledge distillation approach. Extensive experiments are conducted, which
demonstrates the superiority of ReDT in interpretability, compression, and
empirical soundness."
"In this paper, we propose a method for image-set classification based on
convex cone models. Image set classification aims to classify a set of images,
which were usually obtained from video frames or multi-view cameras, into a
target object. To accurately and stably classify a set, it is essential to
represent structural information of the set accurately. There are various
representative image features, such as histogram based features, HLAC, and
Convolutional Neural Network (CNN) features. We should note that most of them
have non-negativity and thus can be effectively represented by a convex cone.
This leads us to introduce the convex cone representation to image-set
classification. To establish a convex cone based framework, we mathematically
define multiple angles between two convex cones, and then define the geometric
similarity between the cones using the angles. Moreover, to enhance the
framework, we introduce a discriminant space that maximizes the between-class
variance (gaps) and minimizes the within-class variance of the projected convex
cones onto the discriminant space, similar to the Fisher discriminant analysis.
Finally, the classification is performed based on the similarity between
projected convex cones. The effectiveness of the proposed method is
demonstrated experimentally by using five databases: CMU PIE dataset, ETH-80,
CMU Motion of Body dataset, Youtube Celebrity dataset, and a private database
of multi-view hand shapes."
"In the present scenario of domestic flights in USA, there have been numerous
instances of flight delays and cancellations. In the United States, the
American Airlines, Inc. have been one of the most entrusted and the world's
largest airline in terms of number of destinations served. But when it comes to
domestic flights, AA has not lived up to the expectations in terms of
punctuality or on-time performance. Flight Delays also result in airline
companies operating commercial flights to incur huge losses. So, they are
trying their best to prevent or avoid Flight Delays and Cancellations by taking
certain measures. This study aims at analyzing flight information of US
domestic flights operated by American Airlines, covering top 5 busiest airports
of US and predicting possible arrival delay of the flight using Data Mining and
Machine Learning Approaches. The Gradient Boosting Classifier Model is deployed
by training and hyper-parameter tuning it, achieving a maximum accuracy of
85.73%. Such an Intelligent System is very essential in foretelling
flights'on-time performance."
"Modelling sequential music skips provides streaming companies the ability to
better understand the needs of the user base, resulting in a better user
experience by reducing the need to manually skip certain music tracks. This
paper describes the solution of the University of Copenhagen DIKU-IR team in
the 'Spotify Sequential Skip Prediction Challenge', where the task was to
predict the skip behaviour of the second half in a music listening session
conditioned on the first half. We model this task using a Multi-RNN approach
consisting of two distinct stacked recurrent neural networks, where one network
focuses on encoding the first half of the session and the other network focuses
on utilizing the encoding to make sequential skip predictions. The encoder
network is initialized by a learned session-wide music encoding, and both of
them utilize a learned track embedding. Our final model consists of a majority
voted ensemble of individually trained models, and ranked 2nd out of 45
participating teams in the competition with a mean average accuracy of 0.641
and an accuracy on the first skip prediction of 0.807. Our code is released at
https://github.com/Varyn/WSDM-challenge-2019-spotify."
"The CLEVR dataset of natural-looking questions about 3D-rendered scenes has
recently received much attention from the research community. A number of
models have been proposed for this task, many of which achieved very high
accuracies of around 97-99%. In this work, we study how systematic the
generalization of such models is, that is to which extent they are capable of
handling novel combinations of known linguistic constructs. To this end, we
test models' understanding of referring expressions based on matching object
properties (such as e.g. ""another cube that is the same size as the brown
cube"") in novel contexts. Our experiments on the thereby constructed CLOSURE
benchmark show that state-of-the-art models often do not exhibit systematicity
after being trained on CLEVR. Surprisingly, we find that an explicitly
compositional Neural Module Network model also generalizes badly on CLOSURE,
even when it has access to the ground-truth programs at test time. We improve
the NMN's systematic generalization by developing a novel Vector-NMN module
architecture with vector-valued inputs and outputs. Lastly, we investigate how
much few-shot transfer learning can help models that are pretrained on CLEVR to
adapt to CLOSURE. Our few-shot learning experiments contrast the adaptation
behavior of the models with intermediate discrete programs with that of the
end-to-end continuous models."
"We introduce a new high resolution, high frame rate stereo video dataset,
which we call SPIN, for tracking and action recognition in the game of ping
pong. The corpus consists of ping pong play with three main annotation streams
that can be used to learn tracking and action recognition models -- tracking of
the ping pong ball and poses of humans in the videos and the spin of the ball
being hit by humans. The training corpus consists of 53 hours of data with
labels derived from previous models in a semi-supervised method. The testing
corpus contains 1 hour of data with the same information, except that crowd
compute was used to obtain human annotations of the ball position, from which
ball spin has been derived. Along with the dataset we introduce several
baseline models that were trained on this data. The models were specifically
chosen to be able to perform inference at the same rate as the images are
generated -- specifically 150 fps. We explore the advantages of multi-task
training on this data, and also show interesting properties of ping pong ball
trajectories that are derived from our observational data, rather than from
prior physics models. To our knowledge this is the first large scale dataset of
ping pong; we offer it to the community as a rich dataset that can be used for
a large variety of machine learning and vision tasks such as tracking, pose
estimation, semi-supervised and unsupervised learning and generative modeling."
"A simple approach to obtaining uncertainty-aware neural networks for
regression is to do Bayesian linear regression (BLR) on the representation from
the last hidden layer. Recent work [Riquelme et al., 2018, Azizzadenesheli et
al., 2018] indicates that the method is promising, though it has been limited
to homoscedastic noise. In this paper, we propose a novel variation that
enables the method to flexibly model heteroscedastic noise. The method is
benchmarked against two prominent alternative methods on a set of standard
datasets, and finally evaluated as an uncertainty-aware model in model-based
reinforcement learning. Our experiments indicate that the method is competitive
with standard ensembling, and ensembles of BLR outperforms the methods we
compared to."
"Global Autoregressive Models (GAMs) are a recent proposal [Parshakova et al.,
CoNLL 2019] for exploiting global properties of sequences for data-efficient
learning of seq2seq models. In the first phase of training, an Energy-Based
model (EBM) over sequences is derived. This EBM has high representational
power, but is unnormalized and cannot be directly exploited for sampling. To
address this issue [Parshakova et al., CoNLL 2019] proposes a distillation
technique, which can only be applied under limited conditions. By relating this
problem to Policy Gradient techniques in RL, but in a \emph{distributional}
rather than \emph{optimization} perspective, we propose a general approach
applicable to any sequential EBM. Its effectiveness is illustrated on GAM-based
experiments."
"In this paper, we present our position for a neuralsymbolic integration
strategy, arguing in favor of a hybrid representation to promote an effective
integration. Such description differs from others fundamentally, since its
entities aim at representing AI models in general, allowing to describe both
nonsymbolic and symbolic knowledge, the integration between them and their
corresponding processors. Moreover, the entities also support representing
workflows, leveraging traceability to keep track of every change applied to
models and their related entities (e.g., data or concepts) throughout the
lifecycle of the models."
"We study the best-arm identification problem in multi-armed bandits with
stochastic, potentially private rewards, when the goal is to identify the arm
with the highest quantile at a fixed, prescribed level. First, we propose a
(non-private) successive elimination algorithm for strictly optimal best-arm
identification, we show that our algorithm is $\delta$-PAC and we characterize
its sample complexity. Further, we provide a lower bound on the expected number
of pulls, showing that the proposed algorithm is essentially optimal up to
logarithmic factors. Both upper and lower complexity bounds depend on a special
definition of the associated suboptimality gap, designed in particular for the
quantile bandit problem, as we show when the gap approaches zero, best-arm
identification is impossible. Second, motivated by applications where the
rewards are private, we provide a differentially private successive elimination
algorithm whose sample complexity is finite even for distributions with
infinite support-size, and we characterize its sample complexity. Our
algorithms do not require prior knowledge of either the suboptimality gap or
other statistical information related to the bandit problem at hand."
"Session-based recommendation (SR) has become an important and popular
component of various e-commerce platforms, which aims to predict the next
interacted item based on a given session. Most of existing SR models only focus
on exploiting the consecutive items in a session interacted by a certain user,
to capture the transition pattern among the items. Although some of them have
been proven effective, the following two insights are often neglected. First, a
user's micro-behaviors, such as the manner in which the user locates an item,
the activities that the user commits on an item (e.g., reading comments, adding
to cart), offer fine-grained and deep understanding of the user's preference.
Second, the item attributes, also known as item knowledge, provide side
information to model the transition pattern among interacted items and
alleviate the data sparsity problem. These insights motivate us to propose a
novel SR model MKM-SR in this paper, which incorporates user Micro-behaviors
and item Knowledge into Multi-task learning for Session-based Recommendation.
Specifically, a given session is modeled on micro-behavior level in MKM-SR,
i.e., with a sequence of item-operation pairs rather than a sequence of items,
to capture the transition pattern in the session sufficiently. Furthermore, we
propose a multi-task learning paradigm to involve learning knowledge embeddings
which plays a role as an auxiliary task to promote the major task of SR. It
enables our model to obtain better session representations, resulting in more
precise SR recommendation results. The extensive evaluations on two benchmark
datasets demonstrate MKM-SR's superiority over the state-of-the-art SR models,
justifying the strategy of incorporating knowledge learning."
"Learning with the \textit{instance-dependent} label noise is challenging,
because it is hard to model such real-world noise. Note that there are
psychological and physiological evidences showing that we humans perceive
instances by decomposing them into parts. Annotators are therefore more likely
to annotate instances based on the parts rather than the whole instances, where
a wrong mapping from parts to classes may cause the instance-dependent label
noise. Motivated by this human cognition, in this paper, we approximate the
instance-dependent label noise by exploiting \textit{part-dependent} label
noise. Specifically, since instances can be approximately reconstructed by a
combination of parts, we approximate the instance-dependent \textit{transition
matrix} for an instance by a combination of the transition matrices for the
parts of the instance. The transition matrices for parts can be learned by
exploiting anchor points (i.e., data points that belong to a specific class
almost surely). Empirical evaluations on synthetic and real-world datasets
demonstrate our method is superior to the state-of-the-art approaches for
learning from the instance-dependent label noise."
"The identification and analysis of student satisfaction is a challenging
issue. This is becoming increasingly important since a measure of student
satisfaction is taken as an indication of how well a course has been taught.
However, it remains a challenging problem as student satisfaction has various
aspects. In this paper, we formulate the student satisfaction estimation as a
prediction problem where we predict different levels of student satisfaction
and infer the influential predictors related to course and instructor. We
present five different aspects of student satisfaction in terms of 1) course
content, 2) class participation, 3) achievement of initial expectations about
the course, 4) relevancy towards professional development, and 5) if the course
connects them and helps to explore the real-world situations. We employ
state-of-the-art machine learning techniques to predict each of these aspects
of student satisfaction levels. For our experiment, we utilize a large student
evaluation dataset which includes student perception using different attributes
related to courses and the instructors. Our experimental results and
comprehensive analysis reveal that student satisfaction is more influenced by
course attributes in comparison to instructor related attributes."
"Detecting out-of-distribution (OOD) data is crucial for robust machine
learning systems. Normalizing flows are flexible deep generative models that
often surprisingly fail to distinguish between in- and out-of-distribution
data: a flow trained on pictures of clothing assigns higher likelihood to
handwritten digits. We investigate why normalizing flows perform poorly for OOD
detection. We demonstrate that flows learn local pixel correlations and generic
image-to-latent-space transformations which are not specific to the target
image dataset. We show that by modifying the architecture of flow coupling
layers we can bias the flow towards learning the semantic structure of the
target data, improving OOD detection. Our investigation reveals that properties
that enable flows to generate high-fidelity images can have a detrimental
effect on OOD detection."
"Clinical trials play important roles in drug development but often suffer
from expensive, inaccurate and insufficient patient recruitment. The
availability of massive electronic health records (EHR) data and trial
eligibility criteria (EC) bring a new opportunity to data driven patient
recruitment. One key task named patient-trial matching is to find qualified
patients for clinical trials given structured EHR and unstructured EC text
(both inclusion and exclusion criteria). How to match complex EC text with
longitudinal patient EHRs? How to embed many-to-many relationships between
patients and trials? How to explicitly handle the difference between inclusion
and exclusion criteria? In this paper, we proposed CrOss-Modal PseudO-SiamEse
network (COMPOSE) to address these challenges for patient-trial matching. One
path of the network encodes EC using convolutional highway network. The other
path processes EHR with multi-granularity memory network that encodes
structured patient records into multiple levels based on medical ontology.
Using the EC embedding as query, COMPOSE performs attentional record alignment
and thus enables dynamic patient-trial matching. COMPOSE also introduces a
composite loss term to maximize the similarity between patient records and
inclusion criteria while minimize the similarity to the exclusion criteria.
Experiment results show COMPOSE can reach 98.0% AUC on patient-criteria
matching and 83.7% accuracy on patient-trial matching, which leads 24.3%
improvement over the best baseline on real-world patient-trial matching tasks."
"While theoretically appealing, the application of the Wasserstein distance to
large-scale machine learning problems has been hampered by its prohibitive
computational cost. The sliced Wasserstein distance and its variants improve
the computational efficiency through the random projection, yet they suffer
from low accuracy if the number of projections is not sufficiently large,
because the majority of projections result in trivially small values. In this
work, we propose a new family of distance metrics, called augmented sliced
Wasserstein distances (ASWDs), constructed by first mapping samples to
higher-dimensional hypersurfaces parameterized by neural networks. It is
derived from a key observation that (random) linear projections of samples
residing on these hypersurfaces would translate to much more flexible nonlinear
projections in the original sample space, so they can capture complex
structures of the data distribution. We show that the hypersurfaces can be
optimized by gradient ascent efficiently. We provide the condition under which
the ASWD is a valid metric and show that this can be obtained by an injective
neural network architecture. Numerical results demonstrate that the ASWD
significantly outperforms other Wasserstein variants for both synthetic and
real-world problems."
"Complex black-box machine learning models are regularly used in critical
decision-making domains. This has given rise to several calls for algorithmic
explainability. Many explanation algorithms proposed in literature assign
importance to each feature individually. However, such explanations fail to
capture the joint effects of sets of features. Indeed, few works so far
formally analyze high-dimensional model explanations. In this paper, we propose
a novel high dimension model explanation method that captures the joint effect
of feature subsets.
  We propose a new axiomatization for a generalization of the Banzhaf index;
our method can also be thought of as an approximation of a black-box model by a
higher-order polynomial. In other words, this work justifies the use of the
generalized Banzhaf index as a model explanation by showing that it uniquely
satisfies a set of natural desiderata and that it is the optimal local
approximation of a black-box model.
  Our empirical evaluation of our measure highlights how it manages to capture
desirable behavior, whereas other measures that do not satisfy our axioms
behave in an unpredictable manner."
"The task of subgroup discovery (SD) is to find interpretable descriptions of
subsets of a dataset that stand out with respect to a target attribute. To
address the problem of mining large numbers of redundant subgroups, subgroup
set discovery (SSD) has been proposed. State-of-the-art SSD methods have their
limitations though, as they typically heavily rely on heuristics and/or
user-chosen hyperparameters.
  We propose a dispersion-aware problem formulation for subgroup set discovery
that is based on the minimum description length (MDL) principle and subgroup
lists. We argue that the best subgroup list is the one that best summarizes the
data given the overall distribution of the target. We restrict our focus to a
single numeric target variable and show that our formalization coincides with
an existing quality measure when finding a single subgroup, but that-in
addition-it allows to trade off subgroup quality with the complexity of the
subgroup. We next propose SSD++, a heuristic algorithm for which we empirically
demonstrate that it returns outstanding subgroup lists: non-redundant sets of
compact subgroups that stand out by having strongly deviating means and small
spread."
"This paper presents a framework of successive functional gradient
optimization for training nonconvex models such as neural networks, where
training is driven by mirror descent in a function space. We provide a
theoretical analysis and empirical study of the training method derived from
this framework. It is shown that the method leads to better performance than
that of standard training techniques."
"Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning
of hyperparameters to successfully resolve problems. One of the most
influential parameters in optimization procedures based on stochastic gradient
descent (SGD) is the learning rate. We investigate cyclical learning and
propose a method for defining a general cyclical learning rate for various DRL
problems. In this paper we present a method for cyclical learning applied to
complex DRL problems. Our experiments show that, utilizing cyclical learning
achieves similar or even better results than highly tuned fixed learning rates.
This paper presents the first application of cyclical learning rates in DRL
settings and is a step towards overcoming manual hyperparameter tuning."
"The current practice of manually processing features for high-dimensional and
heterogeneous aviation data is labor-intensive, does not scale well to new
problems, and is prone to information loss, affecting the effectiveness and
maintainability of machine learning (ML) procedures. This research explored an
unsupervised learning method, autoencoder, to extract effective features for
aviation machine learning problems. The study explored variants of autoencoders
with the aim of forcing the learned representations of the input to assume
useful properties. A flight track anomaly detection autoencoder was developed
to demonstrate the versatility of the technique. The research results show that
the autoencoder can not only automatically extract effective features for the
flight track data, but also efficiently deep clean data, thereby reducing the
workload of data scientists. Moreover, the research leveraged transfer learning
to efficiently train models for multiple airports. Transfer learning can reduce
model training times from days to hours, as well as improving model
performance. The developed applications and techniques are shared with the
whole aviation community to improve effectiveness of ongoing and future machine
learning studies."
"Decentralized optimization methods enable on-device training of machine
learning models without a central coordinator. In many scenarios communication
between devices is energy demanding and time consuming and forms the bottleneck
of the entire system.
  We propose a new randomized first-order method which tackles the
communication bottleneck by applying randomized compression operators to the
communicated messages. By combining our scheme with a new variance reduction
technique that progressively throughout the iterations reduces the adverse
effect of the injected quantization noise, we obtain the first scheme that
converges linearly on strongly convex decentralized problems while using
compressed communication only.
  We prove that our method can solve the problems without any increase in the
number of communications compared to the baseline which does not perform any
communication compression while still allowing for a significant compression
factor which depends on the conditioning of the problem and the topology of the
network. Our key theoretical findings are supported by numerical experiments."
"A limitation of model-based reinforcement learning (MBRL) is the exploitation
of errors in the learned models. Black-box models can fit complex dynamics with
high fidelity, but their behavior is undefined outside of the data
distribution.Physics-based models are better at extrapolating, due to the
general validity of their informed structure, but underfit in the real world
due to the presence of unmodeled phenomena. In this work, we demonstrate
experimentally that for the offline model-based reinforcement learning setting,
physics-based models can be beneficial compared to high-capacity function
approximators if the mechanical structure is known. Physics-based models can
learn to perform the ball in a cup (BiC) task on a physical manipulator using
only 4 minutes of sampled data using offline MBRL. We find that black-box
models consistently produce unviable policies for BiC as all predicted
trajectories diverge to physically impossible state, despite having access to
more data than the physics-based model. In addition, we generalize the approach
of physics parameter identification from modeling holonomic multi-body systems
to systems with nonholonomic dynamics using end-to-end automatic
differentiation.
  Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/"
"In this paper, the flexibility, versatility and predictive power of kernel
regression are combined with now lavishly available network data to create
regression models with even greater predictive performances. Building from
previous work featuring generalized linear models built in the presence of
network cohesion data, we construct a kernelized extension that captures
subtler nonlinearities in extremely high dimensional spaces and also produces
far better predictive performances. Applications of seamless yet substantial
adaptation to simulated and real-life data demonstrate the appeal and strength
of our work."
"Digital Twin was introduced over a decade ago, as an innovative
all-encompassing tool, with perceived benefits including real-time monitoring,
simulation and forecasting. However, the theoretical framework and practical
implementations of digital twins (DT) are still far from this vision. Although
successful implementations exist, sufficient implementation details are not
publicly available, therefore it is difficult to assess their effectiveness,
draw comparisons and jointly advance the DT methodology. This work explores the
various DT features and current approaches, the shortcomings and reasons behind
the delay in the implementation and adoption of digital twin. Advancements in
machine learning, internet of things and big data have contributed hugely to
the improvements in DT with regards to its real-time monitoring and forecasting
properties. Despite this progress and individual company-based efforts, certain
research gaps exist in the field, which have caused delay in the widespread
adoption of this concept. We reviewed relevant works and identified that the
major reasons for this delay are the lack of a universal reference framework,
domain dependence, security concerns of shared data, reliance of digital twin
on other technologies, and lack of quantitative metrics. We define the
necessary components of a digital twin required for a universal reference
framework, which also validate its uniqueness as a concept compared to similar
concepts like simulation, autonomous systems, etc. This work further assesses
the digital twin applications in different domains and the current state of
machine learning and big data in it. It thus answers and identifies novel
research questions, both of which will help to better understand and advance
the theory and practice of digital twins."
"Continuous-time event data are common in applications such as individual
behavior data, financial transactions, and medical health records. Modeling
such data can be very challenging, in particular for applications with many
different types of events, since it requires a model to predict the event types
as well as the time of occurrence. Recurrent neural networks that parameterize
time-varying intensity functions are the current state-of-the-art for
predictive modeling with such data. These models typically assume that all
event sequences come from the same data distribution. However, in many
applications event sequences are generated by different sources, or users, and
their characteristics can be very different. In this paper, we extend the broad
class of neural marked point process models to mixtures of latent embeddings,
where each mixture component models the characteristic traits of a given user.
Our approach relies on augmenting these models with a latent variable that
encodes user characteristics, represented by a mixture model over user behavior
that is trained via amortized variational inference. We evaluate our methods on
four large real-world datasets and demonstrate systematic improvements from our
approach over existing work for a variety of predictive metrics such as
log-likelihood, next event ranking, and source-of-sequence identification."
"Despite considerable progress, ab initio protein structure prediction remains
suboptimal. A crowdsourcing approach is the online puzzle video game Foldit,
that provided several useful results that matched or even outperformed
algorithmically computed solutions. Using Foldit, the WeFold crowd had several
successful participations in the Critical Assessment of Techniques for Protein
Structure Prediction. Based on the recent Foldit standalone version, we trained
a deep reinforcement neural network called DeepFoldit to improve the score
assigned to an unfolded protein, using the Q-learning method with experience
replay. This paper is focused on model improvement through hyperparameter
tuning. We examined various implementations by examining different model
architectures and changing hyperparameter values to improve the accuracy of the
model. The new model hyper-parameters also improved its ability to generalize.
Initial results, from the latest implementation, show that given a set of small
unfolded training proteins, DeepFoldit learns action sequences that improve the
score both on the training set and on novel test proteins. Our approach
combines the intuitive user interface of Foldit with the efficiency of deep
reinforcement learning."
"Lack of data and data quality issues are among the main bottlenecks that
prevent further artificial intelligence adoption within many organizations,
pushing data scientists to spend most of their time cleaning data before being
able to answer analytical questions. Hence, there is a need for more effective
and efficient data cleaning solutions, which, not surprisingly, is rife with
theoretical and engineering problems. This report addresses the problem of
performing holistic data cleaning incrementally, given a fixed rule set and an
evolving categorical relational dataset acquired in sequential batches. To the
best of our knowledge, our contributions compose the first incremental
framework that cleans data (i) independently of user interventions, (ii)
without requiring knowledge about the incoming dataset, such as the number of
classes per attribute, and (iii) holistically, enabling multiple error types to
be repaired simultaneously, and thus avoiding conflicting repairs. Extensive
experiments show that our approach outperforms the competitors with respect to
repair quality, execution time, and memory consumption."
"This work describes a self-supervised data augmentation approach used to
improve learning models' performances when only a moderate amount of labeled
data is available. Multiple copies of the original model are initially trained
on the downstream task. Their predictions are then used to annotate a large set
of unlabeled examples. Finally, multi-task training is performed on the
parallel annotations of the resulting training set, and final scores are
obtained by averaging annotator-specific head predictions. Neural language
models are fine-tuned using this procedure in the context of the AcCompl-it
shared task at EVALITA 2020, obtaining considerable improvements in prediction
quality."
"We present methods to serialize and deserialize tree ensembles that optimize
inference latency when models are not already loaded into memory. This arises
whenever models are larger than memory, but also systematically when models are
deployed on low-resource devices, such as in the Internet of Things, or run as
Web micro-services where resources are allocated on demand. Our packed
serialized trees (PACSET) encode reference locality in the layout of a tree
ensemble using principles from external memory algorithms. The layout
interleaves correlated nodes across multiple trees, uses leaf cardinality to
collocate the nodes on the most popular paths and is optimized for the I/O
blocksize. The result is that each I/O yields a higher fraction of useful data,
leading to a 2-6 times reduction in classification latency for interactive
workloads."
"Critical for the coexistence of humans and robots in dynamic environments is
the capability for agents to understand each other's actions, and anticipate
their movements. This paper presents Stochastic Process Anticipatory Navigation
(SPAN), a framework that enables nonholonomic robots to navigate in
environments with crowds, while anticipating and accounting for the motion
patterns of pedestrians. To this end, we learn a predictive model to predict
continuous-time stochastic processes to model future movement of pedestrians.
Anticipated pedestrian positions are used to conduct chance constrained
collision-checking, and are incorporated into a time-to-collision control
problem. An occupancy map is also integrated to allow for probabilistic
collision-checking with static obstacles. We demonstrate the capability of SPAN
in crowded simulation environments, as well as with a real-world pedestrian
dataset."
"Domain adaptation solves the learning problem in a target domain by
leveraging the knowledge in a relevant source domain. While remarkable advances
have been made, almost all existing domain adaptation methods heavily require
large amounts of unlabeled target domain data for learning domain invariant
representations to achieve good generalizability on the target domain. In fact,
in many real-world applications, target domain data may not always be
available. In this paper, we study the cases where at the training phase the
target domain data is unavailable and only well-labeled source domain data is
available, called robust domain adaptation. To tackle this problem, under the
assumption that causal relationships between features and the class variable
are robust across domains, we propose a novel Causal AutoEncoder (CAE), which
integrates deep autoencoder and causal structure learning into a unified model
to learn causal representations only using data from a single source domain.
Specifically, a deep autoencoder model is adopted to learn low-dimensional
representations, and a causal structure learning model is designed to separate
the low-dimensional representations into two groups: causal representations and
task-irrelevant representations. Using three real-world datasets the extensive
experiments have validated the effectiveness of CAE compared to eleven
state-of-the-art methods."
"Machine learning (ML) continues to grow in importance across nearly all
domains and is a natural tool in modeling to learn from data. Often a tradeoff
exists between a model's ability to minimize bias and variance. In this paper,
we utilize ensemble learning to combine linear, nonlinear, and tree-/rule-based
ML methods to cope with the bias-variance tradeoff and result in more accurate
models. Hardware performance counter values are correlated with properties of
applications that impact performance and power on the underlying system. We use
the datasets collected for two parallel cancer deep learning CANDLE benchmarks,
NT3 (weak scaling) and P1B2 (strong scaling), to build performance and power
models based on hardware performance counters using single-object and
multiple-objects ensemble learning to identify the most important counters for
improvement. Based on the insights from these models, we improve the
performance and energy of P1B2 and NT3 by optimizing the deep learning
environments TensorFlow, Keras, Horovod, and Python under the huge page size of
8 MB on the Cray XC40 Theta at Argonne National Laboratory. Experimental
results show that ensemble learning not only produces more accurate models but
also provides more robust performance counter ranking. We achieve up to 61.15%
performance improvement and up to 62.58% energy saving for P1B2 and up to
55.81% performance improvement and up to 52.60% energy saving for NT3 on up to
24,576 cores."
"Semi-parametric regression models are used in several applications which
require comprehensibility without sacrificing accuracy. Typical examples are
spline interpolation in geophysics, or non-linear time series problems, where
the system includes a linear and non-linear component. We discuss here the use
of a finite Determinantal Point Process (DPP) for approximating semi-parametric
models. Recently, Barthelm\'e, Tremblay, Usevich, and Amblard introduced a
novel representation of some finite DPPs. These authors formulated extended
L-ensembles that can conveniently represent partial-projection DPPs and suggest
their use for optimal interpolation. With the help of this formalism, we derive
a key identity illustrating the implicit regularization effect of determinantal
sampling for semi-parametric regression and interpolation. Also, a novel
projected Nystr\""om approximation is defined and used to derive a bound on the
expected risk for the corresponding approximation of semi-parametric
regression. This work naturally extends similar results obtained for kernel
ridge regression."
"Catastrophic forgetting in continual learning is a common destructive
phenomenon in gradient-based neural networks that learn sequential tasks, and
it is much different from forgetting in humans, who can learn and accumulate
knowledge throughout their whole lives. Catastrophic forgetting is the fatal
shortcoming of a large decrease in performance on previous tasks when the model
is learning a novel task. To alleviate this problem, the model should have the
capacity to learn new knowledge and preserve learned knowledge. We propose an
average gradient episodic memory (A-GEM) with a soft constraint $\epsilon \in
[0, 1]$, which is a balance factor between learning new knowledge and
preserving learned knowledge; our method is called gradient episodic memory
with a soft constraint $\epsilon$ ($\epsilon$-SOFT-GEM). $\epsilon$-SOFT-GEM
outperforms A-GEM and several continual learning benchmarks in a single
training epoch; additionally, it has state-of-the-art average accuracy and
efficiency for computation and memory, like A-GEM, and provides a better
trade-off between the stability of preserving learned knowledge and the
plasticity of learning new knowledge."
"Cardiovascular disease (CVD) is a serious illness affecting millions
world-wide and is the leading cause of death in the US. Recent years, however,
have seen tremendous growth in the area of personalized medicine, a field of
medicine that places the patient at the center of the medical decision-making
and treatment process. Many CVD-focused personalized medicine innovations focus
on genetic biomarkers, which provide person-specific CVD insights at the
genetic level, but do not focus on the practical steps a patient could take to
mitigate their risk of CVD development. In this work we propose longitudinal
inverse classification, a recommendation framework that provides personalized
lifestyle recommendations that minimize the predicted probability of CVD risk.
Our framework takes into account historical CVD risk, as well as other patient
characteristics, to provide recommendations. Our experiments show that earlier
adoption of the recommendations elicited from our framework produce significant
CVD risk reduction."
"In the machine learning algorithms, the choice of the hyperparameter is often
an art more than a science, requiring labor-intensive search with expert
experience. Therefore, automation on hyperparameter optimization to exclude
human intervention is a great appeal, especially for the black-box functions.
Recently, there have been increasing demands of solving such concealed tasks
for better generalization, though the task-dependent issue is not easy to
solve. The Black-Box Optimization challenge (NeurIPS 2020) required competitors
to build a robust black-box optimizer across different domains of standard
machine learning problems. This paper describes the approach of team KAIST OSI
in a step-wise manner, which outperforms the baseline algorithms by up to
+20.39%. We first strengthen the local Bayesian search under the concept of
region reliability. Then, we design a combinatorial kernel for a Gaussian
process kernel. In a similar vein, we combine the methodology of Bayesian and
multi-armed bandit,(MAB) approach to select the values with the consideration
of the variable types; the real and integer variables are with Bayesian, while
the boolean and categorical variables are with MAB. Empirical evaluations
demonstrate that our method outperforms the existing methods across different
tasks."
"Sensor-based time series analysis is an essential task for applications such
as activity recognition and brain-computer interface. Recently, features
extracted with deep neural networks (DNNs) are shown to be more effective than
conventional hand-crafted ones. However, most of these solutions rely solely on
the network to extract application-specific information carried in the sensor
data. Motivated by the fact that usually a small subset of the frequency
components carries the primary information for sensor data, we propose a novel
tree-structured wavelet neural network for sensor data analysis, namely
\emph{T-WaveNet}. To be specific, with T-WaveNet, we first conduct a power
spectrum analysis for the sensor data and decompose the input signal into
various frequency subbands accordingly. Then, we construct a tree-structured
network, and each node on the tree (corresponding to a frequency subband) is
built with an invertible neural network (INN) based wavelet transform. By doing
so, T-WaveNet provides more effective representation for sensor information
than existing DNN-based techniques, and it achieves state-of-the-art
performance on various sensor datasets, including UCI-HAR for activity
recognition, OPPORTUNITY for gesture recognition, BCICIV2a for intention
recognition, and NinaPro DB1 for muscular movement recognition."
"In the current context of Big Data, the nature of many forecasting problems
has changed from predicting isolated time series to predicting many time series
from similar sources. This has opened up the opportunity to develop competitive
global forecasting models that simultaneously learn from many time series. But,
it still remains unclear when global forecasting models can outperform the
univariate benchmarks, especially along the dimensions of the
homogeneity/heterogeneity of series, the complexity of patterns in the series,
the complexity of forecasting models, and the lengths/number of series. Our
study attempts to address this problem through investigating the effect from
these factors, by simulating a number of datasets that have controllable time
series characteristics. Specifically, we simulate time series from simple data
generating processes (DGP), such as Auto Regressive (AR) and Seasonal AR, to
complex DGPs, such as Chaotic Logistic Map, Self-Exciting Threshold
Auto-Regressive, and Mackey-Glass Equations. The data heterogeneity is
introduced by mixing time series generated from several DGPs into a single
dataset. The lengths and the number of series in the dataset are varied in
different scenarios. We perform experiments on these datasets using global
forecasting models including Recurrent Neural Networks (RNN), Feed-Forward
Neural Networks, Pooled Regression (PR) models and Light Gradient Boosting
Models (LGBM), and compare their performance against standard statistical
univariate forecasting techniques. Our experiments demonstrate that when
trained as global forecasting models, techniques such as RNNs and LGBMs, which
have complex non-linear modelling capabilities, are competitive methods in
general under challenging forecasting scenarios such as series having short
lengths, datasets with heterogeneous series and having minimal prior knowledge
of the patterns of the series."
"In autonomous driving (AD), accurately predicting changes in the environment
can effectively improve safety and comfort. Due to complex interactions among
traffic participants, however, it is very hard to achieve accurate prediction
for a long horizon. To address this challenge, we propose prediction by
anticipation, which views interaction in terms of a latent probabilistic
generative process wherein some vehicles move partly in response to the
anticipated motion of other vehicles. Under this view, consecutive data frames
can be factorized into sequential samples from an action-conditional
distribution that effectively generalizes to a wider range of actions and
driving situations. Our proposed prediction model, variational Bayesian in
nature, is trained to maximize the evidence lower bound (ELBO) of the
log-likelihood of this conditional distribution. Evaluations of our approach
with prominent AD datasets NGSIM I-80 and Argoverse show significant
improvement over current state-of-the-art in both accuracy and generalization."
"The question of whether to use one classifier or a combination of classifiers
is a central topic in Machine Learning. We propose here a method for finding an
optimal linear combination of classifiers derived from a bias-variance
framework for the classification task."
"Optimal treatment regimes are personalized policies for making a treatment
decision based on subject characteristics, with the policy chosen to maximize
some value. It is common to aim to maximize the mean outcome in the population,
via a regime assigning treatment only to those whose mean outcome is higher
under treatment versus control. However, the mean can be an unstable measure of
centrality, resulting in imprecise statistical procedures, as well as unrobust
decisions that can be overly influenced by a small fraction of subjects. In
this work, we propose a new median optimal treatment regime that instead treats
individuals whose conditional median is higher under treatment. This ensures
that optimal decisions for individuals from the same group are not overly
influenced either by (i) a small fraction of the group (unlike the mean
criterion), or (ii) unrelated subjects from different groups (unlike marginal
median/quantile criteria). We introduce a new measure of value, the Average
Conditional Median Effect (ACME), which summarizes across-group median
treatment outcomes of a policy, and which the median optimal treatment regime
maximizes. After developing key motivating examples that distinguish median
optimal treatment regimes from mean and marginal median optimal treatment
regimes, we give a nonparametric efficiency bound for estimating the ACME of a
policy, and propose a new doubly robust-style estimator that achieves the
efficiency bound under weak conditions. To construct the median optimal
treatment regime, we introduce a new doubly robust-style estimator for the
conditional median treatment effect. Finite-sample properties are explored via
numerical simulations and the proposed algorithm is illustrated using data from
a randomized clinical trial in patients with HIV."
"Strategic classification regards the problem of learning in settings where
users can strategically modify their features to improve outcomes. This setting
applies broadly and has received much recent attention. But despite its
practical significance, work in this space has so far been predominantly
theoretical. In this paper we present a learning framework for strategic
classification that is practical. Our approach directly minimizes the
""strategic"" empirical risk, achieved by differentiating through the strategic
response of users. This provides flexibility that allows us to extend beyond
the original problem formulation and towards more realistic learning scenarios.
A series of experiments demonstrates the effectiveness of our approach on
various learning settings."
"This paper presents a clustering algorithm that is an extension of the
Category Trees algorithm. Category Trees is a clustering method that creates
tree structures that branch on category type and not feature. The development
in this paper is to consider a secondary order of clustering that is not the
category to which the data row belongs, but the tree, representing a single
classifier, that it is eventually clustered with. Each tree branches to store
subsets of other categories, but the rows in those subsets may also be related.
This paper is therefore concerned with looking at that second level of
clustering between the other category subsets, to try to determine if there is
any consistency over it. It is argued that Principal Components may be a
related and reciprocal type of structure, and there is an even bigger question
about the relation between exemplars and principal components, in general. The
theory is demonstrated using the Portugal Forest Fires dataset as a case study.
The Category Trees are then combined with other Self-Organising algorithms from
the author and it is suggested that they all belong to the same family type,
which is an Entropy-style of classifier."
"We derive improved regret bounds for the Tsallis-INF algorithm of Zimmert and
Seldin (2021). We show that in adversarial regimes with a $(\Delta,C,T)$
self-bounding constraint the algorithm achieves
$\mathcal{O}\left(\left(\sum_{i\neq i^*}
\frac{1}{\Delta_i}\right)\log_+\left(\frac{(K-1)T}{\left(\sum_{i\neq i^*}
\frac{1}{\Delta_i}\right)^2}\right)+\sqrt{C\left(\sum_{i\neq
i^*}\frac{1}{\Delta_i}\right)\log_+\left(\frac{(K-1)T}{C\sum_{i\neq
i^*}\frac{1}{\Delta_i}}\right)}\right)$ regret bound, where $T$ is the time
horizon, $K$ is the number of arms, $\Delta_i$ are the suboptimality gaps,
$i^*$ is the best arm, $C$ is the corruption magnitude, and $\log_+(x) =
\max\left(1,\log x\right)$. The regime includes stochastic bandits,
stochastically constrained adversarial bandits, and stochastic bandits with
adversarial corruptions as special cases. Additionally, we provide a general
analysis, which allows to achieve the same kind of improvement for
generalizations of Tsallis-INF to other settings beyond multiarmed bandits."
"Given the rapid changes in telecommunication systems and their higher
dependence on artificial intelligence, it is increasingly important to have
models that can perform well under different, possibly adverse, conditions.
Deep Neural Networks (DNNs) using convolutional layers are state-of-the-art in
many tasks in communications. However, in other domains, like image
classification, DNNs have been shown to be vulnerable to adversarial
perturbations, which consist of imperceptible crafted noise that when added to
the data fools the model into misclassification. This puts into question the
security of DNNs in communication tasks, and in particular in modulation
recognition. We propose a novel framework to test the robustness of current
state-of-the-art models where the adversarial perturbation strength is
dependent on the signal strength and measured with the ""signal to perturbation
ratio"" (SPR). We show that current state-of-the-art models are susceptible to
these perturbations. In contrast to current research on the topic of image
classification, modulation recognition allows us to have easily accessible
insights on the usefulness of the features learned by DNNs by looking at the
constellation space. When analyzing these vulnerable models we found that
adversarial perturbations do not shift the symbols towards the nearest classes
in constellation space. This shows that DNNs do not base their decisions on
signal statistics that are important for the Bayes-optimal modulation
recognition model, but spurious correlations in the training data. Our feature
analysis and proposed framework can help in the task of finding better models
for communication systems."
"Neuromorphic hardware equipped with learning capabilities can adapt to new,
real-time data. While models of Spiking Neural Networks (SNNs) can now be
trained using gradient descent to reach an accuracy comparable to equivalent
conventional neural networks, such learning often relies on external labels.
However, real-world data is unlabeled which can make supervised methods
inapplicable. To solve this problem, we propose a Hybrid Guided Variational
Autoencoder (VAE) which encodes event based data sensed by a Dynamic Vision
Sensor (DVS) into a latent space representation using an SNN. These
representations can be used as an embedding to measure data similarity and
predict labels in real-world data. We show that the Hybrid Guided-VAE achieves
87% classification accuracy on the DVSGesture dataset and it can encode the
sparse, noisy inputs into an interpretable latent space representation,
visualized through T-SNE plots. We also implement the encoder component of the
model on neuromorphic hardware and discuss the potential for our algorithm to
enable real-time learning from real-world event data."
"There has been increasing interest in building deep hierarchy-aware
classifiers that aim to quantify and reduce the severity of mistakes, and not
just reduce the number of errors. The idea is to exploit the label hierarchy
(e.g., the WordNet ontology) and consider graph distances as a proxy for
mistake severity. Surprisingly, on examining mistake-severity distributions of
the top-1 prediction, we find that current state-of-the-art hierarchy-aware
deep classifiers do not always show practical improvement over the standard
cross-entropy baseline in making better mistakes. The reason for the reduction
in average mistake-severity can be attributed to the increase in low-severity
mistakes, which may also explain the noticeable drop in their accuracy. To this
end, we use the classical Conditional Risk Minimization (CRM) framework for
hierarchy-aware classification. Given a cost matrix and a reliable estimate of
likelihoods (obtained from a trained network), CRM simply amends mistakes at
inference time; it needs no extra hyperparameters and requires adding just a
few lines of code to the standard cross-entropy baseline. It significantly
outperforms the state-of-the-art and consistently obtains large reductions in
the average hierarchical distance of top-$k$ predictions across datasets, with
very little loss in accuracy. CRM, because of its simplicity, can be used with
any off-the-shelf trained model that provides reliable likelihood estimates."
"This paper presents TULIP, a new architecture for a binary neural network
(BNN) that uses an optimal schedule for executing the operations of an
arbitrary BNN. It was constructed with the goal of maximizing energy efficiency
per classification. At the top-level, TULIP consists of a collection of unique
processing elements (TULIP-PEs) that are organized in a SIMD fashion. Each
TULIP-PE consists of a small network of binary neurons, and a small amount of
local memory per neuron. The unique aspect of the binary neuron is that it is
implemented as a mixed-signal circuit that natively performs the inner-product
and thresholding operation of an artificial binary neuron. Moreover, the binary
neuron, which is implemented as a single CMOS standard cell, is reconfigurable,
and with a change in a single parameter, can implement all standard operations
involved in a BNN. We present novel algorithms for mapping arbitrary nodes of a
BNN onto the TULIP-PEs. TULIP was implemented as an ASIC in TSMC 40nm-LP
technology. To provide a fair comparison, a recently reported BNN that employs
a conventional MAC-based arithmetic processor was also implemented in the same
technology. The results show that TULIP is consistently 3X more
energy-efficient than the conventional design, without any penalty in
performance, area, or accuracy."
"The amount of data, manpower and capital required to understand, evaluate and
agree on a group of symptoms for the elementary prognosis of pandemic diseases
is enormous. In this paper, we present FedPandemic, a novel noise
implementation algorithm integrated with cross-device Federated learning for
Elementary symptom prognosis during a pandemic, taking COVID-19 as a case
study. Our results display consistency and enhance robustness in recovering the
common symptoms displayed by the disease, paving a faster and cheaper path
towards symptom retrieval while also preserving the privacy of patient's
symptoms via Federated learning."
"Academic citation graphs represent citation relationships between
publications across the full range of academic fields. Top cited papers
typically reveal future trends in their corresponding domains which is of
importance to both researchers and practitioners. Prior citation prediction
methods often require initial citation trends to be established and do not take
advantage of the recent advancements in graph neural networks (GNNs). We
present GNN-based architecture that predicts the top set of papers at the time
of publication. For experiments, we curate a set of academic citation graphs
for a variety of conferences and show that the proposed model outperforms other
classic machine learning models in terms of the F1-score."
"Real-world data is often unbalanced and long-tailed, but deep models struggle
to recognize rare classes in the presence of frequent classes. To address
unbalanced data, most studies try balancing the data, the loss, or the
classifier to reduce classification bias towards head classes. Far less
attention has been given to the latent representations learned with unbalanced
data. We show that the feature extractor part of deep networks suffers greatly
from this bias. We propose a new loss based on robustness theory, which
encourages the model to learn high-quality representations for both head and
tail classes. While the general form of the robustness loss may be hard to
compute, we further derive an easy-to-compute upper bound that can be minimized
efficiently. This procedure reduces representation bias towards head classes in
the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT,
and iNaturalist long-tail benchmarks. We find that training with robustness
increases recognition accuracy of tail classes while largely maintaining the
accuracy of head classes. The new robustness loss can be combined with various
classifier balancing techniques and can be applied to representations at
several layers of the deep model."
"Convolutional Neural Networks (CNN) are used mainly to treat problems with
many images characteristic of Deep Learning. In this work, we propose a hybrid
image classification model to take advantage of quantum and classical
computing. The method will use the potential that convolutional networks have
shown in artificial intelligence by replacing classical filters with
variational quantum filters. Similarly, this work will compare with other
classification methods and the system's execution on different servers. The
algorithm's quantum feasibility is modelled and tested on Amazon Braket
Notebook instances and experimented on the Pennylane's philosophy and
framework."
"The tritium breeding ratio (TBR) is an essential quantity for the design of
modern and next-generation D-T fueled nuclear fusion reactors. Representing the
ratio between tritium fuel generated in breeding blankets and fuel consumed
during reactor runtime, the TBR depends on reactor geometry and material
properties in a complex manner. In this work, we explored the training of
surrogate models to produce a cheap but high-quality approximation for a Monte
Carlo TBR model in use at the UK Atomic Energy Authority. We investigated
possibilities for dimensional reduction of its feature space, reviewed 9
families of surrogate models for potential applicability, and performed
hyperparameter optimisation. Here we present the performance and scaling
properties of these models, the fastest of which, an artificial neural network,
demonstrated $R^2=0.985$ and a mean prediction time of $0.898\ \mu\mathrm{s}$,
representing a relative speedup of $8\cdot 10^6$ with respect to the expensive
MC model. We further present a novel adaptive sampling algorithm,
Quality-Adaptive Surrogate Sampling, capable of interfacing with any of the
individually studied surrogates. Our preliminary testing on a toy TBR theory
has demonstrated the efficacy of this algorithm for accelerating the surrogate
modelling process."
"In multi-class classification tasks, like human activity recognition, it is
often assumed that classes are separable. In real applications, this assumption
becomes strong and generates inconsistencies. Besides, the most commonly used
approach is to learn classes one-by-one against the others. This computational
simplification principle introduces strong inductive biases on the learned
theories. In fact, the natural connections among some classes, and not others,
deserve to be taken into account. In this paper, we show that the organization
of overlapping classes (multiple inheritances) into hierarchies considerably
improves classification performances. This is particularly true in the case of
activity recognition tasks featured in the SHL dataset. After theoretically
showing the exponential complexity of possible class hierarchies, we propose an
approach based on transfer affinity among the classes to determine an optimal
hierarchy for the learning process. Extensive experiments show improved
performances and a reduction in the number of examples needed to learn."
"It is of significance for an agent to learn a widely applicable and
general-purpose policy that can achieve diverse goals including images and text
descriptions. Considering such perceptually-specific goals, the frontier of
deep reinforcement learning research is to learn a goal-conditioned policy
without hand-crafted rewards. To learn this kind of policy, recent works
usually take as the reward the non-parametric distance to a given goal in an
explicit embedding space. From a different viewpoint, we propose a novel
unsupervised learning approach named goal-conditioned policy with intrinsic
motivation (GPIM), which jointly learns both an abstract-level policy and a
goal-conditioned policy. The abstract-level policy is conditioned on a latent
variable to optimize a discriminator and discovers diverse states that are
further rendered into perceptually-specific goals for the goal-conditioned
policy. The learned discriminator serves as an intrinsic reward function for
the goal-conditioned policy to imitate the trajectory induced by the
abstract-level policy. Experiments on various robotic tasks demonstrate the
effectiveness and efficiency of our proposed GPIM method which substantially
outperforms prior techniques."
"A broad class of unsupervised deep learning methods such as Generative
Adversarial Networks (GANs) involve training of overparameterized models where
the number of parameters of the model exceeds a certain threshold. A large body
of work in supervised learning have shown the importance of model
overparameterization in the convergence of the gradient descent (GD) to
globally optimal solutions. In contrast, the unsupervised setting and GANs in
particular involve non-convex concave mini-max optimization problems that are
often trained using Gradient Descent/Ascent (GDA). The role and benefits of
model overparameterization in the convergence of GDA to a global saddle point
in non-convex concave problems is far less understood. In this work, we present
a comprehensive analysis of the importance of model overparameterization in
GANs both theoretically and empirically. We theoretically show that in an
overparameterized GAN model with a $1$-layer neural network generator and a
linear discriminator, GDA converges to a global saddle point of the underlying
non-convex concave min-max problem. To the best of our knowledge, this is the
first result for global convergence of GDA in such settings. Our theory is
based on a more general result that holds for a broader class of nonlinear
generators and discriminators that obey certain assumptions (including deeper
generators and random feature discriminators). We also empirically study the
role of model overparameterization in GANs using several large-scale
experiments on CIFAR-10 and Celeb-A datasets. Our experiments show that
overparameterization improves the quality of generated samples across various
model architectures and datasets. Remarkably, we observe that
overparameterization leads to faster and more stable convergence behavior of
GDA across the board."
"This report presents a preliminary analysis of an LSTM neural network
designed to predict the accuracy of magnitude estimates computed by Early-est
during the first minutes after an earthquake occurs."
"The SARS-CoV-2 virus and COVID-19 disease have posed unprecedented and
overwhelming demand, challenges and opportunities to domain, model and data
driven modeling. This paper provides a comprehensive review of the challenges,
tasks, methods, progress, gaps and opportunities in relation to modeling
COVID-19 problems, data and objectives. It constructs a research landscape of
COVID-19 modeling tasks and methods, and further categorizes, summarizes,
compares and discusses the related methods and progress of modeling COVID-19
epidemic transmission processes and dynamics, case identification and tracing,
infection diagnosis and medical treatments, non-pharmaceutical interventions
and their effects, drug and vaccine development, psychological, economic and
social influence and impact, and misinformation, etc. The modeling methods
involve mathematical and statistical models, domain-driven modeling by
epidemiological compartmental models, medical and biomedical analysis, AI and
data science in particular shallow and deep machine learning, simulation
modeling, social science methods, and hybrid modeling."
"In this conceptual work, we present Deep Convolutional Gaussian Mixture
Models (DCGMMs): a new formulation of deep hierarchical Gaussian Mixture Models
(GMMs) that is particularly suitable for describing and generating images.
Vanilla (i.e., flat) GMMs require a very large number of components to describe
images well, leading to long training times and memory issues. DCGMMs avoid
this by a stacked architecture of multiple GMM layers, linked by convolution
and pooling operations. This allows to exploit the compositionality of images
in a similar way as deep CNNs do. DCGMMs can be trained end-to-end by
Stochastic Gradient Descent. This sets them apart from vanilla GMMs which are
trained by Expectation-Maximization, requiring a prior k-means initialization
which is infeasible in a layered structure. For generating sharp images with
DCGMMs, we introduce a new gradient-based technique for sampling through
non-invertible operations like convolution and pooling. Based on the MNIST and
FashionMNIST datasets, we validate the DCGMMs model by demonstrating its
superiority over flat GMMs for clustering, sampling and outlier detection."
"We trained deep neural networks (DNNs) as a function of the neutrino energy
density, flux, and the fluid velocity to reproduce the Eddington tensor for
neutrinos obtained in our first-principles core-collapse supernova (CCSN)
simulations. Although the moment method, which is one of the most popular
approximations for neutrino transport, requires a closure relation, none of the
analytical closure relations commonly employed in the literature captures all
aspects of the neutrino angular distribution in momentum space. In this paper,
we developed a closure relation by using the DNN that takes the neutrino energy
density, flux, and the fluid velocity as the input and the Eddington tensor as
the output. We consider two kinds of DNNs: a conventional DNN named a
component-wise neural network (CWNN) and a tensor-basis neural network (TBNN).
We found that the diagonal component of the Eddington tensor is reproduced
better by the DNNs than the M1-closure relation especially for low to
intermediate energies. For the off-diagonal component, the DNNs agree better
with the Boltzmann solver than the M1 closure at large radii. In the comparison
between the two DNNs, the TBNN has slightly better performance than the CWNN.
With the new closure relations at hand based on the DNNs that well reproduce
the Eddington tensor with much smaller costs, we opened up a new possibility
for the moment method."
"In this paper, we develop a ytopt autotuning framework that leverages
Bayesian optimization to explore the parameter space search and compare four
different supervised learning methods within Bayesian optimization and evaluate
their effectiveness. We select six of the most complex PolyBench benchmarks and
apply the newly developed LLVM Clang/Polly loop optimization pragmas to the
benchmarks to optimize them. We then use the autotuning framework to optimize
the pragma parameters to improve their performance. The experimental results
show that our autotuning approach outperforms the other compiling methods to
provide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu,
and covariance with two large datasets in 200 code evaluations for effectively
searching the parameter spaces with up to 170,368 different configurations. We
find that the Floyd-Warshall benchmark did not benefit from autotuning because
Polly uses heuristics to optimize the benchmark to make it run much slower. To
cope with this issue, we provide some compiler option solutions to improve the
performance. Then we present loop autotuning without a user's knowledge using a
simple mctree autotuning framework to further improve the performance of the
Floyd-Warshall benchmark. We also extend the ytopt autotuning framework to tune
a deep learning application."
"Deep neural networks can be unreliable in the real world especially when they
heavily use {\it spurious} features for their predictions. Focusing on image
classifications, we define {\it core features} as the set of visual features
that are always a part of the object definition while {\it spurious features}
are the ones that are likely to {\it co-occur} with the object but not a part
of it (e.g., attribute ""fingers"" for class ""band aid""). Traditional methods for
discovering spurious features either require extensive human annotations (thus,
not scalable), or are useful on specific models. In this work, we introduce a
{\it general} framework to discover a subset of spurious and core visual
features used in inferences of a general model and localize them on a large
number of images with minimal human supervision. Our methodology is based on
this key idea: to identify spurious or core \textit{visual features} used in
model predictions, we identify spurious or core \textit{neural features}
(penultimate layer neurons of a robust model) via limited human supervision
(e.g., using top 5 activating images per feature). We then show that these
neural feature annotations {\it generalize} extremely well to many more images
{\it without} any human supervision. We use the activation maps for these
neural features as the soft masks to highlight spurious or core visual
features. Using this methodology, we introduce the {\it Salient Imagenet}
dataset containing core and spurious masks for a large set of samples from
Imagenet. Using this dataset, we show that several popular Imagenet models rely
heavily on various spurious features in their predictions, indicating the
standard accuracy alone is not sufficient to fully assess model performance.
Code and dataset for reproducing all experiments in the paper is available at
\url{https://github.com/singlasahil14/salient_imagenet}."
"Current Open-Domain Question Answering (ODQA) model paradigm often contains a
retrieving module and a reading module. Given an input question, the reading
module predicts the answer from the relevant passages which are retrieved by
the retriever. The recent proposed Fusion-in-Decoder (FiD), which is built on
top of the pretrained generative model T5, achieves the state-of-the-art
performance in the reading module. Although being effective, it remains
constrained by inefficient attention on all retrieved passages which contain a
lot of noise. In this work, we propose a novel method KG-FiD, which filters
noisy passages by leveraging the structural relationship among the retrieved
passages with a knowledge graph. We initiate the passage node embedding from
the FiD encoder and then use graph neural network (GNN) to update the
representation for reranking. To improve the efficiency, we build the GNN on
top of the intermediate layer output of the FiD encoder and only pass a few top
reranked passages into the higher layers of encoder and decoder for answer
generation. We also apply the proposed GNN based reranking method to enhance
the passage retrieval results in the retrieving module. Extensive experiments
on common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate
that KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score
and achieve comparable performance with FiD with only 40% of computation cost."
"Developing an automatic part-of-speech (POS) tagging for any new language is
considered a necessary step for further computational linguistics methodology
beyond tagging, like chunking and parsing, to be fully applied to the language.
Many POS disambiguation technologies have been developed for this type of
research and there are factors that influence the choice of choosing one. This
could be either corpus-based or non-corpus-based. In this paper, we present a
review of POS tagging technologies."
"Consider a heterogeneous data stream being generated by the nodes of a graph.
The data stream is in essence composed by multiple streams, possibly of
different nature that depends on each node. At a given moment $\tau$, a
change-point occurs for a subset of nodes $C$, signifying the change in the
probability distribution of their associated streams. In this paper we propose
an online non-parametric method to infer $\tau$ based on the direct estimation
of the likelihood-ratio between the post-change and the pre-change distribution
associated with the data stream of each node. We propose a kernel-based method,
under the hypothesis that connected nodes of the graph are expected to have
similar likelihood-ratio estimates when there is no change-point. We
demonstrate the quality of our method on synthetic experiments and real-world
applications."
"This paper considers the problem of resilient distributed optimization and
stochastic machine learning in a server-based architecture. The system
comprises a server and multiple agents, where each agent has a local cost
function. The agents collaborate with the server to find a minimum of their
aggregate cost functions. We consider the case when some of the agents may be
asynchronous and/or Byzantine faulty. In this case, the classical algorithm of
distributed gradient descent (DGD) is rendered ineffective. Our goal is to
design techniques improving the efficacy of DGD with asynchrony and Byzantine
failures. To do so, we start by proposing a way to model the agents' cost
functions by the generic notion of $(f, \,r; \epsilon)$-redundancy where $f$
and $r$ are the parameters of Byzantine failures and asynchrony, respectively,
and $\epsilon$ characterizes the closeness between agents' cost functions. This
allows us to quantify the level of redundancy present amongst the agents' cost
functions, for any given distributed optimization problem. We demonstrate, both
theoretically and empirically, the merits of our proposed redundancy model in
improving the robustness of DGD against asynchronous and Byzantine agents, and
their extensions to distributed stochastic gradient descent (D-SGD) for robust
distributed machine learning with asynchronous and Byzantine agents."
"Deep Learning Recommendation Models (DLRM) are widespread, account for a
considerable data center footprint, and grow by more than 1.5x per year. With
model size soon to be in terabytes range, leveraging Storage ClassMemory (SCM)
for inference enables lower power consumption and cost. This paper evaluates
the major challenges in extending the memory hierarchy to SCM for DLRM, and
presents different techniques to improve performance through a Software Defined
Memory. We show how underlying technologies such as Nand Flash and 3DXP
differentiate, and relate to real world scenarios, enabling from 5% to 29%
power savings."
"Block Floating Point (BFP) can efficiently support quantization for Deep
Neural Network (DNN) training by providing a wide dynamic range via a shared
exponent across a group of values. In this paper, we propose a Fast First,
Accurate Second Training (FAST) system for DNNs, where the weights,
activations, and gradients are represented in BFP. FAST supports matrix
multiplication with variable precision BFP input operands, enabling incremental
increases in DNN precision throughout training. By increasing the BFP precision
across both training iterations and DNN layers, FAST can greatly shorten the
training time while reducing overall hardware resource usage. Our FAST
Multipler-Accumulator (fMAC) supports dot product computations under multiple
BFP precisions. We validate our FAST system on multiple DNNs with different
datasets, demonstrating a 2-6$\times$ speedup in training on a single-chip
platform over prior work based on \textbf{mixed-precision or block} floating
point number systems while achieving similar performance in validation
accuracy."
"With regard to the implementation of WiFi sensing agnostic according to the
availability of channel state information (CSI), we investigate the possibility
of estimating a CSI matrix based on its compressed version, which is known as
beamforming feedback matrix (BFM). Being different from the CSI matrix that is
processed and discarded in physical layer components, the BFM can be captured
using a medium-access-layer frame-capturing technique because this is exchanged
among an access point (AP) and stations (STAs) over the air. This indicates
that WiFi sensing that leverages the BFM matrix is more practical to implement
using the pre-installed APs. However, the ability of BFM-based sensing has been
evaluated in a few tasks, and more general insights into its performance should
be provided. To fill this gap, we propose a CSI estimation method based on BFM,
approximating the estimation function with a machine learning model. In
addition, to improve the estimation accuracy, we leverage the inter-subcarrier
dependency using the BFMs at multiple subcarriers in orthogonal frequency
division multiplexing transmissions. Our simulation evaluation reveals that the
estimated CSI matches the ground-truth amplitude. Moreover, compared to CSI
estimation at each individual subcarrier, the effect of the BFMs at multiple
subcarriers on the CSI estimation accuracy is validated."
"We present a data-driven framework to automate the vectorization and machine
interpretation of 2D engineering part drawings. In industrial settings, most
manufacturing engineers still rely on manual reads to identify the topological
and manufacturing requirements from drawings submitted by designers. The
interpretation process is laborious and time-consuming, which severely inhibits
the efficiency of part quotation and manufacturing tasks. While recent advances
in image-based computer vision methods have demonstrated great potential in
interpreting natural images through semantic segmentation approaches, the
application of such methods in parsing engineering technical drawings into
semantically accurate components remains a significant challenge. The severe
pixel sparsity in engineering drawings also restricts the effective
featurization of image-based data-driven methods. To overcome these challenges,
we propose a deep learning based framework that predicts the semantic type of
each vectorized component. Taking a raster image as input, we vectorize all
components through thinning, stroke tracing, and cubic bezier fitting. Then a
graph of such components is generated based on the connectivity between the
components. Finally, a graph convolutional neural network is trained on this
graph data to identify the semantic type of each component. We test our
framework in the context of semantic segmentation of text, dimension and,
contour components in engineering drawings. Results show that our method yields
the best performance compared to recent image, and graph-based segmentation
methods."
"Protecting user data privacy can be achieved via many methods, from
statistical transformations to generative models. However, all of them have
critical drawbacks. For example, creating a transformed data set using
traditional techniques is highly time-consuming. Also, recent deep
learning-based solutions require significant computational resources in
addition to long training phases, and differentially private-based solutions
may undermine data utility. In this paper, we propose $\epsilon$-PrivateSMOTE,
a technique designed for safeguarding against re-identification and linkage
attacks, particularly addressing cases with a high \sloppy re-identification
risk. Our proposal combines synthetic data generation via noise-induced
interpolation with differential privacy principles to obfuscate high-risk
cases. We demonstrate how $\epsilon$-PrivateSMOTE is capable of achieving
competitive results in privacy risk and better predictive performance when
compared to multiple traditional and state-of-the-art privacy-preservation
methods, including generative adversarial networks, variational autoencoders,
and differential privacy baselines. We also show how our method improves time
requirements by at least a factor of 9 and is a resource-efficient solution
that ensures high performance without specialised hardware."
"Labeling large image datasets with attributes such as facial age or object
type is tedious and sometimes infeasible. Supervised machine learning methods
provide a highly accurate solution, but require manual labels which are often
unavailable. Zero-shot models (e.g., CLIP) do not require manual labels but are
not as accurate as supervised ones, particularly when the attribute is numeric.
We propose a new approach, CLIPPR (CLIP with Priors), which adapts zero-shot
models for regression and classification on unlabelled datasets. Our method
does not use any annotated images. Instead, we assume a prior over the label
distribution in the dataset. We then train an adapter network on top of CLIP
under two competing objectives: i) minimal change of predictions from the
original CLIP model ii) minimal distance between predicted and prior
distribution of labels. Additionally, we present a novel approach for selecting
prompts for Vision & Language models using a distributional prior. Our method
is effective and presents a significant improvement over the original model. We
demonstrate an improvement of 28% in mean absolute error on the UTK age
regression task. We also present promising results for classification
benchmarks, improving the classification accuracy on the ImageNet dataset by
2.83%, without using any labels."
"Learning about physical systems from quantum-enhanced experiments, relying on
a quantum memory and quantum processing, can outperform learning from
experiments in which only classical memory and processing are available.
Whereas quantum advantages have been established for a variety of state
learning tasks, quantum process learning allows for comparable advantages only
with a careful problem formulation and is less understood. We establish an
exponential quantum advantage for learning an unknown $n$-qubit quantum process
$\mathcal{N}$. We show that a quantum memory allows to efficiently solve the
following tasks: (a) learning the Pauli transfer matrix of an arbitrary
$\mathcal{N}$, (b) predicting expectation values of bounded Pauli-sparse
observables measured on the output of an arbitrary $\mathcal{N}$ upon input of
a Pauli-sparse state, and (c) predicting expectation values of arbitrary
bounded observables measured on the output of an unknown $\mathcal{N}$ with
sparse Pauli transfer matrix upon input of an arbitrary state. With quantum
memory, these tasks can be solved using linearly-in-$n$ many copies of the Choi
state of $\mathcal{N}$, and even time-efficiently in the case of (b). In
contrast, any learner without quantum memory requires exponentially-in-$n$ many
queries, even when querying $\mathcal{N}$ on subsystems of adaptively chosen
states and performing adaptively chosen measurements. In proving this
separation, we extend existing shadow tomography upper and lower bounds from
states to channels via the Choi-Jamiolkowski isomorphism. Moreover, we combine
Pauli transfer matrix learning with polynomial interpolation techniques to
develop a procedure for learning arbitrary Hamiltonians, which may have
non-local all-to-all interactions, from short-time dynamics. Our results
highlight the power of quantum-enhanced experiments for learning highly complex
quantum dynamics."
"Graph Neural Networks(GNNs) are a family of neural models tailored for
graph-structure data and have shown superior performance in learning
representations for graph-structured data. However, training GNNs on large
graphs remains challenging and a promising direction is distributed GNN
training, which is to partition the input graph and distribute the workload
across multiple machines. The key bottleneck of the existing distributed GNNs
training framework is the across-machine communication induced by the
dependency on the graph data and aggregation operator of GNNs. In this paper,
we study the communication complexity during distributed GNNs training and
propose a simple lossless communication reduction method, termed the
Aggregation before Communication (ABC) method. ABC method exploits the
permutation-invariant property of the GNNs layer and leads to a paradigm where
vertex-cut is proved to admit a superior communication performance than the
currently popular paradigm (edge-cut). In addition, we show that the new
partition paradigm is particularly ideal in the case of dynamic graphs where it
is infeasible to control the edge placement due to the unknown stochastic of
the graph-changing process."
"Continuous long-term monitoring of motor health is crucial for the early
detection of abnormalities such as bearing faults (up to 51% of motor failures
are attributed to bearing faults). Despite numerous methodologies proposed for
bearing fault detection, most of them require normal (healthy) and abnormal
(faulty) data for training. Even with the recent deep learning (DL)
methodologies trained on the labeled data from the same machine, the
classification accuracy significantly deteriorates when one or few conditions
are altered. Furthermore, their performance suffers significantly or may
entirely fail when they are tested on another machine with entirely different
healthy and faulty signal patterns. To address this need, in this pilot study,
we propose a zero-shot bearing fault detection method that can detect any fault
on a new (target) machine regardless of the working conditions, sensor
parameters, or fault characteristics. To accomplish this objective, a 1D
Operational Generative Adversarial Network (Op-GAN) first characterizes the
transition between normal and fault vibration signals of (a) source machine(s)
under various conditions, sensor parameters, and fault types. Then for a target
machine, the potential faulty signals can be generated, and over its actual
healthy and synthesized faulty signals, a compact, and lightweight 1D Self-ONN
fault detector can then be trained to detect the real faulty condition in real
time whenever it occurs. To validate the proposed approach, a new benchmark
dataset is created using two different motors working under different
conditions and sensor locations. Experimental results demonstrate that this
novel approach can accurately detect any bearing fault achieving an average
recall rate of around 89% and 95% on two target machines regardless of its
type, severity, and location."
"Machine learning (ML) has recently facilitated many advances in solving
problems related to many-body physical systems. Given the intrinsic quantum
nature of these problems, it is natural to speculate that quantum-enhanced
machine learning will enable us to unveil even greater details than we
currently have. With this motivation, this paper examines a quantum machine
learning approach based on shallow variational ansatz inspired by tensor
networks for supervised learning tasks. In particular, we first look at the
standard image classification tasks using the Fashion-MNIST dataset and study
the effect of repeating tensor network layers on ansatz's expressibility and
performance. Finally, we use this strategy to tackle the problem of quantum
phase recognition for the transverse-field Ising and Heisenberg spin models in
one and two dimensions, where we were able to reach $\geq 98\%$ test-set
accuracies with both multi-scale entanglement renormalization ansatz (MERA) and
tree tensor network (TTN) inspired parametrized quantum circuits."
"Obtaining labelled data in a particular context could be expensive and time
consuming. Although different algorithms, including unsupervised learning,
semi-supervised learning, self-learning have been adopted, the performance of
text classification varies with context. Given the lack of labelled dataset, we
proposed a novel and simple unsupervised text classification model to classify
cargo content in international shipping industry using the Standard
International Trade Classification (SITC) codes. Our method stems from
representing words using pretrained Glove Word Embeddings and finding the most
likely label using Cosine Similarity. To compare unsupervised text
classification model with supervised classification, we also applied several
Transformer models to classify cargo content. Due to lack of training data, the
SITC numerical codes and the corresponding textual descriptions were used as
training data. A small number of manually labelled cargo content data was used
to evaluate the classification performances of the unsupervised classification
and the Transformer based supervised classification. The comparison reveals
that unsupervised classification significantly outperforms Transformer based
supervised classification even after increasing the size of the training
dataset by 30%. Lacking training data is a key bottleneck that prohibits deep
learning models (such as Transformers) from successful practical applications.
Unsupervised classification can provide an alternative efficient and effective
method to classify text when there is scarce training data."
"To improve the uncertainty quantification of variance networks, we propose a
novel tree-structured local neural network model that partitions the feature
space into multiple regions based on uncertainty heterogeneity. A tree is built
upon giving the training data, whose leaf nodes represent different regions
where region-specific neural networks are trained to predict both the mean and
the variance for quantifying uncertainty. The proposed Uncertainty-Splitting
Neural Regression Tree (USNRT) employs novel splitting criteria. At each node,
a neural network is trained on the full data first, and a statistical test for
the residuals is conducted to find the best split, corresponding to the two
sub-regions with the most significant uncertainty heterogeneity between them.
USNRT is computationally friendly because very few leaf nodes are sufficient
and pruning is unnecessary. Furthermore, an ensemble version can be easily
constructed to estimate the total uncertainty including the aleatory and
epistemic. On extensive UCI datasets, USNRT or its ensemble shows superior
performance compared to some recent popular methods for quantifying uncertainty
with variances. Through comprehensive visualization and analysis, we uncover
how USNRT works and show its merits, revealing that uncertainty heterogeneity
does exist in many datasets and can be learned by USNRT."
"Due to the environmental impacts caused by the construction industry,
repurposing existing buildings and making them more energy-efficient has become
a high-priority issue. However, a legitimate concern of land developers is
associated with the buildings' state of conservation. For that reason, infrared
thermography has been used as a powerful tool to characterize these buildings'
state of conservation by detecting pathologies, such as cracks and humidity.
Thermal cameras detect the radiation emitted by any material and translate it
into temperature-color-coded images. Abnormal temperature changes may indicate
the presence of pathologies, however, reading thermal images might not be quite
simple. This research project aims to combine infrared thermography and machine
learning (ML) to help stakeholders determine the viability of reusing existing
buildings by identifying their pathologies and defects more efficiently and
accurately. In this particular phase of this research project, we've used an
image classification machine learning model of Convolutional Neural Networks
(DCNN) to differentiate three levels of cracks in one particular building. The
model's accuracy was compared between the MSX and thermal images acquired from
two distinct thermal cameras and fused images (formed through multisource
information) to test the influence of the input data and network on the
detection results."
"In this paper, we investigate a multivariate multi-response (MVMR) linear
regression problem, which contains multiple linear regression models with
differently distributed design matrices, and different regression and output
vectors. The goal is to recover the support union of all regression vectors
using $l_1/l_2$-regularized Lasso. We characterize sufficient and necessary
conditions on sample complexity \emph{as a sharp threshold} to guarantee
successful recovery of the support union. Namely, if the sample size is above
the threshold, then $l_1/l_2$-regularized Lasso correctly recovers the support
union; and if the sample size is below the threshold, $l_1/l_2$-regularized
Lasso fails to recover the support union. In particular, the threshold
precisely captures the impact of the sparsity of regression vectors and the
statistical properties of the design matrices on sample complexity. Therefore,
the threshold function also captures the advantages of joint support union
recovery using multi-task Lasso over individual support recovery using
single-task Lasso."
"Natural gradient descent is an optimization method traditionally motivated
from the perspective of information geometry, and works well for many
applications as an alternative to stochastic gradient descent. In this paper we
critically analyze this method and its properties, and show how it can be
viewed as a type of 2nd-order optimization method, with the Fisher information
matrix acting as a substitute for the Hessian. In many important cases, the
Fisher information matrix is shown to be equivalent to the Generalized
Gauss-Newton matrix, which both approximates the Hessian, but also has certain
properties that favor its use over the Hessian. This perspective turns out to
have significant implications for the design of a practical and robust natural
gradient optimizer, as it motivates the use of techniques like trust regions
and Tikhonov regularization. Additionally, we make a series of contributions to
the understanding of natural gradient and 2nd-order methods, including: a
thorough analysis of the convergence speed of stochastic natural gradient
descent (and more general stochastic 2nd-order methods) as applied to convex
quadratics, a critical examination of the oft-used ""empirical"" approximation of
the Fisher matrix, and an analysis of the (approximate) parameterization
invariance property possessed by natural gradient methods (which we show also
holds for certain other curvature, but notably not the Hessian)."
"Stacked denoising auto encoders (DAEs) are well known to learn useful deep
representations, which can be used to improve supervised training by
initializing a deep network. We investigate a training scheme of a deep DAE,
where DAE layers are gradually added and keep adapting as additional layers are
added. We show that in the regime of mid-sized datasets, this gradual training
provides a small but consistent improvement over stacked training in both
reconstruction quality and classification error over stacked training on MNIST
and CIFAR datasets."
"Human activity recognition (HAR) has become a popular topic in research
because of its wide application. With the development of deep learning, new
ideas have appeared to address HAR problems. Here, a deep network architecture
using residual bidirectional long short-term memory (LSTM) cells is proposed.
The advantages of the new network include that a bidirectional connection can
concatenate the positive time direction (forward state) and the negative time
direction (backward state). Second, residual connections between stacked cells
act as highways for gradients, which can pass underlying information directly
to the upper layer, effectively avoiding the gradient vanishing problem.
Generally, the proposed network shows improvements on both the temporal (using
bidirectional cells) and the spatial (residual connections stacked deeply)
dimensions, aiming to enhance the recognition rate. When tested with the
Opportunity data set and the public domain UCI data set, the accuracy was
increased by 4.78% and 3.68%, respectively, compared with previously reported
results. Finally, the confusion matrix of the public domain UCI data set was
analyzed."
"We present an algorithm to identify sparse dependence structure in continuous
and non-Gaussian probability distributions, given a corresponding set of data.
The conditional independence structure of an arbitrary distribution can be
represented as an undirected graph (or Markov random field), but most
algorithms for learning this structure are restricted to the discrete or
Gaussian cases. Our new approach allows for more realistic and accurate
descriptions of the distribution in question, and in turn better estimates of
its sparse Markov structure. Sparsity in the graph is of interest as it can
accelerate inference, improve sampling methods, and reveal important
dependencies between variables. The algorithm relies on exploiting the
connection between the sparsity of the graph and the sparsity of transport
maps, which deterministically couple one probability measure to another."
"Many problems in machine learning are naturally expressed in the language of
undirected graphical models. Here, we propose black-box learning and inference
algorithms for undirected models that optimize a variational approximation to
the log-likelihood of the model. Central to our approach is an upper bound on
the log-partition function parametrized by a function q that we express as a
flexible neural network. Our bound makes it possible to track the partition
function during learning, to speed-up sampling, and to train a broad class of
hybrid directed/undirected models via a unified variational inference
framework. We empirically demonstrate the effectiveness of our method on
several popular generative modeling datasets."
"Neuroblastoma is a strongly heterogeneous cancer with very diverse clinical
courses that may vary from spontaneous regression to fatal progression; an
accurate patient's risk estimation at diagnosis is essential to design
appropriate tumor treatment strategies. Neuroblastoma is a paradigm disease
where different diagnostic and prognostic endpoints should be predicted from
common molecular and clinical information, with increasing complexity, as shown
in the FDA MAQC-II study. Here we introduce the novel multiobjective deep
learning architecture CDRP (Concatenated Diagnostic Relapse Prognostic)
composed by 8 layers to obtain a combined diagnostic and prognostic prediction
from high-throughput transcriptomics data. Two distinct loss functions are
optimized for the Event Free Survival (EFS) and Overall Survival (OS)
prognosis, respectively. We use the High-Risk (HR) diagnostic information as an
additional input generated by an autoencoder embedding. The latter is used as
network regulariser, based on a clinical algorithm commonly adopted for
stratifying patients from cancer stage, age at insurgence of disease, and MYCN,
the specific molecular marker. The architecture was applied to Illumina
HiSeq2000 RNA-Seq for 498 neuroblastoma patients (176 at high risk) from the
Sequencing Quality Control (SEQC) study, obtaining state-of-art on the
diagnostic endpoint and improving prediction of prognosis over the HR cohort."
"Neural networks dominate the modern machine learning landscape, but their
training and success still suffer from sensitivity to empirical choices of
hyperparameters such as model architecture, loss function, and optimisation
algorithm. In this work we present \emph{Population Based Training (PBT)}, a
simple asynchronous optimisation algorithm which effectively utilises a fixed
computational budget to jointly optimise a population of models and their
hyperparameters to maximise performance. Importantly, PBT discovers a schedule
of hyperparameter settings rather than following the generally sub-optimal
strategy of trying to find a single fixed set to use for the whole course of
training. With just a small modification to a typical distributed
hyperparameter training framework, our method allows robust and reliable
training of models. We demonstrate the effectiveness of PBT on deep
reinforcement learning problems, showing faster wall-clock convergence and
higher final performance of agents by optimising over a suite of
hyperparameters. In addition, we show the same method can be applied to
supervised learning for machine translation, where PBT is used to maximise the
BLEU score directly, and also to training of Generative Adversarial Networks to
maximise the Inception score of generated images. In all cases PBT results in
the automatic discovery of hyperparameter schedules and model selection which
results in stable training and better final performance."
"Machine-assisted treatment recommendations hold a promise to reduce physician
time and decision errors. We formulate the task as a sequence-to-sequence
prediction model that takes the entire time-ordered medical history as input,
and predicts a sequence of future clinical procedures and medications. It is
built on the premise that an effective treatment plan may have long-term
dependencies from previous medical history. We approach the problem by using a
memory-augmented neural network, in particular, by leveraging the recent
differentiable neural computer that consists of a neural controller and an
external memory module. But differing from the original model, we use dual
controllers, one for encoding the history followed by another for decoding the
treatment sequences. In the encoding phase, the memory is updated as new input
is read; at the end of this phase, the memory holds not only the medical
history but also the information about the current illness. During the decoding
phase, the memory is write-protected. The decoding controller generates a
treatment sequence, one treatment option at a time. The resulting dual
controller write-protected memory-augmented neural network is demonstrated on
the MIMIC-III dataset on two tasks: procedure prediction and medication
prescription. The results show improved performance over both traditional
bag-of-words and sequence-to-sequence methods."
"The costs associated with refrigerator equipment often represent more than
half of the total energy costs in supermarkets. This presents a good motivation
for running these systems efficiently. In this study, we investigate different
ways to construct a reference behavior, which can serve as a baseline for
judging the performance of energy consumption. We used 3 distinct learning
models: Multiple Linear Regression, Random Forests, and Artificial Neural
Networks. During our experiments we used a variation of the sliding window
method in combination with learning curves. We applied this approach on five
different supermarkets, across Portugal. We are able to create baselines using
off-the-shelf data mining techniques. Moreover, we found a way to create them
based on short term historical data. We believe that our research will serve as
a base for future studies, for which we provide interesting directions."
"Advances in unsupervised learning enable reconstruction and generation of
samples from complex distributions, but this success is marred by the
inscrutability of the representations learned. We propose an
information-theoretic approach to characterizing disentanglement and dependence
in representation learning using multivariate mutual information, also called
total correlation. The principle of total Cor-relation Ex-planation (CorEx) has
motivated successful unsupervised learning applications across a variety of
domains, but under some restrictive assumptions. Here we relax those
restrictions by introducing a flexible variational lower bound to CorEx.
Surprisingly, we find that this lower bound is equivalent to the one in
variational autoencoders (VAE) under certain conditions. This
information-theoretic view of VAE deepens our understanding of hierarchical VAE
and motivates a new algorithm, AnchorVAE, that makes latent codes more
interpretable through information maximization and enables generation of richer
and more realistic samples."
"The DEBS Grand Challenge 2018 is set in the context of maritime route
prediction. Vessel routes are modeled as streams of Automatic Identification
System (AIS) data points selected from real-world tracking data. The challenge
requires to correctly estimate the destination ports and arrival times of
vessel trips, as early as possible. Our proposed solution partitions the
training vessel routes by reported destination port and uses a nearest neighbor
search to find the training routes that are closer to the query AIS point.
Particular improvements have been included as well, such as a way to avoid
changing the predicted ports frequently within one query route and automating
the parameters tuning by the use of a genetic algorithm. This leads to
significant improvements on the final score."
"It is very useful to integrate human knowledge and experience into
traditional neural networks for faster learning speed, fewer training samples
and better interpretability. However, due to the obscured and indescribable
black box model of neural networks, it is very difficult to design its
architecture, interpret its features and predict its performance. Inspired by
human visual cognition process, we propose a knowledge-guided semantic
computing network which includes two modules: a knowledge-guided semantic tree
and a data-driven neural network. The semantic tree is pre-defined to describe
the spatial structural relations of different semantics, which just corresponds
to the tree-like description of objects based on human knowledge. The object
recognition process through the semantic tree only needs simple forward
computing without training. Besides, to enhance the recognition ability of the
semantic tree in aspects of the diversity, randomicity and variability, we use
the traditional neural network to aid the semantic tree to learn some
indescribable features. Only in this case, the training process is needed. The
experimental results on MNIST and GTSRB datasets show that compared with the
traditional data-driven network, our proposed semantic computing network can
achieve better performance with fewer training samples and lower computational
complexity. Especially, Our model also has better adversarial robustness than
traditional neural network with the help of human knowledge."
"Stochastic optimization techniques are standard in variational inference
algorithms. These methods estimate gradients by approximating expectations with
independent Monte Carlo samples. In this paper, we explore a technique that
uses correlated, but more representative , samples to reduce estimator
variance. Specifically, we show how to generate antithetic samples that match
sample moments with the true moments of an underlying importance distribution.
Combining a differentiable antithetic sampler with modern stochastic
variational inference, we showcase the effectiveness of this approach for
learning a deep generative model."
"We develop a new theoretical framework, the \emph{envelope complexity}, to
analyze the minimax regret with logarithmic loss functions and derive a
Bayesian predictor that adaptively achieves the minimax regret over
high-dimensional $\ell_1$-balls within a factor of two. The prior is newly
derived for achieving the minimax regret and called the
\emph{spike-and-tails~(ST) prior} as it looks like. The resulting regret bound
is so simple that it is completely determined with the smoothness of the loss
function and the radius of the balls except with logarithmic factors, and it
has a generalized form of existing regret/risk bounds. In the preliminary
experiment, we confirm that the ST prior outperforms the conventional
minimax-regret prior under non-high-dimensional asymptotics."
"We consider the problem of building a state representation model in a
continual fashion. As the environment changes, the aim is to efficiently
compress the sensory state's information without losing past knowledge. The
learned features are then fed to a Reinforcement Learning algorithm to learn a
policy. We propose to use Variational Auto-Encoders for state representation,
and Generative Replay, i.e. the use of generated samples, to maintain past
knowledge. We also provide a general and statistically sound method for
automatic environment change detection. Our method provides efficient state
representation as well as forward transfer, and avoids catastrophic forgetting.
The resulting model is capable of incrementally learning information without
using past data and with a bounded system size."
"Distance metric learning (DML) aims to find an appropriate way to reveal the
underlying data relationship. It is critical in many machine learning, pattern
recognition and data mining algorithms, and usually require large amount of
label information (such as class labels or pair/triplet constraints) to achieve
satisfactory performance. However, the label information may be insufficient in
real-world applications due to the high-labeling cost, and DML may fail in this
case. Transfer metric learning (TML) is able to mitigate this issue for DML in
the domain of interest (target domain) by leveraging knowledge/information from
other related domains (source domains). Although achieved a certain level of
development, TML has limited success in various aspects such as selective
transfer, theoretical understanding, handling complex data, big data and
extreme cases. In this survey, we present a systematic review of the TML
literature. In particular, we group TML into different categories according to
different settings and metric transfer strategies, such as direct metric
approximation, subspace approximation, distance approximation, and distribution
approximation. A summarization and insightful discussion of the various TML
approaches and their applications will be presented. Finally, we indicate some
challenges and provide possible future directions."
"In the $k$-nearest neighborhood model ($k$-NN), we are given a set of points
$P$, and we shall answer queries $q$ by returning the $k$ nearest neighbors of
$q$ in $P$ according to some metric. This concept is crucial in many areas of
data analysis and data processing, e.g., computer vision, document retrieval
and machine learning. Many $k$-NN algorithms have been published and
implemented, but often the relation between parameters and accuracy of the
computed $k$-NN is not explicit. We study property testing of $k$-NN graphs in
theory and evaluate it empirically: given a point set $P \subset
\mathbb{R}^\delta$ and a directed graph $G=(P,E)$, is $G$ a $k$-NN graph, i.e.,
every point $p \in P$ has outgoing edges to its $k$ nearest neighbors, or is it
$\epsilon$-far from being a $k$-NN graph? Here, $\epsilon$-far means that one
has to change more than an $\epsilon$-fraction of the edges in order to make
$G$ a $k$-NN graph. We develop a randomized algorithm with one-sided error that
decides this question, i.e., a property tester for the $k$-NN property, with
complexity $O(\sqrt{n} k^2 / \epsilon^2)$ measured in terms of the number of
vertices and edges it inspects, and we prove a lower bound of $\Omega(\sqrt{n /
\epsilon k})$. We evaluate our tester empirically on the $k$-NN models computed
by various algorithms and show that it can be used to detect $k$-NN models with
bad accuracy in significantly less time than the building time of the $k$-NN
model."
"Understanding the uncertainty of a neural network's (NN) predictions is
essential for many purposes. The Bayesian framework provides a principled
approach to this, however applying it to NNs is challenging due to large
numbers of parameters and data. Ensembling NNs provides an easily
implementable, scalable method for uncertainty quantification, however, it has
been criticised for not being Bayesian. This work proposes one modification to
the usual process that we argue does result in approximate Bayesian inference;
regularising parameters about values drawn from a distribution which can be set
equal to the prior. A theoretical analysis of the procedure in a simplified
setting suggests the recovered posterior is centred correctly but tends to have
an underestimated marginal variance, and overestimated correlation. However,
two conditions can lead to exact recovery. We argue that these conditions are
partially present in NNs. Empirical evaluations demonstrate it has an advantage
over standard ensembling, and is competitive with variational methods."
"This paper presents a technology for simple and computationally efficient
improvements of a generic Artificial Intelligence (AI) system, including
Multilayer and Deep Learning neural networks. The improvements are, in essence,
small network ensembles constructed on top of the existing AI architectures.
Theoretical foundations of the technology are based on Stochastic Separation
Theorems and the ideas of the concentration of measure. We show that, subject
to mild technical assumptions on statistical properties of internal signals in
the original AI system, the technology enables instantaneous and
computationally efficient removal of spurious and systematic errors with
probability close to one on the datasets which are exponentially large in
dimension. The method is illustrated with numerical examples and a case study
of ten digits recognition from American Sign Language."
"Batch Normalization (BN) has been used extensively in deep learning to
achieve faster training process and better resulting models. However, whether
BN works strongly depends on how the batches are constructed during training
and it may not converge to a desired solution if the statistics on a batch are
not close to the statistics over the whole dataset. In this paper, we try to
understand BN from an optimization perspective by formulating the optimization
problem which motivates BN. We show when BN works and when BN does not work by
analyzing the optimization problem. We then propose a refinement of BN based on
compositional optimization techniques called Full Normalization (FN) to
alleviate the issues of BN when the batches are not constructed ideally. We
provide convergence analysis for FN and empirically study its effectiveness to
refine BN."
"It has been shown that neural network classifiers are not robust. This raises
concerns about their usage in safety-critical systems. We propose in this paper
a regularization scheme for ReLU networks which provably improves the
robustness of the classifier by maximizing the linear regions of the classifier
as well as the distance to the decision boundary. Our techniques allow even to
find the minimal adversarial perturbation for a fraction of test points for
large networks. In the experiments we show that our approach improves upon
adversarial training both in terms of lower and upper bounds on the robustness
and is comparable or better than the state-of-the-art in terms of test error
and robustness."
"We consider a finite-armed structured bandit problem in which mean rewards of
different arms are known functions of a common hidden parameter $\theta^*$.
Since we do not place any restrictions of these functions, the problem setting
subsumes several previously studied frameworks that assume linear or invertible
reward functions. We propose a novel approach to gradually estimate the hidden
$\theta^*$ and use the estimate together with the mean reward functions to
substantially reduce exploration of sub-optimal arms. This approach enables us
to fundamentally generalize any classic bandit algorithm including UCB and
Thompson Sampling to the structured bandit setting. We prove via regret
analysis that our proposed UCB-C algorithm (structured bandit versions of UCB)
pulls only a subset of the sub-optimal arms $O(\log T)$ times while the other
sub-optimal arms (referred to as non-competitive arms) are pulled $O(1)$ times.
As a result, in cases where all sub-optimal arms are non-competitive, which can
happen in many practical scenarios, the proposed algorithms achieve bounded
regret. We also conduct simulations on the Movielens recommendations dataset to
demonstrate the improvement of the proposed algorithms over existing structured
bandit algorithms."
"Probabilistic matrix factorization (PMF) plays a crucial role in
recommendation systems. It requires a large amount of user data (such as user
shopping records and movie ratings) to predict personal preferences, and
thereby provides users high-quality recommendation services, which expose the
risk of leakage of user privacy. Differential privacy, as a provable privacy
protection framework, has been applied widely to recommendation systems. It is
common that different individuals have different levels of privacy requirements
on items. However, traditional differential privacy can only provide a uniform
level of privacy protection for all users.
  In this paper, we mainly propose a probabilistic matrix factorization
recommendation scheme with personalized differential privacy (PDP-PMF). It aims
to meet users' privacy requirements specified at the item-level instead of
giving the same level of privacy guarantees for all. We then develop a modified
sampling mechanism (with bounded differential privacy) for achieving PDP. We
also perform a theoretical analysis of the PDP-PMF scheme and demonstrate the
privacy of the PDP-PMF scheme. In addition, we implement the probabilistic
matrix factorization schemes both with traditional and with personalized
differential privacy (DP-PMF, PDP-PMF) and compare them through a series of
experiments. The results show that the PDP-PMF scheme performs well on
protecting the privacy of each user and its recommendation quality is much
better than the DP-PMF scheme."
"This paper examines, if it is possible to learn structural invariants of city
images by using only a single reference picture when producing transformations
along the variants in the dataset. Previous work explored the problem of
learning from only a few examples and showed that data augmentation techniques
benefit performance and generalization for machine learning approaches. First a
principal component analysis in conjunction with a Fourier transform is trained
on a single reference augmentation training dataset using the city images.
Secondly a convolutional neural network is trained on a similar dataset with
more samples. The findings are that the convolutional neural network is capable
of finding images of the same category whereas the applied principal component
analysis in conjunction with a Fourier transform failed to solve this task."
"Our main motivation is to propose an efficient approach to generate novel
multi-element stable chemical compounds that can be used in real world
applications. This task can be formulated as a combinatorial problem, and it
takes many hours of human experts to construct, and to evaluate new data.
Unsupervised learning methods such as Generative Adversarial Networks (GANs)
can be efficiently used to produce new data. Cross-domain Generative
Adversarial Networks were reported to achieve exciting results in image
processing applications. However, in the domain of materials science, there is
a need to synthesize data with higher order complexity compared to observed
samples, and the state-of-the-art cross-domain GANs can not be adapted
directly. In this contribution, we propose a novel GAN called CrystalGAN which
generates new chemically stable crystallographic structures with increased
domain complexity. We introduce an original architecture, we provide the
corresponding loss functions, and we show that the CrystalGAN generates very
reasonable data. We illustrate the efficiency of the proposed method on a real
original problem of novel hydrides discovery that can be further used in
development of hydrogen storage materials."
"Techniques for understanding the functioning of complex machine learning
models are becoming increasingly popular, not only to improve the validation
process, but also to extract new insights about the data via exploratory
analysis. Though a large class of such tools currently exists, most assume that
predictions are point estimates and use a sensitivity analysis of these
estimates to interpret the model. Using lightweight probabilistic networks we
show how including prediction uncertainties in the sensitivity analysis leads
to: (i) more robust and generalizable models; and (ii) a new approach for model
interpretation through uncertainty decomposition. In particular, we introduce a
new regularization that takes both the mean and variance of a prediction into
account and demonstrate that the resulting networks provide improved
generalization to unseen data. Furthermore, we propose a new technique to
explain prediction uncertainties through uncertainties in the input domain,
thus providing new ways to validate and interpret deep learning models."
"Homomorphic Encryption (HE) is one of the most promising security solutions
to emerging Machine Learning as a Service (MLaaS). Leveled-HE (LHE)-enabled
Convolutional Neural Networks (LHECNNs) are proposed to implement MLaaS to
avoid large bootstrapping overhead. However, prior LHECNNs have to pay
significant computing overhead but achieve only low inference accuracy, due to
their polynomial approximation activations and poolings. Stacking many
polynomial approximation activation layers in a network greatly reduces
inference accuracy, since the polynomial approximation activation errors lead
to a low distortion of the output distribution of the next batch normalization
layer. So the polynomial approximation activations and poolings have become the
obstacle to a fast and accurate LHECNN model.
  In this paper, we propose a Shift-accumulation-based LHE-enabled deep neural
network (SHE) for fast and accurate inferences on encrypted data. We use the
binary-operation-friendly Leveled Fast Homomorphic Encryption over Torus
(LTFHE) encryption scheme to implement ReLU activations and max poolings. We
also adopt the logarithmic quantization to accelerate inferences by replacing
expensive LTFHE multiplications with cheap LTFHE shifts. We propose a mixed
bitwidth accumulator to accelerate accumulations. Since the LTFHE ReLU
activations, max poolings, shifts and accumulations have small multiplicative
depth overhead, SHE can implement much deeper network architectures with more
convolutional and activation layers. Our experimental results show SHE achieves
the state-of-the-art inference accuracy and reduces the inference latency by
76.21% ~ 94.23% over prior LHECNNs on MNIST and CIFAR-10. The source code of
SHE is available at https://github.com/qianlou/SHE."
"Fingerprint presentation attack detection (FPAD) is becoming an increasingly
challenging problem due to the continuous advancement of attack techniques,
which generate `realistic-looking' fake fingerprint presentations. Recently,
laser speckle contrast imaging (LSCI) has been introduced as a new sensing
modality for FPAD. LSCI has the interesting characteristic of capturing the
blood flow under the skin surface. Toward studying the importance and
effectiveness of LSCI for FPAD, we conduct a comprehensive study using
different patch-based deep neural network architectures. Our studied
architectures include 2D and 3D convolutional networks as well as a recurrent
network using long short-term memory (LSTM) units. The study demonstrates that
strong FPAD performance can be achieved using LSCI. We evaluate the different
models over a new large dataset. The dataset consists of 3743 bona fide
samples, collected from 335 unique subjects, and 218 presentation attack
samples, including six different types of attacks. To examine the effect of
changing the training and testing sets, we conduct a 3-fold cross validation
evaluation. To examine the effect of the presence of an unseen attack, we apply
a leave-one-attack out strategy. The FPAD classification results of the
networks, which are separately optimized and tuned for the temporal and spatial
patch-sizes, indicate that the best performance is achieved by LSTM."
"This paper presents a novel method which simultaneously learns the number of
filters and network features repeatedly over multiple epochs. We propose a
novel pruning loss to explicitly enforces the optimizer to focus on promising
candidate filters while suppressing contributions of less relevant ones. In the
meanwhile, we further propose to enforce the diversities between filters and
this diversity-based regularization term improves the trade-off between model
sizes and accuracies. It turns out the interplay between architecture and
feature optimizations improves the final compressed models, and the proposed
method is compared favorably to existing methods, in terms of both models sizes
and accuracies for a wide range of applications including image classification,
image compression and audio classification."
"Adversarial training is an effective methodology for training deep neural
networks that are robust against adversarial, norm-bounded perturbations.
However, the computational cost of adversarial training grows prohibitively as
the size of the model and number of input dimensions increase. Further,
training against less expensive and therefore weaker adversaries produces
models that are robust against weak attacks but break down under attacks that
are stronger. This is often attributed to the phenomenon of gradient
obfuscation; such models have a highly non-linear loss surface in the vicinity
of training examples, making it hard for gradient-based attacks to succeed even
though adversarial examples still exist. In this work, we introduce a novel
regularizer that encourages the loss to behave linearly in the vicinity of the
training data, thereby penalizing gradient obfuscation while encouraging
robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that
models trained with our regularizer avoid gradient obfuscation and can be
trained significantly faster than adversarial training. Using this regularizer,
we exceed current state of the art and achieve 47% adversarial accuracy for
ImageNet with l-infinity adversarial perturbations of radius 4/255 under an
untargeted, strong, white-box attack. Additionally, we match state of the art
results for CIFAR-10 at 8/255."
"Sparse Principal Component Analysis (sPCA) is a popular matrix factorization
approach based on Principal Component Analysis (PCA) that combines variance
maximization and sparsity with the ultimate goal of improving data
interpretation. When moving from PCA to sPCA, there are a number of
implications that the practitioner needs to be aware of. A relevant one is that
scores and loadings in sPCA may not be orthogonal. For this reason, the
traditional way of computing scores, residuals and variance explained that is
used in the classical PCA cannot directly be applied to sPCA models. This also
affects how sPCA components should be visualized. In this paper we illustrate
this problem both theoretically and numerically using simulations for several
state-of-the-art sPCA algorithms, and provide proper computation of the
different elements mentioned. We show that sPCA approaches present disparate
and limited performance when modeling noise-free, sparse data. In a follow-up
paper, we discuss the theoretical properties that lead to this problem."
"The Dynamical Gaussian Process Latent Variable Models provide an elegant
non-parametric framework for learning the low dimensional representations of
the high-dimensional time-series. Real world observational studies, however,
are often ill-conditioned: the observations can be noisy, not assuming the
luxury of relatively complete and equally spaced like those in time series.
Such conditions make it difficult to learn reasonable representations in the
high dimensional longitudinal data set by way of Gaussian Process Latent
Variable Model as well as other dimensionality reduction procedures. In this
study, we approach the inference of Gaussian Process Dynamical Systems in
Longitudinal scenario by augmenting the bound in the variational approximation
to include systematic samples of the unseen observations. We demonstrate the
usefulness of this approach on synthetic as well as the human motion capture
data set."
"Recent literature on unsupervised learning focused on designing structural
priors with the aim of learning meaningful features, but without considering
the description length of the representations. In this thesis, first we
introduce the $\varphi$ metric that evaluates unsupervised models based on
their reconstruction accuracy and the degree of compression of their internal
representations. We then present and define two activation functions (Identity,
ReLU) as base of reference and three sparse activation functions (top-k
absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize
the previously defined metric $\varphi$. We lastly present Sparsely Activated
Networks (SANs) that consist of kernels with shared weights that, during
encoding, are convolved with the input and then passed through a sparse
activation function. During decoding, the same weights are convolved with the
sparse activation map and subsequently the partial reconstructions from each
weight are summed to reconstruct the input. We compare SANs using the five
previously defined activation functions on a variety of datasets (Physionet,
UCI-epilepsy, MNIST, FMNIST) and show that models that are selected using
$\varphi$ have small description representation length and consist of
interpretable kernels."
"We propose a new model for supervised learning to rank. In our model, the
relevance labels are assumed to follow a categorical distribution whose
probabilities are constructed based on a scoring function. We optimize the
training objective with respect to the multivariate categorical variables with
an unbiased and low-variance gradient estimator. Learning-to-rank methods can
generally be categorized into pointwise, pairwise, and listwise approaches.
Although our scoring function is pointwise, the proposed framework permits
flexibility over the choice of the loss function. In our new model, the loss
function need not be differentiable and can either be pointwise or listwise.
Our proposed method achieves better or comparable results on two datasets
compared with existing pairwise and listwise methods."
"The Responsibility-Sensitive Safety (RSS) model offers provable safety for
vehicle behaviors such as minimum safe following distance. However, handling
worst-case variability and uncertainty may significantly lower vehicle
permissiveness, and in some situations safety cannot be guaranteed. Digging
deeper into Newtonian mechanics, we identify complications that result from
considering vehicle status, road geometry and environmental parameters. An
especially challenging situation occurs if these parameters change during the
course of a collision avoidance maneuver such as hard braking. As part of our
analysis, we expand the original RSS following distance equation to account for
edge cases involving potential collisions mid-way through a braking process. We
additionally propose a Micro-Operational Design Domain ({\mu}ODD) approach to
subdividing the operational space as a way of improving permissiveness.
Confining probabilistic aspects of safety to {\mu}ODD transitions permits
proving safety (when possible) under the assumption that the system has
transitioned to the correct {\mu}ODD for the situation. Each {\mu}ODD can
additionally be used to encode system fault responses, take credit for advisory
information (e.g., from vehicle-to-vehicle communication), and anticipate
likely emergent situations."
"This paper extends the recently introduced assignment flow approach for
supervised image labeling to unsupervised scenarios where no labels are given.
The resulting self-assignment flow takes a pairwise data affinity matrix as
input data and maximizes the correlation with a low-rank matrix that is
parametrized by the variables of the assignment flow, which entails an
assignment of the data to themselves through the formation of latent labels
(feature prototypes). A single user parameter, the neighborhood size for the
geometric regularization of assignments, drives the entire process. By smooth
geodesic interpolation between different normalizations of self-assignment
matrices on the positive definite matrix manifold, a one-parameter family of
self-assignment flows is defined. Accordingly, our approach can be
characterized from different viewpoints, e.g. as performing spatially
regularized, rank-constrained discrete optimal transport, or as computing
spatially regularized normalized spectral cuts. Regarding combinatorial
optimization, our approach successfully determines completely positive
factorizations of self-assignments in large-scale scenarios, subject to spatial
regularization. Various experiments including the unsupervised learning of
patch dictionaries using a locally invariant distance function, illustrate the
properties of the approach."
"Pretrained language models have led to significant performance gains in many
NLP tasks. However, the intensive computing resources to train such models
remain an issue. Knowledge distillation alleviates this problem by learning a
light-weight student model. So far the distillation approaches are all
task-specific. In this paper, we explore knowledge distillation under the
multi-task learning setting. The student is jointly distilled across different
tasks. It acquires more general representation capacity through multi-tasking
distillation and can be further fine-tuned to improve the model in the target
domain. Unlike other BERT distillation methods which specifically designed for
Transformer-based architectures, we provide a general learning framework. Our
approach is model agnostic and can be easily applied on different future
teacher model architectures. We evaluate our approach on a Transformer-based
and LSTM based student model. Compared to a strong, similarly LSTM-based
approach, we achieve better quality under the same computational constraints.
Compared to the present state of the art, we reach comparable results with much
faster inference speed."
"In this paper, a new approach to computing the generalisation performance is
presented that assumes the distribution of risks, $\rho(r)$, for a learning
scenario is known. From this, the expected error of a learning machine using
empirical risk minimisation is computed for both classification and regression
problems. A critical quantity in determining the generalisation performance is
the power-law behaviour of $\rho(r)$ around its minimum value---a quantity we
call attunement. The distribution $\rho(r)$ is computed for the case of all
Boolean functions and for the perceptron used in two different problem
settings. Initially a simplified analysis is presented where an independence
assumption about the losses is made. A more accurate analysis is carried out
taking into account chance correlations in the training set. This leads to
corrections in the typical behaviour that is observed."
"Click-Through Rate (CTR) prediction has been an indispensable component for
many industrial applications, such as recommendation systems and online
advertising. CTR prediction systems are usually based on multi-field
categorical features, i.e., every feature is categorical and belongs to one and
only one field. Modeling feature conjunctions is crucial for CTR prediction
accuracy. However, it requires a massive number of parameters to explicitly
model all feature conjunctions, which is not scalable for real-world production
systems. In this paper, we describe a novel Field-Leveraged Embedding Network
(FLEN) which has been deployed in the commercial recommender system in Meitu
and serves the main traffic. FLEN devises a field-wise bi-interaction pooling
technique. By suitably exploiting field information, the field-wise
bi-interaction pooling captures both inter-field and intra-field feature
conjunctions with a small number of model parameters and an acceptable time
complexity for industrial applications. We show that a variety of
state-of-the-art CTR models can be expressed under this technique. Furthermore,
we develop Dicefactor: a dropout technique to prevent independent latent
features from co-adapting. Extensive experiments, including offline evaluations
and online A/B testing on real production systems, demonstrate the
effectiveness and efficiency of FLEN against the state-of-the-arts. Notably,
FLEN has obtained 5.19% improvement on CTR with 1/6 of memory usage and
computation time, compared to last version (i.e. NFM)."
"Value iteration networks (VINs) have been demonstrated to have a good
generalization ability for reinforcement learning tasks across similar domains.
However, based on our experiments, a policy learned by VINs still fail to
generalize well on the domain whose action space and feature space are not
identical to those in the domain where it is trained. In this paper, we propose
a transfer learning approach on top of VINs, termed Transfer VINs (TVINs), such
that a learned policy from a source domain can be generalized to a target
domain with only limited training data, even if the source domain and the
target domain have domain-specific actions and features. We empirically verify
that our proposed TVINs outperform VINs when the source and the target domains
have similar but not identical action and feature spaces. Furthermore, we show
that the performance improvement is consistent across different environments,
maze sizes, dataset sizes as well as different values of hyperparameters such
as number of iteration and kernel size."
"We propose a generative model for adversarial attack. The model generates
subtle but predictive patterns from the input. To perform an attack, it
replaces the patterns of the input with those generated based on examples from
some other class. We demonstrate our model by attacking CNN on MNIST."
"Federated machine learning systems have been widely used to facilitate the
joint data analytics across the distributed datasets owned by the different
parties that do not trust each others. In this paper, we proposed a novel
Gradient Boosting Machines (GBM) framework SecureGBM built-up with a
multi-party computation model based on semi-homomorphic encryption, where every
involved party can jointly obtain a shared Gradient Boosting machines model
while protecting their own data from the potential privacy leakage and
inferential identification. More specific, our work focused on a specific
""dual--party"" secure learning scenario based on two parties -- both party own
an unique view (i.e., attributes or features) to the sample group of samples
while only one party owns the labels. In such scenario, feature and label data
are not allowed to share with others. To achieve the above goal, we firstly
extent -- LightGBM -- a well known implementation of tree-based GBM through
covering its key operations for training and inference with SEAL homomorphic
encryption schemes. However, the performance of such re-implementation is
significantly bottle-necked by the explosive inflation of the communication
payloads, based on ciphertexts subject to the increasing length of plaintexts.
In this way, we then proposed to use stochastic approximation techniques to
reduced the communication payloads while accelerating the overall training
procedure in a statistical manner. Our experiments using the real-world data
showed that SecureGBM can well secure the communication and computation of
LightGBM training and inference procedures for the both parties while only
losing less than 3% AUC, using the same number of iterations for gradient
boosting, on a wide range of benchmark datasets."
"Predicting road traffic speed is a challenging task due to different types of
roads, abrupt speed change and spatial dependencies between roads; it requires
the modeling of dynamically changing spatial dependencies among roads and
temporal patterns over long input sequences. This paper proposes a novel
spatio-temporal graph attention (ST-GRAT) that effectively captures the
spatio-temporal dynamics in road networks. The novel aspects of our approach
mainly include spatial attention, temporal attention, and spatial sentinel
vectors. The spatial attention takes the graph structure information (e.g.,
distance between roads) and dynamically adjusts spatial correlation based on
road states. The temporal attention is responsible for capturing traffic speed
changes, and the sentinel vectors allow the model to retrieve new features from
spatially correlated nodes or preserve existing features. The experimental
results show that ST-GRAT outperforms existing models, especially in difficult
conditions where traffic speeds rapidly change (e.g., rush hours). We
additionally provide a qualitative study to analyze when and where ST-GRAT
tended to make accurate predictions during rush-hour times."
"Reliable uncertainty estimates are an important tool for helping autonomous
agents or human decision makers understand and leverage predictive models.
However, existing approaches to estimating uncertainty largely ignore the
possibility of covariate shift--i.e., where the real-world data distribution
may differ from the training distribution. As a consequence, existing
algorithms can overestimate certainty, possibly yielding a false sense of
confidence in the predictive model. We propose an algorithm for calibrating
predictions that accounts for the possibility of covariate shift, given labeled
examples from the training distribution and unlabeled examples from the
real-world distribution. Our algorithm uses importance weighting to correct for
the shift from the training to the real-world distribution. However, importance
weighting relies on the training and real-world distributions to be
sufficiently close. Building on ideas from domain adaptation, we additionally
learn a feature map that tries to equalize these two distributions. In an
empirical evaluation, we show that our proposed approach outperforms existing
approaches to calibrated prediction when there is covariate shift."
"We present a new method to transform the spectral pixel information of a
micrograph into an affine geometric description, which allows us to analyze the
morphology of granular materials. We use spectral and pulse-coupled neural
network based segmentation techniques to generate blobs, and a newly developed
algorithm to extract dilated contours. A constrained Delaunay tesselation of
the contour points results in a triangular mesh. This mesh is the basic
ingredient of the Chodal Axis Transform, which provides a morphological
decomposition of shapes. Such decomposition allows for grain separation and the
efficient computation of the statistical features of granular materials."
"In this article we propose a novel face recognition method based on Principal
Component Analysis (PCA) and Log-Gabor filters. The main advantages of the
proposed method are its simple implementation, training, and very high
recognition accuracy. For recognition experiments we used 5151 face images of
1311 persons from different sets of the FERET and AR databases that allow to
analyze how recognition accuracy is affected by the change of facial
expressions, illumination, and aging. Recognition experiments with the FERET
database (containing photographs of 1196 persons) showed that our method can
achieve maximal 97-98% first one recognition rate and 0.3-0.4% Equal Error
Rate. The experiments also showed that the accuracy of our method is less
affected by eye location errors and used image normalization method than of
traditional PCA -based recognition method."
"Regularization functionals that lower level set boundary length when used
with L^1 fidelity functionals on signal de-noising on images create artifacts.
These are (i) rounding of corners, (ii) shrinking of radii, (iii) shrinking of
cusps, and (iv) non-smoothing of staircasing. Regularity functionals based upon
total curvature of level set boundaries do not create artifacts (i) and (ii).
An adjusted fidelity term based on the flat norm on the current (a
distributional graph) representing the density of curvature of level sets
boundaries can minimize (iii) by weighting the position of a cusp. A regularity
term to eliminate staircasing can be based upon the mass of the current
representing the graph of an image function or its second derivatives.
Densities on the Grassmann bundle of the Grassmann bundle of the ambient space
of the graph can be used to identify patterns, textures, occlusion and lines."
"Interest point detection is a common task in various computer vision
applications. Although a big variety of detector are developed so far
computational efficiency of interest point based image analysis remains to be
the problem. Current paper proposes a system-theoretic approach to interest
point detection. Starting from the analysis of interdependency between detector
and descriptor it is shown that given a descriptor it is possible to introduce
to notion of detector redundancy. Furthermore for each detector it is possible
to construct its irredundant and equivalent modification. Modified detector
possesses lower computational complexity and is preferable. It is also shown
that several known approaches to reduce computational complexity of image
registration can be generalized in terms of proposed theory."
"In this paper, a new method for handwritten signature identification based on
rotated complex wavelet filters is proposed. We have proposed to use the
rotated complex wavelet filters (RCWF) and dual tree complex wavelet
transform(DTCWT) together to derive signature feature extraction, which
captures information in twelve different directions. In identification phase,
Canberra distance measure is used. The proposed method is compared with
discrete wavelet transform (DWT). From experimental results it is found that
signature identification rate of proposed method is superior over DWT"
"Matlab version 7.1 had been used to detect playing cards on a Casino table
and the suits and ranks of these cards had been identified. The process gives
an example of an application of computer vision to a problem where rectangular
objects are to be detected and the information content of the objects are
extracted out. In the case of playing cards, it is the suit and rank of each
card. The image processing system is done in two passes. Pass 1 detects
rectangular shapes and template matched with a template of the left and right
edges of the cards. Pass 2 extracts the suit and rank of the cards by matching
the top left portion of the card that contains both rank and suit information,
with stored templates of ranks and suits of the playing cards using a series of
if-then statements."
"In this paper we present a novel slanted-plane MRF model which reasons
jointly about occlusion boundaries as well as depth. We formulate the problem
as the one of inference in a hybrid MRF composed of both continuous (i.e.,
slanted 3D planes) and discrete (i.e., occlusion boundaries) random variables.
This allows us to define potentials encoding the ownership of the pixels that
compose the boundary between segments, as well as potentials encoding which
junctions are physically possible. Our approach outperforms the
state-of-the-art on Middlebury high resolution imagery as well as in the more
challenging KITTI dataset, while being more efficient than existing slanted
plane MRF-based methods, taking on average 2 minutes to perform inference on
high resolution imagery."
"Feature extraction is one of the fundamental problems of character
recognition. The performance of character recognition system is depends on
proper feature extraction and correct classifier selection. In this article, a
rapid feature extraction method is proposed and named as Celled Projection (CP)
that compute the projection of each section formed through partitioning an
image. The recognition performance of the proposed method is compared with
other widely used feature extraction methods that are intensively studied for
many different scripts in literature. The experiments have been conducted using
Bangla handwritten numerals along with three different well known classifiers
which demonstrate comparable results including 94.12% recognition accuracy
using celled projection."
"Biometric time and attendance system is one of the most successful
applications of biometric technology. One of the main advantage of a biometric
time and attendance system is it avoids ""buddy-punching"". Buddy punching was a
major loophole which will be exploiting in the traditional time attendance
systems. Fingerprint recognition is an established field today, but still
identifying individual from a set of enrolled fingerprints is a time taking
process. Most fingerprint-based biometric systems store the minutiae template
of a user in the database. It has been traditionally assumed that the minutiae
template of a user does not reveal any information about the original
fingerprint. This belief has now been shown to be false; several algorithms
have been proposed that can reconstruct fingerprint images from minutiae
templates. In this paper, a novel fingerprint reconstruction algorithm is
proposed to reconstruct the phase image, which is then converted into the
grayscale image. The proposed reconstruction algorithm reconstructs the phase
image from minutiae. The proposed reconstruction algorithm is used to automate
the whole process of taking attendance, manually which is a laborious and
troublesome work and waste a lot of time, with its managing and maintaining the
records for a period of time is also a burdensome task. The proposed
reconstruction algorithm has been evaluated with respect to the success rates
of type-I attack (match the reconstructed fingerprint against the original
fingerprint) and type-II attack (match the reconstructed fingerprint against
different impressions of the original fingerprint) using a commercial
fingerprint recognition system. Given the reconstructed image from our
algorithm, we show that both types of attacks can be effectively launched
against a fingerprint recognition system."
"A variety of new and powerful algorithms have been developed for image
compression over the years. Among them the wavelet-based image compression
schemes have gained much popularity due to their overlapping nature which
reduces the blocking artifacts that are common phenomena in JPEG compression
and multiresolution character which leads to superior energy compaction with
high quality reconstructed images. This paper provides a detailed survey on
some of the popular wavelet coding techniques such as the Embedded Zerotree
Wavelet (EZW) coding, Set Partitioning in Hierarchical Tree (SPIHT) coding, the
Set Partitioned Embedded Block (SPECK) Coder, and the Embedded Block Coding
with Optimized Truncation (EBCOT) algorithm. Other wavelet-based coding
techniques like the Wavelet Difference Reduction (WDR) and the Adaptive Scanned
Wavelet Difference Reduction (ASWDR) algorithms, the Space Frequency
Quantization (SFQ) algorithm, the Embedded Predictive Wavelet Image Coder
(EPWIC), Compression with Reversible Embedded Wavelet (CREW), the Stack-Run
(SR) coding and the recent Geometric Wavelet (GW) coding are also discussed.
Based on the review, recommendations and discussions are presented for
algorithm development and implementation."
"Automatic head frontal-view identification is challenging due to appearance
variations caused by pose changes, especially without any training samples. In
this paper, we present an unsupervised algorithm for identifying frontal view
among multiple facial images under various yaw poses (derived from the same
person). Our approach is based on Locally Linear Embedding (LLE), with the
assumption that with yaw pose being the only variable, the facial images should
lie in a smooth and low dimensional manifold. We horizontally flip the facial
images and present two K-nearest neighbor protocols for the original images and
the flipped images, respectively. In the proposed extended LLE, for any facial
image (original or flipped one), we search (1) the Ko nearest neighbors among
the original facial images and (2) the Kf nearest neighbors among the flipped
facial images to construct the same neighborhood graph. The extended LLE
eliminates the differences (because of background, face position and scale in
the whole image and some asymmetry of left-right face) between the original
facial image and the flipped facial image at the same yaw pose so that the
flipped facial images can be used effectively. Our approach does not need any
training samples as prior information. The experimental results show that the
frontal view of head can be identified reliably around the lowest point of the
pose manifold for multiple facial images, especially the cropped facial images
(little background and centered face)."
"This paper proposes a novel framework for multi-group shape analysis relying
on a hierarchical graphical statistical model on shapes within a population.The
framework represents individual shapes as point setsmodulo translation,
rotation, and scale, following the notion in Kendall shape space.While
individual shapes are derived from their group shape model, each group shape
model is derived from a single population shape model. The hierarchical model
follows the natural organization of population data and the top level in the
hierarchy provides a common frame of reference for multigroup shape analysis,
e.g. classification and hypothesis testing. Unlike typical shape-modeling
approaches, the proposed model is a generative model that defines a joint
distribution of object-boundary data and the shape-model variables.
Furthermore, it naturally enforces optimal correspondences during the process
of model fitting and thereby subsumes the so-called correspondence problem. The
proposed inference scheme employs an expectation maximization (EM) algorithm
that treats the individual and group shape variables as hidden random variables
and integrates them out before estimating the parameters (population mean and
variance and the group variances). The underpinning of the EM algorithm is the
sampling of pointsets, in Kendall shape space, from their posterior
distribution, for which we exploit a highly-efficient scheme based on
Hamiltonian Monte Carlo simulation. Experiments in this paper use the fitted
hierarchical model to perform (1) hypothesis testing for comparison between
pairs of groups using permutation testing and (2) classification for image
retrieval. The paper validates the proposed framework on simulated data and
demonstrates results on real data."
"Hand Gesture is a popular way to interact or control machines and it has been
implemented in many applications. The geometry of hand is such that it is hard
to construct in virtual environment and control the joints but the
functionality and DOF encourage researchers to make a hand like instrument.
This paper presents a novel method for fingertips detection and centres of
palms detection distinctly for both hands using MS KINECT in 3D from the input
image. KINECT facilitates us by providing the depth information of foreground
objects. The hands were segmented using the depth vector and centres of palms
were detected using distance transformation on inverse image. This result would
be used to feed the inputs to the robotic hands to emulate human hands
operation."
"In this paper we propose an easiest approach for facial expression
recognition. Here we are using concept of SVM for Expression Classification.
Main problem is sub divided in three main modules. First one is Face detection
in which we are using skin filter and Face segmentation. We are given more
stress on feature Extraction. This method is effective enough for application
where fast execution is required. Second, Facial Feature Extraction which is
essential part for expression recognition. In this module we used Edge
Projection Analysis. Finally extracted features vector is passed towards SVM
classifier for Expression Recognition. We are considering six basic Expressions
(Anger, Fear, Disgust, Joy, Sadness, and Surprise)"
"Image inpainting is the art of predicting damaged regions of an image. The
manual way of image inpainting is a time consuming. Therefore, there must be an
automatic digital method for image inpainting that recovers the image from the
damaged regions. In this paper, a novel statistical image inpainting algorithm
based on Kriging interpolation technique was proposed. Kriging technique
automatically fills the damaged region in an image using the information
available from its surrounding regions in such away that it uses the spatial
correlation structure of points inside the k-by-k block. Kriging has the
ability to face the challenge of keeping the structure and texture information
as the size of damaged region heighten. Experimental results showed that,
Kriging has a high PSNR value when recovering a variety of test images from
scratches and text as damaged regions."
"One of the fundamental requirements for visual surveillance using
non-overlapping camera networks is the correct labeling of tracked objects on
each camera in a consistent way,in the sense that the captured tracklets, or
observations in this paper, of the same object at different cameras should be
assigned with the same label. In this paper, we formulate this task as a
Bayesian inference problem and propose a distributed inference framework in
which the posterior distribution of labeling variable corresponding to each
observation, conditioned on all history appearance and spatio-temporal evidence
made in the whole networks, is calculated based solely on local information
processing on each camera and mutual information exchanging between neighboring
cameras. In our framework, the number of objects presenting in the monitored
region, i.e. the sampling space of labeling variables, does not need to be
specified beforehand. Instead, it can be determined automatically on the fly.
In addition, we make no assumption about the appearance distribution of a
single object, but use similarity scores between appearance pairs, given by
advanced object re-identification algorithm, as appearance likelihood for
inference. This feature makes our method very flexible and competitive when
observing condition undergoes large changes across camera views. To cope with
the problem of missing detection, which is critical for distributed inference,
we consider an enlarged neighborhood of each camera during inference and use a
mixture model to describe the higher order spatio-temporal constraints. The
robustness of the algorithm against missing detection is improved at the cost
of slightly increased computation and communication burden at each camera node.
Finally, we demonstrate the effectiveness of our method through experiments on
an indoor Office Building dataset and an outdoor Campus Garden dataset."
"We propose a state of the art method for intelligent object recognition and
video surveillance based on human visual attention. Bottom up and top down
attention are applied respectively in the process of acquiring interested
object(saliency map) and object recognition. The revision of 4 channel PFT
method is proposed for bottom up attention and enhances the speed and accuracy.
Inhibit of return (IOR) is applied in judging the sequence of saliency object
pop out. Euclidean distance of color distribution, object center coordinates
and speed are considered in judging whether the target is match and suspicious.
The extensive tests on videos and images show that our method in video analysis
has high accuracy and fast speed compared with traditional method. The method
can be applied into many fields such as video surveillance and security."
"The analysis of historical documents is still a topical issue given the
importance of information that can be extracted and also the importance given
by the institutions to preserve their heritage. The main idea in order to
characterize the content of the images of ancient documents after attempting to
clean the image is segmented blocks texts from the same image and tries to find
similar blocks in either the same image or the entire image database. Most
approaches of offline handwriting recognition proceed by segmenting words into
smaller pieces (usually characters) which are recognized separately.
Recognition of a word then requires the recognition of all characters (OCR)
that compose it. Our work focuses mainly on the characterization of classes in
images of old documents. We use Som toolbox for finding classes in documents.
We applied also fractal dimensions and points of interest to categorize and
match ancient documents."
"Character identification plays a vital role in the contemporary world of
Image processing. It can solve many composite problems and makes humans work
easier. An instance is Handwritten Character detection. Handwritten recognition
is not a novel expertise, but it has not gained community notice until Now. The
eventual aim of designing Handwritten Character recognition structure with an
accurateness rate of 100% is pretty illusionary. Tamil Handwritten Character
recognition system uses the Neural Networks to distinguish them. Neural Network
and structural characteristics are used to instruct and recognize written
characters. After training and testing the exactness rate reached 99%. This
correctness rate is extremely high. In this paper we are exploring image
processing through the Hilditch algorithm foundation and structural
characteristics of a character in the image. And we recognized some character
of the Tamil language, and we are trying to identify all the character of Tamil
In our future works."
"This paper exploits the feature extraction capabilities of the discrete
cosine transform (DCT) together with an illumination normalization approach in
the logarithm domain that increase its robustness to variations in facial
geometry and illumination. Secondly in the same domain the entropy measures are
applied on the DCT coefficients so that maximum entropy preserving pixels can
be extracted as the feature vector. Thus the informative features of a face can
be extracted in a low dimensional space. Finally, the kernel entropy component
analysis (KECA) with an extension of arc cosine kernels is applied on the
extracted DCT coefficients that contribute most to the entropy estimate to
obtain only those real kernel ECA eigenvectors that are associated with
eigenvalues having high positive entropy contribution. The resulting system was
successfully tested on real image sequences and is robust to significant
partial occlusion and illumination changes, validated with the experiments on
the FERET, AR, FRAV2D and ORL face databases. Experimental comparison is
demonstrated to prove the superiority of the proposed approach in respect to
recognition accuracy. Using specificity and sensitivity we find that the best
is achieved when Renyi entropy is applied on the DCT coefficients. Extensive
experimental comparison is demonstrated to prove the superiority of the
proposed approach in respect to recognition accuracy. Moreover, the proposed
approach is very simple, computationally fast and can be implemented in any
real-time face recognition system."
"We present an integrated framework for using Convolutional Networks for
classification, localization and detection. We show how a multiscale and
sliding window approach can be efficiently implemented within a ConvNet. We
also introduce a novel deep learning approach to localization by learning to
predict object boundaries. Bounding boxes are then accumulated rather than
suppressed in order to increase detection confidence. We show that different
tasks can be learned simultaneously using a single shared network. This
integrated framework is the winner of the localization task of the ImageNet
Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very
competitive results for the detection and classifications tasks. In
post-competition work, we establish a new state of the art for the detection
task. Finally, we release a feature extractor from our best model called
OverFeat."
"To effectively retrieve objects from large corpus with high accuracy is a
challenge task. In this paper, we propose a method that propagates visual
feature level similarities on a Markov random field (MRF) to obtain a high
level correspondence in image space for image pairs. The proposed
correspondence between image pair reflects not only the similarity of low-level
visual features but also the relations built through other images in the
database and it can be easily integrated into the existing
bag-of-visual-words(BoW) based systems to reduce the missing rate. We evaluate
our method on the standard Oxford-5K, Oxford-105K and Paris-6K dataset. The
experiment results show that the proposed method significantly improves the
retrieval accuracy on three datasets and exceeds the current state-of-the-art
retrieval performance."
"Dermoscopy is one of the major imaging modalities used in the diagnosis of
melanoma and other pigmented skin lesions. Due to the difficulty and
subjectivity of human interpretation, automated analysis of dermoscopy images
has become an important research area. Border detection is often the first step
in this analysis. In many cases, the lesion can be roughly separated from the
background skin using a thresholding method applied to the blue channel.
However, no single thresholding method appears to be robust enough to
successfully handle the wide variety of dermoscopy images encountered in
clinical practice. In this paper, we present an automated method for detecting
lesion borders in dermoscopy images using ensembles of thresholding methods.
Experiments on a difficult set of 90 images demonstrate that the proposed
method is robust, fast, and accurate when compared to nine state-of-the-art
methods."
"Extraction and recognition of Bangla text from video frame images is
challenging due to complex color background, low-resolution etc. In this paper,
we propose an algorithm for extraction and recognition of Bangla text form such
video frames with complex background. Here, a two-step approach has been
proposed. First, the text line is segmented into words using information based
on line contours. First order gradient value of the text blocks are used to
find the word gap. Next, a local binarization technique is applied on each word
and text line is reconstructed using those words. Secondly, this binarized text
block is sent to OCR for recognition purpose."
"This paper presents the maneuver of mouse pointer and performs various mouse
operations such as left click, right click, double click, drag etc using
gestures recognition technique. Recognizing gestures is a complex task which
involves many aspects such as motion modeling, motion analysis, pattern
recognition and machine learning. Keeping all the essential factors in mind a
system has been created which recognizes the movement of fingers and various
patterns formed by them. Color caps have been used for fingers to distinguish
it from the background color such as skin color. Thus recognizing the gestures
various mouse events have been performed. The application has been created on
MATLAB environment with operating system as windows 7."
"The connectivity and structural integrity of the white matter of the brain is
nowadays known to be implicated into a wide range of brain-related disorders.
However, it was not before the advent of diffusion Magnetic Resonance Imaging
(dMRI) that researches have been able to examine the properties of white matter
in vivo. Presently, among a range of various methods of dMRI, high angular
resolution diffusion imaging (HARDI) is known to excel in its ability to
provide reliable information about the local orientations of neural fasciculi
(aka fibre tracts). Moreover, as opposed to the more traditional diffusion
tensor imaging (DTI), HARDI is capable of distinguishing the orientations of
multiple fibres passing through a given spatial voxel. Unfortunately, the
ability of HARDI to discriminate between neural fibres that cross each other at
acute angles is always limited, which is the main reason behind the development
of numerous post-processing tools, aiming at the improvement of the directional
resolution of HARDI. Among such tools is spherical deconvolution (SD). Due to
its ill-posed nature, however, SD standardly relies on a number of a priori
assumptions which are to render its results unique and stable. In this paper,
we propose a different approach to the problem of SD in HARDI, which accounts
for the spatial continuity of neural fibres as well as the presence of
isotropic diffusion. Subsequently, we demonstrate how the proposed solution can
be used to successfully overcome the effect of partial voluming, while
preserving the spatial coherency of cerebral diffusion at moderate-to-severe
noise levels. In a series of both in silico and in vivo experiments, the
performance of the proposed method is compared with that of several available
alternatives, with the comparative results clearly supporting the viability and
usefulness of our approach."
"There are many difficulties facing a handwritten Arabic recognition system
such as unlimited variation in human handwriting, similarities of distinct
character shapes, interconnections of neighbouring characters and their
position in the word. The typical Optical Character Recognition (OCR) systems
are based mainly on three stages, preprocessing, features extraction and
recognition. This paper proposes new methods for handwritten Arabic character
recognition which is based on novel preprocessing operations including
different kinds of noise removal also different kind of features like
structural, Statistical and Morphological features from the main body of the
character and also from the secondary components. Evaluation of the accuracy of
the selected features is made. The system was trained and tested by back
propagation neural network with CENPRMI dataset. The proposed algorithm
obtained promising results as it is able to recognize 88% of our test set
accurately. In Comparable with other related works we find that our result is
the highest among other published works."
"Dimensionality reduction, cluster analysis, and sparse representation are
basic components in machine learning. However, their relationships have not yet
been fully investigated. In this paper, we find that the spectral graph theory
underlies a series of these elementary methods and can unify them into a
complete framework. The methods include PCA, K-means, Laplacian eigenmap (LE),
ratio cut (Rcut), and a new sparse representation method developed by us,
called spectral sparse representation (SSR). Further, extended relations to
conventional over-complete sparse representations (e.g., method of optimal
directions, KSVD), manifold learning (e.g., kernel PCA, multidimensional
scaling, Isomap, locally linear embedding), and subspace clustering (e.g.,
sparse subspace clustering, low-rank representation) are incorporated. We show
that, under an ideal condition from the spectral graph theory, PCA, K-means,
LE, and Rcut are unified together. And when the condition is relaxed, the
unification evolves to SSR, which lies in the intermediate between PCA/LE and
K-mean/Rcut. An efficient algorithm, NSCrt, is developed to solve the sparse
codes of SSR. SSR combines merits of both sides: its sparse codes reduce
dimensionality of data meanwhile revealing cluster structure. For its inherent
relation to cluster analysis, the codes of SSR can be directly used for
clustering. Scut, a clustering approach derived from SSR reaches the
state-of-the-art performance in the spectral clustering family. The one-shot
solution obtained by Scut is comparable to the optimal result of K-means that
are run many times. Experiments on various data sets demonstrate the properties
and strengths of SSR, NSCrt, and Scut."
"Optimization techniques have been widely used in deformable registration,
allowing for the incorporation of similarity metrics with regularization
mechanisms. These regularization mechanisms are designed to mitigate the
effects of trivial solutions to ill-posed registration problems and to
otherwise ensure the resulting deformation fields are well-behaved. This paper
introduces a novel deformable registration algorithm, RANCOR, which uses
iterative convexification to address deformable registration problems under
total-variation regularization. Initial comparative results against four
state-of-the-art registration algorithms are presented using the Internet Brain
Segmentation Repository (IBSR) database."
"Key frame extraction algorithms consider the problem of selecting a subset of
the most informative frames from a video to summarize its content."
"Nonlocal filters are simple and powerful techniques for image denoising. In
this paper we study the reformulation of a broad class of nonlocal filters in
terms of two functional rearrangements: the decreasing and the relative
rearrangements.
  Independently of the dimension of the image, we reformulate these filters as
integral operators defined in a one-dimensional space corresponding to the
level sets measures.
  We prove the equivalency between the original and the rearranged versions of
the filters and propose a discretization in terms of constant-wise
interpolators, which we prove to be convergent to the solution of the
continuous setting.
  For some particular cases, this new formulation allows us to perform a
detailed analysis of the filtering properties. Among others, we prove that the
filtered image is a contrast change of the original image, and that the
filtering procedure behaves asymptotically as a shock filter combined with a
border diffusive term, responsible for the staircaising effect and the loss of
contrast."
"Nowadays in developing or developed countries, the Intelligent Transportation
System (ITS) technology has attracted so much attention to itself. License
Plate Recognition (LPR) systems have many applications in ITSs, such as the
payment of parking fee, controlling the traffic volume, traffic data
collection, etc. This paper presents a new and fast method for license plate
extraction based on edge analysis. our proposed method consist of four stage,
which are edge detection, non-useable edge and noise removing, edge analysis
and morphology-based license plate extraction. In the result part, the proposed
algorithm is applied on vehicle database and the accuracy rate reached 98%.
From the experimental results it is shown that the proposed method gives fairly
acceptable level of accuracy for practical license plate recognition system."
"Automatically describing the content of an image is a fundamental problem in
artificial intelligence that connects computer vision and natural language
processing. In this paper, we present a generative model based on a deep
recurrent architecture that combines recent advances in computer vision and
machine translation and that can be used to generate natural sentences
describing an image. The model is trained to maximize the likelihood of the
target description sentence given the training image. Experiments on several
datasets show the accuracy of the model and the fluency of the language it
learns solely from image descriptions. Our model is often quite accurate, which
we verify both qualitatively and quantitatively. For instance, while the
current state-of-the-art BLEU-1 score (the higher the better) on the Pascal
dataset is 25, our approach yields 59, to be compared to human performance
around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66,
and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we
achieve a BLEU-4 of 27.7, which is the current state-of-the-art."
"We address the problem of action detection in videos. Driven by the latest
progress in object detection from 2D images, we build action models using rich
feature hierarchies derived from shape and kinematic cues. We incorporate
appearance and motion in two ways. First, starting from image region proposals
we select those that are motion salient and thus are more likely to contain the
action. This leads to a significant reduction in the number of regions being
processed and allows for faster computations. Second, we extract
spatio-temporal feature representations to build strong classifiers using
Convolutional Neural Networks. We link our predictions to produce detections
consistent in time, which we call action tubes. We show that our approach
outperforms other techniques in the task of action detection."
"We consider the problem of depth estimation from a single monocular image in
this work. It is a challenging task as no reliable depth cues are available,
e.g., stereo correspondences, motions, etc. Previous efforts have been focusing
on exploiting geometric priors or additional sources of information, with all
using hand-crafted features. Recently, there is mounting evidence that features
from deep convolutional neural networks (CNN) are setting new records for
various vision applications. On the other hand, considering the continuous
characteristic of the depth values, depth estimations can be naturally
formulated into a continuous conditional random field (CRF) learning problem.
Therefore, we in this paper present a deep convolutional neural field model for
estimating depths from a single image, aiming to jointly explore the capacity
of deep CNN and continuous CRF. Specifically, we propose a deep structured
learning scheme which learns the unary and pairwise potentials of continuous
CRF in a unified deep CNN framework.
  The proposed method can be used for depth estimations of general scenes with
no geometric priors nor any extra information injected. In our case, the
integral of the partition function can be analytically calculated, thus we can
exactly solve the log-likelihood optimization. Moreover, solving the MAP
problem for predicting depths of a new image is highly efficient as closed-form
solutions exist. We experimentally demonstrate that the proposed method
outperforms state-of-the-art depth estimation methods on both indoor and
outdoor scene datasets."
"We present a novel approach for discovering human interactions in videos.
Activity understanding techniques usually require a large number of labeled
examples, which are not available in many practical cases. Here, we focus on
recovering semantically meaningful clusters of human-human and human-object
interaction in an unsupervised fashion. A new iterative solution is introduced
based on Maximum Margin Clustering (MMC), which also accepts user feedback to
refine clusters. This is achieved by formulating the whole process as a unified
constrained latent max-margin clustering problem. Extensive experiments have
been carried out over three challenging datasets, Collective Activity, VIRAT,
and UT-interaction. Empirical results demonstrate that the proposed algorithm
can efficiently discover perfect semantic clusters of human interactions with
only a small amount of labeling effort."
"Broadly speaking, the objective in cardiac image segmentation is to delineate
the outer and inner walls of the heart to segment out either the entire or
parts of the organ boundaries. This paper will focus on MR images as they are
the most widely used in cardiac segmentation -- as a result of the accurate
morphological information and better soft tissue contrast they provide. This
cardiac segmentation information is very useful as it eases physical
measurements that provides useful metrics for cardiac diagnosis such as
infracted volumes, ventricular volumes, ejection fraction, myocardial mass,
cardiac movement, and the like. But, this task is difficult due to the
intensity and texture similarities amongst the different cardiac and background
structures on top of some noisy artifacts present in MR images. Thus far,
various researchers have proposed different techniques to solve some of the
pressing issues. This seminar paper presents an overview of representative
medical image segmentation techniques. The paper also highlights preferred
approaches for segmentation of the four cardiac chambers: the left ventricle
(LV), right ventricle (RV), left atrium (LA) and right atrium (RA), on short
axis image planes."
"Domain adaptation aims at training a classifier in one dataset and applying
it to a related but not identical dataset. One successfully used framework of
domain adaptation is to learn a transformation to match both the distribution
of the features (marginal distribution), and the distribution of the labels
given features (conditional distribution). In this paper, we propose a new
domain adaptation framework named Deep Transfer Network (DTN), where the highly
flexible deep neural networks are used to implement such a distribution
matching process.
  This is achieved by two types of layers in DTN: the shared feature extraction
layers which learn a shared feature subspace in which the marginal
distributions of the source and the target samples are drawn close, and the
discrimination layers which match conditional distributions by classifier
transduction. We also show that DTN has a computation complexity linear to the
number of training samples, making it suitable to large-scale problems. By
combining the best paradigms in both worlds (deep neural networks in
recognition, and matching marginal and conditional distributions in domain
adaptation), we demonstrate by extensive experiments that DTN improves
significantly over former methods in both execution time and classification
accuracy."
"The Principal Component Analysis Network (PCANet), which is one of the
recently proposed deep learning architectures, achieves the state-of-the-art
classification accuracy in various databases. However, the performance of
PCANet may be degraded when dealing with color images. In this paper, a
Quaternion Principal Component Analysis Network (QPCANet), which is an
extension of PCANet, is proposed for color images classification. Compared to
PCANet, the proposed QPCANet takes into account the spatial distribution
information of color images and ensures larger amount of intra-class invariance
of color images. Experiments conducted on different color image datasets such
as Caltech-101, UC Merced Land Use, Georgia Tech face and CURet have revealed
that the proposed QPCANet achieves higher classification accuracy than PCANet."
"Tensors or multiarray data are generalizations of matrices. Tensor clustering
has become a very important research topic due to the intrinsically rich
structures in real-world multiarray datasets. Subspace clustering based on
vectorizing multiarray data has been extensively researched. However,
vectorization of tensorial data does not exploit complete structure
information. In this paper, we propose a subspace clustering algorithm without
adopting any vectorization process. Our approach is based on a novel
heterogeneous Tucker decomposition model. In contrast to existing techniques,
we propose a new clustering algorithm that alternates between different modes
of the proposed heterogeneous tensor model. All but the last mode have
closed-form updates. Updating the last mode reduces to optimizing over the
so-called multinomial manifold, for which we investigate second order
Riemannian geometry and propose a trust-region algorithm. Numerical experiments
show that our proposed algorithm compete effectively with state-of-the-art
clustering algorithms that are based on tensor factorization."
"Determining dense semantic correspondences across objects and scenes is a
difficult problem that underpins many higher-level computer vision algorithms.
Unlike canonical dense correspondence problems which consider images that are
spatially or temporally adjacent, semantic correspondence is characterized by
images that share similar high-level structures whose exact appearance and
geometry may differ.
  Motivated by object recognition literature and recent work on rapidly
estimating linear classifiers, we treat semantic correspondence as a
constrained detection problem, where an exemplar LDA classifier is learned for
each pixel. LDA classifiers have two distinct benefits: (i) they exhibit higher
average precision than similarity metrics typically used in correspondence
problems, and (ii) unlike exemplar SVM, can output globally interpretable
posterior probabilities without calibration, whilst also being significantly
faster to train.
  We pose the correspondence problem as a graphical model, where the unary
potentials are computed via convolution with the set of exemplar classifiers,
and the joint potentials enforce smoothly varying correspondence assignment."
"We propose a method for guiding a photographer to rotate her/his smartphone
camera to obtain an image that overlaps with another image of the same scene.
The other image is taken by another photographer from a different viewpoint.
Our method is applicable even when the images do not have overlapping fields of
view. Straightforward applications of our method include sharing attention to
regions of interest for social purposes, or adding missing images to improve
structure for motion results. Our solution uses additional images of the scene,
which are often available since many people use their smartphone cameras
regularly. These images may be available online from other photographers who
are present at the scene. Our method avoids 3D scene reconstruction; it relies
instead on a new representation that consists of the spatial orders of the
scene points on two axes, x and y. This representation allows a sequence of
points to be chosen efficiently and projected onto the photographers images,
using epipolar point transfer. Overlaying these epipolar lines on the live
preview of the camera produces a convenient interface to guide the user. The
method was tested on challenging datasets of images and succeeded in guiding a
photographer from one view to a non-overlapping destination view."
"Different from face verification, face identification is much more demanding.
To reach comparable performance, an identifier needs to be roughly N times
better than a verifier. To expect a breakthrough in face identification, we
need a fresh look at the fundamental building blocks of face recognition. In
this paper we focus on the selection of a suitable signal representation and
better matching strategy for face identification. We demonstrate how Gabor
phase could be leveraged to improve the performance of face identification by
using the Block Matching method. Compared to the existing approaches, the
proposed method features much lower algorithmic complexity: face images are
only filtered by a single-scale Gabor filter pair and the matching is performed
between any pairs of face images at hand without involving any training
process. Benchmark evaluations show that the proposed approach is totally
comparable to and even better than state-of-the-art algorithms, which are
typically based on more features extracted from a large set of Gabor faces
and/or rely on heavy training processes."
"Subspace clustering is the problem of clustering data that lie close to a
union of linear subspaces. In the abstract form of the problem, where no noise
or other corruptions are present, the data are assumed to lie in general
position inside the algebraic variety of a union of subspaces, and the
objective is to decompose the variety into its constituent subspaces. Prior
algebraic-geometric approaches to this problem require the subspaces to be of
equal dimension, or the number of subspaces to be known. Subspaces of arbitrary
dimensions can still be recovered in closed form, in terms of all homogeneous
polynomials of degree $m$ that vanish on their union, when an upper bound m on
the number of the subspaces is given. In this paper, we propose an alternative,
provably correct, algorithm for addressing a union of at most $m$
arbitrary-dimensional subspaces, based on the idea of descending filtrations of
subspace arrangements. Our algorithm uses the gradient of a vanishing
polynomial at a point in the variety to find a hyperplane containing the
subspace S passing through that point. By intersecting the variety with this
hyperplane, we obtain a subvariety that contains S, and recursively applying
the procedure until no non-trivial vanishing polynomial exists, our algorithm
eventually identifies S. By repeating this procedure for other points, our
algorithm eventually identifies all the subspaces by returning a basis for
their orthogonal complement. Finally, we develop a variant of the abstract
algorithm, suitable for computations with noisy data. We show by experiments on
synthetic and real data that the proposed algorithm outperforms
state-of-the-art methods on several occasions, thus demonstrating the merit of
the idea of filtrations."
"Security concerns has been kept on increasing, so it is important for
everyone to keep their property safe from thefts and destruction. So the need
for surveillance techniques are also increasing. The system has been developed
to detect the motion in a video. A system has been developed for real time
applications by using the techniques of background subtraction and frame
differencing. In this system, motion is detected from the webcam or from the
real time video. Background subtraction and frames differencing method has been
used to detect the moving target. In background subtraction method, current
frame is subtracted from the referenced frame and then the threshold is
applied. If the difference is greater than the threshold then it is considered
as the pixel from the moving object, otherwise it is considered as background
pixel. Similarly, two frames difference method takes difference between two
continuous frames. Then that resultant difference frame is thresholded and the
amount of difference pixels is calculated."
"We introduce a novel matching algorithm, called DeepMatching, to compute
dense correspondences between images. DeepMatching relies on a hierarchical,
multi-layer, correlational architecture designed for matching images and was
inspired by deep convolutional approaches. The proposed matching algorithm can
handle non-rigid deformations and repetitive textures and efficiently
determines dense correspondences in the presence of significant changes between
images. We evaluate the performance of DeepMatching, in comparison with
state-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al
2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013)
datasets. DeepMatching outperforms the state-of-the-art algorithms and shows
excellent results in particular for repetitive textures.We also propose a
method for estimating optical flow, called DeepFlow, by integrating
DeepMatching in the large displacement optical flow (LDOF) approach of Brox and
Malik (2011). Compared to existing matching algorithms, additional robustness
to large displacements and complex motion is obtained thanks to our matching
approach. DeepFlow obtains competitive performance on public benchmarks for
optical flow estimation."
"Semantic segmentation is the task of assigning a class-label to each pixel in
an image. We propose a region-based semantic segmentation framework which
handles both full and weak supervision, and addresses three common problems:
(1) Objects occur at multiple scales and therefore we should use regions at
multiple scales. However, these regions are overlapping which creates
conflicting class predictions at the pixel-level. (2) Class frequencies are
highly imbalanced in realistic datasets. (3) Each pixel can only be assigned to
a single class, which creates competition between classes. We address all three
problems with a joint calibration method which optimizes a multi-class loss
defined over the final pixel-level output labeling, as opposed to simply region
classification. Our method outperforms the state-of-the-art on the popular SIFT
Flow [18] dataset in both the fully and weakly supervised setting by a
considerably margin (+6% and +10%, respectively)."
"An algorithm to improve performance parameter for unsupervised decision
forest clustering and density estimation is presented. Specifically, a dual
assignment parameter is introduced as a density estimator by combining Random
Forest and Gaussian Mixture Model. The Random Forest method has been
specifically applied to construct a robust affinity graph that provides
information on the underlying structure of data objects used in clustering. The
proposed algorithm differs from the commonly used spectral clustering methods
where the computed distance metric is used to find similarities between data
points. Experiments were conducted using five datasets. A comparison with six
other state-of-the-art methods shows that our model is superior to existing
approaches. Efficiency of the proposed model is in capturing the underlying
structure for a given set of data points. The proposed method is also robust,
and can discriminate between the complex features of data points among
different clusters."
"State-of-the-art image-set matching techniques typically implicitly model
each image-set with a Gaussian distribution. Here, we propose to go beyond
these representations and model image-sets as probability distribution
functions (PDFs) using kernel density estimators. To compare and match
image-sets, we exploit Csiszar f-divergences, which bear strong connections to
the geodesic distance defined on the space of PDFs, i.e., the statistical
manifold. Furthermore, we introduce valid positive definite kernels on the
statistical manifolds, which let us make use of more powerful classification
schemes to match image-sets. Finally, we introduce a supervised dimensionality
reduction technique that learns a latent space where f-divergences reflect the
class labels of the data. Our experiments on diverse problems, such as
video-based face recognition and dynamic texture classification, evidence the
benefits of our approach over the state-of-the-art image-set matching methods."
"Counting the number of people is something many security application focus
on, when dealing with controlling accesses in restricted areas, as it occurs
with banks, airports, railway stations and governmental offices. This paper
presents an automated solution for detecting the presence of more than one
person into interlocked doors adopted in many accesses. In most cases,
interlocked doors are small areas where other pieces of information and sensors
are placed in order to detect the presence of guns, explosive, etc. The general
goals and the required environmental condition, allowed us to implement a
detection system at lower costs and complexity, with respect to other existing
techniques. The system consists of a fixed array of microwave transceiver
modules, whose received signals are processed to collect information related to
a sort of volume occupied in the interlocked door cabin. The proposed solution
has been statistically validated by using statistical analysis. The whole
solution has been also implemented to be used in a real time environment and
thus validated against real experimental measures."
"In this paper, we propose a novel deep neural network framework embedded with
low-level features (LCNN) for salient object detection in complex images. We
utilise the advantage of convolutional neural networks to automatically learn
the high-level features that capture the structured information and semantic
context in the image. In order to better adapt a CNN model into the saliency
task, we redesign the network architecture based on the small-scale datasets.
Several low-level features are extracted, which can effectively capture
contrast and spatial information in the salient regions, and incorporated to
compensate with the learned high-level features at the output of the last fully
connected layer. The concatenated feature vector is further fed into a
hinge-loss SVM detector in a joint discriminative learning manner and the final
saliency score of each region within the bounding box is obtained by the linear
combination of the detector's weights. Experiments on three challenging
benchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to be
effective and superior than most low-level oriented state-of-the-arts in terms
of P-R curves, F-measure and mean absolute errors."
"Railroad tracks need to be periodically inspected and monitored to ensure
safe transportation. Automated track inspection using computer vision and
pattern recognition methods have recently shown the potential to improve safety
by allowing for more frequent inspections while reducing human errors.
Achieving full automation is still very challenging due to the number of
different possible failure modes as well as the broad range of image variations
that can potentially trigger false alarms. Also, the number of defective
components is very small, so not many training examples are available for the
machine to learn a robust anomaly detector. In this paper, we show that
detection performance can be improved by combining multiple detectors within a
multi-task learning framework. We show that this approach results in better
accuracy in detecting defects on railway ties and fasteners."
"Conventional single image based localization methods usually fail to localize
a querying image when there exist large variations between the querying image
and the pre-built scene. To address this, we propose an image-set querying
based localization approach. When the localization by a single image fails to
work, the system will ask the user to capture more auxiliary images. First, a
local 3D model is established for the querying image set. Then, the pose of the
querying image set is estimated by solving a nonlinear optimization problem,
which aims to match the local 3D model against the pre-built scene. Experiments
have shown the effectiveness and feasibility of the proposed approach."
"In this work, we propose to divide each class (a person) into subclasses
using spatial partition trees which helps in better capturing the
intra-personal variances arising from the appearances of the same individual.
We perform a comprehensive analysis on within-class and within-subclass
eigenspectrums of face images and propose a novel method of eigenspectrum
modeling which extracts discriminative features of faces from both
within-subclass and total or between-subclass scatter matrices. Effective
low-dimensional face discriminative features are extracted for face recognition
(FR) after performing discriminant evaluation in the entire eigenspace.
Experimental results on popular face databases (AR, FERET) and the challenging
unconstrained YouTube Face database show the superiority of our proposed
approach on all three databases."
"The common graph Laplacian regularizer is well-established in semi-supervised
learning and spectral dimensionality reduction. However, as a first-order
regularizer, it can lead to degenerate functions in high-dimensional manifolds.
The iterated graph Laplacian enables high-order regularization, but it has a
high computational complexity and so cannot be applied to large problems. We
introduce a new regularizer which is globally high order and so does not suffer
from the degeneracy of the graph Laplacian regularizer, but is also sparse for
efficient computation in semi-supervised learning applications. We reduce
computational complexity by building a local first-order approximation of the
manifold as a surrogate geometry, and construct our high-order regularizer
based on local derivative evaluations therein. Experiments on human body shape
and pose analysis demonstrate the effectiveness and efficiency of our method."
"State-of-the-art video deblurring methods cannot handle blurry videos
recorded in dynamic scenes, since they are built under a strong assumption that
the captured scenes are static. Contrary to the existing methods, we propose a
video deblurring algorithm that can deal with general blurs inherent in dynamic
scenes. To handle general and locally varying blurs caused by various sources,
such as moving objects, camera shake, depth variation, and defocus, we estimate
pixel-wise non-uniform blur kernels. We infer bidirectional optical flows to
handle motion blurs, and also estimate Gaussian blur maps to remove optical
blur from defocus in our new blur model. Therefore, we propose a single energy
model that jointly estimates optical flows, defocus blur maps and latent
frames. We also provide a framework and efficient solvers to minimize the
proposed energy model. By optimizing the energy model, we achieve significant
improvements in removing general blurs, estimating optical flows, and extending
depth-of-field in blurry frames. Moreover, in this work, to evaluate the
performance of non-uniform deblurring methods objectively, we have constructed
a new realistic dataset with ground truths. In addition, extensive experimental
on publicly available challenging video data demonstrate that the proposed
method produces qualitatively superior performance than the state-of-the-art
methods which often fail in either deblurring or optical flow estimation."
"Diabetic retinopathy (DR) and age related macular degeneration (ARMD) are
among the major causes of visual impairment worldwide. DR is mainly
characterized by red spots, namely microaneurysms and bright lesions,
specifically exudates whereas ARMD is mainly identified by tiny yellow or white
deposits called drusen. Since exudates might be the only manifestation of the
early diabetic retinopathy, there is an increase demand for automatic
retinopathy diagnosis. Exudates and drusen may share similar appearances, thus
discriminating between them is of interest to enhance screening performance. In
this research, we investigative the role of bag of words approach in the
automatic diagnosis of retinopathy diabetes. We proposed to use a single based
and multiple based methods for the construction of the visual dictionary by
combining the histogram of word occurrences from each dictionary and building a
single histogram. The introduced approach is evaluated for automatic diagnosis
of normal and abnormal color fundus images with bright lesions. This approach
has been implemented on 430 fundus images, including six publicly available
datasets, in addition to one local dataset. The mean accuracies reported are
97.2% and 99.77% for single based and multiple based dictionaries respectively."
"In this paper we approach the novel problem of segmenting an image based on a
natural language expression. This is different from traditional semantic
segmentation over a predefined set of semantic classes, as e.g., the phrase
""two men sitting on the right bench"" requires segmenting only the two people on
the right bench and no one standing or sitting on another bench. Previous
approaches suitable for this task were limited to a fixed set of categories
and/or rectangular regions. To produce pixelwise segmentation for the language
expression, we propose an end-to-end trainable recurrent and convolutional
network model that jointly learns to process visual and linguistic information.
In our model, a recurrent LSTM network is used to encode the referential
expression into a vector representation, and a fully convolutional network is
used to a extract a spatial feature map from the image and output a spatial
response map for the target object. We demonstrate on a benchmark dataset that
our model can produce quality segmentation output from the natural language
expression, and outperforms baseline methods by a large margin."
"Attribute recognition, particularly facial, extracts many labels for each
image. While some multi-task vision problems can be decomposed into separate
tasks and stages, e.g., training independent models for each task, for a
growing set of problems joint optimization across all tasks has been shown to
improve performance. We show that for deep convolutional neural network (DCNN)
facial attribute extraction, multi-task optimization is better. Unfortunately,
it can be difficult to apply joint optimization to DCNNs when training data is
imbalanced, and re-balancing multi-label data directly is structurally
infeasible, since adding/removing data to balance one label will change the
sampling of the other labels. This paper addresses the multi-label imbalance
problem by introducing a novel mixed objective optimization network (MOON) with
a loss function that mixes multiple task objectives with domain adaptive
re-weighting of propagated loss. Experiments demonstrate that not only does
MOON advance the state of the art in facial attribute recognition, but it also
outperforms independently trained DCNNs using the same data. When using facial
attributes for the LFW face recognition task, we show that our balanced (domain
adapted) network outperforms the unbalanced trained network."
"Understanding the camera wearer's activity is central to egocentric vision,
yet one key facet of that activity is inherently invisible to the camera--the
wearer's body pose. Prior work focuses on estimating the pose of hands and arms
when they come into view, but this 1) gives an incomplete view of the full body
posture, and 2) prevents any pose estimate at all in many frames, since the
hands are only visible in a fraction of daily life activities. We propose to
infer the ""invisible pose"" of a person behind the egocentric camera. Given a
single video, our efficient learning-based approach returns the full body 3D
joint positions for each frame. Our method exploits cues from the dynamic
motion signatures of the surrounding scene--which changes predictably as a
function of body pose--as well as static scene structures that reveal the
viewpoint (e.g., sitting vs. standing). We further introduce a novel energy
minimization scheme to infer the pose sequence. It uses soft predictions of the
poses per time instant together with a non-parametric model of human pose
dynamics over longer windows. Our method outperforms an array of possible
alternatives, including deep learning approaches for direct pose regression
from images."
"In this paper, a fresh procedure to handle image mixtures by means of blind
signal separation relying on a combination of second order and higher order
statistics techniques are introduced. The problem of blind signal separation is
reassigned to the wavelet domain. The key idea behind this method is that the
image mixture can be decomposed into the sum of uncorrelated and/or independent
sub-bands using wavelet transform. Initially, the observed image is
pre-whitened in the space domain. Afterwards, an initial separation matrix is
estimated from the second order statistics de-correlation model in the wavelet
domain. Later, this matrix will be used as an initial separation matrix for the
higher order statistics stage in order to find the best separation matrix. The
suggested algorithm was tested using natural images.Experiments have confirmed
that the use of the proposed process provides promising outcomes in identifying
an image from noisy mixtures of images."
"Object detection with deep neural networks is often performed by passing a
few thousand candidate bounding boxes through a deep neural network for each
image. These bounding boxes are highly correlated since they originate from the
same image. In this paper we investigate how to exploit feature occurrence at
the image scale to prune the neural network which is subsequently applied to
all bounding boxes. We show that removing units which have near-zero activation
in the image allows us to significantly reduce the number of parameters in the
network. Results on the PASCAL 2007 Object Detection Challenge demonstrate that
up to 40% of units in some fully-connected layers can be entirely eliminated
with little change in the detection result."
"In this paper we present a tracker, which is radically different from
state-of-the-art trackers: we apply no model updating, no occlusion detection,
no combination of trackers, no geometric matching, and still deliver
state-of-the-art tracking performance, as demonstrated on the popular online
tracking benchmark (OTB) and six very challenging YouTube videos. The presented
tracker simply matches the initial patch of the target in the first frame with
candidates in a new frame and returns the most similar patch by a learned
matching function. The strength of the matching function comes from being
extensively trained generically, i.e., without any data of the target, using a
Siamese deep neural network, which we design for tracking. Once learned, the
matching function is used as is, without any adapting, to track previously
unseen targets. It turns out that the learned matching function is so powerful
that a simple tracker built upon it, coined Siamese INstance search Tracker,
SINT, which only uses the original observation of the target from the first
frame, suffices to reach state-of-the-art performance. Further, we show the
proposed tracker even allows for target re-identification after the target was
absent for a complete video shot."
"In this paper, we propose a novel sparse coding and counting method under
Bayesian framwork for visual tracking. In contrast to existing methods, the
proposed method employs the combination of L0 and L1 norm to regularize the
linear coefficients of incrementally updated linear basis. The sparsity
constraint enables the tracker to effectively handle difficult challenges, such
as occlusion or image corruption. To achieve realtime processing, we propose a
fast and efficient numerical algorithm for solving the proposed model. Although
it is an NP-hard problem, the proposed accelerated proximal gradient (APG)
approach is guaranteed to converge to a solution quickly. Besides, we provide a
closed solution of combining L0 and L1 regularized representation to obtain
better sparsity. Experimental results on challenging video sequences
demonstrate that the proposed method achieves state-of-the-art results both in
accuracy and speed."
"Estimation of the fetal heart rate (FHR) has gained interest in the last
century, low heart rate variability has been studied to identify intrauterine
growth restricted fetuses (prepartum), and abnormal FHR patterns have been
associated with fetal distress during delivery (intrapartum). Several
monitoring techniques have been proposed for FHR estimation, including
auscultation and Doppler ultrasound. This thesis focuses on the extraction of
the non-invasive fetal electrocardiogram (NI-FECG) recorded from a limited set
of abdominal sensors. The main challenge with NI-FECG extraction techniques is
the low signal-to-noise ratio of the FECG signal on the abdominal mixture
signal which consists of a dominant maternal ECG component, FECG and noise.
However the NI-FECG offers many advantages over the alternative fetal
monitoring techniques, the most important one being the opportunity to enable
morphological analysis of the FECG which is vital for determining whether an
observed FHR event is normal or pathological. In order to advance the field of
NI-FECG signal processing, the development of standardised public databases and
benchmarking of a number of published and novel algorithms was necessary."
"It is well accepted that image segmentation can benefit from utilizing
multilevel cues. The paper focuses on utilizing the FCNN-based dense semantic
predictions in the bottom-up image segmentation, arguing to take semantic cues
into account from the very beginning. By this we can avoid merging regions of
similar appearance but distinct semantic categories as possible. The semantic
inefficiency problem is handled. We also propose a straightforward way to use
the contour cues to suppress the noise in multilevel cues, thus to improve the
segmentation robustness. The evaluation on the BSDS500 shows that we obtain the
competitive region and boundary performance. Furthermore, since all individual
regions can be assigned with appropriate semantic labels during the
computation, we are capable of extracting the adjusted semantic segmentations.
The experiment on Pascal VOC 2012 shows our improvement to the original
semantic segmentations which derives directly from the dense predictions."
"The Computer Vision Research Lab at the University of Notre Dame began
collecting iris images in the spring semester of 2004. The initial data
collections used an LG 2200 iris imaging system for image acquisition. Image
datasets acquired in 2004-2005 at Notre Dame with this LG 2200 have been used
in the ICE 2005 and ICE 2006 iris biometric evaluations. The ICE 2005 iris
image dataset has been distributed to over 100 research groups around the
world. The purpose of this document is to describe the content of the
ND-IRIS-0405 iris image dataset. This dataset is a superset of the iris image
datasets used in ICE 2005 and ICE 2006. The ND 2004-2005 iris image dataset
contains 64,980 images corresponding to 356 unique subjects, and 712 unique
irises. The age range of the subjects is 18 to 75 years old. 158 of the
subjects are female, and 198 are male. 250 of the subjects are Caucasian, 82
are Asian, and 24 are other ethnicities."
"Poisson denoising is an essential issue for various imaging applications,
such as night vision, medical imaging and microscopy. State-of-the-art
approaches are clearly dominated by patch-based non-local methods in recent
years. In this paper, we aim to propose a local Poisson denoising model with
both structure simplicity and good performance. To this end, we consider a
variational modeling to integrate the so-called Fields of Experts (FoE) image
prior, that has proven an effective higher-order Markov Random Fields (MRF)
model for many classic image restoration problems. We exploit several feasible
variational variants for this task. We start with a direct modeling in the
original image domain by taking into account the Poisson noise statistics,
which performs generally well for the cases of high SNR. However, this strategy
encounters problem in cases of low SNR. Then we turn to an alternative modeling
strategy by using the Anscombe transform and Gaussian statistics derived data
term. We retrain the FoE prior model directly in the transform domain. With the
newly trained FoE model, we end up with a local variational model providing
strongly competitive results against state-of-the-art non-local approaches,
meanwhile bearing the property of simple structure. Furthermore, our proposed
model comes along with an additional advantage, that the inference is very
efficient as it is well-suited for parallel computation on GPUs. For images of
size $512 \times 512$, our GPU implementation takes less than 1 second to
produce state-of-the-art Poisson denoising performance."
"Image denoising is a fundamental operation in image processing and holds
considerable practical importance for various real-world applications. Arguably
several thousands of papers are dedicated to image denoising. In the past
decade, sate-of-the-art denoising algorithm have been clearly dominated by
non-local patch-based methods, which explicitly exploit patch self-similarity
within image. However, in recent two years, discriminatively trained local
approaches have started to outperform previous non-local models and have been
attracting increasing attentions due to the additional advantage of
computational efficiency. Successful approaches include cascade of shrinkage
fields (CSF) and trainable nonlinear reaction diffusion (TNRD). These two
methods are built on filter response of linear filters of small size using feed
forward architectures. Due to the locality inherent in local approaches, the
CSF and TNRD model become less effective when noise level is high and
consequently introduces some noise artifacts. In order to overcome this
problem, in this paper we introduce a multi-scale strategy. To be specific, we
build on our newly-developed TNRD model, adopting the multi-scale pyramid image
representation to devise a multi-scale nonlinear diffusion process. As
expected, all the parameters in the proposed multi-scale diffusion model,
including the filters and the influence functions across scales, are learned
from training data through a loss based approach. Numerical results on Gaussian
and Poisson denoising substantiate that the exploited multi-scale strategy can
successfully boost the performance of the original TNRD model with single
scale. As a consequence, the resulting multi-scale diffusion models can
significantly suppress the typical incorrect features for those noisy images
with heavy noise."
"Computed tomography imaging is a standard modality for detecting and
assessing lung cancer. In order to evaluate the malignancy of lung nodules,
clinical practice often involves expert qualitative ratings on several criteria
describing a nodule's appearance and shape. Translating these features for
computer-aided diagnostics is challenging due to their subjective nature and
the difficulties in gaining a complete description. In this paper, we propose a
computerized approach to quantitatively evaluate both appearance distinctions
and 3D surface variations. Nodule shape was modeled and parameterized using
spherical harmonics, and appearance features were extracted using deep
convolutional neural networks. Both sets of features were combined to estimate
the nodule malignancy using a random forest classifier. The proposed algorithm
was tested on the publicly available Lung Image Database Consortium dataset,
achieving high accuracy. By providing lung nodule characterization, this method
can provide a robust alternative reference opinion for lung cancer diagnosis."
"Multi-view face detection in open environment is a challenging task due to
diverse variations of face appearances and shapes. Most multi-view face
detectors depend on multiple models and organize them in parallel, pyramid or
tree structure, which compromise between the accuracy and time-cost. Aiming at
a more favorable multi-view face detector, we propose a novel funnel-structured
cascade (FuSt) detection framework. In a coarse-to-fine flavor, our FuSt
consists of, from top to bottom, 1) multiple view-specific fast LAB cascade for
extremely quick face proposal, 2) multiple coarse MLP cascade for further
candidate window verification, and 3) a unified fine MLP cascade with
shape-indexed features for accurate face detection. Compared with other
structures, on the one hand, the proposed one uses multiple computationally
efficient distributed classifiers to propose a small number of candidate
windows but with a high recall of multi-view faces. On the other hand, by using
a unified MLP cascade to examine proposals of all views in a centralized style,
it provides a favorable solution for multi-view face detection with high
accuracy and low time-cost. Besides, the FuSt detector is alignment-aware and
performs a coarse facial part prediction which is beneficial for subsequent
face alignment. Extensive experiments on two challenging datasets, FDDB and
AFW, demonstrate the effectiveness of our FuSt detector in both accuracy and
speed."
"This paper presents a novel framework for visual object recognition using
infinite-dimensional covariance operators of input features in the paradigm of
kernel methods on infinite-dimensional Riemannian manifolds. Our formulation
provides in particular a rich representation of image features by exploiting
their non-linear correlations. Theoretically, we provide a finite-dimensional
approximation of the Log-Hilbert-Schmidt (Log-HS) distance between covariance
operators that is scalable to large datasets, while maintaining an effective
discriminating capability. This allows us to efficiently approximate any
continuous shift-invariant kernel defined using the Log-HS distance. At the
same time, we prove that the Log-HS inner product between covariance operators
is only approximable by its finite-dimensional counterpart in a very limited
scenario. Consequently, kernels defined using the Log-HS inner product, such as
polynomial kernels, are not scalable in the same way as shift-invariant
kernels. Computationally, we apply the approximate Log-HS distance formulation
to covariance operators of both handcrafted and convolutional features,
exploiting both the expressiveness of these features and the power of the
covariance representation. Empirically, we tested our framework on the task of
image classification on twelve challenging datasets. In almost all cases, the
results obtained outperform other state of the art methods, demonstrating the
competitiveness and potential of our framework."
"Recent progress in face detection (including keypoint detection), and
recognition is mainly being driven by (i) deeper convolutional neural network
architectures, and (ii) larger datasets. However, most of the large datasets
are maintained by private companies and are not publicly available. The
academic computer vision community needs larger and more varied datasets to
make further progress.
  In this paper we introduce a new face dataset, called UMDFaces, which has
367,888 annotated faces of 8,277 subjects. We also introduce a new face
recognition evaluation protocol which will help advance the state-of-the-art in
this area. We discuss how a large dataset can be collected and annotated using
human annotators and deep networks. We provide human curated bounding boxes for
faces. We also provide estimated pose (roll, pitch and yaw), locations of
twenty-one key-points and gender information generated by a pre-trained neural
network. In addition, the quality of keypoint annotations has been verified by
humans for about 115,000 images. Finally, we compare the quality of the dataset
with other publicly available face datasets at similar scales."
"Recently, very deep convolutional neural networks (CNNs) have shown
outstanding performance in object recognition and have also been the first
choice for dense classification problems such as semantic segmentation.
However, repeated subsampling operations like pooling or convolution striding
in deep CNNs lead to a significant decrease in the initial image resolution.
Here, we present RefineNet, a generic multi-path refinement network that
explicitly exploits all the information available along the down-sampling
process to enable high-resolution prediction using long-range residual
connections. In this way, the deeper layers that capture high-level semantic
features can be directly refined using fine-grained features from earlier
convolutions. The individual components of RefineNet employ residual
connections following the identity mapping mindset, which allows for effective
end-to-end training. Further, we introduce chained residual pooling, which
captures rich background context in an efficient manner. We carry out
comprehensive experiments and set new state-of-the-art results on seven public
datasets. In particular, we achieve an intersection-over-union score of 83.4 on
the challenging PASCAL VOC 2012 dataset, which is the best reported result to
date."
"One popular approach for blind deconvolution is to formulate a maximum a
posteriori (MAP) problem with sparsity priors on the gradients of the latent
image, and then alternatingly estimate the blur kernel and the latent image.
While several successful MAP based methods have been proposed, there has been
much controversy and confusion about their convergence, because sparsity priors
have been shown to prefer blurry images to sharp natural images. In this paper,
we revisit this problem and provide an analysis on the convergence of MAP based
approaches. We first introduce a slight modification to a conventional joint
energy function for blind deconvolution. The reformulated energy function
yields the same alternating estimation process, but more clearly reveals how
blind deconvolution works. We then show the energy function can actually favor
the right solution instead of the no-blur solution under certain conditions,
which explains the success of previous MAP based approaches. The reformulated
energy function and our conditions for the convergence also provide a way to
compare the qualities of different blur kernels, and we demonstrate its
applicability to automatic blur kernel size selection, blur kernel estimation
using light streaks, and defocus estimation."
"This paper presents a method to predict the future movements (location and
gaze direction) of basketball players as a whole from their first person
videos. The predicted behaviors reflect an individual physical space that
affords to take the next actions while conforming to social behaviors by
engaging to joint attention. Our key innovation is to use the 3D reconstruction
of multiple first person cameras to automatically annotate each other's the
visual semantics of social configurations.
  We leverage two learning signals uniquely embedded in first person videos.
Individually, a first person video records the visual semantics of a spatial
and social layout around a person that allows associating with past similar
situations. Collectively, first person videos follow joint attention that can
link the individuals to a group. We learn the egocentric visual semantics of
group movements using a Siamese neural network to retrieve future trajectories.
We consolidate the retrieved trajectories from all players by maximizing a
measure of social compatibility---the gaze alignment towards joint attention
predicted by their social formation, where the dynamics of joint attention is
learned by a long-term recurrent convolutional network. This allows us to
characterize which social configuration is more plausible and predict future
group trajectories."
"Boundary incompleteness raises great challenges to automatic prostate
segmentation in ultrasound images. Shape prior can provide strong guidance in
estimating the missing boundary, but traditional shape models often suffer from
hand-crafted descriptors and local information loss in the fitting procedure.
In this paper, we attempt to address those issues with a novel framework. The
proposed framework can seamlessly integrate feature extraction and shape prior
exploring, and estimate the complete boundary with a sequential manner. Our
framework is composed of three key modules. Firstly, we serialize the static 2D
prostate ultrasound images into dynamic sequences and then predict prostate
shapes by sequentially exploring shape priors. Intuitively, we propose to learn
the shape prior with the biologically plausible Recurrent Neural Networks
(RNNs). This module is corroborated to be effective in dealing with the
boundary incompleteness. Secondly, to alleviate the bias caused by different
serialization manners, we propose a multi-view fusion strategy to merge shape
predictions obtained from different perspectives. Thirdly, we further implant
the RNN core into a multiscale Auto-Context scheme to successively refine the
details of the shape prediction map. With extensive validation on challenging
prostate ultrasound images, our framework bridges severe boundary
incompleteness and achieves the best performance in prostate boundary
delineation when compared with several advanced methods. Additionally, our
approach is general and can be extended to other medical image segmentation
tasks, where boundary incompleteness is one of the main challenges."
"Medical image segmentation requires consensus ground truth segmentations to
be derived from multiple expert annotations. A novel approach is proposed that
obtains consensus segmentations from experts using graph cuts (GC) and semi
supervised learning (SSL). Popular approaches use iterative Expectation
Maximization (EM) to estimate the final annotation and quantify annotator's
performance. Such techniques pose the risk of getting trapped in local minima.
We propose a self consistency (SC) score to quantify annotator consistency
using low level image features. SSL is used to predict missing annotations by
considering global features and local image consistency. The SC score also
serves as the penalty cost in a second order Markov random field (MRF) cost
function optimized using graph cuts to derive the final consensus label. Graph
cut obtains a global maximum without an iterative procedure. Experimental
results on synthetic images, real data of Crohn's disease patients and retinal
images show our final segmentation to be accurate and more consistent than
competing methods."
"Co-localization is the problem of localizing objects of the same class using
only the set of images that contain them. This is a challenging task because
the object detector must be built without negative examples that can lead to
more informative supervision signals. The main idea of our method is to cluster
the feature space of a generically pre-trained CNN, to find a set of CNN
features that are consistently and highly activated for an object category,
which we call category-consistent CNN features. Then, we propagate their
combined activation map using superpixel geodesic distances for
co-localization. In our first set of experiments, we show that the proposed
method achieves state-of-the-art performance on three related benchmarks:
PASCAL 2007, PASCAL-2012, and the Object Discovery dataset. We also show that
our method is able to detect and localize truly unseen categories, on six
held-out ImageNet categories with accuracy that is significantly higher than
previous state-of-the-art. Our intuitive approach achieves this success without
any region proposals or object detectors and can be based on a CNN that was
pre-trained purely on image classification tasks without further fine-tuning."
"Social event detection in a static image is a very challenging problem and
it's very useful for internet of things applications including automatic photo
organization, ads recommender system, or image captioning. Several publications
show that variety of objects, scene, and people can be very ambiguous for the
system to decide the event that occurs in the image. We proposed the spatial
pyramid configuration of convolutional neural network (CNN) classifier for
social event detection in a static image. By applying the spatial pyramid
configuration to the CNN classifier, the detail that occurs in the image can
observe more accurately by the classifier. USED dataset provided by Ahmad et
al. is used to evaluate our proposed method, which consists of two different
image sets, EiMM, and SED dataset. As a result, the average accuracy of our
system outperforms the baseline method by 15% and 2% respectively."
"Visual tracking addresses the problem of identifying and localizing an
unknown target in a video given the target specified by a bounding box in the
first frame. In this paper, we propose a dual network to better utilize
features among layers for visual tracking. It is observed that features in
higher layers encode semantic context while its counterparts in lower layers
are sensitive to discriminative appearance. Thus we exploit the hierarchical
features in different layers of a deep model and design a dual structure to
obtain better feature representation from various streams, which is rarely
investigated in previous work. To highlight geometric contours of the target,
we integrate the hierarchical feature maps with an edge detector as the coarse
prior maps to further embed local details around the target. To leverage the
robustness of our dual network, we train it with random patches measuring the
similarities between the network activation and target appearance, which serves
as a regularization to enforce the dual network to focus on target object. The
proposed dual network is updated online in a unique manner based on the
observation that the target being tracked in consecutive frames should share
more similar feature representations than those in the surrounding background.
It is also found that for a target object, the prior maps can help further
enhance performance by passing message into the output maps of the dual
network. Therefore, an independent component analysis with reference algorithm
(ICA-R) is employed to extract target context using prior maps as guidance.
Online tracking is conducted by maximizing the posterior estimate on the final
maps with stochastic and periodic update. Quantitative and qualitative
evaluations on two large-scale benchmark data sets show that the proposed
algorithm performs favourably against the state-of-the-arts."
"Microalgae counting is used to measure biomass quantity. Usually, it is
performed in a manual way using a Neubauer chamber and expert criterion, with
the risk of a high error rate. This paper addresses the methodology for
automatic identification of Scenedesmus microalgae (used in the methane
production and food industry) and applies it to images captured by a digital
microscope. The use of contrast adaptive histogram equalization for
pre-processing, and active contours for segmentation are presented. The
calculation of statistical features (Histogram of Oriented Gradients, Hu and
Zernike moments) with texture features (Haralick and Local Binary Patterns
descriptors) are proposed for algae characterization. Scenedesmus algae can
build coenobia consisting of 1, 2, 4 and 8 cells. The amount of algae of each
coenobium helps to determine the amount of lipids, proteins, and other
substances in a given sample of a algae crop. The knowledge of the quantity of
those elements improves the quality of bioprocess applications. Classification
of coenobia achieves accuracies of 98.63% and 97.32% with Support Vector
Machine (SVM) and Artificial Neural Network (ANN), respectively. According to
the results it is possible to consider the proposed methodology as an
alternative to the traditional technique for algae counting. The database used
in this paper is publicly available for download."
"We present a new deep supervised learning method for intrinsic decomposition
of a single image into its albedo and shading components. Our contributions are
based on a new fully convolutional neural network that estimates absolute
albedo and shading jointly. Our solution relies on a single end-to-end deep
sequence of residual blocks and a perceptually-motivated metric formed by two
adversarially trained discriminators. As opposed to classical intrinsic image
decomposition work, it is fully data-driven, hence does not require any
physical priors like shading smoothness or albedo sparsity, nor does it rely on
geometric information such as depth. Compared to recent deep learning
techniques, we simplify the architecture, making it easier to build and train,
and constrain it to generate a valid and reversible decomposition. We rediscuss
and augment the set of quantitative metrics so as to account for the more
challenging recovery of non scale-invariant quantities. We train and
demonstrate our architecture on the publicly available MPI Sintel dataset and
its intrinsic image decomposition, show attenuated overfitting issues and
discuss generalizability to other data. Results show that our work outperforms
the state of the art deep algorithms both on the qualitative and quantitative
aspect."
"Nowadays the CNN is widely used in practical applications for image
classification task. However the design of the CNN model is very professional
work and which is very difficult for ordinary users. Besides, even for experts
of CNN, to select an optimal model for specific task may still need a lot of
time (to train many different models). In order to solve this problem, we
proposed an automated CNN recommendation system for image classification task.
Our system is able to evaluate the complexity of the classification task and
the classification ability of the CNN model precisely. By using the evaluation
results, the system can recommend the optimal CNN model and which can match the
task perfectly. The recommendation process of the system is very fast since we
don't need any model training. The experiment results proved that the
evaluation methods are very accurate and reliable."
"We present a descriptor, called fully convolutional self-similarity (FCSS),
for dense semantic correspondence. To robustly match points among different
instances within the same object class, we formulate FCSS using local
self-similarity (LSS) within a fully convolutional network. In contrast to
existing CNN-based descriptors, FCSS is inherently insensitive to intra-class
appearance variations because of its LSS-based structure, while maintaining the
precise localization ability of deep neural networks. The sampling patterns of
local structure and the self-similarity measure are jointly learned within the
proposed network in an end-to-end and multi-scale manner. As training data for
semantic correspondence is rather limited, we propose to leverage object
candidate priors provided in existing image datasets and also correspondence
consistency between object pairs to enable weakly-supervised learning.
Experiments demonstrate that FCSS outperforms conventional handcrafted
descriptors and CNN-based descriptors on various benchmarks."
"Image diffusion plays a fundamental role for the task of image denoising.
Recently proposed trainable nonlinear reaction diffusion (TNRD) model defines a
simple but very effective framework for image denoising. However, as the TNRD
model is a local model, the diffusion behavior of which is purely controlled by
information of local patches, it is prone to create artifacts in the homogenous
regions and over-smooth highly textured regions, especially in the case of
strong noise levels. Meanwhile, it is widely known that the non-local
self-similarity (NSS) prior stands as an effective image prior for image
denoising, which has been widely exploited in many non-local methods. In this
work, we are highly motivated to embed the NSS prior into the TNRD model to
tackle its weaknesses. In order to preserve the expected property that
end-to-end training is available, we exploit the NSS prior by a set of
non-local filters, and derive our proposed trainable non-local reaction
diffusion (TNLRD) model for image denoising. Together with the local filters
and influence functions, the non-local filters are learned by employing
loss-specific training. The experimental results show that the trained TNLRD
model produces visually plausible recovered images with more textures and less
artifacts, compared to its local versions. Moreover, the trained TNLRD model
can achieve strongly competitive performance to recent state-of-the-art image
denoising methods in terms of peak signal-to-noise ratio (PSNR) and structural
similarity index (SSIM)."
"Purpose To develop a computer based method for the automated assessment of
image quality in the context of diabetic retinopathy (DR) to guide the
photographer.
  Methods A deep learning framework was trained to grade the images
automatically. A large representative set of 7000 color fundus images were used
for the experiment which were obtained from the EyePACS that were made
available by the California Healthcare Foundation. Three retinal image analysis
experts were employed to categorize these images into Accept and Reject classes
based on the precise definition of image quality in the context of DR. A deep
learning framework was trained using 3428 images.
  Results A total of 3572 images were used for the evaluation of the proposed
method. The method shows an accuracy of 100% to successfully categorise Accept
and Reject images.
  Conclusion Image quality is an essential prerequisite for the grading of DR.
In this paper we have proposed a deep learning based automated image quality
assessment method in the context of DR. The method can be easily incorporated
with the fundus image capturing system and thus can guide the photographer
whether a recapture is necessary or not."
"This paper addresses the problem of simultaneous 3D reconstruction and
material recognition and segmentation. Enabling robots to recognise different
materials (concrete, metal etc.) in a scene is important for many tasks, e.g.
robotic interventions in nuclear decommissioning. Previous work on 3D semantic
reconstruction has predominantly focused on recognition of everyday domestic
objects (tables, chairs etc.), whereas previous work on material recognition
has largely been confined to single 2D images without any 3D reconstruction.
Meanwhile, most 3D semantic reconstruction methods rely on computationally
expensive post-processing, using Fully-Connected Conditional Random Fields
(CRFs), to achieve consistent segmentations. In contrast, we propose a deep
learning method which performs 3D reconstruction while simultaneously
recognising different types of materials and labelling them at the pixel level.
Unlike previous methods, we propose a fully end-to-end approach, which does not
require hand-crafted features or CRF post-processing. Instead, we use only
learned features, and the CRF segmentation constraints are incorporated inside
the fully end-to-end learned system. We present the results of experiments, in
which we trained our system to perform real-time 3D semantic reconstruction for
23 different materials in a real-world application. The run-time performance of
the system can be boosted to around 10Hz, using a conventional GPU, which is
enough to achieve real-time semantic reconstruction using a 30fps RGB-D camera.
To the best of our knowledge, this work is the first real-time end-to-end
system for simultaneous 3D reconstruction and material recognition."
"Most state-of-the-art text detection methods are specific to horizontal Latin
text and are not fast enough for real-time applications. We introduce Segment
Linking (SegLink), an oriented text detection method. The main idea is to
decompose text into two locally detectable elements, namely segments and links.
A segment is an oriented box covering a part of a word or text line; A link
connects two adjacent segments, indicating that they belong to the same word or
text line. Both elements are detected densely at multiple scales by an
end-to-end trained, fully-convolutional neural network. Final detections are
produced by combining segments connected by links. Compared with previous
methods, SegLink improves along the dimensions of accuracy, speed, and ease of
training. It achieves an f-measure of 75.0% on the standard ICDAR 2015
Incidental (Challenge 4) benchmark, outperforming the previous best by a large
margin. It runs at over 20 FPS on 512x512 images. Moreover, without
modification, SegLink is able to detect long lines of non-Latin text, such as
Chinese."
"Knowledge transfer impacts the performance of deep learning -- the state of
the art for image classification tasks, including automated melanoma screening.
Deep learning's greed for large amounts of training data poses a challenge for
medical tasks, which we can alleviate by recycling knowledge from models
trained on different tasks, in a scheme called transfer learning. Although much
of the best art on automated melanoma screening employs some form of transfer
learning, a systematic evaluation was missing. Here we investigate the presence
of transfer, from which task the transfer is sourced, and the application of
fine tuning (i.e., retraining of the deep learning model after transfer). We
also test the impact of picking deeper (and more expensive) models. Our results
favor deeper models, pre-trained over ImageNet, with fine-tuning, reaching an
AUC of 80.7% and 84.5% for the two skin-lesion datasets evaluated."
"In recent years, deep learning has achieved great success in many computer
vision applications. Convolutional neural networks (CNNs) have lately emerged
as a major approach to image classification. Most research on CNNs thus far has
focused on developing architectures such as the Inception and residual
networks. The convolution layer is the core of the CNN, but few studies have
addressed the convolution unit itself. In this paper, we introduce a
convolution unit called the active convolution unit (ACU). A new convolution
has no fixed shape, because of which we can define any form of convolution. Its
shape can be learned through backpropagation during training. Our proposed unit
has a few advantages. First, the ACU is a generalization of convolution; it can
define not only all conventional convolutions, but also convolutions with
fractional pixel coordinates. We can freely change the shape of the
convolution, which provides greater freedom to form CNN structures. Second, the
shape of the convolution is learned while training and there is no need to tune
it by hand. Third, the ACU can learn better than a conventional unit, where we
obtained the improvement simply by changing the conventional convolution to an
ACU. We tested our proposed method on plain and residual networks, and the
results showed significant improvement using our method on various datasets and
architectures in comparison with the baseline."
"We present a deep convolutional decoder architecture that can generate
volumetric 3D outputs in a compute- and memory-efficient manner by using an
octree representation. The network learns to predict both the structure of the
octree, and the occupancy values of individual cells. This makes it a
particularly valuable technique for generating 3D shapes. In contrast to
standard decoders acting on regular voxel grids, the architecture does not have
cubic complexity. This allows representing much higher resolution outputs with
a limited memory budget. We demonstrate this in several application domains,
including 3D convolutional autoencoders, generation of objects and whole scenes
from high-level representations, and shape from a single image."
"We motivate and address a human-in-the-loop variant of the monocular
viewpoint estimation task in which the location and class of one semantic
object keypoint is available at test time. In order to leverage the keypoint
information, we devise a Convolutional Neural Network called Click-Here CNN
(CH-CNN) that integrates the keypoint information with activations from the
layers that process the image. It transforms the keypoint information into a 2D
map that can be used to weigh features from certain parts of the image more
heavily. The weighted sum of these spatial features is combined with global
image features to provide relevant information to the prediction layers. To
train our network, we collect a novel dataset of 3D keypoint annotations on
thousands of CAD models, and synthetically render millions of images with 2D
keypoint information. On test instances from PASCAL 3D+, our model achieves a
mean class accuracy of 90.7%, whereas the state-of-the-art baseline only
obtains 85.7% mean class accuracy, justifying our argument for
human-in-the-loop inference."
"We present an algorithm capable of identifying a wide variety of
human-induced change on the surface of the planet by analyzing matches between
local features in time-sequenced remote sensing imagery. We evaluate feature
sets, match protocols, and the statistical modeling of feature matches. With
application of KAZE features, k-nearest-neighbor descriptor matching, and
geometric proximity and bi-directional match consistency checks, average match
rates increase more than two-fold over the previous standard. In testing our
platform, we developed a small, labeled benchmark dataset expressing
large-scale residential, industrial, and civic construction, along with null
instances, in California between the years 2010 and 2012. On the benchmark set,
our algorithm makes precise, accurate change proposals on two-thirds of scenes.
Further, the detection threshold can be tuned so that all or almost all
proposed detections are true positives."
"Unsupervised learning from visual data is one of the most difficult
challenges in computer vision, being a fundamental task for understanding how
visual recognition works. From a practical point of view, learning from
unsupervised visual input has an immense practical value, as very large
quantities of unlabeled videos can be collected at low cost. In this paper, we
address the task of unsupervised learning to detect and segment foreground
objects in single images. We achieve our goal by training a student pathway,
consisting of a deep neural network. It learns to predict from a single input
image (a video frame) the output for that particular frame, of a teacher
pathway that performs unsupervised object discovery in video. Our approach is
different from the published literature that performs unsupervised discovery in
videos or in collections of images at test time. We move the unsupervised
discovery phase during the training stage, while at test time we apply the
standard feed-forward processing along the student pathway. This has a dual
benefit: firstly, it allows in principle unlimited possibilities of learning
and generalization during training, while remaining very fast at testing.
Secondly, the student not only becomes able to detect in single images
significantly better than its unsupervised video discovery teacher, but it also
achieves state of the art results on two important current benchmarks, YouTube
Objects and Object Discovery datasets. Moreover, at test time, our system is at
least two orders of magnitude faster than other previous methods."
"We introduce a deep encoder-decoder architecture for image deformation
prediction from multimodal images. Specifically, we design an image-patch-based
deep network that jointly (i) learns an image similarity measure and (ii) the
relationship between image patches and deformation parameters. While our method
can be applied to general image registration formulations, we focus on the
Large Deformation Diffeomorphic Metric Mapping (LDDMM) registration model. By
predicting the initial momentum of the shooting formulation of LDDMM, we
preserve its mathematical properties and drastically reduce the computation
time, compared to optimization-based approaches. Furthermore, we create a
Bayesian probabilistic version of the network that allows evaluation of
registration uncertainty via sampling of the network at test time. We evaluate
our method on a 3D brain MRI dataset using both T1- and T2-weighted images. Our
experiments show that our method generates accurate predictions and that
learning the similarity measure leads to more consistent registrations than
relying on generic multimodal image similarity measures, such as mutual
information. Our approach is an order of magnitude faster than
optimization-based LDDMM."
"Recent study shows that a wide deep network can obtain accuracy comparable to
a deeper but narrower network. Compared to narrower and deeper networks, wide
networks employ relatively less number of layers and have various important
benefits, such that they have less running time on parallel computing devices,
and they are less affected by gradient vanishing problems. However, the
parameter size of a wide network can be very large due to use of large width of
each layer in the network. In order to keep the benefits of wide networks
meanwhile improve the parameter size and accuracy trade-off of wide networks,
we propose a binary tree architecture to truncate architecture of wide networks
by reducing the width of the networks. More precisely, in the proposed
architecture, the width is continuously reduced from lower layers to higher
layers in order to increase the expressive capacity of network with a less
increase on parameter size. Also, to ease the gradient vanishing problem,
features obtained at different layers are concatenated to form the output of
our architecture. By employing the proposed architecture on a baseline wide
network, we can construct and train a new network with same depth but
considerably less number of parameters. In our experimental analyses, we
observe that the proposed architecture enables us to obtain better parameter
size and accuracy trade-off compared to baseline networks using various
benchmark image classification datasets. The results show that our model can
decrease the classification error of baseline from 20.43% to 19.22% on
Cifar-100 using only 28% of parameters that baseline has. Code is available at
https://github.com/ZhangVision/bitnet."
"Traffic congestion is a widespread problem. Dynamic traffic routing systems
and congestion pricing are getting importance in recent research. Lane
prediction and vehicle density estimation is an important component of such
systems. We introduce a novel problem of vehicle self-positioning which
involves predicting the number of lanes on the road and vehicle's position in
those lanes using videos captured by a dashboard camera. We propose an
integrated closed-loop approach where we use the presence of vehicles to aid
the task of self-positioning and vice-versa. To incorporate multiple factors
and high-level semantic knowledge into the solution, we formulate this problem
as a Bayesian framework. In the framework, the number of lanes, the vehicle's
position in those lanes and the presence of other vehicles are considered as
parameters. We also propose a bounding box selection scheme to reduce the
number of false detections and increase the computational efficiency. We show
that the number of box proposals decreases by a factor of 6 using the selection
approach. It also results in large reduction in the number of false detections.
The entire approach is tested on real-world videos and is found to give
acceptable results."
"In this paper, we present a new method for egocentric video temporal
segmentation based on integrating a statistical mean change detector and
agglomerative clustering(AC) within an energy-minimization framework. Given the
tendency of most AC methods to oversegment video sequences when clustering
their frames, we combine the clustering with a concept drift detection
technique (ADWIN) that has rigorous guarantee of performances. ADWIN serves as
a statistical upper bound for the clustering-based video segmentation. We
integrate both techniques in an energy-minimization framework that serves to
disambiguate the decision of both techniques and to complete the segmentation
taking into account the temporal continuity of video frames descriptors. We
present experiments over egocentric sets of more than 13.000 images acquired
with different wearable cameras, showing that our method outperforms
state-of-the-art clustering methods."
"Given the recent advances in depth prediction from Convolutional Neural
Networks (CNNs), this paper investigates how predicted depth maps from a deep
neural network can be deployed for accurate and dense monocular reconstruction.
We propose a method where CNN-predicted dense depth maps are naturally fused
together with depth measurements obtained from direct monocular SLAM. Our
fusion scheme privileges depth prediction in image locations where monocular
SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa.
We demonstrate the use of depth prediction for estimating the absolute scale of
the reconstruction, hence overcoming one of the major limitations of monocular
SLAM. Finally, we propose a framework to efficiently fuse semantic labels,
obtained from a single frame, with dense SLAM, yielding semantically coherent
scene reconstruction from a single view. Evaluation results on two benchmark
datasets show the robustness and accuracy of our approach."
"In this paper, we propose a novel benchmark for evaluating local image
descriptors. We demonstrate that the existing datasets and evaluation protocols
do not specify unambiguously all aspects of evaluation, leading to ambiguities
and inconsistencies in results reported in the literature. Furthermore, these
datasets are nearly saturated due to the recent improvements in local
descriptors obtained by learning them from large annotated datasets. Therefore,
we introduce a new large dataset suitable for training and testing modern
descriptors, together with strictly defined evaluation protocols in several
tasks such as matching, retrieval and classification. This allows for more
realistic, and thus more reliable comparisons in different application
scenarios. We evaluate the performance of several state-of-the-art descriptors
and analyse their properties. We show that a simple normalisation of
traditional hand-crafted descriptors can boost their performance to the level
of deep learning based descriptors within a realistic benchmarks evaluation."
"Thanks to the recent developments of Convolutional Neural Networks, the
performance of face verification methods has increased rapidly. In a typical
face verification method, feature normalization is a critical step for boosting
performance. This motivates us to introduce and study the effect of
normalization during training. But we find this is non-trivial, despite
normalization being differentiable. We identify and study four issues related
to normalization through mathematical analysis, which yields understanding and
helps with parameter settings. Based on this analysis we propose two strategies
for training using normalized features. The first is a modification of softmax
loss, which optimizes cosine similarity instead of inner-product. The second is
a reformulation of metric learning by introducing an agent vector for each
class. We show that both strategies, and small variants, consistently improve
performance by between 0.2% to 0.4% on the LFW dataset based on two models.
This is significant because the performance of the two models on LFW dataset is
close to saturation at over 98%. Codes and models are released on
https://github.com/happynear/NormFace"
"In this project, we design a real-time human-computer interaction system
based on hand gesture. The whole system consists of three components: hand
detection, gesture recognition and human-computer interaction (HCI) based on
recognition; and realizes the robust control of mouse and keyboard events with
a higher accuracy of gesture recognition. Specifically, we use the
convolutional neural network (CNN) to recognize gestures and makes it
attainable to identify relatively complex gestures using only one cheap
monocular camera. We introduce the Kalman filter to estimate the hand position
based on which the mouse cursor control is realized in a stable and smooth way.
During the HCI stage, we develop a simple strategy to avoid the false
recognition caused by noises - mostly transient, false gestures, and thus to
improve the reliability of interaction. The developed system is highly
extendable and can be used in human-robotic or other human-machine interaction
scenarios with more complex command formats rather than just mouse and keyboard
events."
"We address unsupervised optical flow estimation for ego-centric motion. We
argue that optical flow can be cast as a geometrical warping between two
successive video frames and devise a deep architecture to estimate such
transformation in two stages. First, a dense pixel-level flow is computed with
a geometric prior imposing strong spatial constraints. Such prior is typical of
driving scenes, where the point of view is coherent with the vehicle motion. We
show how such global transformation can be approximated with an homography and
how spatial transformer layers can be employed to compute the flow field
implied by such transformation. The second stage then refines the prediction
feeding a second deeper network. A final reconstruction loss compares the
warping of frame X(t) with the subsequent frame X(t+1) and guides both
estimates. The model, which we named TransFlow, performs favorably compared to
other unsupervised algorithms, and shows better generalization compared to
supervised methods with a 3x reduction in error on unseen data."
"Image or object recognition is an important task in computer vision. With the
hight-speed processing power on modern platforms and the availability of mobile
phones everywhere, millions of photos are uploaded to the internet per minute,
it is critical to establish a generic framework for fast and accurate image
processing for automatic recognition and information retrieval. In this paper,
we proposed an efficient image recognition and matching method that is
originally derived from Naive Bayesian classification method to construct a
probabilistic model. Our method support real-time performance and have very
high ability to distinguish similar images with high details. Experiments are
conducted together with intensive comparison with state-of-the-arts on image
matching, such as Ferns recognition and SIFT recognition. The results
demonstrate satisfactory performance."
"Unlike standard cameras that send intensity images at a constant frame rate,
event-driven cameras asynchronously report pixel-level brightness changes,
offering low latency and high temporal resolution (both in the order of
micro-seconds). As such, they have great potential for fast and low power
vision algorithms for robots. Visual tracking, for example, is easily achieved
even for very fast stimuli, as only moving objects cause brightness changes.
However, cameras mounted on a moving robot are typically non-stationary and the
same tracking problem becomes confounded by background clutter events due to
the robot ego-motion. In this paper, we propose a method for segmenting the
motion of an independently moving object for event-driven cameras. Our method
detects and tracks corners in the event stream and learns the statistics of
their motion as a function of the robot's joint velocities when no
independently moving objects are present. During robot operation, independently
moving objects are identified by discrepancies between the predicted corner
velocities from ego-motion and the measured corner velocities. We validate the
algorithm on data collected from the neuromorphic iCub robot. We achieve a
precision of ~ 90 % and show that the method is robust to changes in speed of
both the head and the target."
"Mobile landmark search (MLS) recently receives increasing attention for its
great practical values. However, it still remains unsolved due to two important
challenges. One is high bandwidth consumption of query transmission, and the
other is the huge visual variations of query images sent from mobile devices.
In this paper, we propose a novel hashing scheme, named as canonical view based
discrete multi-modal hashing (CV-DMH), to handle these problems via a novel
three-stage learning procedure. First, a submodular function is designed to
measure visual representativeness and redundancy of a view set. With it,
canonical views, which capture key visual appearances of landmark with limited
redundancy, are efficiently discovered with an iterative mining strategy.
Second, multi-modal sparse coding is applied to transform visual features from
multiple modalities into an intermediate representation. It can robustly and
adaptively characterize visual contents of varied landmark images with certain
canonical views. Finally, compact binary codes are learned on intermediate
representation within a tailored discrete binary embedding model which
preserves visual relations of images measured with canonical views and removes
the involved noises. In this part, we develop a new augmented Lagrangian
multiplier (ALM) based optimization method to directly solve the discrete
binary codes. We can not only explicitly deal with the discrete constraint, but
also consider the bit-uncorrelated constraint and balance constraint together.
Experiments on real world landmark datasets demonstrate the superior
performance of CV-DMH over several state-of-the-art methods."
"We propose a new method to analyze the impact of errors in algorithms for
multi-instance pose estimation and a principled benchmark that can be used to
compare them. We define and characterize three classes of errors -
localization, scoring, and background - study how they are influenced by
instance attributes and their impact on an algorithm's performance. Our
technique is applied to compare the two leading methods for human pose
estimation on the COCO Dataset, measure the sensitivity of pose estimation with
respect to instance size, type and number of visible keypoints, clutter due to
multiple instances, and the relative score of instances. The performance of
algorithms, and the types of error they make, are highly dependent on all these
variables, but mostly on the number of keypoints and the clutter. The analysis
and software tools we propose offer a novel and insightful approach for
understanding the behavior of pose estimation algorithms and an effective
method for measuring their strengths and weaknesses."
"In this paper, we propose the joint learning attention and recurrent neural
network (RNN) models for multi-label classification. While approaches based on
the use of either model exist (e.g., for the task of image captioning),
training such existing network architectures typically require pre-defined
label sequences. For multi-label classification, it would be desirable to have
a robust inference process, so that the prediction error would not propagate
and thus affect the performance. Our proposed model uniquely integrates
attention and Long Short Term Memory (LSTM) models, which not only addresses
the above problem but also allows one to identify visual objects of interests
with varying sizes without the prior knowledge of particular label ordering.
More importantly, label co-occurrence information can be jointly exploited by
our LSTM model. Finally, by advancing the technique of beam search, prediction
of multiple labels can be efficiently achieved by our proposed network model."
"Traditional works have shown that patches in a natural image tend to
redundantly recur many times inside the image, both within the same scale, as
well as across different scales. Make full use of these multi-scale information
can improve the image restoration performance. However, the current proposed
deep learning based restoration methods do not take the multi-scale information
into account. In this paper, we propose a dilated convolution based inception
module to learn multi-scale information and design a deep network for single
image super-resolution. Different dilated convolution learns different scale
feature, then the inception module concatenates all these features to fuse
multi-scale information. In order to increase the reception field of our
network to catch more contextual information, we cascade multiple inception
modules to constitute a deep network to conduct single image super-resolution.
With the novel dilated convolution based inception module, the proposed
end-to-end single image super-resolution network can take advantage of
multi-scale information to improve image super-resolution performance.
Experimental results show that our proposed method outperforms many
state-of-the-art single image super-resolution methods."
"Standard methods for generating adversarial examples for neural networks do
not consistently fool neural network classifiers in the physical world due to a
combination of viewpoint shifts, camera noise, and other natural
transformations, limiting their relevance to real-world systems. We demonstrate
the existence of robust 3D adversarial objects, and we present the first
algorithm for synthesizing examples that are adversarial over a chosen
distribution of transformations. We synthesize two-dimensional adversarial
images that are robust to noise, distortion, and affine transformation. We
apply our algorithm to complex three-dimensional objects, using 3D-printing to
manufacture the first physical adversarial objects. Our results demonstrate the
existence of 3D adversarial objects in the physical world."
"In this work, we address the problem of improvement of robustness of feature
representations learned using convolutional neural networks (CNNs) to image
deformation. We argue that higher moment statistics of feature distributions
could be shifted due to image deformations, and the shift leads to degrade of
performance and cannot be reduced by ordinary normalization methods as observed
in experimental analyses. In order to attenuate this effect, we apply
additional non-linearity in CNNs by combining power functions with learnable
parameters into convolution operation. In the experiments, we observe that CNNs
which employ the proposed method obtain remarkable boost in both the
generalization performance and the robustness under various types of
deformations using large scale benchmark datasets. For instance, a model
equipped with the proposed method obtains 3.3\% performance boost in mAP on
Pascal Voc object detection task using deformed images, compared to the
reference model, while both models provide the same performance using original
images. To the best of our knowledge, this is the first work that studies
robustness of deep features learned using CNNs to a wide range of deformations
for object recognition and detection."
"Automatic liver segmentation in 3D medical images is essential in many
clinical applications, such as pathological diagnosis of hepatic diseases,
surgical planning, and postoperative assessment. However, it is still a very
challenging task due to the complex background, fuzzy boundary, and various
appearance of liver. In this paper, we propose an automatic and efficient
algorithm to segment liver from 3D CT volumes. A deep image-to-image network
(DI2IN) is first deployed to generate the liver segmentation, employing a
convolutional encoder-decoder architecture combined with multi-level feature
concatenation and deep supervision. Then an adversarial network is utilized
during training process to discriminate the output of DI2IN from ground truth,
which further boosts the performance of DI2IN. The proposed method is trained
on an annotated dataset of 1000 CT volumes with various different scanning
protocols (e.g., contrast and non-contrast, various resolution and position)
and large variations in populations (e.g., ages and pathology). Our approach
outperforms the state-of-the-art solutions in terms of segmentation accuracy
and computing efficiency."
"We propose a new neural network architecture for automatic generation of
missing characters in a Chinese font set. We call the neural network
architecture the Variational Grid Setting Network which is based on the
variational autoencoder (VAE) with some tweaks. The neural network model is
able to generate missing characters relatively large in size ($256 \times 256$
pixels). Moreover, we show that one can use very few samples for training data
set, and get a satisfied result."
"In this paper, we propose a method for cloud removal from visible light RGB
satellite images by extending the conditional Generative Adversarial Networks
(cGANs) from RGB images to multispectral images. Satellite images have been
widely utilized for various purposes, such as natural environment monitoring
(pollution, forest or rivers), transportation improvement and prompt emergency
response to disasters. However, the obscurity caused by clouds makes it
unstable to monitor the situation on the ground with the visible light camera.
Images captured by a longer wavelength are introduced to reduce the effects of
clouds. Synthetic Aperture Radar (SAR) is such an example that improves
visibility even the clouds exist. On the other hand, the spatial resolution
decreases as the wavelength increases. Furthermore, the images captured by long
wavelengths differs considerably from those captured by visible light in terms
of their appearance. Therefore, we propose a network that can remove clouds and
generate visible light images from the multispectral images taken as inputs.
This is achieved by extending the input channels of cGANs to be compatible with
multispectral images. The networks are trained to output images that are close
to the ground truth using the images synthesized with clouds over the ground
truth as inputs. In the available dataset, the proportion of images of the
forest or the sea is very high, which will introduce bias in the training
dataset if uniformly sampled from the original dataset. Thus, we utilize the
t-Distributed Stochastic Neighbor Embedding (t-SNE) to improve the problem of
bias in the training dataset. Finally, we confirm the feasibility of the
proposed network on the dataset of four bands images, which include three
visible light bands and one near-infrared (NIR) band."
"We present an overview and evaluation of a new, systematic approach for
generation of highly realistic, annotated synthetic data for training of deep
neural networks in computer vision tasks. The main contribution is a procedural
world modeling approach enabling high variability coupled with physically
accurate image synthesis, and is a departure from the hand-modeled virtual
worlds and approximate image synthesis methods used in real-time applications.
The benefits of our approach include flexible, physically accurate and scalable
image synthesis, implicit wide coverage of classes and features, and complete
data introspection for annotations, which all contribute to quality and cost
efficiency. To evaluate our approach and the efficacy of the resulting data, we
use semantic segmentation for autonomous vehicles and robotic navigation as the
main application, and we train multiple deep learning architectures using
synthetic data with and without fine tuning on organic (i.e. real-world) data.
The evaluation shows that our approach improves the neural network's
performance and that even modest implementation efforts produce
state-of-the-art results."
"Sea level change, one of the most dire impacts of anthropogenic global
warming, will affect a large amount of the world's population. However, sea
level change is not uniform in time and space, and the skill of conventional
prediction methods is limited due to the ocean's internal variabi-lity on
timescales from weeks to decades. Here we study the potential of neural network
methods which have been used successfully in other applications, but rarely
been applied for this task. We develop a combination of a convolutional neural
network (CNN) and a recurrent neural network (RNN) to ana-lyse both the spatial
and the temporal evolution of sea level and to suggest an independent, accurate
method to predict interannual sea level anomalies (SLA). We test our method for
the northern and equatorial Pacific Ocean, using gridded altimeter-derived SLA
data. We show that the used network designs outperform a simple regression and
that adding a CNN improves the skill significantly. The predictions are stable
over several years."
"Video prediction models based on convolutional networks, recurrent networks,
and their combinations often result in blurry predictions. We identify an
important contributing factor for imprecise predictions that has not been
studied adequately in the literature: blind spots, i.e., lack of access to all
relevant past information for accurately predicting the future. To address this
issue, we introduce a fully context-aware architecture that captures the entire
available past context for each pixel using Parallel Multi-Dimensional LSTM
units and aggregates it using blending units. Our model outperforms a strong
baseline network of 20 recurrent convolutional layers and yields
state-of-the-art performance for next step prediction on three challenging
real-world video datasets: Human 3.6M, Caltech Pedestrian, and UCF-101.
Moreover, it does so with fewer parameters than several recently proposed
models, and does not rely on deep convolutional networks, multi-scale
architectures, separation of background and foreground modeling, motion flow
learning, or adversarial training. These results highlight that full awareness
of past context is of crucial importance for video prediction."
"Reflectance confocal microscopy (RCM) is an effective, non-invasive
pre-screening tool for skin cancer diagnosis, but it requires extensive
training and experience to assess accurately. There are few quantitative tools
available to standardize image acquisition and analysis, and the ones that are
available are not interpretable. In this study, we use a recurrent neural
network with attention on convolutional network features. We apply it to
delineate skin strata in vertically-oriented stacks of transverse RCM image
slices in an interpretable manner. We introduce a new attention mechanism
called Toeplitz attention, which constrains the attention map to have a
Toeplitz structure. Testing our model on an expert labeled dataset of 504 RCM
stacks, we achieve 88.17% image-wise classification accuracy, which is the
current state-of-art."
"In this paper, we adopt 3D Convolutional Neural Networks to segment
volumetric medical images. Although deep neural networks have been proven to be
very effective on many 2D vision tasks, it is still challenging to apply them
to 3D tasks due to the limited amount of annotated 3D data and limited
computational resources. We propose a novel 3D-based coarse-to-fine framework
to effectively and efficiently tackle these challenges. The proposed 3D-based
framework outperforms the 2D counterpart to a large margin since it can
leverage the rich spatial infor- mation along all three axes. We conduct
experiments on two datasets which include healthy and pathological pancreases
respectively, and achieve the current state-of-the-art in terms of
Dice-S{\o}rensen Coefficient (DSC). On the NIH pancreas segmentation dataset,
we outperform the previous best by an average of over 2%, and the worst case is
improved by 7% to reach almost 70%, which indicates the reliability of our
framework in clinical applications."
"In object detection, an intersection over union (IoU) threshold is required
to define positives and negatives. An object detector, trained with low IoU
threshold, e.g. 0.5, usually produces noisy detections. However, detection
performance tends to degrade with increasing the IoU thresholds. Two main
factors are responsible for this: 1) overfitting during training, due to
exponentially vanishing positive samples, and 2) inference-time mismatch
between the IoUs for which the detector is optimal and those of the input
hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is
proposed to address these problems. It consists of a sequence of detectors
trained with increasing IoU thresholds, to be sequentially more selective
against close false positives. The detectors are trained stage by stage,
leveraging the observation that the output of a detector is a good distribution
for training the next higher quality detector. The resampling of progressively
improved hypotheses guarantees that all detectors have a positive set of
examples of equivalent size, reducing the overfitting problem. The same cascade
procedure is applied at inference, enabling a closer match between the
hypotheses and the detector quality of each stage. A simple implementation of
the Cascade R-CNN is shown to surpass all single-model object detectors on the
challenging COCO dataset. Experiments also show that the Cascade R-CNN is
widely applicable across detector architectures, achieving consistent gains
independently of the baseline detector strength. The code will be made
available at https://github.com/zhaoweicai/cascade-rcnn."
"Convolutional neural networks (CNNs) have become popular especially in
computer vision in the last few years because they achieved outstanding
performance on different tasks, such as image classifications. We propose a
nine-layer CNN for leaf identification using the famous Flavia and Foliage
datasets. Usually the supervised learning of deep CNNs requires huge datasets
for training. However, the used datasets contain only a few examples per plant
species. Therefore, we apply data augmentation and transfer learning to prevent
our network from overfitting. The trained CNNs achieve recognition rates above
99% on the Flavia and Foliage datasets, and slightly outperform current methods
for leaf classification."
"Inertial sensors are present in most mobile devices nowadays and such devices
are used by people during most of their daily activities. In this paper, we
present an approach for human activity recognition based on inertial sensors by
employing recurrence plots (RP) and visual descriptors. The pipeline of the
proposed approach is the following: compute RPs from sensor data, compute
visual features from RPs and use them in a machine learning protocol. As RPs
generate texture visual patterns, we transform the problem of sensor data
classification to a problem of texture classification. Experiments for
classifying human activities based on accelerometer data showed that the
proposed approach obtains the highest accuracies, outperforming time- and
frequency-domain features directly extracted from sensor data. The best results
are obtained when using RGB RPs, in which each RGB channel corresponds to the
RP of an independent accelerometer axis."
"We propose to automatically create capsule wardrobes. Given an inventory of
candidate garments and accessories, the algorithm must assemble a minimal set
of items that provides maximal mix-and-match outfits. We pose the task as a
subset selection problem. To permit efficient subset selection over the space
of all outfit combinations, we develop submodular objective functions capturing
the key ingredients of visual compatibility, versatility, and user-specific
preference. Since adding garments to a capsule only expands its possible
outfits, we devise an iterative approach to allow near-optimal submodular
function maximization. Finally, we present an unsupervised approach to learn
visual compatibility from ""in the wild"" full body outfit photos; the
compatibility metric translates well to cleaner catalog photos and improves
over existing methods. Our results on thousands of pieces from popular fashion
websites show that automatic capsule creation has potential to mimic skilled
fashionistas in assembling flexible wardrobes, while being significantly more
scalable."
"Shadow detection is a fundamental and challenging task, since it requires an
understanding of global image semantics and there are various backgrounds
around shadows. This paper presents a novel network for shadow detection by
analyzing image context in a direction-aware manner. To achieve this, we first
formulate the direction-aware attention mechanism in a spatial recurrent neural
network (RNN) by introducing attention weights when aggregating spatial context
features in the RNN. By learning these weights through training, we can recover
direction-aware spatial context (DSC) for detecting shadows. This design is
developed into the DSC module and embedded in a CNN to learn DSC features at
different levels. Moreover, a weighted cross entropy loss is designed to make
the training more effective. We employ two common shadow detection benchmark
datasets and perform various experiments to evaluate our network. Experimental
results show that our network outperforms state-of-the-art methods and achieves
97% accuracy and 38% reduction on balance error rate."
"To work at scale, a complete image indexing system comprises two components:
An inverted file index to restrict the actual search to only a subset that
should contain most of the items relevant to the query; An approximate distance
computation mechanism to rapidly scan these lists. While supervised deep
learning has recently enabled improvements to the latter, the former continues
to be based on unsupervised clustering in the literature. In this work, we
propose a first system that learns both components within a unifying neural
framework of structured binary encoding."
"Recent deep networks that directly handle points in a point set, e.g.,
PointNet, have been state-of-the-art for supervised learning tasks on point
clouds such as classification and segmentation. In this work, a novel
end-to-end deep auto-encoder is proposed to address unsupervised learning
challenges on point clouds. On the encoder side, a graph-based enhancement is
enforced to promote local structures on top of PointNet. Then, a novel
folding-based decoder deforms a canonical 2D grid onto the underlying 3D object
surface of a point cloud, achieving low reconstruction errors even for objects
with delicate structures. The proposed decoder only uses about 7% parameters of
a decoder with fully-connected neural networks, yet leads to a more
discriminative representation that achieves higher linear SVM classification
accuracy than the benchmark. In addition, the proposed decoder structure is
shown, in theory, to be a generic architecture that is able to reconstruct an
arbitrary point cloud from a 2D grid. Our code is available at
http://www.merl.com/research/license#FoldingNet"
"Many image processing tasks can be formulated as translating images between
two image domains, such as colorization, super resolution and conditional image
synthesis. In most of these tasks, an input image may correspond to multiple
outputs. However, current existing approaches only show very minor diversity of
the outputs. In this paper, we present a novel approach to synthesize diverse
realistic images corresponding to a semantic layout. We introduce a diversity
loss objective, which maximizes the distance between synthesized image pairs
and links the input noise to the semantic segments in the synthesized images.
Thus, our approach can not only produce diverse images, but also allow users to
manipulate the output images by adjusting the noise manually. Experimental
results show that images synthesized by our approach are significantly more
diverse than that of the current existing works and equipping our diversity
loss does not degrade the reality of the base networks."
"Virtual and augmented reality technologies have seen significant growth in
the past few years. A key component of such systems is the ability to track the
pose of head mounted displays and controllers in 3D space. We tackle the
problem of efficient 6-DoF tracking of a handheld controller from egocentric
camera perspectives. We collected the HMD Controller dataset which consist of
over 540,000 stereo image pairs labelled with the full 6-DoF pose of the
handheld controller. Our proposed SSD-AF-Stereo3D model achieves a mean average
error of 33.5 millimeters in 3D keypoint prediction and is used in conjunction
with an IMU sensor on the controller to enable 6-DoF tracking. We also present
results on approaches for model based full 6-DoF tracking. All our models
operate under the strict constraints of real time mobile CPU inference."
"Single image rain streaks removal is extremely important since rainy images
adversely affect many computer vision systems. Deep learning based methods have
found great success in image deraining tasks. In this paper, we propose a novel
residual-guide feature fusion network, called ResGuideNet, for single image
deraining that progressively predicts highquality reconstruction. Specifically,
we propose a cascaded network and adopt residuals generated from shallower
blocks to guide deeper blocks. By using this strategy, we can obtain a coarse
to fine estimation of negative residual as the blocks go deeper. The outputs of
different blocks are merged into the final reconstruction. We adopt recursive
convolution to build each block and apply supervision to all intermediate
results, which enable our model to achieve promising performance on synthetic
and real-world data while using fewer parameters than previous required.
ResGuideNet is detachable to meet different rainy conditions. For images with
light rain streaks and limited computational resource at test time, we can
obtain a decent performance even with several building blocks. Experiments
validate that ResGuideNet can benefit other low- and high-level vision tasks."
"Our overarching goal is to develop an accurate and explainable model for
plant disease identification using hyperspectral data. Charcoal rot is a soil
borne fungal disease that affects the yield of soybean crops worldwide.
Hyperspectral images were captured at 240 different wavelengths in the range of
383 - 1032 nm. We developed a 3D Convolutional Neural Network model for soybean
charcoal rot disease identification. Our model has classification accuracy of
95.73\% and an infected class F1 score of 0.87. We infer the trained model
using saliency map and visualize the most sensitive pixel locations that enable
classification. The sensitivity of individual wavelengths for classification
was also determined using the saliency map visualization. We identify the most
sensitive wavelength as 733 nm using the saliency map visualization. Since the
most sensitive wavelength is in the Near Infrared Region(700 - 1000 nm) of the
electromagnetic spectrum, which is also the commonly used spectrum region for
determining the vegetation health of the plant, we were more confident in the
predictions using our model."
"Domain adaptation is widely used in learning problems lacking labels. Recent
studies show that deep adversarial domain adaptation models can make markable
improvements in performance, which include symmetric and asymmetric
architectures. However, the former has poor generalization ability whereas the
latter is very hard to train. In this paper, we propose a novel adversarial
domain adaptation method named Adversarial Residual Transform Networks (ARTNs)
to improve the generalization ability, which directly transforms the source
features into the space of target features. In this model, residual connections
are used to share features and adversarial loss is reconstructed, thus making
the model more generalized and easier to train. Moreover, a special
regularization term is added to the loss function to alleviate a vanishing
gradient problem, which enables its training process stable. A series of
experiments based on Amazon review dataset, digits datasets and Office-31 image
datasets are conducted to show that the proposed ARTN can be comparable with
the methods of the state-of-the-art."
"To overcome the poor scalability of convolutional neural network, recurrent
attention model(RAM) selectively choose what and where to look on the image. By
directing recurrent attention model how to look the image, RAM can be even more
successful in that the given clue narrow down the scope of the possible focus
zone. In this perspective, this work proposes clued recurrent attention model
(CRAM) which add clue or constraint on the RAM better problem solving. CRAM
follows encoder-decoder framework, encoder utilizes recurrent attention model
with spatial transformer network and decoder which varies depending on the
task. To ensure the performance, CRAM tackles two computer vision task. One is
the image classification task, with clue given as the binary image saliency
which indicates the approximate location of object. The other is the inpainting
task, with clue given as binary mask which indicates the occluded part. In both
tasks, CRAM shows better performance than existing methods showing the
successful extension of RAM."
"Image patch matching, which is the process of identifying corresponding
patches across images, has been used as a subroutine for many computer vision
and image processing tasks. State -of-the-art patch matching techniques take
image patches as input to a convolutional neural network to extract the patch
features and evaluate their similarity. Our aim in this paper is to improve on
the state of the art patch matching techniques by observing the fact that a
sparse-overcomplete representation of an image posses statistical properties of
natural visual scenes which can be exploited for patch matching. We propose a
new paradigm which encodes image patch details by encoding the patch and
subsequently using this sparse representation as input to a neural network to
compare the patches. As sparse coding is based on a generative model of natural
image patches, it can represent the patch in terms of the fundamental visual
components from which it has been composed of, leading to similar sparse codes
for patches which are built from similar components. Once the sparse coded
features are extracted, we employ a fully-connected neural network, which
captures the non-linear relationships between features, for comparison. We have
evaluated our approach using the Liberty and Notredame subsets of the popular
UBC patch dataset and set a new benchmark outperforming all state-of-the-art
patch matching techniques for these datasets."
"In this paper, we propose a novel regularization method for Generative
Adversarial Networks, which allows the model to learn discriminative yet
compact binary representations of image patches (image descriptors). We employ
the dimensionality reduction that takes place in the intermediate layers of the
discriminator network and train binarized low-dimensional representation of the
penultimate layer to mimic the distribution of the higher-dimensional preceding
layers. To achieve this, we introduce two loss terms that aim at: (i) reducing
the correlation between the dimensions of the binarized low-dimensional
representation of the penultimate layer i. e. maximizing joint entropy) and
(ii) propagating the relations between the dimensions in the high-dimensional
space to the low-dimensional space. We evaluate the resulting binary image
descriptors on two challenging applications, image matching and retrieval, and
achieve state-of-the-art results."
"Multi-person pose estimation is fundamental to many computer vision tasks and
has made significant progress in recent years. However, few previous methods
explored the problem of pose estimation in crowded scenes while it remains
challenging and inevitable in many scenarios. Moreover, current benchmarks
cannot provide an appropriate evaluation for such cases. In this paper, we
propose a novel and efficient method to tackle the problem of pose estimation
in the crowd and a new dataset to better evaluate algorithms. Our model
consists of two key components: joint-candidate single person pose estimation
(SPPE) and global maximum joints association. With multi-peak prediction for
each joint and global association using graph model, our method is robust to
inevitable interference in crowded scenes and very efficient in inference. The
proposed method surpasses the state-of-the-art methods on CrowdPose dataset by
5.2 mAP and results on MSCOCO dataset demonstrate the generalization ability of
our method. Source code and dataset will be made publicly available."
"Three-dimensional (3D) shape recognition has drawn much research attention in
the field of computer vision. The advances of deep learning encourage various
deep models for 3D feature representation. For point cloud and multi-view data,
two popular 3D data modalities, different models are proposed with remarkable
performance. However the relation between point cloud and views has been rarely
investigated. In this paper, we introduce Point-View Relation Network (PVRNet),
an effective network designed to well fuse the view features and the point
cloud feature with a proposed relation score module. More specifically, based
on the relation score module, the point-single-view fusion feature is first
extracted by fusing the point cloud feature and each single view feature with
point-singe-view relation, then the point-multi-view fusion feature is
extracted by fusing the point cloud feature and the features of different
number of views with point-multi-view relation. Finally, the point-single-view
fusion feature and point-multi-view fusion feature are further combined
together to achieve a unified representation for a 3D shape. Our proposed
PVRNet has been evaluated on ModelNet40 dataset for 3D shape classification and
retrieval. Experimental results indicate our model can achieve significant
performance improvement compared with the state-of-the-art models."
"We tackle the problem of using 3D information in convolutional neural
networks for down-stream recognition tasks. Using depth as an additional
channel alongside the RGB input has the scale variance problem present in image
convolution based approaches. On the other hand, 3D convolution wastes a large
amount of memory on mostly unoccupied 3D space, which consists of only the
surface visible to the sensor. Instead, we propose SurfConv, which ""slides""
compact 2D filters along the visible 3D surface. SurfConv is formulated as a
simple depth-aware multi-scale 2D convolution, through a new Data-Driven Depth
Discretization (D4) scheme. We demonstrate the effectiveness of our method on
indoor and outdoor 3D semantic segmentation datasets. Our method achieves
state-of-the-art performance with less than 30% parameters used by the 3D
convolution-based approaches."
"While deep learning has achieved significant advances in accuracy for medical
image segmentation, its benefits for deformable image registration have so far
remained limited to reduced computation times. Previous work has either focused
on replacing the iterative optimization of distance and smoothness terms with
CNN-layers or using supervised approaches driven by labels. Our method is the
first to combine the complementary strengths of global semantic information
(represented by segmentation labels) and local distance metrics that help align
surrounding structures. We demonstrate significant higher Dice scores (of
86.5\%) for deformable cardiac image registration compared to classic
registration (79.0\%) as well as label-driven deep learning frameworks
(83.4\%)."
"We explore the application of super-resolution techniques to satellite
imagery, and the effects of these techniques on object detection algorithm
performance. Specifically, we enhance satellite imagery beyond its native
resolution, and test if we can identify various types of vehicles, planes, and
boats with greater accuracy than native resolution. Using the Very Deep
Super-Resolution (VDSR) framework and a custom Random Forest Super-Resolution
(RFSR) framework we generate enhancement levels of 2x, 4x, and 8x over five
distinct resolutions ranging from 30 cm to 4.8 meters. Using both native and
super-resolved data, we then train several custom detection models using the
SIMRDWN object detection framework. SIMRDWN combines a number of popular object
detection algorithms (e.g. SSD, YOLO) into a unified framework that is designed
to rapidly detect objects in large satellite images. This approach allows us to
quantify the effects of super-resolution techniques on object detection
performance across multiple classes and resolutions. We also quantify the
performance of object detection as a function of native resolution and object
pixel size. For our test set we note that performance degrades from mean
average precision (mAP) = 0.53 at 30 cm resolution, down to mAP = 0.11 at 4.8 m
resolution. Super-resolving native 30 cm imagery to 15 cm yields the greatest
benefit; a 13-36% improvement in mAP. Super-resolution is less beneficial at
coarser resolutions, though still provides a small improvement in performance."
"Compared with other semantic segmentation tasks, portrait segmentation
requires both higher precision and faster inference speed. However, this
problem has not been well studied in previous works. In this paper, we propose
a lightweight network architecture, called Boundary-Aware Network (BANet) which
selectively extracts detail information in boundary area to make high-quality
segmentation output with real-time( >25FPS) speed. In addition, we design a new
loss function called refine loss which supervises the network with image level
gradient information. Our model is able to produce finer segmentation results
which has richer details than annotations."
"We present a new approach to the problem of estimating the 3D room layout
from a single panoramic image. We represent room layout as three 1D vectors
that encode, at each image column, the boundary positions of floor-wall and
ceiling-wall, and the existence of wall-wall boundary. The proposed network,
HorizonNet, trained for predicting 1D layout, outperforms previous
state-of-the-art approaches. The designed post-processing procedure for
recovering 3D room layouts from 1D predictions can automatically infer the room
shape with low computation cost - it takes less than 20ms for a panorama image
while prior works might need dozens of seconds. We also propose Pano Stretch
Data Augmentation, which can diversify panorama data and be applied to other
panorama-related learning tasks. Due to the limited data available for
non-cuboid layout, we relabel 65 general layout from the current dataset for
finetuning. Our approach shows good performance on general layouts by
qualitative results and cross-validation."
"Feature detectors and descriptors are key low-level vision tools that many
higher-level tasks build on. Unfortunately these fail in the presence of
challenging light transport effects including partial occlusion, low contrast,
and reflective or refractive surfaces. Building on spatio-angular imaging
modalities offered by emerging light field cameras, we introduce a new and
computationally efficient 4D light field feature detector and descriptor: LiFF.
LiFF is scale invariant and utilizes the full 4D light field to detect features
that are robust to changes in perspective. This is particularly useful for
structure from motion (SfM) and other tasks that match features across
viewpoints of a scene. We demonstrate significantly improved 3D reconstructions
via SfM when using LiFF instead of the leading 2D or 4D features, and show that
LiFF runs an order of magnitude faster than the leading 4D approach. Finally,
LiFF inherently estimates depth for each feature, opening a path for future
research in light field-based SfM."
"Graph matching is an important and persistent problem in computer vision and
pattern recognition for finding node-to-node correspondence between
graph-structured data. However, as widely used, graph matching that
incorporates pairwise constraints can be formulated as a quadratic assignment
problem (QAP), which is NP-complete and results in intrinsic computational
difficulties. In this paper, we present a functional representation for graph
matching (FRGM) that aims to provide more geometric insights on the problem and
reduce the space and time complexities of corresponding algorithms. To achieve
these goals, we represent a graph endowed with edge attributes by a linear
function space equipped with a functional such as inner product or metric, that
has an explicit geometric meaning. Consequently, the correspondence between
graphs can be represented as a linear representation map of that functional.
Specifically, we reformulate the linear functional representation map as a new
parameterization for Euclidean graph matching, which is associative with
geometric parameters for graphs under rigid or nonrigid deformations. This
allows us to estimate the correspondence and geometric deformations
simultaneously. The use of the representation of edge attributes rather than
the affinity matrix enables us to reduce the space complexity by two orders of
magnitudes. Furthermore, we propose an efficient optimization strategy with low
time complexity to optimize the objective function. The experimental results on
both synthetic and real-world datasets demonstrate that the proposed FRGM can
achieve state-of-the-art performance."
"Recent research on face detection, which is focused primarily on improving
accuracy of detecting smaller faces, attempt to develop new anchor design
strategies to facilitate increased overlap between anchor boxes and ground
truth faces of smaller sizes. In this work, we approach the problem of small
face detection with the motivation of enriching the feature maps using a
density map estimation module. This module, inspired by recent crowd
counting/density estimation techniques, performs the task of estimating the per
pixel density of people/faces present in the image. Output of this module is
employed to accentuate the feature maps from the backbone network using a
feature enrichment module before being used for detecting smaller faces. The
proposed approach can be used to complement recent anchor-design based novel
methods to further improve their results. Experiments conducted on different
datasets such as WIDER, FDDB and Pascal-Faces demonstrate the effectiveness of
the proposed approach."
"The bilateral and nonlocal means filters are instances of kernel-based
filters that are popularly used in image processing. It was recently shown that
fast and accurate bilateral filtering of grayscale images can be performed
using a low-rank approximation of the kernel matrix. More specifically, based
on the eigendecomposition of the kernel matrix, the overall filtering was
approximated using spatial convolutions, for which efficient algorithms are
available. Unfortunately, this technique cannot be scaled to high-dimensional
data such as color and hyperspectral images. This is simply because one needs
to compute/store a large matrix and perform its eigendecomposition in this
case. We show how this problem can be solved using the Nystr\""om method, which
is generally used for approximating the eigendecomposition of large matrices.
The resulting algorithm can also be used for nonlocal means filtering. We
demonstrate the effectiveness of our proposal for bilateral and nonlocal means
filtering of color and hyperspectral images. In particular, our method is shown
to be competitive with state-of-the-art fast algorithms, and moreover it comes
with a theoretical guarantee on the approximation error."
"Convolutional neural networks (CNNs) have been successfully applied to solve
the problem of correspondence estimation between semantically related images.
Due to non-availability of large training datasets, existing methods resort to
self-supervised or unsupervised training paradigm. In this paper we propose a
semi-supervised learning framework that imposes cyclic consistency constraint
on unlabeled image pairs. Together with the supervised loss the proposed model
achieves state-of-the-art on a benchmark semantic matching dataset."
"In this paper, we propose a novel approach for saliency detection for seismic
applications using 3D-FFT local spectra and multi-dimensional plane
projections. We develop a projection scheme by dividing a 3D-FFT local spectrum
of a data volume into three distinct components, each depicting changes along a
different dimension of the data. The saliency detection results obtained using
each projected component are then combined to yield a saliency map. To
accommodate the directional nature of seismic data, in this work, we modify the
center-surround model, proven to be biologically plausible for visual
attention, to incorporate directional comparisons around each voxel in a 3D
volume. Experimental results on real seismic dataset from the F3 block in
Netherlands offshore in the North Sea prove that the proposed algorithm is
effective, efficient, and scalable. Furthermore, a subjective comparison of the
results shows that it outperforms the state-of-the-art methods for saliency
detection."
"Celiac disease prevalence and diagnosis have increased substantially in
recent years. The current gold standard for celiac disease confirmation is
visual examination of duodenal mucosal biopsies. An accurate computer-aided
biopsy analysis system using deep learning can help pathologists diagnose
celiac disease more efficiently. In this study, we trained a deep learning
model to detect celiac disease on duodenal biopsy images. Our model uses a
state-of-the-art residual convolutional neural network to evaluate patches of
duodenal tissue and then aggregates those predictions for whole-slide
classification. We tested the model on an independent set of 212 images and
evaluated its classification results against reference standards established by
pathologists. Our model identified celiac disease, normal tissue, and
nonspecific duodenitis with accuracies of 95.3%, 91.0%, and 89.2%,
respectively. The area under the receiver operating characteristic curve was
greater than 0.95 for all classes. We have developed an automated biopsy
analysis system that achieves high performance in detecting celiac disease on
biopsy slides. Our system can highlight areas of interest and provide
preliminary classification of duodenal biopsies prior to review by
pathologists. This technology has great potential for improving the accuracy
and efficiency of celiac disease diagnosis."
"In this paper, we focus on obtaining 2D and 3D labels, as well as track IDs
for objects on the road with the help of a novel 3D Bounding Box Annotation
Toolbox (3D BAT). Our open source, web-based 3D BAT incorporates several smart
features to improve usability and efficiency. For instance, this annotation
toolbox supports semi-automatic labeling of tracks using interpolation, which
is vital for downstream tasks like tracking, motion planning and motion
prediction. Moreover, annotations for all camera images are automatically
obtained by projecting annotations from 3D space into the image domain. In
addition to the raw image and point cloud feeds, a Masterview consisting of the
top view (bird's-eye-view), side view and front views is made available to
observe objects of interest from different perspectives. Comparisons of our
method with other publicly available annotation tools reveal that 3D
annotations can be obtained faster and more efficiently by using our toolbox."
"Automatic segmentation of fine-grained brain structures remains a challenging
task. Current segmentation methods mainly utilize 2D and 3D deep neural
networks. The 2D networks take image slices as input to produce coarse
segmentation in less processing time, whereas the 3D networks take the whole
image volumes to generated fine-detailed segmentation with more computational
burden. In order to obtain accurate fine-grained segmentation efficiently, in
this paper, we propose an end-to-end Feature-Fused Context-Encoding Network for
brain structure segmentation from MR (magnetic resonance) images. Our model is
implemented based on a 2D convolutional backbone, which integrates a 2D
encoding module to acquire planar image features and a spatial encoding module
to extract spatial context information. A global context encoding module is
further introduced to capture global context semantics from the fused 2D
encoding and spatial features. The proposed network aims to fully leverage the
global anatomical prior knowledge learned from context semantics, which is
represented by a structure-aware attention factor to recalibrate the outputs of
the network. In this way, the network is guaranteed to be aware of the
class-dependent feature maps to facilitate the segmentation. We evaluate our
model on 2012 Brain Multi-Atlas Labelling Challenge dataset for 134
fine-grained structure segmentation. Besides, we validate our network on 27
coarse structure segmentation tasks. Experimental results have demonstrated
that our model can achieve improved performance compared with the
state-of-the-art approaches."
"Objective: This work addresses two key problems of skin lesion
classification. The first problem is the effective use of high-resolution
images with pretrained standard architectures for image classification. The
second problem is the high class imbalance encountered in real-world
multi-class datasets. Methods: To use high-resolution images, we propose a
novel patch-based attention architecture that provides global context between
small, high-resolution patches. We modify three pretrained architectures and
study the performance of patch-based attention. To counter class imbalance
problems, we compare oversampling, balanced batch sampling, and class-specific
loss weighting. Additionally, we propose a novel diagnosis-guided loss
weighting method which takes the method used for ground-truth annotation into
account. Results: Our patch-based attention mechanism outperforms previous
methods and improves the mean sensitivity by 7%. Class balancing significantly
improves the mean sensitivity and we show that our diagnosis-guided loss
weighting method improves the mean sensitivity by 3% over normal loss
balancing. Conclusion: The novel patch-based attention mechanism can be
integrated into pretrained architectures and provides global context between
local patches while outperforming other patch-based methods. Hence, pretrained
architectures can be readily used with high-resolution images without
downsampling. The new diagnosis-guided loss weighting method outperforms other
methods and allows for effective training when facing class imbalance.
Significance: The proposed methods improve automatic skin lesion
classification. They can be extended to other clinical applications where
high-resolution image data and class imbalance are relevant."
"We present a conceptually simple yet effective algorithm to detect wireframes
in a given image. Compared to the previous methods which first predict an
intermediate heat map and then extract straight lines with heuristic
algorithms, our method is end-to-end trainable and can directly output a
vectorized wireframe that contains semantically meaningful and geometrically
salient junctions and lines. To better understand the quality of the outputs,
we propose a new metric for wireframe evaluation that penalizes overlapped line
segments and incorrect line connectivities. We conduct extensive experiments
and show that our method significantly outperforms the previous
state-of-the-art wireframe and line extraction algorithms. We hope our simple
approach can be served as a baseline for future wireframe parsing studies. Code
has been made publicly available at https://github.com/zhou13/lcnn."
"In this work, we aim to realize a method for embedding human knowledge into
deep neural networks. While the conventional method to embed human knowledge
has been applied for non-deep machine learning, it is challenging to apply it
for deep learning models due to the enormous number of model parameters. To
tackle this problem, we focus on the attention mechanism of an attention branch
network (ABN). In this paper, we propose a fine-tuning method that utilizes a
single-channel attention map which is manually edited by a human expert. Our
fine-tuning method can train a network so that the output attention map
corresponds to the edited ones. As a result, the fine-tuned network can output
an attention map that takes into account human knowledge. Experimental results
with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to
obtain a clear attention map for a visual explanation and improve the
classification performance. Our findings can be a novel framework for
optimizing networks through human intuitive editing via a visual interface and
suggest new possibilities for human-machine cooperation in addition to the
improvement of visual explanations."
"The availability and use of egocentric data are rapidly increasing due to the
growing use of wearable cameras. Our aim is to study the effect (positive,
neutral or negative) of egocentric images or events on an observer. Given
egocentric photostreams capturing the wearer's days, we propose a method that
aims to assign sentiment to events extracted from egocentric photostreams. Such
moments can be candidates to retrieve according to their possibility of
representing a positive experience for the camera's wearer. The proposed
approach obtained a classification accuracy of 75% on the test set, with a
deviation of 8%. Our model makes a step forward opening the door to sentiment
recognition in egocentric photostreams."
"Many Multi-View-Stereo algorithms extract a 3D mesh model of a scene, after
fusing depth maps into a volumetric representation of the space. Due to the
limited scalability of such representations, the estimated model does not
capture fine details of the scene. Therefore a mesh refinement algorithm is
usually applied; it improves the mesh resolution and accuracy by minimizing the
photometric error induced by the 3D model into pairs of cameras. The choice of
these pairs significantly affects the quality of the refinement and usually
relies on sparse 3D points belonging to the surface. Instead, in this paper, to
increase the quality of pairs selection, we exploit the 3D model (before the
refinement) to compute five metrics: scene coverage, mutual image overlap,
image resolution, camera parallax, and a new symmetry term. To improve the
refinement robustness, we also propose an explicit method to manage occlusions,
which may negatively affect the computation of the photometric error. The
proposed method takes into account the depth of the model while computing the
similarity measure and its gradient. We quantitatively and qualitatively
validated our approach on publicly available datasets against state of the art
reconstruction methods."
"Images with visual and scene text content are ubiquitous in everyday life.
However, current image interpretation systems are mostly limited to using only
the visual features, neglecting to leverage the scene text content. In this
paper, we propose to jointly use scene text and visual channels for robust
semantic interpretation of images. We do not only extract and encode visual and
scene text cues, but also model their interplay to generate a contextual joint
embedding with richer semantics. The contextual embedding thus generated is
applied to retrieval and classification tasks on multimedia images, with scene
text content, to demonstrate its effectiveness. In the retrieval framework, we
augment our learned text-visual semantic representation with scene text cues,
to mitigate vocabulary misses that may have occurred during the semantic
embedding. To deal with irrelevant or erroneous recognition of scene text, we
also apply query-based attention to our text channel. We show how the
multi-channel approach, involving visual semantics and scene text, improves
upon state of the art."
"Existing methods for AI-generated artworks still struggle with generating
high-quality stylized content, where high-level semantics are preserved, or
separating fine-grained styles from various artists. We propose a novel
Generative Adversarial Disentanglement Network which can disentangle two
complementary factors of variations when only one of them is labelled in
general, and fully decompose complex anime illustrations into style and content
in particular. Training such model is challenging, since given a style, various
content data may exist but not the other way round. Our approach is divided
into two stages, one that encodes an input image into a style independent
content, and one based on a dual-conditional generator. We demonstrate the
ability to generate high-fidelity anime portraits with a fixed content and a
large variety of styles from over a thousand artists, and vice versa, using a
single end-to-end network and with applications in style transfer. We show this
unique capability as well as superior output to the current state-of-the-art."
"Geospatial object detection of remote sensing imagery has been attracting an
increasing interest in recent years, due to the rapid development in spaceborne
imaging. Most of previously proposed object detectors are very sensitive to
object deformations, such as scaling and rotation. To this end, we propose a
novel and efficient framework for geospatial object detection in this letter,
called Fourier-based rotation-invariant feature boosting (FRIFB). A
Fourier-based rotation-invariant feature is first generated in polar
coordinate. Then, the extracted features can be further structurally refined
using aggregate channel features. This leads to a faster feature computation
and more robust feature representation, which is good fitting for the coming
boosting learning. Finally, in the test phase, we achieve a fast pyramid
feature extraction by estimating a scale factor instead of directly collecting
all features from image pyramid. Extensive experiments are conducted on two
subsets of NWPU VHR-10 dataset, demonstrating the superiority and effectiveness
of the FRIFB compared to previous state-of-the-art methods."
"Latent fingerprint recognition is not a new topic but it has attracted a lot
of attention from researchers in both academia and industry over the past 50
years. With the rapid development of pattern recognition techniques, automated
fingerprint identification systems (AFIS) have become more and more ubiquitous.
However, most AFIS are utilized for live-scan or rolled/slap prints while only
a few systems can work on latent fingerprints with reasonable accuracy. The
question of whether taking higher resolution scans of latent fingerprints and
their rolled/slap mate prints could help improve the identification accuracy
still remains an open question in the forensic community. Because pores are one
of the most reliable features besides minutiae to identify latent fingerprints,
we propose an end-to-end automatic pore extraction and matching system to
analyze the utility of pores in latent fingerprint identification. Hence, this
paper answers two questions in the latent fingerprint domain: (i) does the
incorporation of pores as level-3 features improve the system performance
significantly? and (ii) does the 1,000 ppi image resolution improve the
recognition results? We believe that our proposed end-to-end pore extraction
and matching system will be a concrete baseline for future latent AFIS
development."
"Segmenting salient objects in an image is an important vision task with
ubiquitous applications. The problem becomes more challenging in the presence
of a cluttered and textured background, low resolution and/or low contrast
images. Even though existing algorithms perform well in segmenting most of the
object(s) of interest, they often end up segmenting false positives due to
resembling salient objects in the background. In this work, we tackle this
problem by iteratively attending to image patches in a recurrent fashion and
subsequently enhancing the predicted segmentation mask. Saliency features are
estimated independently for every image patch, which are further combined using
an aggregation strategy based on a Convolutional Gated Recurrent Unit (ConvGRU)
network. The proposed approach works in an end-to-end manner, removing
background noise and false positives incrementally. Through extensive
evaluation on various benchmark datasets, we show superior performance to the
existing approaches without any post-processing."
"Modern machine learning suffers from catastrophic forgetting when learning
new classes incrementally. The performance dramatically degrades due to the
missing data of old classes. Incremental learning methods have been proposed to
retain the knowledge acquired from the old classes, by using knowledge
distilling and keeping a few exemplars from the old classes. However, these
methods struggle to scale up to a large number of classes. We believe this is
because of the combination of two factors: (a) the data imbalance between the
old and new classes, and (b) the increasing number of visually similar classes.
Distinguishing between an increasing number of visually similar classes is
particularly challenging, when the training data is unbalanced. We propose a
simple and effective method to address this data imbalance issue. We found that
the last fully connected layer has a strong bias towards the new classes, and
this bias can be corrected by a linear model. With two bias parameters, our
method performs remarkably well on two large datasets: ImageNet (1000 classes)
and MS-Celeb-1M (10000 classes), outperforming the state-of-the-art algorithms
by 11.1% and 13.2% respectively."
"This paper presents a novel approach for predicting the falls of people in
advance from monocular video. First, all persons in the observed frames are
detected and tracked with the coordinates of their body keypoints being
extracted meanwhile. A keypoints vectorization method is exploited to eliminate
irrelevant information in the initial coordinate representation. Then, the
observed keypoint sequence of each person is input to the pose prediction
module adapted from sequence-to-sequence(seq2seq) architecture to predict the
future keypoint sequence. Finally, the predicted pose is analyzed by the falls
classifier to judge whether the person will fall down in the future. The pose
prediction module and falls classifier are trained separately and tuned jointly
using Le2i dataset, which contains 191 videos of various normal daily
activities as well as falls performed by several actors. The contrast
experiments with mainstream raw RGB-based models show the accuracy improvement
of utilizing body keypoints in falls classification. Moreover, the precognition
of falls is proved effective by comparisons between models that with and
without the pose prediction module."
"Psychological studies have found that human visual tracking system involves
learning, memory, and planning. Despite recent successes, not many works have
focused on memory and planning in deep learning based tracking. We are thus
interested in memory augmented network, where an external memory remembers the
evolving appearance of the target (foreground) object without backpropagation
for updating weights. Our Dual Augmented Memory Network (DAWN) is unique in
remembering both target and background, and using an improved attention LSTM
memory to guide the focus on memorized features. DAWN is effective in
unsupervised tracking in handling total occlusion, severe motion blur, abrupt
changes in target appearance, multiple object instances, and similar foreground
and background features. We present extensive quantitative and qualitative
experimental comparison with state-of-the-art methods including top contenders
in recent VOT challenges. Notably, despite the straightforward implementation,
DAWN is ranked third in both VOT2016 and VOT2017 challenges with excellent
success rate among all VOT fast trackers running at fps > 10 in unsupervised
tracking in both challenges. We propose DAWN-RPN, where we simply augment our
memory and attention LSTM modules to the state-of-the-art SiamRPN, and report
immediate performance gain, thus demonstrating DAWN can work well with and
directly benefit other models to handle difficult cases as well."
"In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a
semi-automatic method which leverages on moving a 2D camera by means of a
robot, proving precise camera tracking, and an augmented reality pen to define
initial object bounding box, to create large labeled datasets with minimal
human intervention. By removing the burden of generating annotated data from
humans, we make the Deep Learning technique applied to computer vision, that
typically requires very large datasets, truly automated and reliable. With the
ARS pipeline, we created effortlessly two novel datasets, one on
electromechanical components (industrial scenario) and one on fruits
(daily-living scenario), and trained robustly two state-of-the-art object
detectors, based on convolutional neural networks, such as YOLO and SSD. With
respect to the conventional manual annotation of 1000 frames that takes us
slightly more than 10 hours, the proposed approach based on ARS allows
annotating 9 sequences of about 35000 frames in less than one hour, with a gain
factor of about 450. Moreover, both the precision and recall of object
detection is increased by about 15\% with respect to manual labeling. All our
software is available as a ROS package in a public repository alongside the
novel annotated datasets."
"Open set domain adaptation aims to diminish the domain shift across domains,
with partially shared classes. There exist unknown target samples out of the
knowledge of source domain. Compared to the close set setting, how to separate
the unknown (unshared) class from the known (shared) ones plays a key role.
Whereas, previous methods did not emphasize the semantic structure of the open
set data, which may introduce bias into the domain alignment and confuse the
classifier around the decision boundary. In this paper, we exploit the semantic
structure of open set data from two aspects: 1) Semantic Categorical Alignment,
which aims to achieve good separability of target known classes by
categorically aligning the centroid of target with the source. 2)Semantic
Contrastive Mapping, which aims to push the unknown class away from the
decision boundary. Empirically, we demonstrate that our method performs
favourably against the state-of-the-art methods on representative benchmarks,
e.g. Digit datasets and Office-31 datasets."
"We present a learning model that makes full use of boundary information for
salient object segmentation. Specifically, we come up with a novel loss
function, i.e., Contour Loss, which leverages object contours to guide models
to perceive salient object boundaries. Such a boundary-aware network can learn
boundary-wise distinctions between salient objects and background, hence
effectively facilitating the saliency detection. Yet the Contour Loss
emphasizes on the local saliency. We further propose the hierarchical global
attention module (HGAM), which forces the model hierarchically attend to global
contexts, thus captures the global visual saliency. Comprehensive experiments
on six benchmark datasets show that our method achieves superior performance
over state-of-the-art ones. Moreover, our model has a real-time speed of 26 fps
on a TITAN X GPU."
"Understanding the spatial relations between objects in images is a
surprisingly challenging task. A chair may be ""behind"" a person even if it
appears to the left of the person in the image (depending on which way the
person is facing). Two students that appear close to each other in the image
may not in fact be ""next to"" each other if there is a third student between
them.
  We introduce SpatialSense, a dataset specializing in spatial relation
recognition which captures a broad spectrum of such challenges, allowing for
proper benchmarking of computer vision techniques. SpatialSense is constructed
through adversarial crowdsourcing, in which human annotators are tasked with
finding spatial relations that are difficult to predict using simple cues such
as 2D spatial configuration or language priors. Adversarial crowdsourcing
significantly reduces dataset bias and samples more interesting relations in
the long tail compared to existing datasets. On SpatialSense, state-of-the-art
recognition models perform comparably to simple baselines, suggesting that they
rely on straightforward cues instead of fully reasoning about this complex
task. The SpatialSense benchmark provides a path forward to advancing the
spatial reasoning capabilities of computer vision systems. The dataset and code
are available at https://github.com/princeton-vl/SpatialSense."
"Age synthesis methods typically take a single image as input and use a
specific number to control the age of the generated image. In this paper, we
propose a novel framework taking two images as inputs, named dual-reference age
synthesis (DRAS), which approaches the task differently; instead of using
""hard"" age information, i.e. a fixed number, our model determines the target
age in a ""soft"" way, by employing a second reference image. Specifically, the
proposed framework consists of an identity agent, an age agent and a generative
adversarial network. It takes two images as input - an identity reference and
an age reference - and outputs a new image that shares corresponding features
with each. Experimental results on two benchmark datasets (UTKFace and CACD)
demonstrate the appealing performance and flexibility of the proposed
framework."
"This paper studies the problem of temporal moment localization in a long
untrimmed video using natural language as the query. Given an untrimmed video
and a sentence as the query, the goal is to determine the starting, and the
ending, of the relevant visual moment in the video, that corresponds to the
query sentence. While previous works have tackled this task by a
propose-and-rank approach, we introduce a more efficient, end-to-end trainable,
and {\em proposal-free approach} that relies on three key components: a dynamic
filter to transfer language information to the visual domain, a new loss
function to guide our model to attend the most relevant parts of the video, and
soft labels to model annotation uncertainty. We evaluate our method on two
benchmark datasets, Charades-STA and ActivityNet-Captions. Experimental results
show that our approach outperforms state-of-the-art methods on both datasets."
"This paper proposes a novel approach to regularize the ill-posed blind image
deconvolution (blind image deblurring) problem using deep generative networks.
We employ two separate deep generative models - one trained to produce sharp
images while the other trained to generate blur kernels from lower dimensional
parameters. To deblur, we propose an alternating gradient descent scheme
operating in the latent lower-dimensional space of each of the pretrained
generative models. Our experiments show excellent deblurring results even under
large blurs and heavy noise. To improve the performance on rich image datasets
not well learned by the generative networks, we present a modification of the
proposed scheme that governs the deblurring process under both generative and
classical priors."
"Single view depth estimation models can be trained from video footage using a
self-supervised end-to-end approach with view synthesis as the supervisory
signal. This is achieved with a framework that predicts depth and camera
motion, with a loss based on reconstructing a target video frame from
temporally adjacent frames. In this context, occlusion relates to parts of a
scene that can be observed in the target frame but not in a frame used for
image reconstruction. Since the image reconstruction is based on sampling from
the adjacent frame, and occluded areas by definition cannot be sampled,
reconstructed occluded areas corrupt to the supervisory signal. In previous
work arXiv:1806.01260 occlusion is handled based on reconstruction error; at
each pixel location, only the reconstruction with the lowest error is included
in the loss. The current study aims to determine whether performance
improvements of depth estimation models can be gained by during training only
ignoring those regions that are affected by occlusion.
  In this work we introduce occlusion mask, a mask that during training can be
used to specifically ignore regions that cannot be reconstructed due to
occlusions. Occlusion mask is based entirely on predicted depth information. We
introduce two novel loss formulations which incorporate the occlusion mask. The
method and implementation of arXiv:1806.01260 serves as the foundation for our
modifications as well as the baseline in our experiments. We demonstrate that
(i) incorporating occlusion mask in the loss function improves the performance
of single image depth prediction models on the KITTI benchmark. (ii) loss
functions that select from reconstructions based on error are able to ignore
some of the reprojection error caused by object motion."
"Lesions are injuries and abnormal tissues in the human body. Detecting
lesions in 3D Computed Tomography (CT) scans can be time-consuming even for
very experienced physicians and radiologists. In recent years, CNN based lesion
detectors have demonstrated huge potentials. Most of current state-of-the-art
lesion detectors employ anchors to enumerate all possible bounding boxes with
respect to the dataset in process. This anchor mechanism greatly improves the
detection performance while also constraining the generalization ability of
detectors. In this paper, we propose an anchor-free lesion detector. The anchor
mechanism is removed and lesions are formalized as single keypoints. By doing
so, we witness a considerable performance gain in terms of both accuracy and
inference speed compared with the anchor-based baseline"
"Image aesthetic quality assessment has got much attention in recent years,
but not many works have been done on a specific genre of photos: Group
photograph. In this work, we designed a set of high-level features based on the
experience and principles of group photography: Opened-eye, Gaze, Smile,
Occluded faces, Face Orientation, Facial blur, Character center. Then we
combined them and 83 generic aesthetic features to build two aesthetic
assessment models. We also constructed a large dataset of group photographs -
GPD- annotated with the aesthetic score. The experimental result shows that our
features perform well for categorizing professional photos and snapshots and
predicting the distinction of multiple group photographs of diverse human
states under the same scene."
"Fully-automatic execution is the ultimate goal for many Computer Vision
applications. However, this objective is not always realistic in tasks
associated with high failure costs, such as medical applications. For these
tasks, semi-automatic methods allowing minimal effort from users to guide
computer algorithms are often preferred due to desirable accuracy and
performance. Inspired by the practicality and applicability of the
semi-automatic approach, this paper proposes a novel deep neural network
architecture, namely SideInfNet that effectively integrates features learnt
from images with side information extracted from user annotations. To evaluate
our method, we applied the proposed network to three semantic segmentation
tasks and conducted extensive experiments on benchmark datasets. Experimental
results and comparison with prior work have verified the superiority of our
model, suggesting the generality and effectiveness of the model in
semi-automatic semantic segmentation."
"Magnetic resonance imaging (MRI) enables plant scientists to non-invasively
study root system development and root-soil interaction. Challenging recording
conditions, such as low resolution and a high level of noise hamper the
performance of traditional root extraction algorithms, though. We propose to
increase signal-to-noise ratio and resolution by segmenting the scanned volumes
into root and soil in super-resolution using a 3D U-Net. Tests on real data
show that the trained network is capable to detect most roots successfully and
even finds roots that were missed by human annotators. Our experiments show
that the segmentation performance can be further improved with modifications of
the loss function."
"Synthetic aperture radar (SAR) images are widely used in target recognition
tasks nowadays. In this letter, we propose an automatic approach for radar
shadow detection and extraction from SAR images utilizing geometric projections
along with the digital elevation model (DEM) which corresponds to the given
geo-referenced SAR image. First, the DEM is rotated into the radar geometry so
that each row would match that of a radar line of sight. Next, we extract the
shadow regions by processing row by row until the image is covered fully. We
test the proposed shadow detection approach on different DEMs and a simulated
1D signals and 2D hills and volleys modeled by various variance based Gaussian
functions. Experimental results indicate the proposed algorithm produces good
results in detecting shadows in SAR images with high resolution."
"This paper considers the problem of localizing actions in videos as a
sequences of bounding boxes. The objective is to generate action proposals that
are likely to include the action of interest, ideally achieving high recall
with few proposals. Our contributions are threefold. First, inspired by
selective search for object proposals, we introduce an approach to generate
action proposals from spatiotemporal super-voxels in an unsupervised manner, we
call them Tubelets. Second, along with the static features from individual
frames our approach advantageously exploits motion. We introduce independent
motion evidence as a feature to characterize how the action deviates from the
background and explicitly incorporate such motion information in various stages
of the proposal generation. Finally, we introduce spatiotemporal refinement of
Tubelets, for more precise localization of actions, and pruning to keep the
number of Tubelets limited. We demonstrate the suitability of our approach by
extensive experiments for action proposal quality and action localization on
three public datasets: UCF Sports, MSR-II and UCF101. For action proposal
quality, our unsupervised proposals beat all other existing approaches on the
three datasets. For action localization, we show top performance on both the
trimmed videos of UCF Sports and UCF101 as well as the untrimmed videos of
MSR-II."
"We describe a novel method for removing speckle (in wavelet domain) of
unknown variance from SAR images. The me-thod is based on the following
procedure: We apply 1) Bidimentional Discrete Wavelet Transform (DWT-2D) to the
speckled image, 2) scaling and rounding to the coefficients of the highest
subbands (to obtain integer and positive coefficients), 3) bit-slicing to the
new highest subbands (to obtain bit-planes), 4) then we apply the Systholic
Boolean Orthonormalizer Network (SBON) to the input bit-plane set and we obtain
two orthonormal output bit-plane sets (in a Boolean sense), we project a set on
the other one, by means of an AND operation, and then, 5) we apply
re-assembling, and, 6) re-sca-ling. Finally, 7) we apply Inverse DWT-2D and
reconstruct a SAR image from the modified wavelet coefficients. Despeckling
results compare favorably to the most of methods in use at the moment."
"It is increasingly common in many types of natural and physical systems
(especially biological systems) to have different types of measurements
performed on the same underlying system. In such settings, it is important to
align the manifolds arising from each measurement in order to integrate such
data and gain an improved picture of the system. We tackle this problem using
generative adversarial networks (GANs). Recently, GANs have been utilized to
try to find correspondences between sets of samples. However, these GANs are
not explicitly designed for proper alignment of manifolds. We present a new GAN
called the Manifold-Aligning GAN (MAGAN) that aligns two manifolds such that
related points in each measurement space are aligned together. We demonstrate
applications of MAGAN in single-cell biology in integrating two different
measurement types together. In our demonstrated examples, cells from the same
tissue are measured with both genomic (single-cell RNA-sequencing) and
proteomic (mass cytometry) technologies. We show that the MAGAN successfully
aligns them such that known correlations between measured markers are improved
compared to other recently proposed models."
"Recent advances in object detection are mainly driven by deep learning with
large-scale detection benchmarks. However, the fully-annotated training set is
often limited for a target detection task, which may deteriorate the
performance of deep detectors. To address this challenge, we propose a novel
low-shot transfer detector (LSTD) in this paper, where we leverage rich
source-domain knowledge to construct an effective target-domain detector with
very few training examples. The main contributions are described as follows.
First, we design a flexible deep architecture of LSTD to alleviate transfer
difficulties in low-shot detection. This architecture can integrate the
advantages of both SSD and Faster RCNN in a unified deep framework. Second, we
introduce a novel regularized transfer learning framework for low-shot
detection, where the transfer knowledge (TK) and background depression (BD)
regularizations are proposed to leverage object knowledge respectively from
source and target domains, in order to further enhance fine-tuning with a few
target images. Finally, we examine our LSTD on a number of challenging low-shot
detection experiments, where LSTD outperforms other state-of-the-art
approaches. The results demonstrate that LSTD is a preferable deep detector for
low-shot scenarios."
"Face analysis is a core part of computer vision, in which remarkable progress
has been observed in the past decades. Current methods achieve recognition and
tracking with invariance to fundamental modes of variation such as
illumination, 3D pose, expressions. Notwithstanding, a much less standing mode
of variation is motion deblurring, which however presents substantial
challenges in face analysis. Recent approaches either make oversimplifying
assumptions, e.g. in cases of joint optimization with other tasks, or fail to
preserve the highly structured shape/identity information. Therefore, we
propose a data-driven method that encourages identity preservation. The
proposed model includes two parallel streams (sub-networks): the first deblurs
the image, the second implicitly extracts and projects the identity of both the
sharp and the blurred image in similar subspaces. We devise a method for
creating realistic motion blur by averaging a variable number of frames to
train our model. The averaged images originate from a 2MF2 dataset with 10
million facial frames, which we introduce for the task. Considering deblurring
as an intermediate step, we utilize the deblurred outputs to conduct a thorough
experimentation on high-level face analysis tasks, i.e. landmark localization
and face verification. The experimental evaluation demonstrates the superiority
of our method."
"Deep convolutional neural networks have demonstrated high performances for
fixation prediction in recent years. How they achieve this, however, is less
explored and they remain to be black box models. Here, we attempt to shed light
on the internal structure of deep saliency models and study what features they
extract for fixation prediction. Specifically, we use a simple yet powerful
architecture, consisting of only one CNN and a single resolution input,
combined with a new loss function for pixel-wise fixation prediction during
free viewing of natural scenes. We show that our simple method is on par or
better than state-of-the-art complicated saliency models. Furthermore, we
propose a method, related to saliency model evaluation metrics, to visualize
deep models for fixation prediction. Our method reveals the inner
representations of deep models for fixation prediction and provides evidence
that saliency, as experienced by humans, is likely to involve high-level
semantic knowledge in addition to low-level perceptual cues. Our results can be
useful to measure the gap between current saliency models and the human
inter-observer model and to build new models to close this gap."
"Near field depth estimation around a self driving car is an important
function that can be achieved by four wide angle fisheye cameras having a field
of view of over 180. Depth estimation based on convolutional neural networks
(CNNs) produce state of the art results, but progress is hindered because depth
annotation cannot be obtained manually. Synthetic datasets are commonly used
but they have limitations. For instance, they do not capture the extensive
variability in the appearance of objects like vehicles present in real
datasets. There is also a domain shift while performing inference on natural
images illustrated by many attempts to handle the domain adaptation explicitly.
In this work, we explore an alternate approach of training using sparse LiDAR
data as ground truth for depth estimation for fisheye camera. We built our own
dataset using our self driving car setup which has a 64 beam Velodyne LiDAR and
four wide angle fisheye cameras. To handle the difference in view points of
LiDAR and fisheye camera, an occlusion resolution mechanism was implemented. We
started with Eigen's multiscale convolutional network architecture and improved
by modifying activation function and optimizer. We obtained promising results
on our dataset with RMSE errors comparable to the state of the art results
obtained on KITTI."
"We present an automatic moment capture system that runs in real-time on
mobile cameras. The system is designed to run in the viewfinder mode and
capture a burst sequence of frames before and after the shutter is pressed. For
each frame, the system predicts in real-time a ""goodness"" score, based on which
the best moment in the burst can be selected immediately after the shutter is
released, without any user interference. To solve the problem, we develop a
highly efficient deep neural network ranking model, which implicitly learns a
""latent relative attribute"" space to capture subtle visual differences within a
sequence of burst images. Then the overall goodness is computed as a linear
aggregation of the goodnesses of all the latent attributes. The latent relative
attributes and the aggregation function can be seamlessly integrated in one
fully convolutional network and trained in an end-to-end fashion. To obtain a
compact model which can run on mobile devices in real-time, we have explored
and evaluated a wide range of network design choices, taking into account the
constraints of model size, computational cost, and accuracy. Extensive studies
show that the best frame predicted by our model hit users' top-1 (out of 11 on
average) choice for $64.1\%$ cases and top-3 choices for $86.2\%$ cases.
Moreover, the model(only 0.47M Bytes) can run in real time on mobile devices,
e.g. only 13ms on iPhone 7 for one frame prediction."
"In this paper, we propose an efficient architecture for semantic image
segmentation using the depth-to-space (D2S) operation. Our D2S model is
comprised of a standard CNN encoder followed by a depth-to-space reordering of
the final convolutional feature maps. Our approach eliminates the decoder
portion of traditional encoder-decoder segmentation models and reduces the
amount of computation almost by half. As a participant of the DeepGlobe Road
Extraction competition, we evaluate our models on the corresponding road
segmentation dataset. Our highly efficient D2S models exhibit comparable
performance to standard segmentation models with much lower computational cost."
"The ability to interact and understand the environment is a fundamental
prerequisite for a wide range of applications from robotics to augmented
reality. In particular, predicting how deformable objects will react to applied
forces in real time is a significant challenge. This is further confounded by
the fact that shape information about encountered objects in the real world is
often impaired by occlusions, noise and missing regions e.g. a robot
manipulating an object will only be able to observe a partial view of the
entire solid. In this work we present a framework, 3D-PhysNet, which is able to
predict how a three-dimensional solid will deform under an applied force using
intuitive physics modelling. In particular, we propose a new method to encode
the physical properties of the material and the applied force, enabling
generalisation over materials. The key is to combine deep variational
autoencoders with adversarial training, conditioned on the applied force and
the material properties. We further propose a cascaded architecture that takes
a single 2.5D depth view of the object and predicts its deformation. Training
data is provided by a physics simulator. The network is fast enough to be used
in real-time applications from partial views. Experimental results show the
viability and the generalisation properties of the proposed architecture."
"In this paper, we develop a new method that recognizes facial expressions, on
the basis of an innovative local motion patterns feature, with three main
contributions. The first one is the analysis of the face skin temporal
elasticity and face deformations during expression. The second one is a unified
approach for both macro and micro expression recognition. And, the third one is
the step forward towards in-the-wild expression recognition, dealing with
challenges such as various intensity and various expression activation
patterns, illumination variation and small head pose variations. Our method
outperforms state-of-the-art methods for micro expression recognition and
positions itself among top-rank state-of-the-art methods for macro expression
recognition."
"Recently, generative adversarial networks (GANs) have shown promising
performance in generating realistic images. However, they often struggle in
learning complex underlying modalities in a given dataset, resulting in
poor-quality generated images. To mitigate this problem, we present a novel
approach called mixture of experts GAN (MEGAN), an ensemble approach of
multiple generator networks. Each generator network in MEGAN specializes in
generating images with a particular subset of modalities, e.g., an image class.
Instead of incorporating a separate step of handcrafted clustering of multiple
modalities, our proposed model is trained through an end-to-end learning of
multiple generators via gating networks, which is responsible for choosing the
appropriate generator network for a given condition. We adopt the categorical
reparameterization trick for a categorical decision to be made in selecting a
generator while maintaining the flow of the gradients. We demonstrate that
individual generators learn different and salient subparts of the data and
achieve a multiscale structural similarity (MS-SSIM) score of 0.2470 for CelebA
and a competitive unsupervised inception score of 8.33 in CIFAR-10."
"We study weakly-supervised video object grounding: given a video segment and
a corresponding descriptive sentence, the goal is to localize objects that are
mentioned from the sentence in the video. During training, no object bounding
boxes are available, but the set of possible objects to be grounded is known
beforehand. Existing approaches in the image domain use Multiple Instance
Learning (MIL) to ground objects by enforcing matches between visual and
semantic features. A naive extension of this approach to the video domain is to
treat the entire segment as a bag of spatial object proposals. However, an
object existing sparsely across multiple frames might not be detected
completely since successfully spotting it from one single frame would trigger a
satisfactory match. To this end, we propagate the weak supervisory signal from
the segment level to frames that likely contain the target object. For frames
that are unlikely to contain the target objects, we use an alternative penalty
loss. We also leverage the interactions among objects as a textual guide for
the grounding. We evaluate our model on the newly-collected benchmark
YouCook2-BoundingBox and show improvements over competitive baselines."
"Dynamic Vision Sensors (DVS), which output asynchronous log intensity change
events, have potential applications in high-speed robotics, autonomous cars and
drones. The precise event timing, sparse output, and wide dynamic range of the
events are well suited for optical flow, but conventional optical flow (OF)
algorithms are not well matched to the event stream data. This paper proposes
an event-driven OF algorithm called adaptive block-matching optical flow
(ABMOF). ABMOF uses time slices of accumulated DVS events. The time slices are
adaptively rotated based on the input events and OF results. Compared with
other methods such as gradient-based OF, ABMOF can efficiently be implemented
in compact logic circuits. Results show that ABMOF achieves comparable accuracy
to conventional standards such as Lucas-Kanade (LK). The main contributions of
our paper are new adaptive time-slice rotation methods that ensure the
generated slices have sufficient features for matching,including a feedback
mechanism that controls the generated slices to have average slice displacement
within the block search range. An LK method using our adapted slices is also
implemented. The ABMOF accuracy is compared with this LK method on natural
scene data including sparse and dense texture, high dynamic range, and fast
motion exceeding 30,000 pixels per second.The paper dataset and source code are
available from http://sensors.ini.uzh.ch/databases.html."
"Deep learning for predicting or generating 3D human pose sequences is an
active research area. Previous work regresses either joint rotations or joint
positions. The former strategy is prone to error accumulation along the
kinematic chain, as well as discontinuities when using Euler angle or
exponential map parameterizations. The latter requires re-projection onto
skeleton constraints to avoid bone stretching and invalid configurations. This
work addresses both limitations. Our recurrent network, QuaterNet, represents
rotations with quaternions and our loss function performs forward kinematics on
a skeleton to penalize absolute position errors instead of angle errors. On
short-term predictions, QuaterNet improves the state-of-the-art quantitatively.
For long-term generation, our approach is qualitatively judged as realistic as
recent neural strategies from the graphics literature."
"Although face recognition systems have achieved impressive performance in
recent years, the low-resolution face recognition (LRFR) task remains
challenging, especially when the LR faces are captured under non-ideal
conditions, as is common in surveillance-based applications. Faces captured in
such conditions are often contaminated by blur, nonuniform lighting, and
nonfrontal face pose. In this paper, we analyze face recognition techniques
using data captured under low-quality conditions in the wild. We provide a
comprehensive analysis of experimental results for two of the most important
applications in real surveillance applications, and demonstrate practical
approaches to handle both cases that show promising performance. The following
three contributions are made: {\em (i)} we conduct experiments to evaluate
super-resolution methods for low-resolution face recognition; {\em (ii)} we
study face re-identification on various public face datasets including real
surveillance and low-resolution subsets of large-scale datasets, present a
baseline result for several deep learning based approaches, and improve them by
introducing a GAN pre-training approach and fully convolutional architecture;
and {\em (iii)} we explore low-resolution face identification by employing a
state-of-the-art supervised discriminative learning approach. Evaluations are
conducted on challenging portions of the SCFace and UCCSface datasets."
"Deep neural networks have demonstrated impressive performance in various
machine learning tasks. However, they are notoriously sensitive to changes in
data distribution. Often, even a slight change in the distribution can lead to
drastic performance reduction. Artificially augmenting the data may help to
some extent, but in most cases, fails to achieve model invariance to the data
distribution. Some examples where this sub-class of domain adaptation can be
valuable are various imaging modalities such as thermal imaging, X-ray,
ultrasound, and MRI, where changes in acquisition parameters or acquisition
device manufacturer will result in a different representation of the same
input. Our work shows that standard fine-tuning fails to adapt the model in
certain important cases. We propose a novel method of adapting to a new data
source, and demonstrate near perfect adaptation on a customized ImageNet
benchmark. Moreover, our method does not require any samples from the original
data set, it is completely explainable and can be tailored to the task."
"Fashion is an increasingly important topic in computer vision, in particular
the so-called street-to-shop task of matching street images with shop images
containing similar fashion items. Solving this problem promises new means of
making fashion searchable and helping shoppers find the articles they are
looking for. This paper focuses on finding pieces of clothing worn by a person
in full-body or half-body images with neutral backgrounds. Such images are
ubiquitous on the web and in fashion blogs, and are typically studio photos, we
refer to this setting as studio-to-shop. Recent advances in computational
fashion include the development of domain-specific numerical representations.
Our model Studio2Shop builds on top of such representations and uses a deep
convolutional network trained to match a query image to the numerical feature
vectors of all the articles annotated in this image. Top-$k$ retrieval
evaluation on test query images shows that the correct items are most often
found within a range that is sufficiently small for building realistic visual
search engines for the studio-to-shop setting."
"In this paper, we propose PointSeg, a real-time end-to-end semantic
segmentation method for road-objects based on spherical images. We take the
spherical image, which is transformed from the 3D LiDAR point clouds, as input
of the convolutional neural networks (CNNs) to predict the point-wise semantic
map. To make PointSeg applicable on a mobile system, we build the model based
on the light-weight network, SqueezeNet, with several improvements. It
maintains a good balance between memory cost and prediction performance. Our
model is trained on spherical images and label masks projected from the KITTI
3D object detection dataset. Experiments show that PointSeg can achieve
competitive accuracy with 90fps on a single GPU 1080ti. which makes it quite
compatible for autonomous driving applications."
"This work addresses the problem of semantic scene understanding under dense
fog. Although considerable progress has been made in semantic scene
understanding, it is mainly related to clear-weather scenes. Extending
recognition methods to adverse weather conditions such as fog is crucial for
outdoor applications. In this paper, we propose a novel method, named
Curriculum Model Adaptation (CMAda), which gradually adapts a semantic
segmentation model from light synthetic fog to dense real fog in multiple
steps, using both synthetic and real foggy data. In addition, we present three
other main stand-alone contributions: 1) a novel method to add synthetic fog to
real, clear-weather scenes using semantic input; 2) a new fog density
estimator; 3) the Foggy Zurich dataset comprising $3808$ real foggy images,
with pixel-level semantic annotations for $16$ images with dense fog. Our
experiments show that 1) our fog simulation slightly outperforms a
state-of-the-art competing simulation with respect to the task of semantic
foggy scene understanding (SFSU); 2) CMAda improves the performance of
state-of-the-art models for SFSU significantly by leveraging unlabeled real
foggy data. The datasets and code are publicly available."
"We present a novel method for high detail-preserving human avatar creation
from monocular video. A parameterized body model is refined and optimized to
maximally resemble subjects from a video showing them from all sides. Our
avatars feature a natural face, hairstyle, clothes with garment wrinkles, and
high-resolution texture. Our paper contributes facial landmark and
shading-based human body shape refinement, a semantic texture prior, and a
novel texture stitching strategy, resulting in the most sophisticated-looking
human avatars obtained from a single video to date. Numerous results show the
robustness and versatility of our method. A user study illustrates its
superiority over the state-of-the-art in terms of identity preservation, level
of detail, realism, and overall user preference."
"Human pose estimation is an important topic in computer vision with many
applications including gesture and activity recognition. However, pose
estimation from image is challenging due to appearance variations, occlusions,
clutter background, and complex activities. To alleviate these problems, we
develop a robust pose estimation method based on the recent deep conv-deconv
modules with two improvements: (1) multi-scale supervision of body keypoints,
and (2) a global regression to improve structural consistency of keypoints. We
refine keypoint detection heatmaps using layer-wise multi-scale supervision to
better capture local contexts. Pose inference via keypoint association is
optimized globally using a regression network at the end. Our method can
effectively disambiguate keypoint matches in close proximity including the
mismatch of left-right body parts, and better infer occluded parts.
Experimental results show that our method achieves competitive performance
among state-of-the-art methods on the MPII and FLIC datasets."
"This paper presents a model for head and body pose estimation (HBPE) when
labelled samples are highly sparse. The current state-of-the-art multimodal
approach to HBPE utilizes the matrix completion method in a transductive
setting to predict pose labels for unobserved samples. Based on this approach,
the proposed method tackles HBPE when manually annotated ground truth labels
are temporally sparse. We posit that the current state of the art approach
oversimplifies the temporal sparsity assumption by using Laplacian smoothing.
Our final solution uses: i) Gaussian process regression in place of Laplacian
smoothing, ii) head and body coupling, and iii) nuclear norm minimization in
the matrix completion setting. The model is applied to the challenging SALSA
dataset for benchmark against the state-of-the-art method. Our presented
formulation outperforms the state-of-the-art significantly in this particular
setting, e.g. at 5% ground truth labels as training data, head pose accuracy
and body pose accuracy is approximately 62% and 70%, respectively. As well as
fitting a more flexible model to missing labels in time, we posit that our
approach also loosens the head and body coupling constraint, allowing for a
more expressive model of the head and body pose typically seen during
conversational interaction in groups. This provides a new baseline to improve
upon for future integration of multimodal sensor data for the purpose of HBPE."
"For fine-grained visual classification, objects usually share similar
geometric structure but present variant local appearance and different pose.
Therefore, localizing and extracting discriminative local features play a
crucial role in accurate category prediction. Existing works either pay
attention to limited object parts or train isolated networks for locating and
classification. In this paper, we propose Weakly Supervised Bilinear Attention
Network (WS-BAN) to solve these issues. It jointly generates a set of attention
maps (region-of-interest maps) to indicate the locations of object's parts and
extracts sequential part features by Bilinear Attention Pooling (BAP). Besides,
we propose attention regularization and attention dropout to weakly supervise
the generating process of attention maps. WS-BAN can be trained end-to-end and
achieves the state-of-the-art performance on multiple fine-grained
classification datasets, including CUB-200-2011, Stanford Car and
FGVC-Aircraft, which demonstrated its effectiveness."
"Estimating uncertainty of camera parameters computed in Structure from Motion
(SfM) is an important tool for evaluating the quality of the reconstruction and
guiding the reconstruction process. Yet, the quality of the estimated
parameters of large reconstructions has been rarely evaluated due to the
computational challenges. We present a new algorithm which employs the sparsity
of the uncertainty propagation and speeds the computation up about ten times
\wrt previous approaches. Our computation is accurate and does not use any
approximations. We can compute uncertainties of thousands of cameras in tens of
seconds on a standard PC. We also demonstrate that our approach can be
effectively used for reconstructions of any size by applying it to smaller
sub-reconstructions."
"In this paper, we propose to train a network with binary weights and
low-bitwidth activations, designed especially for mobile devices with limited
power consumption. Most previous works on quantizing CNNs uncritically assume
the same architecture, though with reduced precision. However, we take the view
that for best performance it is possible (and even likely) that a different
architecture may be better suited to dealing with low precision weights and
activations.
  Specifically, we propose a ""network expansion"" strategy in which we aggregate
a set of homogeneous low-precision branches to implicitly reconstruct the
full-precision intermediate feature maps. Moreover, we also propose a
group-wise feature approximation strategy which is very flexible and highly
accurate. Experiments on ImageNet classification tasks demonstrate the superior
performance of the proposed model, named Group-Net, over various popular
architectures. In particular, with binary weights and activations, we
outperform the previous best binary neural network in terms of accuracy as well
as saving more than 5 times computational complexity on ImageNet with ResNet-18
and ResNet-50."
"This paper presents an efficient object detection method from satellite
imagery. Among a number of machine learning algorithms, we proposed a
combination of two convolutional neural networks (CNN) aimed at high precision
and high recall, respectively. We validated our models using golf courses as
target objects. The proposed deep learning method demonstrated higher accuracy
than previous object identification methods."
"The amounts of muscle and fat in a person's body, known as body composition,
are correlated with cancer risks, cancer survival, and cardiovascular risk. The
current gold standard for measuring body composition requires time-consuming
manual segmentation of CT images by an expert reader. In this work, we describe
a two-step process to fully automate the analysis of CT body composition using
a DenseNet to select the CT slice and U-Net to perform segmentation. We train
and test our methods on independent cohorts. Our results show Dice scores
(0.95-0.98) and correlation coefficients (R=0.99) that are favorable compared
to human readers. These results suggest that fully automated body composition
analysis is feasible, which could enable both clinical use and large-scale
population studies."
"Breast cancer is the most common invasive cancer in women, affecting more
than 10% of women worldwide. Microscopic analysis of a biopsy remains one of
the most important methods to diagnose the type of breast cancer. This requires
specialized analysis by pathologists, in a task that i) is highly time- and
cost-consuming and ii) often leads to nonconsensual results. The relevance and
potential of automatic classification algorithms using hematoxylin-eosin
stained histopathological images has already been demonstrated, but the
reported results are still sub-optimal for clinical use. With the goal of
advancing the state-of-the-art in automatic classification, the Grand Challenge
on BreAst Cancer Histology images (BACH) was organized in conjunction with the
15th International Conference on Image Analysis and Recognition (ICIAR 2018). A
large annotated dataset, composed of both microscopy and whole-slide images,
was specifically compiled and made publicly available for the BACH challenge.
Following a positive response from the scientific community, a total of 64
submissions, out of 677 registrations, effectively entered the competition.
From the submitted algorithms it was possible to push forward the
state-of-the-art in terms of accuracy (87%) in automatic classification of
breast cancer with histopathological images. Convolutional neuronal networks
were the most successful methodology in the BACH challenge. Detailed analysis
of the collective results allowed the identification of remaining challenges in
the field and recommendations for future developments. The BACH dataset remains
publically available as to promote further improvements to the field of
automatic classification in digital pathology."
"Existing face recognition using deep neural networks is difficult to know
what kind of features are used to discriminate the identities of face images
clearly. To investigate the effective features for face recognition, we propose
a novel face recognition method, called a pairwise relational network (PRN),
that obtains local appearance patches around landmark points on the feature
map, and captures the pairwise relation between a pair of local appearance
patches. The PRN is trained to capture unique and discriminative pairwise
relations among different identities. Because the existence and meaning of
pairwise relations should be identity dependent, we add a face identity state
feature, which obtains from the long short-term memory (LSTM) units network
with the sequential local appearance patches on the feature maps, to the PRN.
To further improve accuracy of face recognition, we combined the global
appearance representation with the pairwise relational feature. Experimental
results on the LFW show that the PRN using only pairwise relations achieved
99.65% accuracy and the PRN using both pairwise relations and face identity
state feature achieved 99.76% accuracy. On the YTF, both the PRN using only
pairwise relations and the PRN using pairwise relations and the face identity
state feature achieved the state-of-the-art (95.7% and 96.3%). The PRN also
achieved comparable results to the state-of-the-art for both face verification
and face identification tasks on the IJB-A, and the state-of-the-art on the
IJB-B."
"This paper presents an approach for automatic detection of Munro's
Microabscess in stratum corneum (SC) of human skin biopsy in order to realize a
machine assisted diagnosis of Psoriasis. The challenge of detecting neutrophils
in presence of nucleated cells is solved using the recent advances of deep
learning algorithms. Separation of SC layer, extraction of patches from the
layer followed by classification of patches with respect to presence or absence
of neutrophils form the basis of the overall approach which is effected through
an integration of a U-Net based segmentation network and a capsule network for
classification. The novel design of the present capsule net leads to a drastic
reduction in the number of parameters without any noticeable compromise in the
overall performance. The research further addresses the challenge of dealing
with Mega-pixel images (in 10X) vis-a-vis Giga-pixel ones (in 40X). The
promising result coming out of an experiment on a dataset consisting of 273
real-life images shows that a practical system is possible based on the present
research. The implementation of our system is available at
https://github.com/Anabik/CapsDeMM."
"Extracting accurate foreground objects from a scene is an essential step for
many video applications. Traditional background subtraction algorithms can
generate coarse estimates, but generating high quality masks requires
professional softwares with significant human interventions, e.g., providing
trimaps or labeling key frames. We propose an automatic foreground extraction
method in applications where a static but imperfect background is available.
Examples include filming and surveillance where the background can be captured
before the objects enter the scene or after they leave the scene. Our proposed
method is very robust and produces significantly better estimates than
state-of-the-art background subtraction, video segmentation and alpha matting
methods. The key innovation of our method is a novel information fusion
technique. The fusion framework allows us to integrate the individual strengths
of alpha matting, background subtraction and image denoising to produce an
overall better estimate. Such integration is particularly important when
handling complex scenes with imperfect background. We show how the framework is
developed, and how the individual components are built. Extensive experiments
and ablation studies are conducted to evaluate the proposed method."
"Indoor navigation aims at performing navigation within buildings. In scenes
like home and factory, most intelligent mobile devices require an functionality
of routing to guide itself precisely through indoor scenes to complete various
tasks in order to serve human. In most scenarios, we expected an intelligent
device capable of navigating itself in unseen environment. Although several
solutions have been proposed to deal with this issue, they usually require
pre-installed beacons or a map pre-built with SLAM, which means that they are
not capable of working in novel environments. To address this, we proposed
NavigationNet, a computer vision dataset and benchmark to allow the utilization
of deep reinforcement learning on scene-understanding-based indoor navigation.
We also proposed and formalized several typical indoor routing problems that
are suitable for deep reinforcement learning."
"Video traffic is increasing at a considerable rate due to the spread of
personal media and advancements in media technology. Accordingly, there is a
growing need for techniques to automatically classify moving images. This paper
use NetVLAD and NetFV models and the Huber loss function for video
classification problem and YouTube-8M dataset to verify the experiment. We
tried various attempts according to the dataset and optimize hyperparameters,
ultimately obtain a GAP score of 0.8668."
"The ultimate goal of a baby detection task concerns detecting the presence of
a baby and other objects in a sequence of 2D images, tracking them and
understanding the semantic contents of the scene. Recent advances in deep
learning and computer vision offer various powerful tools in general object
detection and can be applied to a baby detection task. In this paper, the
Faster Region-based Convolutional Neural Network and the Single-Shot Multi-Box
Detection approaches are explored. They are the two state-of-the-art object
detectors based on the region proposal tactic and the multi-box tactic. The
presence of a baby in the scene obtained from these detectors, tested using
different pre-trained models, are discussed. This study is important since the
behaviors of these detectors in a baby detection task using different
pre-trained models are still not well understood. This exploratory study
reveals many useful insights into the applications of these object detectors in
the smart nursery domain."
"CapsNet (Capsule Network) was first proposed by~\citet{capsule} and later
another version of CapsNet was proposed by~\citet{emrouting}. CapsNet has been
proved effective in modeling spatial features with much fewer parameters.
However, the routing procedures in both papers are not well incorporated into
the whole training process. The optimal number of routing procedure is misery
which has to be found manually. To overcome this disadvantages of current
routing procedures in CapsNet, we embed the routing procedure into the
optimization procedure with all other parameters in neural networks, namely,
make coupling coefficients in the routing procedure become completely
trainable. We call it Generalized CapsNet (G-CapsNet). We implement both
""full-connected"" version of G-CapsNet and ""convolutional"" version of G-CapsNet.
G-CapsNet achieves a similar performance in the dataset MNIST as in the
original papers. We also test two capsule packing method (cross feature maps or
with feature maps) from previous convolutional layers and see no evident
difference. Besides, we also explored possibility of stacking multiple capsule
layers. The code is shared on
\hyperlink{https://github.com/chenzhenhua986/CAFFE-CapsNet}{CAFFE-CapsNet}."
"Channel-based pruning has achieved significant successes in accelerating deep
convolutional neural network, whose pipeline is an iterative three-step
procedure: ranking, pruning and fine-tuning. However, this iterative procedure
is computationally expensive. In this study, we present a novel computationally
efficient channel pruning approach based on the coarse ranking that utilizes
the intermediate results during fine-tuning to rank the importance of filters,
built upon state-of-the-art works with data-driven ranking criteria. The goal
of this work is not to propose a single improved approach built upon a specific
channel pruning method, but to introduce a new general framework that works for
a series of channel pruning methods. Various benchmark image datasets
(CIFAR-10, ImageNet, Birds-200, and Flowers-102) and network architectures
(AlexNet and VGG-16) are utilized to evaluate the proposed approach for object
classification purpose. Experimental results show that the proposed method can
achieve almost identical performance with the corresponding state-of-the-art
works (baseline) while our ranking time is negligibly short. In specific, with
the proposed method, 75% and 54% of the total computation time for the whole
pruning procedure can be reduced for AlexNet on CIFAR-10, and for VGG-16 on
ImageNet, respectively. Our approach would significantly facilitate pruning
practice, especially on resource-constrained platforms."
"Action recognition in videos has attracted a lot of attention in the past
decade. In order to learn robust models, previous methods usually assume videos
are trimmed as short sequences and require ground-truth annotations of each
video frame/sequence, which is quite costly and time-consuming. In this paper,
given only video-level annotations, we propose a novel weakly supervised
framework to simultaneously locate action frames as well as recognize actions
in untrimmed videos. Our proposed framework consists of two major components.
First, for action frame localization, we take advantage of the self-attention
mechanism to weight each frame, such that the influence of background frames
can be effectively eliminated. Second, considering that there are trimmed
videos publicly available and also they contain useful information to leverage,
we present an additional module to transfer the knowledge from trimmed videos
for improving the classification performance in untrimmed ones. Extensive
experiments are conducted on two benchmark datasets (i.e., THUMOS14 and
ActivityNet1.3), and experimental results clearly corroborate the efficacy of
our method."
"Automatic age estimation from facial images represents an important task in
computer vision. This paper analyses the effect of gender, age, ethnic, makeup
and expression attributes of faces as sources of bias to improve deep apparent
age prediction. Following recent works where it is shown that apparent age
labels benefit real age estimation, rather than direct real to real age
regression, our main contribution is the integration, in an end-to-end
architecture, of face attributes for apparent age prediction with an additional
loss for real age regression. Experimental results on the APPA-REAL dataset
indicate the proposed network successfully take advantage of the adopted
attributes to improve both apparent and real age estimation. Our model
outperformed a state-of-the-art architecture proposed to separately address
apparent and real age regression. Finally, we present preliminary results and
discussion of a proof of concept application using the proposed model to
regress the apparent age of an individual based on the gender of an external
observer."
"Skeleton-based action recognition is an important task that requires the
adequate understanding of movement characteristics of a human action from the
given skeleton sequence. Recent studies have shown that exploring spatial and
temporal features of the skeleton sequence is vital for this task.
Nevertheless, how to effectively extract discriminative spatial and temporal
features is still a challenging problem. In this paper, we propose a novel
Attention Enhanced Graph Convolutional LSTM Network (AGC-LSTM) for human action
recognition from skeleton data. The proposed AGC-LSTM can not only capture
discriminative features in spatial configuration and temporal dynamics but also
explore the co-occurrence relationship between spatial and temporal domains. We
also present a temporal hierarchical architecture to increases temporal
receptive fields of the top AGC-LSTM layer, which boosts the ability to learn
the high-level semantic representation and significantly reduces the
computation cost. Furthermore, to select discriminative spatial information,
the attention mechanism is employed to enhance information of key joints in
each AGC-LSTM layer. Experimental results on two datasets are provided: NTU
RGB+D dataset and Northwestern-UCLA dataset. The comparison results demonstrate
the effectiveness of our approach and show that our approach outperforms the
state-of-the-art methods on both datasets."
"Visual place recognition is particularly challenging when places suffer
changes in its appearance. Such changes are indeed common, e.g., due to
weather, night/day or seasons. In this paper we leverage on recent research
using deep networks, and explore how they can be improved by exploiting the
temporal sequence information. Specifically, we propose 3 different
alternatives (Descriptor Grouping, Fusion and Recurrent Descriptors) for deep
networks to use several frames of a sequence. We show that our approaches
produce more compact and best performing descriptors than single- and
multi-view baselines in the literature in two public databases."
"Interest in image-to-image translation has grown substantially in recent
years with the success of unsupervised models based on the cycle-consistency
assumption. The achievements of these models have been limited to a particular
subset of domains where this assumption yields good results, namely homogeneous
domains that are characterized by style or texture differences. We tackle the
challenging problem of image-to-image translation where the domains are defined
by high-level shapes and contexts, as well as including significant clutter and
heterogeneity. For this purpose, we introduce a novel GAN based on preserving
intra-domain vector transformations in a latent space learned by a siamese
network. The traditional GAN system introduced a discriminator network to guide
the generator into generating images in the target domain. To this two-network
system we add a third: a siamese network that guides the generator so that each
original image shares semantics with its generated version. With this new
three-network system, we no longer need to constrain the generators with the
ubiquitous cycle-consistency restraint. As a result, the generators can learn
mappings between more complex domains that differ from each other by large
differences - not just style or texture."
"In this paper we address the problem of representing 3D visual data with
parameterized volumetric shape primitives. Specifically, we present a
(two-stage) approach built around convolutional neural networks (CNNs) capable
of segmenting complex depth scenes into the simpler geometric structures that
can be represented with superquadric models. In the first stage, our approach
uses a Mask RCNN model to identify superquadric-like structures in depth scenes
and then fits superquadric models to the segmented structures using a specially
designed CNN regressor. Using our approach we are able to describe complex
structures with a small number of interpretable parameters. We evaluated the
proposed approach on synthetic as well as real-world depth data and show that
our solution does not only result in competitive performance in comparison to
the state-of-the-art, but is able to decompose scenes into a number of
superquadric models at a fraction of the time required by competing approaches.
We make all data and models used in the paper available from
https://lmi.fe.uni-lj.si/en/research/resources/sq-seg."
"Until recently Intelligence, Surveillance, and Reconnaissance (ISR) focused
on acquiring behavioral information of the targets and their activities.
Continuous evolution of intelligence being gathered of the human centric
activities has put increased focus on the humans, especially inferring their
innate characteristics - size, shapes and physiology. These bio-signatures
extracted from the surveillance sensors can be used to deduce age, ethnicity,
gender and actions, and further characterize human actions in unseen scenarios.
However, recovery of pose and shape of humans in such monocular videos is
inherently an ill-posed problem, marked by frequent depth and view based
ambiguities due to self-occlusion, foreshortening and misalignment. The
likelihood function often yields a highly multimodal posterior that is
difficult to propagate even using the most advanced particle filtering(PF)
algorithms. Motivated by the recent success of the discriminative approaches to
efficiently predict 3D poses directly from the 2D images, we present several
principled approaches to integrate predictive cues using learned regression
models to sustain multimodality of the posterior during tracking. Additionally,
these learned priors can be actively adapted to the test data using a
likelihood based feedback mechanism. Estimated 3D poses are then used to fit 3D
human shape model to each frame independently for inferring anthropometric
bio-signatures. The proposed system is fully automated, robust to noisy test
data and has ability to swiftly recover from tracking failures even after
confronting with significant errors. We evaluate the system on a large number
of monocular human motion sequences."
"Hypergraph is a powerful representation in several computer vision, machine
learning and pattern recognition problems. In the last decade, many researchers
have been keen to develop different hypergraph models. In contrast, no much
attention has been paid to the design of hyperedge weights. However, many
studies on pairwise graphs show that the choice of edge weight can
significantly influence the performances of such graph algorithms. We argue
that this also applies to hypegraphs. In this paper, we empirically discuss the
influence of hyperedge weight on hypegraph learning via proposing three novel
hyperedge weights from the perspectives of geometry, multivariate statistical
analysis and linear regression. Extensive experiments on ORL, COIL20, JAFFE,
Sheffield, Scene15 and Caltech256 databases verify our hypothesis. Similar to
graph learning, several representative hyperedge weighting schemes can be
concluded by our experimental studies. Moreover, the experiments also
demonstrate that the combinations of such weighting schemes and conventional
hypergraph models can get very promising classification and clustering
performances in comparison with some recent state-of-the-art algorithms."
"Deep learning techniques are being used in skeleton based action recognition
tasks and outstanding performance has been reported. Compared with RNN based
methods which tend to overemphasize temporal information, CNN-based approaches
can jointly capture spatio-temporal information from texture color images
encoded from skeleton sequences. There are several skeleton-based features that
have proven effective in RNN-based and handcrafted-feature-based methods.
However, it remains unknown whether they are suitable for CNN-based approaches.
This paper proposes to encode five spatial skeleton features into images with
different encoding methods. In addition, the performance implication of
different joints used for feature extraction is studied. The proposed method
achieved state-of-the-art performance on NTU RGB+D dataset for 3D human action
analysis. An accuracy of 75.32\% was achieved in Large Scale 3D Human Activity
Analysis Challenge in Depth Videos."
"Current state-of-the-art approaches for spatio-temporal action localization
rely on detections at the frame level that are then linked or tracked across
time. In this paper, we leverage the temporal continuity of videos instead of
operating at the frame level. We propose the ACtion Tubelet detector
(ACT-detector) that takes as input a sequence of frames and outputs tubelets,
i.e., sequences of bounding boxes with associated scores. The same way
state-of-the-art object detectors rely on anchor boxes, our ACT-detector is
based on anchor cuboids. We build upon the SSD framework. Convolutional
features are extracted for each frame, while scores and regressions are based
on the temporal stacking of these features, thus exploiting information from a
sequence. Our experimental results show that leveraging sequences of frames
significantly improves detection performance over using individual frames. The
gain of our tubelet detector can be explained by both more accurate scores and
more precise localization. Our ACT-detector outperforms the state-of-the-art
methods for frame-mAP and video-mAP on the J-HMDB and UCF-101 datasets, in
particular at high overlap thresholds."
"Universal style transfer aims to transfer arbitrary visual styles to content
images. Existing feed-forward based methods, while enjoying the inference
efficiency, are mainly limited by inability of generalizing to unseen styles or
compromised visual quality. In this paper, we present a simple yet effective
method that tackles these limitations without training on any pre-defined
styles. The key ingredient of our method is a pair of feature transforms,
whitening and coloring, that are embedded to an image reconstruction network.
The whitening and coloring transforms reflect a direct matching of feature
covariance of the content image to a given style image, which shares similar
spirits with the optimization of Gram matrix based cost in neural style
transfer. We demonstrate the effectiveness of our algorithm by generating
high-quality stylized images with comparisons to a number of recent methods. We
also analyze our method by visualizing the whitened features and synthesizing
textures via simple feature coloring."
"In this paper, we will study the simplest kind of beauty which can be found
in simple visual patterns. The proposed approach shows that aesthetically
appealing patterns deliver higher amount of information over multiple levels in
comparison with less aesthetically appealing patterns when the same amount of
energy is used. The proposed approach is used to classify aesthetically
appealing patterns."
"In this paper, we design a multimodal framework for object detection,
recognition and mapping based on the fusion of stereo camera frames, point
cloud Velodyne Lidar scans, and Vehicle-to-Vehicle (V2V) Basic Safety Messages
(BSMs) exchanged using Dedicated Short Range Communication (DSRC). We merge the
key features of rich texture descriptions of objects from 2D images, depth and
distance between objects provided by 3D point cloud and awareness of hidden
vehicles from BSMs' 3D information. We present a joint pixel to point cloud and
pixel to V2V correspondences of objects in frames from the Kitti Vision
Benchmark Suite by using a semi-supervised manifold alignment approach to
achieve camera-Lidar and camera-V2V mapping of their recognized objects that
have the same underlying manifold."
"In recent years, dynamic vision sensors (DVS), also known as event-based
cameras or neuromorphic sensors, have seen increased use due to various
advantages over conventional frame-based cameras. Using principles inspired by
the retina, its high temporal resolution overcomes motion blurring, its high
dynamic range overcomes extreme illumination conditions and its low power
consumption makes it ideal for embedded systems on platforms such as drones and
self-driving cars. However, event-based data sets are scarce and labels are
even rarer for tasks such as object detection. We transferred discriminative
knowledge from a state-of-the-art frame-based convolutional neural network
(CNN) to the event-based modality via intermediate pseudo-labels, which are
used as targets for supervised learning. We show, for the first time,
event-based car detection under ego-motion in a real environment at 100 frames
per second with a test average precision of 40.3% relative to our annotated
ground truth. The event-based car detector handles motion blur and poor
illumination conditions despite not explicitly trained to do so, and even
complements frame-based CNN detectors, suggesting that it has learnt
generalized visual representations."
"Recent work has shown impressive success in transferring painterly style to
images. These approaches, however, fall short of photorealistic style transfer.
Even when both the input and reference images are photographs, the output still
exhibits distortions reminiscent of a painting. In this paper we propose an
approach that takes as input a stylized image and makes it more photorealistic.
It relies on the Screened Poisson Equation, maintaining the fidelity of the
stylized image while constraining the gradients to those of the original input
image. Our method is fast, simple, fully automatic and shows positive progress
in making a stylized image photorealistic. Our results exhibit finer details
and are less prone to artifacts than the state-of-the-art."
"Exploiting multiple modalities for semantic scene parsing has been shown to
improve accuracy over the singlemodality scenario. However multimodal datasets
often suffer from problems such as data misalignment and label inconsistencies,
where the existing methods assume that corresponding regions in two modalities
must have identical labels. We propose to address this issue, by formulating
multimodal semantic labeling as inference in a CRF and introducing latent nodes
to explicitly model inconsistencies between two modalities. These latent nodes
allow us not only to leverage information from both domains to improve their
labeling, but also to cut the edges between inconsistent regions. We propose to
learn intradomain and inter-domain potential functions from training data to
avoid hand-tuning of the model parameters. We evaluate our approach on two
publicly available datasets containing 2D and 3D data. Thanks to our latent
nodes and our learning strategy, our method outperforms the state-of-the-art in
both cases. Moreover, in order to highlight the benefits of the geometric
information and the potential of our method in simultaneous 2D/3D semantic and
geometric inference, we performed simultaneous inference of semantic and
geometric classes both in 2D and 3D that led to satisfactory improvements of
the labeling results in both datasets."
"In contrast to traditional cameras, whose pixels have a common exposure time,
event-based cameras are novel bio-inspired sensors whose pixels work
independently and asynchronously output intensity changes (called ""events""),
with microsecond resolution. Since events are caused by the apparent motion of
objects, event-based cameras sample visual information based on the scene
dynamics and are, therefore, a more natural fit than traditional cameras to
acquire motion, especially at high speeds, where traditional cameras suffer
from motion blur. However, distinguishing between events caused by different
moving objects and by the camera's ego-motion is a challenging task. We present
the first per-event segmentation method for splitting a scene into
independently moving objects. Our method jointly estimates the event-object
associations (i.e., segmentation) and the motion parameters of the objects (or
the background) by maximization of an objective function, which builds upon
recent results on event-based motion-compensation. We provide a thorough
evaluation of our method on a public dataset, outperforming the
state-of-the-art by as much as 10%. We also show the first quantitative
evaluation of a segmentation algorithm for event cameras, yielding around 90%
accuracy at 4 pixels relative displacement."
"Semantic segmentation generates comprehensive understanding of scenes through
densely predicting the category for each pixel. High-level features from Deep
Convolutional Neural Networks already demonstrate their effectiveness in
semantic segmentation tasks, however the coarse resolution of high-level
features often leads to inferior results for small/thin objects where detailed
information is important. It is natural to consider importing low level
features to compensate for the lost detailed information in high-level
features.Unfortunately, simply combining multi-level features suffers from the
semantic gap among them. In this paper, we propose a new architecture, named
Gated Fully Fusion (GFF), to selectively fuse features from multiple levels
using gates in a fully connected way. Specifically, features at each level are
enhanced by higher-level features with stronger semantics and lower-level
features with more details, and gates are used to control the propagation of
useful information which significantly reduces the noises during fusion. We
achieve the state of the art results on four challenging scene parsing datasets
including Cityscapes, Pascal Context, COCO-stuff and ADE20K."
"With explosive growth of data volume and ever-increasing diversity of data
modalities, cross-modal similarity search, which conducts nearest neighbor
search across different modalities, has been attracting increasing interest.
This paper presents a deep compact code learning solution for efficient
cross-modal similarity search. Many recent studies have proven that
quantization-based approaches perform generally better than hashing-based
approaches on single-modal similarity search. In this paper, we propose a deep
quantization approach, which is among the early attempts of leveraging deep
neural networks into quantization-based cross-modal similarity search. Our
approach, dubbed shared predictive deep quantization (SPDQ), explicitly
formulates a shared subspace across different modalities and two private
subspaces for individual modalities, and representations in the shared subspace
and the private subspaces are learned simultaneously by embedding them to a
reproducing kernel Hilbert space, where the mean embedding of different
modality distributions can be explicitly compared. In addition, in the shared
subspace, a quantizer is learned to produce the semantics preserving compact
codes with the help of label alignment. Thanks to this novel network
architecture in cooperation with supervised quantization training, SPDQ can
preserve intramodal and intermodal similarities as much as possible and greatly
reduce quantization error. Experiments on two popular benchmarks corroborate
that our approach outperforms state-of-the-art methods."
"Deep convolutional networks based super-resolution is a fast-growing field
with numerous practical applications. In this exposition, we extensively
compare 30+ state-of-the-art super-resolution Convolutional Neural Networks
(CNNs) over three classical and three recently introduced challenging datasets
to benchmark single image super-resolution. We introduce a taxonomy for
deep-learning based super-resolution networks that groups existing methods into
nine categories including linear, residual, multi-branch, recursive,
progressive, attention-based and adversarial designs. We also provide
comparisons between the models in terms of network complexity, memory
footprint, model input and output, learning details, the type of network losses
and important architectural differences (e.g., depth, skip-connections,
filters). The extensive evaluation performed, shows the consistent and rapid
growth in the accuracy in the past few years along with a corresponding boost
in model complexity and the availability of large-scale datasets. It is also
observed that the pioneering methods identified as the benchmark have been
significantly outperformed by the current contenders. Despite the progress in
recent years, we identify several shortcomings of existing techniques and
provide future research directions towards the solution of these open problems."
"We present a novel deep convolutional network pipeline, LO-Net, for real-time
lidar odometry estimation. Unlike most existing lidar odometry (LO) estimations
that go through individually designed feature selection, feature matching, and
pose estimation pipeline, LO-Net can be trained in an end-to-end manner. With a
new mask-weighted geometric constraint loss, LO-Net can effectively learn
feature representation for LO estimation, and can implicitly exploit the
sequential dependencies and dynamics in the data. We also design a scan-to-map
module, which uses the geometric and semantic information learned in LO-Net, to
improve the estimation accuracy. Experiments on benchmark datasets demonstrate
that LO-Net outperforms existing learning based approaches and has similar
accuracy with the state-of-the-art geometry-based approach, LOAM."
"Watching cartoons can be useful for children's intellectual, social and
emotional development. However, the most popular video sharing platform today
provides many videos with Elsagate content. Elsagate is a phenomenon that
depicts childhood characters in disturbing circumstances (e.g., gore, toilet
humor, drinking urine, stealing). Even with this threat easily available for
children, there is no work in the literature addressing the problem. As the
first to explore disturbing content in cartoons, we proceed from the most
recent pornography detection literature applying deep convolutional neural
networks combined with static and motion information of the video. Our solution
is compatible with mobile platforms and achieved 92.6% of accuracy. Our goal is
not only to introduce the first solution but also to bring up the discussion
around Elsagate."
"This paper proposes a RANSAC-based algorithm for determining the axial
rotation angle of an object from a pair of its tomographic projections. An
equation is derived for calculating the rotation angle using one correct
keypoints correspondence of two tomographic projections. The proposed algorithm
consists of the following steps: keypoints detection and matching, rotation
angle estimation for each correspondence, outliers filtering with the RANSAC
algorithm, finally, calculation of the desired angle by minimizing the
re-projection error from the remaining correspondences. To validate the
proposed method an experimental comparison against methods based on analysis of
the distribution of the angles computed from all correspondences is conducted."
"In the last few years, there has been a growing interest in taking advantage
of the 360 panoramic images potential, while managing the new challenges they
imply. While several tasks have been improved thanks to the contextual
information these images offer, object recognition in indoor scenes still
remains a challenging problem that has not been deeply investigated. This paper
provides an object recognition system that performs object detection and
semantic segmentation tasks by using a deep learning model adapted to match the
nature of equirectangular images. From these results, instance segmentation
masks are recovered, refined and transformed into 3D bounding boxes that are
placed into the 3D model of the room. Quantitative and qualitative results
support that our method outperforms the state of the art by a large margin and
show a complete understanding of the main objects in indoor scenes."
"Recent work on 3D object detection advocates point cloud voxelization in
birds-eye view, where objects preserve their physical dimensions and are
naturally separable. When represented in this view, however, point clouds are
sparse and have highly variable point density, which may cause detectors
difficulties in detecting distant or small objects (pedestrians, traffic signs,
etc.). On the other hand, perspective view provides dense observations, which
could allow more favorable feature encoding for such cases. In this paper, we
aim to synergize the birds-eye view and the perspective view and propose a
novel end-to-end multi-view fusion (MVF) algorithm, which can effectively learn
to utilize the complementary information from both. Specifically, we introduce
dynamic voxelization, which has four merits compared to existing voxelization
methods, i) removing the need of pre-allocating a tensor with fixed size; ii)
overcoming the information loss due to stochastic point/voxel dropout; iii)
yielding deterministic voxel embeddings and more stable detection outcomes; iv)
establishing the bi-directional relationship between points and voxels, which
potentially lays a natural foundation for cross-view feature fusion. By
employing dynamic voxelization, the proposed feature fusion architecture
enables each point to learn to fuse context information from different views.
MVF operates on points and can be naturally extended to other approaches using
LiDAR point clouds. We evaluate our MVF model extensively on the newly released
Waymo Open Dataset and on the KITTI dataset and demonstrate that it
significantly improves detection accuracy over the comparable single-view
PointPillars baseline."
"An effective person re-identification (re-ID) model should learn feature
representations that are both discriminative, for distinguishing
similar-looking people, and generalisable, for deployment across datasets
without any adaptation. In this paper, we develop novel CNN architectures to
address both challenges. First, we present a re-ID CNN termed omni-scale
network (OSNet) to learn features that not only capture different spatial
scales but also encapsulate a synergistic combination of multiple scales,
namely omni-scale features. The basic building block consists of multiple
convolutional streams, each detecting features at a certain scale. For
omni-scale feature learning, a unified aggregation gate is introduced to
dynamically fuse multi-scale features with channel-wise weights. OSNet is
lightweight as its building blocks comprise factorised convolutions. Second, to
improve generalisable feature learning, we introduce instance normalisation
(IN) layers into OSNet to cope with cross-dataset discrepancies. Further, to
determine the optimal placements of these IN layers in the architecture, we
formulate an efficient differentiable architecture search algorithm. Extensive
experiments show that, in the conventional same-dataset setting, OSNet achieves
state-of-the-art performance, despite being much smaller than existing re-ID
models. In the more challenging yet practical cross-dataset setting, OSNet
beats most recent unsupervised domain adaptation methods without using any
target data. Our code and models are released at
\texttt{https://github.com/KaiyangZhou/deep-person-reid}."
"Global context information is vital in visual understanding problems,
especially in pixel-level semantic segmentation. The mainstream methods adopt
the self-attention mechanism to model global context information. However,
pixels belonging to different classes usually have weak feature correlation.
Modeling the global pixel-level correlation matrix indiscriminately is
extremely redundant in the self-attention mechanism. In order to solve the
above problem, we propose a hierarchical context network to differentially
model homogeneous pixels with strong correlations and heterogeneous pixels with
weak correlations. Specifically, we first propose a multi-scale guided
pre-segmentation module to divide the entire feature map into different
classed-based homogeneous regions. Within each homogeneous region, we design
the pixel context module to capture pixel-level correlations. Subsequently,
different from the self-attention mechanism that still models weak
heterogeneous correlations in a dense pixel-level manner, the region context
module is proposed to model sparse region-level dependencies using a unified
representation of each region. Through aggregating fine-grained pixel context
features and coarse-grained region context features, our proposed network can
not only hierarchically model global context information but also harvest
multi-granularity representations to more robustly identify multi-scale
objects. We evaluate our approach on Cityscapes and the ISPRS Vaihingen
dataset. Without Bells or Whistles, our approach realizes a mean IoU of 82.8%
and overall accuracy of 91.4% on Cityscapes and ISPRS Vaihingen test set,
achieving state-of-the-art results."
"Neural architecture search (NAS) aims to produce the optimal sparse solution
from a high-dimensional space spanned by all candidate connections. Current
gradient-based NAS methods commonly ignore the constraint of sparsity in the
search phase, but project the optimized solution onto a sparse one by
post-processing. As a result, the dense super-net for search is inefficient to
train and has a gap with the projected architecture for evaluation. In this
paper, we formulate neural architecture search as a sparse coding problem. We
perform the differentiable search on a compressed lower-dimensional space that
has the same validation loss as the original sparse solution space, and recover
an architecture by solving the sparse coding problem. The differentiable search
and architecture recovery are optimized in an alternate manner. By doing so,
our network for search at each update satisfies the sparsity constraint and is
efficient to train. In order to also eliminate the depth and width gap between
the network in search and the target-net in evaluation, we further propose a
method to search and evaluate in one stage under the target-net settings. When
training finishes, architecture variables are absorbed into network weights.
Thus we get the searched architecture and optimized parameters in a single run.
In experiments, our two-stage method on CIFAR-10 requires only 0.05 GPU-day for
search. Our one-stage method produces state-of-the-art performances on both
CIFAR-10 and ImageNet at the cost of only evaluation time."
"The whole slide histopathology images (WSIs) play a critical role in gastric
cancer diagnosis. However, due to the large scale of WSIs and various sizes of
the abnormal area, how to select informative regions and analyze them are quite
challenging during the automatic diagnosis process. The multi-instance learning
based on the most discriminative instances can be of great benefit for whole
slide gastric image diagnosis. In this paper, we design a recalibrated
multi-instance deep learning method (RMDL) to address this challenging problem.
We first select the discriminative instances, and then utilize these instances
to diagnose diseases based on the proposed RMDL approach. The designed RMDL
network is capable of capturing instance-wise dependencies and recalibrating
instance features according to the importance coefficient learned from the
fused features. Furthermore, we build a large whole-slide gastric
histopathology image dataset with detailed pixel-level annotations.
Experimental results on the constructed gastric dataset demonstrate the
significant improvement on the accuracy of our proposed framework compared with
other state-of-the-art multi-instance learning methods. Moreover, our method is
general and can be extended to other diagnosis tasks of different cancer types
based on WSIs."
"Consecutive LiDAR scans compose dynamic 3D sequences, which contain more
abundant information than a single frame. Similar to the development history of
image and video perception, dynamic 3D sequence perception starts to come into
sight after inspiring research on static 3D data perception. This work proposes
a spatio-temporal neural network for human segmentation with the dynamic LiDAR
point clouds. It takes a sequence of depth images as input. It has a two-branch
structure, i.e., the spatial segmentation branch and the temporal velocity
estimation branch. The velocity estimation branch is designed to capture motion
cues from the input sequence and then propagates them to the other branch. So
that the segmentation branch segments humans according to both spatial and
temporal features. These two branches are jointly learned on a generated
dynamic point cloud dataset for human recognition. Our works fill in the blank
of dynamic point cloud perception with the spherical representation of point
cloud and achieves high accuracy. The experiments indicate that the
introduction of temporal feature benefits the segmentation of dynamic point
cloud."
"3D object detectors based only on LiDAR point clouds hold the
state-of-the-art on modern street-view benchmarks. However, LiDAR-based
detectors poorly generalize across domains due to domain shift. In the case of
LiDAR, in fact, domain shift is not only due to changes in the environment and
in the object appearances, as for visual data from RGB cameras, but is also
related to the geometry of the point clouds (e.g., point density variations).
This paper proposes SF-UDA$^{3D}$, the first Source-Free Unsupervised Domain
Adaptation (SF-UDA) framework to domain-adapt the state-of-the-art PointRCNN 3D
detector to target domains for which we have no annotations (unsupervised),
neither we hold images nor annotations of the source domain (source-free).
SF-UDA$^{3D}$ is novel on both aspects. Our approach is based on
pseudo-annotations, reversible scale-transformations and motion coherency.
SF-UDA$^{3D}$ outperforms both previous domain adaptation techniques based on
features alignment and state-of-the-art 3D object detection methods which
additionally use few-shot target annotations or target annotation statistics.
This is demonstrated by extensive experiments on two large-scale datasets,
i.e., KITTI and nuScenes."
"We propose a method for fusing stereo disparity estimation with
movement-induced prior information. Instead of independent inference
frame-by-frame, we formulate the problem as a non-parametric learning task in
terms of a temporal Gaussian process prior with a movement-driven kernel for
inter-frame reasoning. We present a hierarchy of three Gaussian process kernels
depending on the availability of motion information, where our main focus is on
a new gyroscope-driven kernel for handheld devices with low-quality MEMS
sensors, thus also relaxing the requirement of having full 6D camera poses
available. We show how our method can be combined with two state-of-the-art
deep stereo methods. The method either work in a plug-and-play fashion with
pre-trained deep stereo networks, or further improved by jointly training the
kernels together with encoder-decoder architectures, leading to consistent
improvement."
"Unsupervised Domain Adaptation (UDA) for semantic segmentation has been
favorably applied to real-world scenarios in which pixel-level labels are hard
to be obtained. In most of the existing UDA methods, all target data are
assumed to be introduced simultaneously. Yet, the data are usually presented
sequentially in the real world. Moreover, Continual UDA, which deals with more
practical scenarios with multiple target domains in the continual learning
setting, has not been actively explored. In this light, we propose Continual
UDA for semantic segmentation based on a newly designed Expanding
Target-specific Memory (ETM) framework. Our novel ETM framework contains
Target-specific Memory (TM) for each target domain to alleviate catastrophic
forgetting. Furthermore, a proposed Double Hinge Adversarial (DHA) loss leads
the network to produce better UDA performance overall. Our design of the TM and
training objectives let the semantic segmentation network adapt to the current
target domain while preserving the knowledge learned on previous target
domains. The model with the proposed framework outperforms other
state-of-the-art models in continual learning settings on standard benchmarks
such as GTA5, SYNTHIA, CityScapes, IDD, and Cross-City datasets. The source
code is available at https://github.com/joonh-kim/ETM."
"Driving in a state of drowsiness is a major cause of road accidents,
resulting in tremendous damage to life and property. Developing robust,
automatic, real-time systems that can infer drowsiness states of drivers has
the potential of making life-saving impact. However, developing drowsiness
detection systems that work well in real-world scenarios is challenging because
of the difficulties associated with collecting high-volume realistic drowsy
data and modeling the complex temporal dynamics of evolving drowsy states. In
this paper, we propose a data collection protocol that involves outfitting
vehicles of overnight shift workers with camera kits that record their faces
while driving. We develop a drowsiness annotation guideline to enable humans to
label the collected videos into 4 levels of drowsiness: `alert', `slightly
drowsy', `moderately drowsy' and `extremely drowsy'. We experiment with
different convolutional and temporal neural network architectures to predict
drowsiness states from pose, expression and emotion-based representation of the
input video of the driver's face. Our best performing model achieves a macro
ROC-AUC of 0.78, compared to 0.72 for a baseline model."
"Existing state-of-the-art disparity estimation works mostly leverage the 4D
concatenation volume and construct a very deep 3D convolution neural network
(CNN) for disparity regression, which is inefficient due to the high memory
consumption and slow inference speed. In this paper, we propose a network named
EDNet for efficient disparity estimation. Firstly, we construct a combined
volume which incorporates contextual information from the squeezed
concatenation volume and feature similarity measurement from the correlation
volume. The combined volume can be next aggregated by 2D convolutions which are
faster and require less memory than 3D convolutions. Secondly, we propose an
attention-based spatial residual module to generate attention-aware residual
features. The attention mechanism is applied to provide intuitive spatial
evidence about inaccurate regions with the help of error maps at multiple
scales and thus improve the residual learning efficiency. Extensive experiments
on the Scene Flow and KITTI datasets show that EDNet outperforms the previous
3D CNN based works and achieves state-of-the-art performance with significantly
faster speed and less memory consumption."
"Facial expression recognition (FER), aiming to classify the expression
present in the facial image or video, has attracted a lot of research interests
in the field of artificial intelligence and multimedia. In terms of video based
FER task, it is sensible to capture the dynamic expression variation among the
frames to recognize facial expression. However, existing methods directly
utilize CNN-RNN or 3D CNN to extract the spatial-temporal features from
different facial units, instead of concentrating on a certain region during
expression variation capturing, which leads to limited performance in FER. In
our paper, we introduce a Graph Convolutional Network (GCN) layer into a common
CNN-RNN based model for video-based FER. First, the GCN layer is utilized to
learn more significant facial expression features which concentrate on certain
regions after sharing information between extracted CNN features of nodes.
Then, a LSTM layer is applied to learn long-term dependencies among the GCN
learned features to model the variation. In addition, a weight assignment
mechanism is also designed to weight the output of different nodes for final
classification by characterizing the expression intensities in each frame. To
the best of our knowledge, it is the first time to use GCN in FER task. We
evaluate our method on three widely-used datasets, CK+, Oulu-CASIA and MMI, and
also one challenging wild dataset AFEW8.0, and the experimental results
demonstrate that our method has superior performance to existing methods."
"Most face applications depend heavily on the accuracy of the face and facial
landmarks detectors employed. Prediction of attributes such as gender, age, and
identity usually completely fail when the faces are badly aligned due to
inaccurate facial landmark detection. Despite the impressive recent advances in
face and facial landmark detection, little study is on the recovery from and
detection of failures or inaccurate predictions. In this work we study two top
recent facial landmark detectors and devise confidence models for their
outputs. We validate our failure detection approaches on standard benchmarks
(AFLW, HELEN) and correctly identify more than 40% of the failures in the
outputs of the landmark detectors. Moreover, with our failure detection we can
achieve a 12% error reduction on a gender estimation application at the cost of
a small increase in computation."
"Correlation filtering based tracking model has received lots of attention and
achieved great success in real-time tracking, however, the lost function in
current correlation filtering paradigm could not reliably response to the
appearance changes caused by occlusion and illumination variations. This study
intends to promote the robustness of the correlation filter learning. By
exploiting the anisotropy of the filter response, three sparsity related loss
functions are proposed to alleviate the overfitting issue of previous methods
and improve the overall tracking performance. As a result, three real-time
trackers are implemented. Extensive experiments in various challenging
situations demonstrate that the robustness of the learned correlation filter
has been greatly improved via the designed loss functions. In addition, the
study reveals, from an experimental perspective, how different loss functions
essentially influence the tracking performance. An important conclusion is that
the sensitivity of the peak values of the filter in successive frames is
consistent with the tracking performance. This is a useful reference criterion
in designing a robust correlation filter for visual tracking."
"An effort has been made to show mathematicians some new ideas applied to
image analysis. Gray images are presented as tilings. Based on topological
properties of the tiling, a number of gray convex hulls: maximal, minimal, and
oriented ones are constructed and some are proved. They are constructed with
only one operation. Two tilings are used in the Constraint and Allowance types
of operations. New type of concavity described: a dale. All operations are
parallel, possible to realize clock-less. Convexities define what is the
background. They are treated as separate gray objects. There are multiple
relations among them and their descendants. Via that, topological size of
concavities is proposed. Constructed with the same type of operations, Rays and
Angles in a tiling define possible spatial relations. Notions like ""strokes""
are defined through concavities. Unusual effects on levelized gray objects are
shown. It is illustrated how alphabet and complex hieroglyphs can be described
through concavities and their relations. A hypothesis of living organisms image
analysis is proposed. A number of examples with symbols and a human face are
calculated with new Asynchwave C++ software library."
"Thanks to the availability and increasing popularity of Egocentric cameras
such as GoPro cameras, glasses, and etc. we have been provided with a plethora
of videos captured from the first person perspective. Surveillance cameras and
Unmanned Aerial Vehicles(also known as drones) also offer tremendous amount of
videos, mostly with top-down or oblique view-point. Egocentric vision and
top-view surveillance videos have been studied extensively in the past in the
computer vision community. However, the relationship between the two has yet to
be explored thoroughly. In this effort, we attempt to explore this relationship
by approaching two questions. First, having a set of egocentric videos and a
top-view video, can we verify if the top-view video contains all, or some of
the egocentric viewers present in the egocentric set? And second, can we
identify the egocentric viewers in the content of the top-view video? In other
words, can we find the cameramen in the surveillance videos? These problems can
become more challenging when the videos are not time-synchronous. Thus we
formalize the problem in a way which handles and also estimates the unknown
relative time-delays between the egocentric videos and the top-view video. We
formulate the problem as a spectral graph matching instance, and jointly seek
the optimal assignments and relative time-delays of the videos. As a result, we
spatiotemporally localize the egocentric observers in the top-view video. We
model each view (egocentric or top) using a graph, and compute the assignment
and time-delays in an iterative-alternative fashion."
"This paper presents a robust multi-class multi-object tracking (MCMOT)
formulated by a Bayesian filtering framework. Multi-object tracking for
unlimited object classes is conducted by combining detection responses and
changing point detection (CPD) algorithm. The CPD model is used to observe
abrupt or abnormal changes due to a drift and an occlusion based spatiotemporal
characteristics of track states. The ensemble of convolutional neural network
(CNN) based object detector and Lucas-Kanede Tracker (KLT) based motion
detector is employed to compute the likelihoods of foreground regions as the
detection responses of different object classes. Extensive experiments are
performed using lately introduced challenging benchmark videos; ImageNet VID
and MOT benchmark dataset. The comparison to state-of-the-art video tracking
techniques shows very encouraging results."
"We present an empirical model for noises in color measurements from OLED
displays. According to measured data the noise is not isotropic in the XYZ
space, instead most of the noise is along an axis that is parallel to a vector
from origin to measured XYZ vector. The presented empirical model is simple and
depends only on the measured XYZ values. Our tests show that the variations
between multiple panels of the same type have similar distribution as the
temporal noise in measurements from a single panel, but a larger magnitude."
"With the immense number of videos being uploaded to the video sharing sites,
issue of copyright infringement arises with uploading of illicit copies or
transformed versions of original video. Thus safeguarding copyright of digital
media has become matter of concern. To address this concern, it is obliged to
have a video copy detection system which is sufficiently robust to detect these
transformed videos with ability to pinpoint location of copied segments. This
paper outlines recent advancement in content based video copy detection, mainly
focusing on different visual features employed by video copy detection systems.
Finally we evaluate performance of existing video copy detection systems."
"Motion segmentation is currently an active area of research in computer
Vision. The task of comparing different methods of motion segmentation is
complicated by the fact that researchers may use subtly different definitions
of the problem. Questions such as ""Which objects are moving?"", ""What is
background?"", and ""How can we use motion of the camera to segment objects,
whether they are static or moving?"" are clearly related to each other, but lead
to different algorithms, and imply different versions of the ground truth. This
report has two goals. The first is to offer a precise definition of motion
segmentation so that the intent of an algorithm is as well-defined as possible.
The second is to report on new versions of three previously existing data sets
that are compatible with this definition. We hope that this more detailed
definition, and the three data sets that go with it, will allow more meaningful
comparisons of certain motion segmentation methods."
"Generalization performance of trained computer vision systems that use
computer graphics (CG) generated data is not yet effective due to the concept
of 'domain-shift' between virtual and real data. Although simulated data
augmented with a few real world samples has been shown to mitigate domain shift
and improve transferability of trained models, guiding or bootstrapping the
virtual data generation with the distributions learnt from target real world
domain is desired, especially in the fields where annotating even few real
images is laborious (such as semantic labeling, and intrinsic images etc.). In
order to address this problem in an unsupervised manner, our work combines
recent advances in CG (which aims to generate stochastic scene layouts coupled
with large collections of 3D object models) and generative adversarial training
(which aims train generative models by measuring discrepancy between generated
and real data in terms of their separability in the space of a deep
discriminatively-trained classifier). Our method uses iterative estimation of
the posterior density of prior distributions for a generative graphical model.
This is done within a rejection sampling framework. Initially, we assume
uniform distributions as priors on the parameters of a scene described by a
generative graphical model. As iterations proceed the prior distributions get
updated to distributions that are closer to the (unknown) distributions of
target data. We demonstrate the utility of adversarially tuned scene generation
on two real-world benchmark datasets (CityScapes and CamVid) for traffic scene
semantic labeling with a deep convolutional net (DeepLab). We realized
performance improvements by 2.28 and 3.14 points (using the IoU metric) between
the DeepLab models trained on simulated sets prepared from the scene generation
models before and after tuning to CityScapes and CamVid respectively."
"Automatic photo cropping is an important tool for improving visual quality of
digital photos without resorting to tedious manual selection. Traditionally,
photo cropping is accomplished by determining the best proposal window through
visual quality assessment or saliency detection. In essence, the performance of
an image cropper highly depends on the ability to correctly rank a number of
visually similar proposal windows. Despite the ranking nature of automatic
photo cropping, little attention has been paid to learning-to-rank algorithms
in tackling such a problem. In this work, we conduct an extensive study on
traditional approaches as well as ranking-based croppers trained on various
image features. In addition, a new dataset consisting of high quality cropping
and pairwise ranking annotations is presented to evaluate the performance of
various baselines. The experimental results on the new dataset provide useful
insights into the design of better photo cropping algorithms."
"In this paper, we introduce a framework for classifying images according to
high-level sentiment. We subdivide the task into three primary problems:
emotion classification on faces, human pose estimation, and 3D estimation and
clustering of groups of people. We introduce novel algorithms for matching body
parts to a common individual and clustering people in images based on physical
location and orientation. Our results outperform several baseline approaches."
"Based on different projection geometry, a fisheye image can be presented as a
parameterized non-rectilinear image. Deep neural networks(DNN) is one of the
solutions to extract parameters for fisheye image feature description. However,
a large number of images are required for training a reasonable prediction
model for DNN. In this paper, we propose to extend the scale of the training
dataset using parameterized synthetic images. It effectively boosts the
diversity of images and avoids the data scale limitation. To simulate different
viewing angles and distances, we adopt controllable parameterized projection
processes on transformation. The reliability of the proposed method is proved
by testing images captured by our fisheye camera. The synthetic dataset is the
first dataset that is able to extend to a big scale labeled fisheye image
dataset. It is accessible via: http://www2.leuphana.de/misl/fisheye-data-set/."
"We present a system to recover the 3D shape and motion of a wide variety of
quadrupeds from video. The system comprises a machine learning front-end which
predicts candidate 2D joint positions, a discrete optimization which finds
kinematically plausible joint correspondences, and an energy minimization stage
which fits a detailed 3D model to the image. In order to overcome the limited
availability of motion capture training data from animals, and the difficulty
of generating realistic synthetic training images, the system is designed to
work on silhouette data. The joint candidate predictor is trained on
synthetically generated silhouette images, and at test time, deep learning
methods or standard video segmentation tools are used to extract silhouettes
from real data. The system is tested on animal videos from several species, and
shows accurate reconstructions of 3D shape and pose."
"We propose a simple yet effective deep tree-structured fusion model based on
feature aggregation for the deraining problem. We argue that by effectively
aggregating features, a relatively simple network can still handle tough image
deraining problems well. First, to capture the spatial structure of rain we use
dilated convolutions as our basic network block. We then design a
tree-structured fusion architecture which is deployed within each block
(spatial information) and across all blocks (content information). Our method
is based on the assumption that adjacent features contain redundant
information. This redundancy obstructs generation of new representations and
can be reduced by hierarchically fusing adjacent features. Thus, the proposed
model is more compact and can effectively use spatial and content information.
Experiments on synthetic and real-world datasets show that our network achieves
better deraining results with fewer parameters."
"The past decade has witnessed great success in applying deep learning to
enhance the quality of compressed video. However, the existing approaches aim
at quality enhancement on a single frame, or only using fixed neighboring
frames. Thus they fail to take full advantage of the inter-frame correlation in
the video. This paper proposes the Quality-Gated Convolutional Long Short-Term
Memory (QG-ConvLSTM) network with bi-directional recurrent structure to fully
exploit the advantageous information in a large range of frames. More
importantly, due to the obvious quality fluctuation among compressed frames,
higher quality frames can provide more useful information for other frames to
enhance quality. Therefore, we propose learning the ""forget"" and ""input"" gates
in the ConvLSTM cell from quality-related features. As such, the frames with
various quality contribute to the memory in ConvLSTM with different importance,
making the information of each frame reasonably and adequately used. Finally,
the experiments validate the effectiveness of our QG-ConvLSTM approach in
advancing the state-of-the-art quality enhancement of compressed video, and the
ablation study shows that our QG-ConvLSTM approach is learnt to make a
trade-off between quality and correlation when leveraging multi-frame
information. The project page: https://github.com/ryangchn/QG-ConvLSTM.git."
"Deep neural network (DNN) based approaches have been widely investigated and
deployed in medical image analysis. For example, fully convolutional neural
networks (FCN) achieve the state-of-the-art performance in several applications
of 2D/3D medical image segmentation. Even the baseline neural network models
(U-Net, V-Net, etc.) have been proven to be very effective and efficient when
the training process is set up properly. Nevertheless, to fully exploit the
potentials of neural networks, we propose an automated searching approach for
the optimal training strategy with reinforcement learning. The proposed
approach can be utilized for tuning hyper-parameters, and selecting necessary
data augmentation with certain probabilities. The proposed approach is
validated on several tasks of 3D medical image segmentation. The performance of
the baseline model is boosted after searching, and it can achieve comparable
accuracy to other manually-tuned state-of-the-art segmentation approaches."
"Small objects are difficult to detect because of their low resolution and
small size. The existing small object detection methods mainly focus on data
preprocessing or narrowing the differences between large and small objects.
Inspired by human vision ""attention"" mechanism, we exploit two feature
extraction methods to mine the most useful information of small objects. Both
methods are based on multiresolution feature extraction. We initially design
and explore the soft attention method, but we find that its convergence speed
is slow. Then we present the second method, an attention-based feature
interaction method, called a MultiResolution Attention Extractor (MRAE),
showing significant improvement as a generic feature extractor in small object
detection. After each building block in the vanilla feature extractor, we
append a small network to generate attention weights followed by a weighted-sum
operation to get the final attention maps. Our attention-based feature
extractor is 2.0 times the AP of the ""hard"" attention counterpart (plain
architecture) on the COCO small object detection benchmark, proving that MRAE
can capture useful location and contextual information through adaptive
learning."
"Knowledge distillation, which involves extracting the ""dark knowledge"" from a
teacher network to guide the learning of a student network, has emerged as an
important technique for model compression and transfer learning. Unlike
previous works that exploit architecture-specific cues such as activation and
attention for distillation, here we wish to explore a more general and
model-agnostic approach for extracting ""richer dark knowledge"" from the
pre-trained teacher model. We show that the seemingly different
self-supervision task can serve as a simple yet powerful solution. For example,
when performing contrastive learning between transformed entities, the noisy
predictions of the teacher network reflect its intrinsic composition of
semantic and pose information. By exploiting the similarity between those
self-supervision signals as an auxiliary task, one can effectively transfer the
hidden information from the teacher to the student. In this paper, we discuss
practical ways to exploit those noisy self-supervision signals with selective
transfer for distillation. We further show that self-supervision signals
improve conventional distillation with substantial gains under few-shot and
noisy-label scenarios. Given the richer knowledge mined from self-supervision,
our knowledge distillation approach achieves state-of-the-art performance on
standard benchmarks, i.e., CIFAR100 and ImageNet, under both
similar-architecture and cross-architecture settings. The advantage is even
more pronounced under the cross-architecture setting, where our method
outperforms the state of the art CRD by an average of 2.3% in accuracy rate on
CIFAR100 across six different teacher-student pairs."
"Visual object tracking is an important application of computer vision.
Recently, Siamese based trackers have achieved good accuracy. However, most of
Siamese based trackers are not efficient, as they exhaustively search potential
object locations to define anchors and then classify each anchor (i.e., a
bounding box). This paper develops the first Anchor Free Siamese Network
(AFSN). Specifically, a target object is defined by a bounding box center,
tracking offset, and object size. All three are regressed by Siamese network
with no additional classification or regional proposal, and performed once for
each frame. We also tune the stride and receptive field for Siamese network,
and further perform ablation experiments to quantitatively illustrate the
effectiveness of our AFSN. We evaluate AFSN using five most commonly used
benchmarks and compare to the best anchor-based trackers with source codes
available for each benchmark. AFSN is 3-425 times faster than these best anchor
based trackers. AFSN is also 5.97% to 12.4% more accurate in terms of all
metrics for benchmark sets OTB2015, VOT2015, VOT2016, VOT2018 and TrackingNet,
except that SiamRPN++ is 4% better than AFSN in terms of Expected Average
Overlap (EAO) on VOT2018 (but SiamRPN++ is 3.9 times slower)."
"Video-based person re-identification (Re-ID) is an important computer vision
task. The batch-hard triplet loss frequently used in video-based person Re-ID
suffers from the Distance Variance among Different Positives (DVDP) problem. In
this paper, we address this issue by introducing a new metric learning method
called Attribute-aware Identity-hard Triplet Loss (AITL), which reduces the
intra-class variation among positive samples via calculating attribute
distance. To achieve a complete model of video-based person Re-ID, a multi-task
framework with Attribute-driven Spatio-Temporal Attention (ASTA) mechanism is
also proposed. Extensive experiments on MARS and DukeMTMC-VID datasets shows
that both the AITL and ASTA are very effective. Enhanced by them, even a simple
light-weighted video-based person Re-ID baseline can outperform existing
state-of-the-art approaches. The codes has been published on
https://github.com/yuange250/Video-based-person-ReID-with-Attribute-information."
"Single-view 3D object reconstruction is a challenging fundamental problem in
computer vision, largely due to the morphological diversity of objects in the
natural world. In particular, high curvature regions are not always captured
effectively by methods trained using only set-based loss functions, resulting
in reconstructions short-circuiting the surface or cutting corners. In
particular, high curvature regions are not always captured effectively by
methods trained using only set-based loss functions, resulting in
reconstructions short-circuiting the surface or cutting corners. To address
this issue, we propose learning an image-conditioned mapping function from a
canonical sampling domain to a high dimensional space where the Euclidean
distance is equal to the geodesic distance on the object. The first three
dimensions of a mapped sample correspond to its 3D coordinates. The additional
lifted components contain information about the underlying geodesic structure.
Our results show that taking advantage of these learned lifted coordinates
yields better performance for estimating surface normals and generating
surfaces than using point cloud reconstructions alone. Further, we find that
this learned geodesic embedding space provides useful information for
applications such as unsupervised object decomposition."
"We study the effect of adversarial perturbations on the task of monocular
depth prediction. Specifically, we explore the ability of small, imperceptible
additive perturbations to selectively alter the perceived geometry of the
scene. We show that such perturbations can not only globally re-scale the
predicted distances from the camera, but also alter the prediction to match a
different target scene. We also show that, when given semantic or instance
information, perturbations can fool the network to alter the depth of specific
categories or instances in the scene, and even remove them while preserving the
rest of the scene. To understand the effect of targeted perturbations, we
conduct experiments on state-of-the-art monocular depth prediction methods. Our
experiments reveal vulnerabilities in monocular depth prediction networks, and
shed light on the biases and context learned by them."
"We tackle the problem of establishing dense pixel-wise correspondences
between a pair of images. In this work, we introduce Dual-Resolution
Correspondence Networks (DualRC-Net), to obtain pixel-wise correspondences in a
coarse-to-fine manner. DualRC-Net extracts both coarse- and fine- resolution
feature maps. The coarse maps are used to produce a full but coarse 4D
correlation tensor, which is then refined by a learnable neighbourhood
consensus module. The fine-resolution feature maps are used to obtain the final
dense correspondences guided by the refined coarse 4D correlation tensor. The
selected coarse-resolution matching scores allow the fine-resolution features
to focus only on a limited number of possible matches with high confidence. In
this way, DualRC-Net dramatically increases matching reliability and
localisation accuracy, while avoiding to apply the expensive 4D convolution
kernels on fine-resolution feature maps. We comprehensively evaluate our method
on large-scale public benchmarks including HPatches, InLoc, and Aachen
Day-Night. It achieves the state-of-the-art results on all of them."
"How can we tell whether an image has been mirrored? While we understand the
geometry of mirror reflections very well, less has been said about how it
affects distributions of imagery at scale, despite widespread use for data
augmentation in computer vision. In this paper, we investigate how the
statistics of visual data are changed by reflection. We refer to these changes
as ""visual chirality"", after the concept of geometric chirality - the notion of
objects that are distinct from their mirror image. Our analysis of visual
chirality reveals surprising results, including low-level chiral signals
pervading imagery stemming from image processing in cameras, to the ability to
discover visual chirality in images of people and faces. Our work has
implications for data augmentation, self-supervised learning, and image
forensics."
"In the past few years, numerous Deep Neural Network (DNN) models and
frameworks have been developed to tackle the problem of real-time object
detection from RGB images. Ordinary object detection approaches process
information from the images only, and they are oblivious to the camera pose
with regard to the environment and the scale of the environment. On the other
hand, mobile Augmented Reality (AR) frameworks can continuously track a
camera's pose within the scene and can estimate the correct scale of the
environment by using Visual-Inertial Odometry (VIO). In this paper, we propose
a novel approach that combines the geometric information from VIO with semantic
information from object detectors to improve the performance of object
detection on mobile devices. Our approach includes three components: (1) an
image orientation correction method, (2) a scale-based filtering approach, and
(3) an online semantic map. Each component takes advantage of the different
characteristics of the VIO-based AR framework. We implemented the AR-enhanced
features using ARCore and the SSD Mobilenet model on Android phones. To
validate our approach, we manually labeled objects in image sequences taken
from 12 room-scale AR sessions. The results show that our approach can improve
on the accuracy of generic object detectors by 12% on our dataset."
"Skeleton-based Human Activity Recognition has achieved great interest in
recent years as skeleton data has demonstrated being robust to illumination
changes, body scales, dynamic camera views, and complex background. In
particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated
to be effective in learning both spatial and temporal dependencies on
non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding
of the latent information underlying the 3D skeleton is still an open problem,
especially when it comes to extracting effective information from joint motion
patterns and their correlations. In this work, we propose a novel
Spatial-Temporal Transformer network (ST-TR) which models dependencies between
joints using the Transformer self-attention operator. In our ST-TR model, a
Spatial Self-Attention module (SSA) is used to understand intra-frame
interactions between different body parts, and a Temporal Self-Attention module
(TSA) to model inter-frame correlations. The two are combined in a two-stream
network, whose performance is evaluated on three large-scale datasets,
NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving
backbone results. Compared with methods that use the same input data, the
proposed ST-TR achieves state-of-the-art performance on all datasets when using
joints' coordinates as input, and results on-par with state-of-the-art when
adding bones information."
"Machine learning (ML) systems have introduced significant advances in various
fields, due to the introduction of highly complex models. Despite their
success, it has been shown multiple times that machine learning models are
prone to imperceptible perturbations that can severely degrade their accuracy.
So far, existing studies have primarily focused on models where supervision
across all classes were available. In constrast, Zero-shot Learning (ZSL) and
Generalized Zero-shot Learning (GZSL) tasks inherently lack supervision across
all classes. In this paper, we present a study aimed on evaluating the
adversarial robustness of ZSL and GZSL models. We leverage the well-established
label embedding model and subject it to a set of established adversarial
attacks and defenses across multiple datasets. In addition to creating possibly
the first benchmark on adversarial robustness of ZSL models, we also present
analyses on important points that require attention for better interpretation
of ZSL robustness results. We hope these points, along with the benchmark, will
help researchers establish a better understanding what challenges lie ahead and
help guide their work."
"Depth cameras are a prominent perception system for robotics, especially when
operating in natural unstructured environments. Industrial applications,
however, typically involve reflective objects under harsh lighting conditions,
a challenging scenario for depth cameras, as it induces numerous reflections
and deflections, leading to loss of robustness and deteriorated accuracy. Here,
we developed a deep model to correct the depth channel in RGBD images, aiming
to restore the depth information to the required accuracy. To train the model,
we created a novel industrial dataset that we now present to the public. The
data was collected with low-end depth cameras and the ground truth depth was
generated by multi-view fusion."
"Seeking effective neural networks is a critical and practical field in deep
learning. Besides designing the depth, type of convolution, normalization, and
nonlinearities, the topological connectivity of neural networks is also
important. Previous principles of rule-based modular design simplify the
difficulty of building an effective architecture, but constrain the possible
topologies in limited spaces. In this paper, we attempt to optimize the
connectivity in neural networks. We propose a topological perspective to
represent a network into a complete graph for analysis, where nodes carry out
aggregation and transformation of features, and edges determine the flow of
information. By assigning learnable parameters to the edges which reflect the
magnitude of connections, the learning process can be performed in a
differentiable manner. We further attach auxiliary sparsity constraint to the
distribution of connectedness, which promotes the learned topology focus on
critical connections. This learning process is compatible with existing
networks and owns adaptability to larger search spaces and different tasks.
Quantitative results of experiments reflect the learned connectivity is
superior to traditional rule-based ones, such as random, residual, and
complete. In addition, it obtains significant improvements in image
classification and object detection without introducing excessive computation
burden."
"Differentiable neural architecture search (DARTS) has gained much success in
discovering flexible and diverse cell types. To reduce the evaluation gap, the
supernet is expected to have identical layers with the target network. However,
even for this consistent search, the searched cells often suffer from poor
performance, especially for the supernet with fewer layers, as current DARTS
methods are prone to wide and shallow cells, and this topology collapse induces
sub-optimal searched cells. In this paper, we alleviate this issue by endowing
the cells with explicit stretchability, so the search can be directly
implemented on our stretchable cells for both operation type and topology
simultaneously. Concretely, we introduce a set of topological variables and a
combinatorial probabilistic distribution to explicitly model the target
topology. With more diverse and complex topologies, our method adapts well for
various layer numbers. Extensive experiments on CIFAR-10 and ImageNet show that
our stretchable cells obtain better performance with fewer layers and
parameters. For example, our method can improve DARTS by 0.28\% accuracy on
CIFAR-10 dataset with 45\% parameters reduced or 2.9\% with similar FLOPs on
ImageNet dataset."
"Deep Convolutional Neural Networks (CNNs) are powerful models that have
achieved excellent performance on difficult computer vision tasks. Although
CNNs perform well whenever large labeled training samples are available, they
work badly on video frame synthesis due to objects deforming and moving, scene
lighting changes, and cameras moving in video sequence. In this paper, we
present a novel and general end-to-end architecture, called convolutional
Transformer or ConvTransformer, for video frame sequence learning and video
frame synthesis. The core ingredient of ConvTransformer is the proposed
attention layer, i.e., multi-head convolutional self-attention layer, that
learns the sequential dependence of video sequence. ConvTransformer uses an
encoder, built upon multi-head convolutional self-attention layer, to encode
the sequential dependence between the input frames, and then a decoder decodes
the long-term dependence between the target synthesized frames and the input
frames. Experiments on video future frame extrapolation task show
ConvTransformer to be superior in quality while being more parallelizable to
recent approaches built upon convolutional LSTM (ConvLSTM). To the best of our
knowledge, this is the first time that ConvTransformer architecture is proposed
and applied to video frame synthesis."
"In recent times, there have been increasing accusations on artificial
intelligence systems and algorithms of computer vision of possessing implicit
biases. Even though these conversations are more prevalent now and systems are
improving by performing extensive testing and broadening their horizon, biases
still do exist. One such class of systems where bias is said to exist is facial
recognition systems, where bias has been observed on the basis of gender,
ethnicity, skin tone and other facial attributes. This is even more disturbing,
given the fact that these systems are used in practically every sector of the
industries today. From as critical as criminal identification to as simple as
getting your attendance registered, these systems have gained a huge market,
especially in recent years. That in itself is a good enough reason for
developers of these systems to ensure that the bias is kept to a bare minimum
or ideally non-existent, to avoid major issues like favoring a particular
gender, race, or class of people or rather making a class of people susceptible
to false accusations due to inability of these systems to correctly recognize
those people."
"Instance segmentation methods require large datasets with expensive and thus
limited instance-level mask labels. Partially supervised instance segmentation
aims to improve mask prediction with limited mask labels by utilizing the more
abundant weak box labels. In this work, we show that a class agnostic mask
head, commonly used in partially supervised instance segmentation, has
difficulties learning a general concept of foreground for the weakly annotated
classes using box supervision only. To resolve this problem we introduce an
object mask prior (OMP) that provides the mask head with the general concept of
foreground implicitly learned by the box classification head under the
supervision of all classes. This helps the class agnostic mask head to focus on
the primary object in a region of interest (RoI) and improves generalization to
the weakly annotated classes. We test our approach on the COCO dataset using
different splits of strongly and weakly supervised classes. Our approach
significantly improves over the Mask R-CNN baseline and obtains competitive
performance with the state-of-the-art, while offering a much simpler
architecture."
"This paper addresses the problem of generating dense point clouds from given
sparse point clouds to model the underlying geometric structures of
objects/scenes. To tackle this challenging issue, we propose a novel end-to-end
learning-based framework. Specifically, by taking advantage of the linear
approximation theorem, we first formulate the problem explicitly, which boils
down to determining the interpolation weights and high-order approximation
errors. Then, we design a lightweight neural network to adaptively learn
unified and sorted interpolation weights as well as the high-order refinements,
by analyzing the local geometry of the input point cloud. The proposed method
can be interpreted by the explicit formulation, and thus is more
memory-efficient than existing ones. In sharp contrast to the existing methods
that work only for a pre-defined and fixed upsampling factor, the proposed
framework only requires a single neural network with one-time training to
handle various upsampling factors within a typical range, which is highly
desired in real-world applications. In addition, we propose a simple yet
effective training strategy to drive such a flexible ability. In addition, our
method can handle non-uniformly distributed and noisy data well. Extensive
experiments on both synthetic and real-world data demonstrate the superiority
of the proposed method over state-of-the-art methods both quantitatively and
qualitatively."
"Multi-task learning is widely used in computer vision. Currently, object
detection models utilize shared feature map to complete classification and
localization tasks simultaneously. By comparing the performance between the
original Faster R-CNN and that with partially separated feature maps, we show
that: (1) Sharing high-level features for the classification and localization
tasks is sub-optimal; (2) Large stride is beneficial for classification but
harmful for localization; (3) Global context information could improve the
performance of classification. Based on these findings, we proposed a paradigm
called Gap-optimized region based convolutional network (G-RCN), which aims to
separating these two tasks and optimizing the gap between them. The paradigm
was firstly applied to correct the current ResNet protocol by simply reducing
the stride and moving the Conv5 block from the head to the feature extraction
network, which brings 3.6 improvement of AP70 on the PASCAL VOC dataset and 1.5
improvement of AP on the COCO dataset for ResNet50. Next, the new method is
applied on the Faster R-CNN with backbone of VGG16,ResNet50 and ResNet101,
which brings above 2.0 improvement of AP70 on the PASCAL VOC dataset and above
1.9 improvement of AP on the COCO dataset. Noticeably, the implementation of
G-RCN only involves a few structural modifications, with no extra module added."
"We consider the generic deep image enhancement problem where an input image
is transformed into a perceptually better-looking image. Recent methods for
image enhancement consider the problem by performing style transfer and image
restoration. The methods mostly fall into two categories: training data-based
and training data-independent (deep internal learning methods). We perform
image enhancement in the deep internal learning framework. Our Deep Internal
Learning for Image Enhancement framework enhances content features and style
features and uses contextual content loss for preserving image context in the
enhanced image. We show results on both hazy and noisy image enhancement. To
validate the results, we use structure similarity and perceptual error, which
is efficient in measuring the unrealistic deformation present in the images. We
show that the proposed framework outperforms the relevant state-of-the-art
works for image enhancement."
"Periocular biometric, the peripheral area of the ocular, is a collaborative
alternative to the face, especially when the face is occluded or masked.
However, in practice, sole periocular biometric capture the least salient
facial features, thereby lacking discriminative information, particularly in
wild environments. To address these problems, we transfer discriminatory
information from the face to support the training of a periocular network by
using knowledge distillation. Specifically, we leverage face images for
periocular embedding learning, but periocular alone is utilized for identity
identification or verification. To enhance periocular embeddings by face
effectively, we proposeConsistent Knowledge Distillation (CKD) that imposes
consistency between face and periocular networks across prediction and feature
layers. We find that imposing consistency at the prediction layer enables (1)
extraction of global discriminative relationship information from face images
and (2) effective transfer of the information from the face network to the
periocular network. Particularly, consistency regularizes the prediction units
to extract and store profound inter-class relationship information of face
images. (3) The feature layer consistency, on the other hand, makes the
periocular features robust against identity-irrelevant attributes. Overall, CKD
empowers the sole periocular network to produce robust discriminative
embeddings for periocular recognition in the wild. We theoretically and
empirically validate the core principles of the distillation mechanism in CKD,
discovering that CKD is equivalent to label smoothing with a novel
sparsity-oriented regularizer that helps the network prediction to capture the
global discriminative relationship. Extensive experiments reveal that CKD
achieves state-of-the-art results on standard periocular recognition benchmark
datasets."
"This study proposes a novel framework for spectral unmixing by using 1D
convolution kernels and spectral uncertainty. High-level representations are
computed from data, and they are further modeled with the Multinomial Mixture
Model to estimate fractions under severe spectral uncertainty. Furthermore, a
new trainable uncertainty term based on a nonlinear neural network model is
introduced in the reconstruction step. All uncertainty models are optimized by
Wasserstein Generative Adversarial Network (WGAN) to improve stability and
capture uncertainty. Experiments are performed on both real and synthetic
datasets. The results validate that the proposed method obtains
state-of-the-art performance, especially for the real datasets compared to the
baselines. Project page at: https://github.com/savasozkan/dscn."
"With the development of the economy, the number of financial tickets
increases rapidly. The traditional manual invoice reimbursement and financial
accounting system bring more and more burden to financial accountants.
Therefore, based on the research and analysis of a large number of real
financial ticket data, we designed an accurate and efficient all contents text
detection and recognition method based on deep learning. This method has higher
recognition accuracy and recall rate and can meet the actual requirements of
financial accounting work. In addition, we propose a Financial Ticket Character
Recognition Framework (FTCRF). According to the characteristics of Chinese
character recognition, this framework contains a two-step information
extraction method, which can improve the speed of Chinese character
recognition. The experimental results show that the average recognition
accuracy of this method is 91.75\% for character sequence and 87\% for the
whole ticket. The availability and effectiveness of this method are verified by
a commercial application system, which significantly improves the efficiency of
the financial accounting system."
"Self-attention learns pairwise interactions to model long-range dependencies,
yielding great improvements for video action recognition. In this paper, we
seek a deeper understanding of self-attention for temporal modeling in videos.
We first demonstrate that the entangled modeling of spatio-temporal information
by flattening all pixels is sub-optimal, failing to capture temporal
relationships among frames explicitly. To this end, we introduce Global
Temporal Attention (GTA), which performs global temporal attention on top of
spatial attention in a decoupled manner. We apply GTA on both pixels and
semantically similar regions to capture temporal relationships at different
levels of spatial granularity. Unlike conventional self-attention that computes
an instance-specific attention matrix, GTA directly learns a global attention
matrix that is intended to encode temporal structures that generalize across
different samples. We further augment GTA with a cross-channel multi-head
fashion to exploit channel interactions for better temporal modeling. Extensive
experiments on 2D and 3D networks demonstrate that our approach consistently
enhances temporal modeling and provides state-of-the-art performance on three
video action recognition datasets."
"Non-local operations are usually used to capture long-range dependencies via
aggregating global context to each position recently. However, most of the
methods cannot preserve object shapes since they only focus on feature
similarity but ignore proximity between central and other positions for
capturing long-range dependencies, while shape-awareness is beneficial to many
computer vision tasks. In this paper, we propose a Semi-Global Shape-aware
Network (SGSNet) considering both feature similarity and proximity for
preserving object shapes when modeling long-range dependencies. A hierarchical
way is taken to aggregate global context. In the first level, each position in
the whole feature map only aggregates contextual information in vertical and
horizontal directions according to both similarity and proximity. And then the
result is input into the second level to do the same operations. By this
hierarchical way, each central position gains supports from all other
positions, and the combination of similarity and proximity makes each position
gain supports mostly from the same semantic object. Moreover, we also propose a
linear time algorithm for the aggregation of contextual information, where each
of rows and columns in the feature map is treated as a binary tree to reduce
similarity computation cost. Experiments on semantic segmentation and image
retrieval show that adding SGSNet to existing networks gains solid improvements
on both accuracy and efficiency."
"Although deep learning has enabled a huge leap forward in image inpainting,
current methods are often unable to synthesize realistic high-frequency
details. In this paper, we propose applying super-resolution to coarsely
reconstructed outputs, refining them at high resolution, and then downscaling
the output to the original resolution. By introducing high-resolution images to
the refinement network, our framework is able to reconstruct finer details that
are usually smoothed out due to spectral bias - the tendency of neural networks
to reconstruct low frequencies better than high frequencies. To assist training
the refinement network on large upscaled holes, we propose a progressive
learning technique in which the size of the missing regions increases as
training progresses. Our zoom-in, refine and zoom-out strategy, combined with
high-resolution supervision and progressive learning, constitutes a
framework-agnostic approach for enhancing high-frequency details that can be
applied to any CNN-based inpainting method. We provide qualitative and
quantitative evaluations along with an ablation analysis to show the
effectiveness of our approach. This seemingly simple, yet powerful approach,
outperforms state-of-the-art inpainting methods. Our code is available in
https://github.com/google/zoom-to-inpaint"
"While medical images such as computed tomography (CT) are stored in DICOM
format in hospital PACS, it is still quite routine in many countries to print a
film as a transferable medium for the purposes of self-storage and secondary
consultation. Also, with the ubiquitousness of mobile phone cameras, it is
quite common to take pictures of the CT films, which unfortunately suffer from
geometric deformation and illumination variation. In this work, we study the
problem of recovering a CT film, which marks the first attempt in the
literature, to the best of our knowledge. We start with building a large-scale
head CT film database CTFilm20K, consisting of approximately 20,000 pictures,
using the widely used computer graphics software Blender. We also record all
accompanying information related to the geometric deformation (such as 3D
coordinate, depth, normal, and UV maps) and illumination variation (such as
albedo map). Then we propose a deep framework to disentangle geometric
deformation and illumination variation using the multiple maps extracted from
the CT films to collaboratively guide the recovery process. Extensive
experiments on simulated and real images demonstrate the superiority of our
approach over the previous approaches. We plan to open source the simulated
images and deep models for promoting the research on CT film recovery
(https://anonymous.4open.science/r/e6b1f6e3-9b36-423f-a225-55b7d0b55523/)."
"Interpretation of Airborne Laser Scanning (ALS) point clouds is a critical
procedure for producing various geo-information products like 3D city models,
digital terrain models and land use maps. In this paper, we present a local and
global encoder network (LGENet) for semantic segmentation of ALS point clouds.
Adapting the KPConv network, we first extract features by both 2D and 3D point
convolutions to allow the network to learn more representative local geometry.
Then global encoders are used in the network to exploit contextual information
at the object and point level. We design a segment-based Edge Conditioned
Convolution to encode the global context between segments. We apply a
spatial-channel attention module at the end of the network, which not only
captures the global interdependencies between points but also models
interactions between channels. We evaluate our method on two ALS datasets
namely, the ISPRS benchmark dataset and DCF2019 dataset. For the ISPRS
benchmark dataset, our model achieves state-of-the-art results with an overall
accuracy of 0.845 and an average F1 score of 0.737. With regards to the DFC2019
dataset, our proposed network achieves an overall accuracy of 0.984 and an
average F1 score of 0.834."
"The objective of this work is to deblur face videos. We propose a method that
tackles this problem from two directions: (1) enhancing the blurry frames, and
(2) treating the blurry frames as missing values and estimate them by
interpolation. These approaches are complementary to each other, and their
combination outperforms individual ones. We also introduce a novel module that
leverages the structure of faces for finding positional offsets between video
frames. This module can be integrated into the processing pipelines of both
approaches, improving the quality of the final outcome. Experiments on three
real and synthetically generated blurry video datasets show that our method
outperforms the previous state-of-the-art methods by a large margin in terms of
both quantitative and qualitative results."
"We study the finger vein (FV) sensor model identification task using a deep
learning approach. So far, for this biometric modality, only correlation-based
PRNU and texture descriptor-based methods have been applied. We employ five
prominent CNN architectures covering a wide range of CNN family models,
including VGG16, ResNet, and the Xception model. In addition, a novel
architecture termed FV2021 is proposed in this work, which excels by its
compactness and a low number of parameters to be trained. Original samples, as
well as the region of interest data from eight publicly accessible FV datasets,
are used in experimentation. An excellent sensor identification AUC-ROC score
of 1.0 for patches of uncropped samples and 0.9997 for ROI samples have been
achieved. The comparison with former methods shows that the CNN-based approach
is superior and improved the results."
"Low-quality face image restoration is a popular research direction in today's
computer vision field. It can be used as a pre-work for tasks such as face
detection and face recognition. At present, there is a lot of work to solve the
problem of low-quality faces under various environmental conditions. This paper
mainly focuses on the restoration of motion-blurred faces. In increasingly
abundant mobile scenes, the fast recovery of motion-blurred faces can bring
highly effective speed improvements in tasks such as face matching. In order to
achieve this goal, a deblurring method for motion-blurred facial image signals
based on generative adversarial networks(GANs) is proposed. It uses an
end-to-end method to train a sharp image generator, i.e., a processor for
motion-blurred facial images. This paper introduce the processing progress of
motion-blurred images, the development and changes of GANs and some basic
concepts. After that, it give the details of network structure and training
optimization design of the image processor. Then we conducted a motion blur
image generation experiment on some general facial data set, and used the pairs
of blurred and sharp face image data to perform the training and testing
experiments of the processor GAN, and gave some visual displays. Finally, MTCNN
is used to detect the faces of the image generated by the deblurring processor,
and compare it with the result of the blurred image. From the results, the
processing effect of the deblurring processor on the motion-blurred picture has
a significant improvement both in terms of intuition and evaluation indicators
of face detection."
"When trying to independently apply image-trained algorithms to successive
frames in videos, noxious flickering tends to appear. State-of-the-art
post-processing techniques that aim at fostering temporal consistency, generate
other temporal artifacts and visually alter the style of videos. We propose a
postprocessing model, agnostic to the transformation applied to videos (e.g.
style transfer, image manipulation using GANs, etc.), in the form of a
recurrent neural network. Our model is trained using a Ping Pong procedure and
its corresponding loss, recently introduced for GAN video generation, as well
as a novel style preserving perceptual loss. The former improves long-term
temporal consistency learning, while the latter fosters style preservation. We
evaluate our model on the DAVIS and videvo.net datasets and show that our
approach offers state-of-the-art results concerning flicker removal, and better
keeps the overall style of the videos than previous approaches."
"We present ALADIN (All Layer AdaIN); a novel architecture for searching
images based on the similarity of their artistic style. Representation learning
is critical to visual search, where distance in the learned search embedding
reflects image similarity. Learning an embedding that discriminates
fine-grained variations in style is hard, due to the difficulty of defining and
labelling style. ALADIN takes a weakly supervised approach to learning a
representation for fine-grained style similarity of digital artworks,
leveraging BAM-FG, a novel large-scale dataset of user generated content
groupings gathered from the web. ALADIN sets a new state of the art accuracy
for style-based visual search over both coarse labelled style data (BAM) and
BAM-FG; a new 2.62 million image dataset of 310,000 fine-grained style
groupings also contributed by this work."
"Impressive progress in 3D shape extraction led to representations that can
capture object geometries with high fidelity. In parallel, primitive-based
methods seek to represent objects as semantically consistent part arrangements.
However, due to the simplicity of existing primitive representations, these
methods fail to accurately reconstruct 3D shapes using a small number of
primitives/parts. We address the trade-off between reconstruction quality and
number of parts with Neural Parts, a novel 3D primitive representation that
defines primitives using an Invertible Neural Network (INN) which implements
homeomorphic mappings between a sphere and the target object. The INN allows us
to compute the inverse mapping of the homeomorphism, which in turn, enables the
efficient computation of both the implicit surface function of a primitive and
its mesh, without any additional post-processing. Our model learns to parse 3D
objects into semantically consistent part arrangements without any part-level
supervision. Evaluations on ShapeNet, D-FAUST and FreiHAND demonstrate that our
primitives can capture complex geometries and thus simultaneously achieve
geometrically accurate as well as interpretable reconstructions using an order
of magnitude fewer primitives than state-of-the-art shape abstraction methods."
"The cost volume, capturing the similarity of possible correspondences across
two input images, is a key ingredient in state-of-the-art optical flow
approaches. When sampling correspondences to build the cost volume, a large
neighborhood radius is required to deal with large displacements, introducing a
significant computational burden. To address this, coarse-to-fine or recurrent
processing of the cost volume is usually adopted, where correspondence sampling
in a local neighborhood with a small radius suffices. In this paper, we propose
an alternative by constructing cost volumes with different dilation factors to
capture small and large displacements simultaneously. A U-Net with skip
connections is employed to convert the dilated cost volumes into interpolation
weights between all possible captured displacements to get the optical flow.
Our proposed model DCVNet only needs to process the cost volume once in a
simple feedforward manner and does not rely on the sequential processing
strategy. DCVNet obtains comparable accuracy to existing approaches and
achieves real-time inference (30 fps on a mid-end 1080ti GPU). The code and
model weights are available at https://github.com/neu-vi/ezflow."
"We present a recurrent agent who perceives surroundings through a series of
discrete fixations. At each timestep, the agent imagines a variety of plausible
scenes consistent with the fixation history. The next fixation is planned using
uncertainty in the content of the imagined scenes. As time progresses, the
agent becomes more certain about the content of the surrounding, and the
variety in the imagined scenes reduces. The agent is built using a variational
autoencoder and normalizing flows, and trained in an unsupervised manner on a
proxy task of scene-reconstruction. The latent representations of the imagined
scenes are found to be useful for performing pixel-level and scene-level tasks
by higher-order modules. The agent is tested on various 2D and 3D datasets."
"How to effectively represent camera pose is an essential problem in 3D
computer vision, especially in tasks such as camera pose regression and novel
view synthesis. Traditionally, 3D position of the camera is represented by
Cartesian coordinate and the orientation is represented by Euler angle or
quaternions. These representations are manually designed, which may not be the
most effective representation for downstream tasks. In this work, we propose an
approach to learn neural representations of camera poses and 3D scenes, coupled
with neural representations of local camera movements. Specifically, the camera
pose and 3D scene are represented as vectors and the local camera movement is
represented as a matrix operating on the vector of the camera pose. We
demonstrate that the camera movement can further be parametrized by a matrix
Lie algebra that underlies a rotation system in the neural space. The vector
representations are then concatenated and generate the posed 2D image through a
decoder network. The model is learned from only posed 2D images and
corresponding camera poses, without access to depths or shapes. We conduct
extensive experiments on synthetic and real datasets. The results show that
compared with other camera pose representations, our learned representation is
more robust to noise in novel view synthesis and more effective in camera pose
regression."
"Depth map records distance between the viewpoint and objects in the scene,
which plays a critical role in many real-world applications. However, depth map
captured by consumer-grade RGB-D cameras suffers from low spatial resolution.
Guided depth map super-resolution (DSR) is a popular approach to address this
problem, which attempts to restore a high-resolution (HR) depth map from the
input low-resolution (LR) depth and its coupled HR RGB image that serves as the
guidance.
  The most challenging problems for guided DSR are how to correctly select
consistent structures and propagate them, and properly handle inconsistent
ones. In this paper, we propose a novel attention-based hierarchical
multi-modal fusion (AHMF) network for guided DSR. Specifically, to effectively
extract and combine relevant information from LR depth and HR guidance, we
propose a multi-modal attention based fusion (MMAF) strategy for hierarchical
convolutional layers, including a feature enhance block to select valuable
features and a feature recalibration block to unify the similarity metrics of
modalities with different appearance characteristics. Furthermore, we propose a
bi-directional hierarchical feature collaboration (BHFC) module to fully
leverage low-level spatial information and high-level structure information
among multi-scale features. Experimental results show that our approach
outperforms state-of-the-art methods in terms of reconstruction accuracy,
running speed and memory efficiency."
"People touch their face 23 times an hour, they cross their arms and legs, put
their hands on their hips, etc. While many images of people contain some form
of self-contact, current 3D human pose and shape (HPS) regression methods
typically fail to estimate this contact. To address this, we develop new
datasets and methods that significantly improve human pose estimation with
self-contact. First, we create a dataset of 3D Contact Poses (3DCP) containing
SMPL-X bodies fit to 3D scans as well as poses from AMASS, which we refine to
ensure good contact. Second, we leverage this to create the Mimic-The-Pose
(MTP) dataset of images, collected via Amazon Mechanical Turk, containing
people mimicking the 3DCP poses with selfcontact. Third, we develop a novel HPS
optimization method, SMPLify-XMC, that includes contact constraints and uses
the known 3DCP body pose during fitting to create near ground-truth poses for
MTP images. Fourth, for more image variety, we label a dataset of in-the-wild
images with Discrete Self-Contact (DSC) information and use another new
optimization method, SMPLify-DC, that exploits discrete contacts during pose
optimization. Finally, we use our datasets during SPIN training to learn a new
3D human pose regressor, called TUCH (Towards Understanding Contact in Humans).
We show that the new self-contact training data significantly improves 3D human
pose estimates on withheld test data and existing datasets like 3DPW. Not only
does our method improve results for self-contact poses, but it also improves
accuracy for non-contact poses. The code and data are available for research
purposes at https://tuch.is.tue.mpg.de."
"We introduce DexYCB, a new dataset for capturing hand grasping of objects. We
first compare DexYCB with a related one through cross-dataset evaluation. We
then present a thorough benchmark of state-of-the-art approaches on three
relevant tasks: 2D object and keypoint detection, 6D object pose estimation,
and 3D hand pose estimation. Finally, we evaluate a new robotics-relevant task:
generating safe robot grasps in human-to-robot object handover. Dataset and
code are available at https://dex-ycb.github.io."
"t-distributed stochastic neighbor embedding (t-SNE) is a well-established
visualization method for complex high-dimensional data. However, the original
t-SNE method is nonparametric, stochastic, and often cannot well prevserve the
global structure of data as it emphasizes local neighborhood. With t-SNE as a
reference, we propose to combine the deep neural network (DNN) with the
mathematical-grounded embedding rules for high-dimensional data embedding. We
first introduce a deep embedding network (DEN) framework, which can learn a
parametric mapping from high-dimensional space to low-dimensional embedding.
DEN has a flexible architecture that can accommodate different input data
(vector, image, or tensor) and loss functions. To improve the embedding
performance, a recursive training strategy is proposed to make use of the
latent representations extracted by DEN. Finally, we propose a two-stage loss
function combining the advantages of two popular embedding methods, namely,
t-SNE and uniform manifold approximation and projection (UMAP), for optimal
visualization effect. We name the proposed method Deep Recursive Embedding
(DRE), which optimizes DEN with a recursive training strategy and two-stage
losse. Our experiments demonstrated the excellent performance of the proposed
DRE method on high-dimensional data embedding, across a variety of public
databases. Remarkably, our comparative results suggested that our proposed DRE
could lead to improved global structure preservation."
"We present an efficient high-resolution network, Lite-HRNet, for human pose
estimation. We start by simply applying the efficient shuffle block in
ShuffleNet to HRNet (high-resolution network), yielding stronger performance
over popular lightweight networks, such as MobileNet, ShuffleNet, and Small
HRNet.
  We find that the heavily-used pointwise (1x1) convolutions in shuffle blocks
become the computational bottleneck. We introduce a lightweight unit,
conditional channel weighting, to replace costly pointwise (1x1) convolutions
in shuffle blocks. The complexity of channel weighting is linear w.r.t the
number of channels and lower than the quadratic time complexity for pointwise
convolutions. Our solution learns the weights from all the channels and over
multiple resolutions that are readily available in the parallel branches in
HRNet. It uses the weights as the bridge to exchange information across
channels and resolutions, compensating the role played by the pointwise (1x1)
convolution. Lite-HRNet demonstrates superior results on human pose estimation
over popular lightweight networks. Moreover, Lite-HRNet can be easily applied
to semantic segmentation task in the same lightweight manner. The code and
models have been publicly available at https://github.com/HRNet/Lite-HRNet."
"People's visual experiences of the world are easy to carve up and examine
along natural language boundaries, e.g., by category labels, attribute labels,
etc. However, it is more difficult to elicit detailed visuospatial information
about what a person attends to, e.g., the specific shape of a tree. Paying
attention to the shapes of things not only feeds into well defined tasks like
visual category learning, but it is also what enables us to differentiate
similarly named objects and to take on creative visual pursuits, like
poetically describing the shape of a thing, or finding shapes in the clouds or
stars. We use a new data collection method that elicits people's prioritized
attention to shapes during visual photo inspection by asking them to trace
important parts of the image under varying time constraints. Using data
collected via crowdsourcing over a set of 187 photographs, we examine changes
in patterns of visual attention across individuals, across image types, and
across time constraints."
"Land use as contained in geospatial databases constitutes an essential input
for different applica-tions such as urban management, regional planning and
environmental monitoring. In this paper, a hierarchical deep learning framework
is proposed to verify the land use information. For this purpose, a two-step
strategy is applied. First, given high-resolution aerial images, the land cover
information is determined. To achieve this, an encoder-decoder based
convolutional neural net-work (CNN) is proposed. Second, the pixel-wise land
cover information along with the aerial images serves as input for another CNN
to classify land use. Because the object catalogue of geospatial databases is
frequently constructed in a hierarchical manner, we propose a new CNN-based
method aiming to predict land use in multiple levels hierarchically and
simultaneously. A so called Joint Optimization (JO) is proposed where
predictions are made by selecting the hier-archical tuple over all levels which
has the maximum joint class scores, providing consistent results across the
different levels. The conducted experiments show that the CNN relying on JO
outperforms previous results, achieving an overall accuracy up to 92.5%. In
addition to the individual experiments on two test sites, we investigate
whether data showing different characteristics can improve the results of land
cover and land use classification, when processed together. To do so, we
combine the two datasets and undertake some additional experiments. The results
show that adding more data helps both land cover and land use classification,
especially the identification of underrepre-sented categories, despite their
different characteristics."
"A common strategy to video understanding is to incorporate spatial and motion
information by fusing features derived from RGB frames and optical flow. In
this work, we introduce a new way to leverage semantic segmentation as an
intermediate representation for video understanding and use it in a way that
requires no additional labeling.
  Second, we propose a general framework which learns the intermediate
representations (optical flow and semantic segmentation) jointly with the final
video understanding task and allows the adaptation of the representations to
the end goal. Despite the use of intermediate representations within the
network, during inference, no additional data beyond RGB sequences is needed,
enabling efficient recognition with a single network.
  Finally, we present a way to find the optimal learning configuration by
searching the best loss weighting via evolution. We obtain more powerful visual
representations for videos which lead to performance gains over the
state-of-the-art."
"This work focuses on complete 3D facial geometry prediction, including 3D
facial alignment via 3D face modeling and face orientation estimation using the
proposed multi-task, multi-modal, and multi-representation landmark refinement
network (M$^3$-LRN). Our focus is on the important facial attributes, 3D
landmarks, and we fully utilize their embedded information to guide 3D facial
geometry learning. We first propose a multi-modal and multi-representation
feature aggregation for landmark refinement. Next, we are the first to study
3DMM regression from sparse 3D landmarks and utilize multi-representation
advantage to attain better geometry prediction. We attain the state of the art
from extensive experiments on all tasks of learning 3D facial geometry. We
closely validate contributions of each modality and representation. Our results
are robust across cropped faces, underwater scenarios, and extreme poses.
Specially we adopt only simple and widely used network operations in M$^3$-LRN
and attain a near 20\% improvement on face orientation estimation over the
current best performance. See our project page here."
"Video-text retrieval plays an essential role in multi-modal research and has
been widely used in many real-world web applications. The CLIP (Contrastive
Language-Image Pre-training), an image-language pre-training model, has
demonstrated the power of visual concepts learning from web collected
image-text datasets. In this paper, we propose a CLIP4Clip model to transfer
the knowledge of the CLIP model to video-language retrieval in an end-to-end
manner. Several questions are investigated via empirical studies: 1) Whether
image feature is enough for video-text retrieval? 2) How a post-pretraining on
a large-scale video-text dataset based on the CLIP affect the performance? 3)
What is the practical mechanism to model temporal dependency between video
frames? And 4) The Hyper-parameters sensitivity of the model on video-text
retrieval task. Extensive experimental results present that the CLIP4Clip model
transferred from the CLIP can achieve SOTA results on various video-text
retrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We
release our code at https://github.com/ArrowLuo/CLIP4Clip."
"Learning-based image compression was shown to achieve a competitive
performance with state-of-the-art transform-based codecs. This motivated the
development of new learning-based visual compression standards such as JPEG-AI.
Of particular interest to these emerging standards is the development of
learning-based image compression systems targeting both humans and machines.
This paper is concerned with learning-based compression schemes whose
compressed-domain representations can be utilized to perform visual processing
and computer vision tasks directly in the compressed domain. Such a
characteristic has been incorporated as part of the scope and requirements of
the new emerging JPEG-AI standard. In our work, we adopt the learning-based
JPEG-AI framework for performing material and texture recognition using the
compressed-domain latent representation at varing bit-rates. For comparison,
performance results are presented using compressed but fully decoded images in
the pixel domain as well as original uncompressed images. The obtained
performance results show that even though decoded images can degrade the
classification performance of the model trained with original images,
retraining the model with decoded images will largely reduce the performance
gap for the adopted texture dataset. It is also shown that the
compressed-domain classification can yield a competitive performance in terms
of Top-1 and Top-5 accuracy while using a smaller reduced-complexity
classification model."
"Neural implicit 3D representations have emerged as a powerful paradigm for
reconstructing surfaces from multi-view images and synthesizing novel views.
Unfortunately, existing methods such as DVR or IDR require accurate per-pixel
object masks as supervision. At the same time, neural radiance fields have
revolutionized novel view synthesis. However, NeRF's estimated volume density
does not admit accurate surface reconstruction. Our key insight is that
implicit surface models and radiance fields can be formulated in a unified way,
enabling both surface and volume rendering using the same model. This unified
perspective enables novel, more efficient sampling procedures and the ability
to reconstruct accurate surfaces without input masks. We compare our method on
the DTU, BlendedMVS, and a synthetic indoor dataset. Our experiments
demonstrate that we outperform NeRF in terms of reconstruction quality while
performing on par with IDR without requiring masks."
"The performance of object detection methods based on LiDAR information is
heavily impacted by the availability of training data, usually limited to
certain laser devices. As a result, the use of synthetic data is becoming
popular when training neural network models, as both sensor specifications and
driving scenarios can be generated ad-hoc. However, bridging the gap between
virtual and real environments is still an open challenge, as current simulators
cannot completely mimic real LiDAR operation. To tackle this issue, domain
adaptation strategies are usually applied, obtaining remarkable results on
vehicle detection when applied to range view (RV) and bird's eye view (BEV)
projections while failing for smaller road agents. In this paper, we present a
BEV domain adaptation method based on CycleGAN that uses prior semantic
classification in order to preserve the information of small objects of
interest during the domain adaptation process. The quality of the generated
BEVs has been evaluated using a state-of-the-art 3D object detection framework
at KITTI 3D Object Detection Benchmark. The obtained results show the
advantages of the proposed method over the existing alternatives."
"Continual zero-shot learning(CZSL) is a new domain to classify objects
sequentially the model has not seen during training. It is more suitable than
zero-shot and continual learning approaches in real-case scenarios when data
may come continually with only attributes for a few classes and attributes and
features for other classes. Continual learning(CL) suffers from catastrophic
forgetting, and zero-shot learning(ZSL) models cannot classify objects like
state-of-the-art supervised classifiers due to lack of actual data(or features)
during training. This paper proposes a novel continual zero-shot learning
(DVGR-CZSL) model that grows in size with each task and uses generative replay
to update itself with previously learned classes to avoid forgetting. We
demonstrate our hybrid model(DVGR-CZSL) outperforms the baselines and is
effective on several datasets, i.e., CUB, AWA1, AWA2, and aPY. We show our
method is superior in task sequentially learning with ZSL(Zero-Shot Learning).
We also discuss our results on the SUN dataset."
"We develop an approach to learning visual representations that embraces
multimodal data, driven by a combination of intra- and inter-modal similarity
preservation objectives. Unlike existing visual pre-training methods, which
solve a proxy prediction task in a single domain, our method exploits intrinsic
data properties within each modality and semantic information from cross-modal
correlation simultaneously, hence improving the quality of learned visual
representations. By including multimodal training in a unified framework with
different types of contrastive losses, our method can learn more powerful and
generic visual features. We first train our model on COCO and evaluate the
learned visual representations on various downstream tasks including image
classification, object detection, and instance segmentation. For example, the
visual representations pre-trained on COCO by our method achieve
state-of-the-art top-1 validation accuracy of $55.3\%$ on ImageNet
classification, under the common transfer protocol. We also evaluate our method
on the large-scale Stock images dataset and show its effectiveness on
multi-label image tagging, and cross-modal retrieval tasks."
"Recent one-stage object detectors follow a per-pixel prediction approach that
predicts both the object category scores and boundary positions from every
single grid location. However, the most suitable positions for inferring
different targets, i.e., the object category and boundaries, are generally
different. Predicting all these targets from the same grid location thus may
lead to sub-optimal results. In this paper, we analyze the suitable inference
positions for object category and boundaries, and propose a
prediction-target-decoupled detector named PDNet to establish a more flexible
detection paradigm. Our PDNet with the prediction decoupling mechanism encodes
different targets separately in different locations. A learnable prediction
collection module is devised with two sets of dynamic points, i.e., dynamic
boundary points and semantic points, to collect and aggregate the predictions
from the favorable regions for localization and classification. We adopt a
two-step strategy to learn these dynamic point positions, where the prior
positions are estimated for different targets first, and the network further
predicts residual offsets to the positions with better perceptions of the
object properties. Extensive experiments on the MS COCO benchmark demonstrate
the effectiveness and efficiency of our method. With a single
ResNeXt-64x4d-101-DCN as the backbone, our detector achieves 50.1 AP with
single-scale testing, which outperforms the state-of-the-art methods by an
appreciable margin under the same experimental settings.Moreover, our detector
is highly efficient as a one-stage framework. Our code is public at
https://github.com/yangli18/PDNet."
"We propose a novel Synergistic Attention Network (SA-Net) to address the
light field salient object detection by establishing a synergistic effect
between multi-modal features with advanced attention mechanisms. Our SA-Net
exploits the rich information of focal stacks via 3D convolutional neural
networks, decodes the high-level features of multi-modal light field data with
two cascaded synergistic attention modules, and predicts the saliency map using
an effective feature fusion module in a progressive manner. Extensive
experiments on three widely-used benchmark datasets show that our SA-Net
outperforms 28 state-of-the-art models, sufficiently demonstrating its
effectiveness and superiority. Our code is available at
https://github.com/PanoAsh/SA-Net."
"Vision-and-Language (VL) pre-training has shown great potential on many
related downstream tasks, such as Visual Question Answering (VQA), one of the
most popular problems in the VL field. All of these pre-trained models (such as
VisualBERT, ViLBERT, LXMERT and UNITER) are built with Transformer, which
extends the classical attention mechanism to multiple layers and heads. To
investigate why and how these models work on VQA so well, in this paper we
explore the roles of individual heads and layers in Transformer models when
handling $12$ different types of questions. Specifically, we manually remove
(chop) heads (or layers) from a pre-trained VisualBERT model at a time, and
test it on different levels of questions to record its performance. As shown in
the interesting echelon shape of the result matrices, experiments reveal
different heads and layers are responsible for different question types, with
higher-level layers activated by higher-level visual reasoning questions. Based
on this observation, we design a dynamic chopping module that can automatically
remove heads and layers of the VisualBERT at an instance level when dealing
with different questions. Our dynamic chopping module can effectively reduce
the parameters of the original model by 50%, while only damaging the accuracy
by less than 1% on the VQA task."
"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of five primary body signals including
gaze, face, hand, body, and garment from assorted people. 107 synchronized HD
cameras are used to capture 772 distinctive subjects across gender, ethnicity,
age, and style. With the multiview image streams, we reconstruct high fidelity
body expressions using 3D mesh models, which allows representing view-specific
appearance. We demonstrate that HUMBI is highly effective in learning and
reconstructing a complete human model and is complementary to the existing
datasets of human body expressions with limited views and subjects such as
MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets. Based on HUMBI,
we formulate a new benchmark challenge of a pose-guided appearance rendering
task that aims to substantially extend photorealism in modeling diverse human
expressions in 3D, which is the key enabling factor of authentic social
tele-presence. HUMBI is publicly available at http://humbi-data.net"
"Recognizing images with long-tailed distributions remains a challenging
problem while there lacks an interpretable mechanism to solve this problem. In
this study, we formulate Long-tailed recognition as Domain Adaption (LDA), by
modeling the long-tailed distribution as an unbalanced domain and the general
distribution as a balanced domain. Within the balanced domain, we propose to
slack the generalization error bound, which is defined upon the empirical risks
of unbalanced and balanced domains and the divergence between them. We propose
to jointly optimize empirical risks of the unbalanced and balanced domains and
approximate their domain divergence by intra-class and inter-class distances,
with the aim to adapt models trained on the long-tailed distribution to general
distributions in an interpretable way. Experiments on benchmark datasets for
image recognition, object detection, and instance segmentation validate that
our LDA approach, beyond its interpretability, achieves state-of-the-art
performance. Code is available at https://github.com/pengzhiliang/LDA."
"The research focus of scene text detection and recognition has shifted to
arbitrary shape text in recent years, where the text shape representation is a
fundamental problem. An ideal representation should be compact, complete,
efficient, and reusable for subsequent recognition in our opinion. However,
previous representations have flaws in one or more aspects. Thin-Plate-Spline
(TPS) transformation has achieved great success in scene text recognition.
Inspired by this, we reversely think of its usage and sophisticatedly take TPS
as an exquisite representation for arbitrary shape text representation. The TPS
representation is compact, complete, and efficient. With the predicted TPS
parameters, the detected text region can be directly rectified to a
near-horizontal one to assist the subsequent recognition. To further exploit
the potential of the TPS representation, the Border Alignment Loss is proposed.
Based on these designs, we implement the text detector TPSNet, which can be
extended to a text spotter conveniently. Extensive evaluation and ablation of
several public benchmarks demonstrate the effectiveness and superiority of the
proposed method for text representation and spotting. Particularly, TPSNet
achieves the detection F-Measure improvement of 4.4\% (78.4\% vs. 74.0\%) on
Art dataset and the end-to-end spotting F-Measure improvement of 5.0\% (78.5\%
vs. 73.5\%) on Total-Text, which are large margins with no bells and whistles."
"Recently, Transformer-based networks have shown great promise on
skeleton-based action recognition tasks. The ability to capture global and
local dependencies is the key to success while it also brings quadratic
computation and memory cost. Another problem is that previous studies mainly
focus on the relationships among individual joints, which often suffers from
the noisy skeleton joints introduced by the noisy inputs of sensors or
inaccurate estimations. To address the above issues, we propose a novel
Transformer-based network (IIP-Transformer). Instead of exploiting interactions
among individual joints, our IIP-Transformer incorporates body joints and parts
interactions simultaneously and thus can capture both joint-level (intra-part)
and part-level (inter-part) dependencies efficiently and effectively. From the
data aspect, we introduce a part-level skeleton data encoding that
significantly reduces the computational complexity and is more robust to
joint-level skeleton noise. Besides, a new part-level data augmentation is
proposed to improve the performance of the model. On two large-scale datasets,
NTU-RGB+D 60 and NTU RGB+D 120, the proposed IIP-Transformer achieves
the-state-of-art performance with more than 8x less computational complexity
than DSTA-Net, which is the SOTA Transformer-based method."
"Occluded person re-identification (Re-ID) in images captured by multiple
cameras is challenging because the target person is occluded by pedestrians or
objects, especially in crowded scenes. In addition to the processes performed
during holistic person Re-ID, occluded person Re-ID involves the removal of
obstacles and the detection of partially visible body parts. Most existing
methods utilize the off-the-shelf pose or parsing networks as pseudo labels,
which are prone to error. To address these issues, we propose a novel Occlusion
Correction Network (OCNet) that corrects features through relational-weight
learning and obtains diverse and representative features without using external
networks. In addition, we present a simple concept of a center feature in order
to provide an intuitive solution to pedestrian occlusion scenarios.
Furthermore, we suggest the idea of Separation Loss (SL) for focusing on
different parts between global features and part features. We conduct extensive
experiments on five challenging benchmark datasets for occluded and holistic
Re-ID tasks to demonstrate that our method achieves superior performance to
state-of-the-art methods especially on occluded scene."
"The dominant multi-camera 3D detection paradigm is based on explicit 3D
feature construction, which requires complicated indexing of local image-view
features via 3D-to-2D projection. Other methods implicitly introduce geometric
positional encoding and perform global attention (e.g., PETR) to build the
relationship between image tokens and 3D objects. The 3D-to-2D perspective
inconsistency and global attention lead to a weak correlation between
foreground tokens and queries, resulting in slow convergence. We propose
Focal-PETR with instance-guided supervision and spatial alignment module to
adaptively focus object queries on discriminative foreground regions.
Focal-PETR additionally introduces a down-sampling strategy to reduce the
consumption of global attention. Due to the highly parallelized implementation
and down-sampling strategy, our model, without depth supervision, achieves
leading performance on the large-scale nuScenes benchmark and a superior speed
of 30 FPS on a single RTX3090 GPU. Extensive experiments show that our method
outperforms PETR while consuming 3x fewer training hours. The code will be made
publicly available."
"Although existing semi-supervised learning models achieve remarkable success
in learning with unannotated in-distribution data, they mostly fail to learn on
unlabeled data sampled from novel semantic classes due to their closed-set
assumption. In this work, we target a pragmatic but under-explored Generalized
Novel Category Discovery (GNCD) setting. The GNCD setting aims to categorize
unlabeled training data coming from known and novel classes by leveraging the
information of partially labeled known classes. We propose a two-stage
Contrastive Affinity Learning method with auxiliary visual Prompts, dubbed
PromptCAL, to address this challenging problem. Our approach discovers reliable
pairwise sample affinities to learn better semantic clustering of both known
and novel classes for the class token and visual prompts. First, we propose a
discriminative prompt regularization loss to reinforce semantic
discriminativeness of prompt-adapted pre-trained vision transformer for refined
affinity relationships.Besides, we propose contrastive affinity learning to
calibrate semantic representations based on our iterative semi-supervised
affinity graph generation method for semantically-enhanced supervision.
Extensive experimental evaluation demonstrates that our PromptCAL method is
more effective in discovering novel classes even with limited annotations and
surpasses the current state-of-the-art on generic and fine-grained benchmarks
(e.g., with nearly 11% gain on CUB-200, and 9% on ImageNet-100) on overall
accuracy. Our code is available at https://github.com/sheng-eatamath/PromptCAL."
"Point clouds are characterized by irregularity and unstructuredness, which
pose challenges in efficient data exploitation and discriminative feature
extraction. In this paper, we present an unsupervised deep neural architecture
called Flattening-Net to represent irregular 3D point clouds of arbitrary
geometry and topology as a completely regular 2D point geometry image (PGI)
structure, in which coordinates of spatial points are captured in colors of
image pixels. \mr{Intuitively, Flattening-Net implicitly approximates a locally
smooth 3D-to-2D surface flattening process while effectively preserving
neighborhood consistency.} \mr{As a generic representation modality, PGI
inherently encodes the intrinsic property of the underlying manifold structure
and facilitates surface-style point feature aggregation.} To demonstrate its
potential, we construct a unified learning framework directly operating on PGIs
to achieve \mr{diverse types of high-level and low-level} downstream
applications driven by specific task networks, including classification,
segmentation, reconstruction, and upsampling. Extensive experiments demonstrate
that our methods perform favorably against the current state-of-the-art
competitors. We will make the code and data publicly available at
https://github.com/keeganhk/Flattening-Net."
"Sign language is the preferred method of communication of deaf or mute
people, but similar to any language, it is difficult to learn and represents a
significant barrier for those who are hard of hearing or unable to speak. A
person's entire frontal appearance dictates and conveys specific meaning.
However, this frontal appearance can be quantified as a temporal sequence of
human body pose, leading to Sign Language Recognition through the learning of
spatiotemporal dynamics of skeleton keypoints. We propose a novel,
attention-based approach to Sign Language Recognition exclusively built upon
decoupled graph and temporal self-attention: the Sign Language Graph Time
Transformer (SLGTformer). SLGTformer first deconstructs spatiotemporal pose
sequences separately into spatial graphs and temporal windows. SLGTformer then
leverages novel Learnable Graph Relative Positional Encodings (LGRPE) to guide
spatial self-attention with the graph neighborhood context of the human
skeleton. By modeling the temporal dimension as intra- and inter-window
dynamics, we introduce Temporal Twin Self-Attention (TTSA) as the combination
of locally-grouped temporal attention (LTA) and global sub-sampled temporal
attention (GSTA). We demonstrate the effectiveness of SLGTformer on the
World-Level American Sign Language (WLASL) dataset, achieving state-of-the-art
performance with an ensemble-free approach on the keypoint modality. The code
is available at https://github.com/neilsong/slt"
"Semi-supervised learning (SSL) has made significant strides in the field of
remote sensing. Finding a large number of labeled datasets for SSL methods is
uncommon, and manually labeling datasets is expensive and time-consuming.
Furthermore, accurately identifying remote sensing satellite images is more
complicated than it is for conventional images. Class-imbalanced datasets are
another prevalent phenomenon, and models trained on these become biased towards
the majority classes. This becomes a critical issue with an SSL model's subpar
performance. We aim to address the issue of labeling unlabeled data and also
solve the model bias problem due to imbalanced datasets while achieving better
accuracy. To accomplish this, we create ""artificial"" labels and train a model
to have reasonable accuracy. We iteratively redistribute the classes through
resampling using a distribution alignment technique. We use a variety of class
imbalanced satellite image datasets: EuroSAT, UCM, and WHU-RS19. On UCM
balanced dataset, our method outperforms previous methods MSMatch and FixMatch
by 1.21% and 0.6%, respectively. For imbalanced EuroSAT, our method outperforms
MSMatch and FixMatch by 1.08% and 1%, respectively. Our approach significantly
lessens the requirement for labeled data, consistently outperforms alternative
approaches, and resolves the issue of model bias caused by class imbalance in
datasets."
"Model quantization enables the deployment of deep neural networks under
resource-constrained devices. Vector quantization aims at reducing the model
size by indexing model weights with full-precision embeddings, i.e., codewords,
while the index needs to be restored to 32-bit during computation. Binary and
other low-precision quantization methods can reduce the model size up to
32$\times$, however, at the cost of a considerable accuracy drop. In this
paper, we propose an efficient framework for ternary quantization to produce
smaller and more accurate compressed models. By integrating hyperspherical
learning, pruning and reinitialization, our proposed Hyperspherical
Quantization (HQ) method reduces the cosine distance between the full-precision
and ternary weights, thus reducing the bias of the straight-through gradient
estimator during ternary quantization. Compared with existing work at similar
compression levels ($\sim$30$\times$, $\sim$40$\times$), our method
significantly improves the test accuracy and reduces the model size."
"The United States coastline spans 95,471 miles; a distance that cannot be
effectively patrolled or secured by manual human effort alone. Unmanned Aerial
Vehicles (UAVs) equipped with infrared cameras and deep-learning based
algorithms represent a more efficient alternative for identifying and
segmenting objects of interest - namely, ships. However, standard approaches to
training these algorithms require large-scale datasets of densely labeled
infrared maritime images. Such datasets are not publicly available and manually
annotating every pixel in a large-scale dataset would have an extreme labor
cost. In this work we demonstrate that, in the context of segmenting ships in
infrared imagery, weakly-supervising an algorithm with sparsely labeled data
can drastically reduce data labeling costs with minimal impact on system
performance. We apply weakly-supervised learning to an unlabeled dataset of
7055 infrared images sourced from the Naval Air Warfare Center Aircraft
Division (NAWCAD). We find that by sparsely labeling only 32 points per image,
weakly-supervised segmentation models can still effectively detect and segment
ships, with a Jaccard score of up to 0.756."
"In this work, we investigate improving the generalizability of GAN-generated
image detectors by performing data augmentation in the fingerprint domain.
Specifically, we first separate the fingerprints and contents of the
GAN-generated images using an autoencoder based GAN fingerprint extractor,
followed by random perturbations of the fingerprints. Then the original
fingerprints are substituted with the perturbed fingerprints and added to the
original contents, to produce images that are visually invariant but with
distinct fingerprints. The perturbed images can successfully imitate images
generated by different GANs to improve the generalization of the detectors,
which is demonstrated by the spectra visualization. To our knowledge, we are
the first to conduct data augmentation in the fingerprint domain. Our work
explores a novel prospect that is distinct from previous works on spatial and
frequency domain augmentation. Extensive cross-GAN experiments demonstrate the
effectiveness of our method compared to the state-of-the-art methods in
detecting fake images generated by unknown GANs."
"Fine-grained visual recognition is to classify objects with visually similar
appearances into subcategories, which has made great progress with the
development of deep CNNs. However, handling subtle differences between
different subcategories still remains a challenge. In this paper, we propose to
solve this issue in one unified framework from two aspects, i.e., constructing
feature-level interrelationships, and capturing part-level discriminative
features. This framework, namely PArt-guided Relational Transformers (PART), is
proposed to learn the discriminative part features with an automatic part
discovery module, and to explore the intrinsic correlations with a feature
transformation module by adapting the Transformer models from the field of
natural language processing. The part discovery module efficiently discovers
the discriminative regions which are highly-corresponded to the gradient
descent procedure. Then the second feature transformation module builds
correlations within the global embedding and multiple part embedding, enhancing
spatial interactions among semantic pixels. Moreover, our proposed approach
does not rely on additional part branches in the inference time and reaches
state-of-the-art performance on 3 widely-used fine-grained object recognition
benchmarks. Experimental results and explainable visualizations demonstrate the
effectiveness of our proposed approach. The code can be found at
https://github.com/iCVTEAM/PART."
"Parameter tuning is a common issue for many tracking algorithms. In order to
solve this problem, this paper proposes an online parameter tuning to adapt a
tracking algorithm to various scene contexts. In an offline training phase,
this approach learns how to tune the tracker parameters to cope with different
contexts. In the online control phase, once the tracking quality is evaluated
as not good enough, the proposed approach computes the current context and
tunes the tracking parameters using the learned values. The experimental
results show that the proposed approach improves the performance of the
tracking algorithm and outperforms recent state of the art trackers. This paper
brings two contributions: (1) an online tracking evaluation, and (2) a method
to adapt online tracking parameters to scene contexts."
"This work describes a computer vision system that enables pervasive mapping
and monitoring of human attention. The key contribution is that our methodology
enables full 3D recovery of the gaze pointer, human view frustum and associated
human centered measurements directly into an automatically computed 3D model in
real-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3D
modeling, localization and fully automated annotation of ROIs (regions of
interest) within the acquired 3D model. This innovative methodology will open
new avenues for attention studies in real world environments, bringing new
potential into automated processing for human factors technologies."
"Recent reports suggest that a generic supervised deep CNN model trained on a
large-scale dataset reduces, but does not remove, dataset bias on a standard
benchmark. Fine-tuning deep models in a new domain can require a significant
amount of data, which for many applications is simply not available. We propose
a new CNN architecture which introduces an adaptation layer and an additional
domain confusion loss, to learn a representation that is both semantically
meaningful and domain invariant. We additionally show that a domain confusion
metric can be used for model selection to determine the dimension of an
adaptation layer and the best position for the layer in the CNN architecture.
Our proposed adaptation method offers empirical performance which exceeds
previously published results on a standard benchmark visual domain adaptation
task."
"Non-photorealistic rendering techniques work on image features and often
manipulate a set of characteristics such as edges and texture to achieve a
desired depiction of the scene. Most computational photography methods
decompose an image using edge preserving filters and work on the resulting base
and detail layers independently to achieve desired visual effects. We propose a
new approach for content-aware non-photorealistic rendering of images where we
manipulate the visually salient and the non-salient regions separately. We
propose a novel content-aware framework in order to render an image for
applications such as detail exaggeration, artificial blurring and image
abstraction. The processed regions of the image are blended seamlessly for all
these applications. We demonstrate that content awareness of the proposed
method leads to automatic generation of non-photorealistic rendering of the
same image for the different applications mentioned above."
"Face detection and alignment in unconstrained environment are challenging due
to various poses, illuminations and occlusions. Recent studies show that deep
learning approaches can achieve impressive performance on these two tasks. In
this paper, we propose a deep cascaded multi-task framework which exploits the
inherent correlation between them to boost up their performance. In particular,
our framework adopts a cascaded structure with three stages of carefully
designed deep convolutional networks that predict face and landmark location in
a coarse-to-fine manner. In addition, in the learning process, we propose a new
online hard sample mining strategy that can improve the performance
automatically without manual sample selection. Our method achieves superior
accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER
FACE benchmark for face detection, and AFLW benchmark for face alignment, while
keeps real time performance."
"We present a method for learning discriminative filters using a shallow
Convolutional Neural Network (CNN). We encode rotation invariance directly in
the model by tying the weights of groups of filters to several rotated versions
of the canonical filter in the group. These filters can be used to extract
rotation invariant features well-suited for image classification. We test this
learning procedure on a texture classification benchmark, where the
orientations of the training images differ from those of the test images. We
obtain results comparable to the state-of-the-art. Compared to standard shallow
CNNs, the proposed method obtains higher classification performance while
reducing by an order of magnitude the number of parameters to be learned."
"Depth from defocus (DfD) and stereo matching are two most studied passive
depth sensing schemes. The techniques are essentially complementary: DfD can
robustly handle repetitive textures that are problematic for stereo matching
whereas stereo matching is insensitive to defocus blurs and can handle large
depth range. In this paper, we present a unified learning-based technique to
conduct hybrid DfD and stereo matching. Our input is image triplets: a stereo
pair and a defocused image of one of the stereo views. We first apply
depth-guided light field rendering to construct a comprehensive training
dataset for such hybrid sensing setups. Next, we adopt the hourglass network
architecture to separately conduct depth inference from DfD and stereo.
Finally, we exploit different connection methods between the two separate
networks for integrating them into a unified solution to produce high fidelity
3D disparity maps. Comprehensive experiments on real and synthetic data show
that our new learning-based hybrid 3D sensing technique can significantly
improve accuracy and robustness in 3D reconstruction."
"Data acquired from multi-channel sensors is a highly valuable asset to
interpret the environment for a variety of remote sensing applications.
However, low spatial resolution is a critical limitation for previous sensors
and the constituent materials of a scene can be mixed in different fractions
due to their spatial interactions. Spectral unmixing is a technique that allows
us to obtain the material spectral signatures and their fractions from
hyperspectral data. In this paper, we propose a novel endmember extraction and
hyperspectral unmixing scheme, so called \textit{EndNet}, that is based on a
two-staged autoencoder network. This well-known structure is completely
enhanced and restructured by introducing additional layers and a projection
metric (i.e., spectral angle distance (SAD) instead of inner product) to
achieve an optimum solution. Moreover, we present a novel loss function that is
composed of a Kullback-Leibler divergence term with SAD similarity and
additional penalty terms to improve the sparsity of the estimates. These
modifications enable us to set the common properties of endmembers such as
non-linearity and sparsity for autoencoder networks. Lastly, due to the
stochastic-gradient based approach, the method is scalable for large-scale data
and it can be accelerated on Graphical Processing Units (GPUs). To demonstrate
the superiority of our proposed method, we conduct extensive experiments on
several well-known datasets. The results confirm that the proposed method
considerably improves the performance compared to the state-of-the-art
techniques in literature."
"We propose a graphical user interface based groundtruth generation tool in
this paper. Here, annotation of an input document image is done based on the
foreground pixels. Foreground pixels are grouped together with user interaction
to form labeling units. These units are then labeled by the user with the user
defined labels. The output produced by the tool is an image with an XML file
containing its metadata information. This annotated data can be further used in
different applications of document image analysis."
"Classifying pages or text lines into font categories aids transcription
because single font Optical Character Recognition (OCR) is generally more
accurate than omni-font OCR. We present a simple framework based on
Convolutional Neural Networks (CNNs), where a CNN is trained to classify small
patches of text into predefined font classes. To classify page or line images,
we average the CNN predictions over densely extracted patches. We show that
this method achieves state-of-the-art performance on a challenging dataset of
40 Arabic computer fonts with 98.8\% line level accuracy. This same method also
achieves the highest reported accuracy of 86.6% in predicting paleographic
scribal script classes at the page level on medieval Latin manuscripts.
Finally, we analyze what features are learned by the CNN on Latin manuscripts
and find evidence that the CNN is learning both the defining morphological
differences between scribal script classes as well as overfitting to
class-correlated nuisance factors. We propose a novel form of data augmentation
that improves robustness to text darkness, further increasing classification
performance."
"In this work, we introduce a new algorithm for analyzing a diagram, which
contains visual and textual information in an abstract and integrated way.
Whereas diagrams contain richer information compared with individual
image-based or language-based data, proper solutions for automatically
understanding them have not been proposed due to their innate characteristics
of multi-modality and arbitrariness of layouts. To tackle this problem, we
propose a unified diagram-parsing network for generating knowledge from
diagrams based on an object detector and a recurrent neural network designed
for a graphical structure. Specifically, we propose a dynamic graph-generation
network that is based on dynamic memory and graph theory. We explore the
dynamics of information in a diagram with activation of gates in gated
recurrent unit (GRU) cells. On publicly available diagram datasets, our model
demonstrates a state-of-the-art result that outperforms other baselines.
Moreover, further experiments on question answering shows potentials of the
proposed method for various applications."
"Raindrops adhered to a glass window or camera lens can severely hamper the
visibility of a background scene and degrade an image considerably. In this
paper, we address the problem by visually removing raindrops, and thus
transforming a raindrop degraded image into a clean one. The problem is
intractable, since first the regions occluded by raindrops are not given.
Second, the information about the background scene of the occluded regions is
completely lost for most part. To resolve the problem, we apply an attentive
generative network using adversarial training. Our main idea is to inject
visual attention into both the generative and discriminative networks. During
the training, our visual attention learns about raindrop regions and their
surroundings. Hence, by injecting this information, the generative network will
pay more attention to the raindrop regions and the surrounding structures, and
the discriminative network will be able to assess the local consistency of the
restored regions. This injection of visual attention to both generative and
discriminative networks is the main contribution of this paper. Our experiments
show the effectiveness of our approach, which outperforms the state of the art
methods quantitatively and qualitatively."
"About 8% of the male population of the world are affected by a determined
type of color vision disturbance, which varies from the partial to complete
reduction of the ability to distinguish certain colors. A considerable amount
of color blind people are able to live all life long without knowing they have
color vision disabilities and abnormalities. Nowadays the evolution of
information technology and computer science, specifically image processing
techniques and computer graphics, can be fundamental to aid at the development
of adaptive color blindness correction tools. This paper presents a software
tool based on Fuzzy Logic to evaluate the type and the degree of color
blindness a person suffer from. In order to model several degrees of color
blindness, herein this work we modified the classical linear transform-based
simulation method by the use of fuzzy parameters. We also proposed four new
methods to correct color blindness based on a fuzzy approach: Methods A and B,
with and without histogram equalization. All the methods are based on
combinations of linear transforms and histogram operations. In order to
evaluate the results we implemented a web-based survey to get the best results
according to optimize to distinguish different elements in an image. Results
obtained from 40 volunteers proved that the Method B with histogram
equalization got the best results for about 47% of volunteers."
"Compressive sensing (CS) works to acquire measurements at sub-Nyquist rate
and recover the scene images. Existing CS methods always recover the scene
images in pixel level. This causes the smoothness of recovered images and lack
of structure information, especially at a low measurement rate. To overcome
this drawback, in this paper, we propose perceptual CS to obtain high-level
structured recovery. Our task no longer focuses on pixel level. Instead, we
work to make a better visual effect. In detail, we employ perceptual loss,
defined on feature level, to enhance the structure information of the recovered
images. Experiments show that our method achieves better visual results with
stronger structure information than existing CS methods at the same measurement
rate."
"Recent years, compressive sensing (CS) has improved greatly for the
application of deep learning technology. For convenience, the input image is
usually measured and reconstructed block by block. This usually causes block
effect in reconstructed images. In this paper, we present a novel CNN-based
network to solve this problem. In measurement part, the input image is
adaptively measured block by block to acquire a group of measurements. While in
reconstruction part, all the measurements from one image are used to
reconstruct the full image at the same time. Different from previous method
recovering block by block, the structure information destroyed in measurement
part is recovered in our framework. Block effect is removed accordingly. We
train the proposed framework by mean square error (MSE) loss function.
Experiments show that there is no block effect at all in the proposed method.
And our results outperform 1.8 dB compared with existing methods."
"In this paper we propose the use of image pixel position coordinate system to
improve image classification accuracy in various applications. Specifically, we
hypothesize that the use of pixel coordinates will lead to (a) Resolution
invariant performance. Here, by resolution we mean the spacing between the
pixels rather than the size of the image matrix. (b) Overall improvement in
classification accuracy in comparison with network models trained without local
pixel coordinates. This is due to position coordinates enabling the network to
learn relationship between parts of objects, mimicking the human vision system.
We demonstrate our hypothesis using empirical results and intuitive
explanations of the feature maps learnt by deep neural networks. Specifically,
our approach showed improvements in MNIST digit classification and beats state
of the results on the SVHN database. We also show that the performance of our
networks is unaffected despite training the same using blurred images of the
MNIST database and predicting on the high resolution database."
"Image-based localization, or camera relocalization, is a fundamental problem
in computer vision and robotics, and it refers to estimating camera pose from
an image. Recent state-of-the-art approaches use learning based methods, such
as Random Forests (RFs) and Convolutional Neural Networks (CNNs), to regress
for each pixel in the image its corresponding position in the scene's world
coordinate frame, and solve the final pose via a RANSAC-based optimization
scheme using the predicted correspondences. In this paper, instead of in a
patch-based manner, we propose to perform the scene coordinate regression in a
full-frame manner to make the computation efficient at test time and, more
importantly, to add more global context to the regression process to improve
the robustness. To do so, we adopt a fully convolutional encoder-decoder neural
network architecture which accepts a whole image as input and produces scene
coordinate predictions for all pixels in the image. However, using more global
context is prone to overfitting. To alleviate this issue, we propose to use
data augmentation to generate more data for training. In addition to the data
augmentation in 2D image space, we also augment the data in 3D space. We
evaluate our approach on the publicly available 7-Scenes dataset, and
experiments show that it has better scene coordinate predictions and achieves
state-of-the-art results in localization with improved robustness on the
hardest frames (e.g., frames with repeated structures)."
"Major winning Convolutional Neural Networks (CNNs), such as VGGNet, ResNet,
DenseNet, \etc, include tens to hundreds of millions of parameters, which
impose considerable computation and memory overheads. This limits their
practical usage in training and optimizing for real-world applications. On the
contrary, light-weight architectures, such as SqueezeNet, are being proposed to
address this issue. However, they mainly suffer from low accuracy, as they have
compromised between the processing power and efficiency. These inefficiencies
mostly stem from following an ad-hoc designing procedure. In this work, we
discuss and propose several crucial design principles for an efficient
architecture design and elaborate intuitions concerning different aspects of
the design procedure. Furthermore, we introduce a new layer called {\it
SAF-pooling} to improve the generalization power of the network while keeping
it simple by choosing best features. Based on such principles, we propose a
simple architecture called {\it SimpNet}. We empirically show that SimpNet
provides a good trade-off between the computation/memory efficiency and the
accuracy solely based on these primitive but crucial principles. SimpNet
outperforms the deeper and more complex architectures such as VGGNet, ResNet,
WideResidualNet \etc, on several well-known benchmarks, while having 2 to 25
times fewer number of parameters and operations. We obtain state-of-the-art
results (in terms of a balance between the accuracy and the number of involved
parameters) on standard datasets, such as CIFAR10, CIFAR100, MNIST and SVHN.
The implementations are available at
\href{url}{https://github.com/Coderx7/SimpNet}."
"With the recent proliferation of consumer-grade 360{\deg} cameras, it is
worth revisiting visual perception challenges with spherical cameras given the
potential benefit of their global field of view. To this end we introduce a
spherical convolutional hourglass network (SCHN) for the dense labeling on the
sphere. The SCHN is invariant to camera orientation (lifting the usual
requirement for `upright' panoramic images), and its design is scalable for
larger practical datasets. Initial experiments show promising results on a
spherical semantic segmentation task."
"This paper presents an efficient module named spatial bottleneck for
accelerating the convolutional layers in deep neural networks. The core idea is
to decompose convolution into two stages, which first reduce the spatial
resolution of the feature map, and then restore it to the desired size. This
operation decreases the sampling density in the spatial domain, which is
independent yet complementary to network acceleration approaches in the channel
domain. Using different sampling rates, we can tradeoff between recognition
accuracy and model complexity.
  As a basic building block, spatial bottleneck can be used to replace any
single convolutional layer, or the combination of two convolutional layers. We
empirically verify the effectiveness of spatial bottleneck by applying it to
the deep residual networks. Spatial bottleneck achieves 2x and 1.4x speedup on
the regular and channel-bottlenecked residual blocks, respectively, with the
accuracies retained in recognizing low-resolution images, and even improved in
recognizing high-resolution images."
"Sum-of-squares objective functions are very popular in computer vision
algorithms. However, these objective functions are not always easy to optimize.
The underlying assumptions made by solvers are often not satisfied and many
problems are inherently ill-posed. In this paper, we propose LS-Net, a neural
nonlinear least squares optimization algorithm which learns to effectively
optimize these cost functions even in the presence of adversities. Unlike
traditional approaches, the proposed solver requires no hand-crafted
regularizers or priors as these are implicitly learned from the data. We apply
our method to the problem of motion stereo ie. jointly estimating the motion
and scene geometry from pairs of images of a monocular sequence. We show that
our learned optimizer is able to efficiently and effectively solve this
challenging optimization problem."
"FPGA becomes a popular technology for implementing Convolutional Neural
Network (CNN) in recent years. Most CNN applications on FPGA are
domain-specific, e.g., detecting objects from specific categories, in which
commonly-used CNN models pre-trained on general datasets may not be efficient
enough. This paper presents TuRF, an end-to-end CNN acceleration framework to
efficiently deploy domain-specific applications on FPGA by transfer learning
that adapts pre-trained models to specific domains, replacing standard
convolution layers with efficient convolution blocks, and applying layer fusion
to enhance hardware design performance. We evaluate TuRF by deploying a
pre-trained VGG-16 model for a domain-specific image recognition task onto a
Stratix V FPGA. Results show that designs generated by TuRF achieve better
performance than prior methods for the original VGG-16 and ResNet-50 models,
while for the optimised VGG-16 model TuRF designs are more accurate and easier
to process."
"Label information plays an important role in supervised hyperspectral image
classification problem. However, current classification methods all ignore an
important and inevitable problem---labels may be corrupted and collecting clean
labels for training samples is difficult, and often impractical. Therefore, how
to learn from the database with noisy labels is a problem of great practical
importance. In this paper, we study the influence of label noise on
hyperspectral image classification, and develop a random label propagation
algorithm (RLPA) to cleanse the label noise. The key idea of RLPA is to exploit
knowledge (e.g., the superpixel based spectral-spatial constraints) from the
observed hyperspectral images and apply it to the process of label propagation.
Specifically, RLPA first constructs a spectral-spatial probability transfer
matrix (SSPTM) that simultaneously considers the spectral similarity and
superpixel based spatial information. It then randomly chooses some training
samples as ""clean"" samples and sets the rest as unlabeled samples, and
propagates the label information from the ""clean"" samples to the rest unlabeled
samples with the SSPTM. By repeating the random assignment (of ""clean"" labeled
samples and unlabeled samples) and propagation, we can obtain multiple labels
for each training sample. Therefore, the final propagated label can be
calculated by a majority vote algorithm. Experimental studies show that RLPA
can reduce the level of noisy label and demonstrates the advantages of our
proposed method over four major classifiers with a significant margin---the
gains in terms of the average OA, AA, Kappa are impressive, e.g., 9.18%, 9.58%,
and 0.1043. The Matlab source code is available at
https://github.com/junjun-jiang/RLPA"
"Temporal action detection aims at not only recognizing action category but
also detecting start time and end time for each action instance in an untrimmed
video. The key challenge of this task is to accurately classify the action and
determine the temporal boundaries of each action instance. In temporal action
detection benchmark: THUMOS 2014, large variations exist in the same action
category while many similarities exist in different action categories, which
always limit the performance of temporal action detection. To address this
problem, we propose to use joint Identification-Verification network to reduce
the intra-action variations and enlarge inter-action differences. The joint
Identification-Verification network is a siamese network based on 3D ConvNets,
which can simultaneously predict the action categories and the similarity
scores for the input pairs of video proposal segments. Extensive experimental
results on the challenging THUMOS 2014 dataset demonstrate the effectiveness of
our proposed method compared to the existing state-of-art methods for temporal
action detection in untrimmed videos."
"Objective: Deformable image registration is a fundamental problem in medical
image analysis, with applications such as longitudinal studies, population
modeling, and atlas based image segmentation. Registration is often phrased as
an optimization problem, i.e., finding a deformation field that is optimal
according to a given objective function. Discrete, combinatorial, optimization
techniques have successfully been employed to solve the resulting optimization
problem. Specifically, optimization based on $\alpha$-expansion with minimal
graph cuts has been proposed as a powerful tool for image registration. The
high computational cost of the graph-cut based optimization approach, however,
limits the utility of this approach for registration of large volume images.
Methods: Here, we propose to accelerate graph-cut based deformable registration
by dividing the image into overlapping sub-regions and restricting the
$\alpha$-expansion moves to a single sub-region at a time. Results: We
demonstrate empirically that this approach can achieve a large reduction in
computation time -- from days to minutes -- with only a small penalty in terms
of solution quality. Conclusion: The reduction in computation time provided by
the proposed method makes graph cut based deformable registration viable for
large volume images. Significance: Graph cut based image registration has
previously been shown to produce excellent results, but the high computational
cost has hindered the adoption of the method for registration of large medical
volume images. Our proposed method lifts this restriction, requiring only a
small fraction of the computational cost to produce results of comparable
quality."
"One-shot image semantic segmentation poses a challenging task of recognizing
the object regions from unseen categories with only one annotated example as
supervision. In this paper, we propose a simple yet effective Similarity
Guidance network to tackle the One-shot (SG-One) segmentation problem. We aim
at predicting the segmentation mask of a query image with the reference to one
densely labeled support image of the same category. To obtain the robust
representative feature of the support image, we firstly adopt a masked average
pooling strategy for producing the guidance features by only taking the pixels
belonging to the support image into account. We then leverage the cosine
similarity to build the relationship between the guidance features and features
of pixels from the query image. In this way, the possibilities embedded in the
produced similarity maps can be adapted to guide the process of segmenting
objects. Furthermore, our SG-One is a unified framework which can efficiently
process both support and query images within one network and be learned in an
end-to-end manner. We conduct extensive experiments on Pascal VOC 2012. In
particular, our SGOne achieves the mIoU score of 46.3%, surpassing the baseline
methods."
"Recently, adversarial erasing for weakly-supervised object attention has been
deeply studied due to its capability in localizing integral object regions.
However, such a strategy raises one key problem that attention regions will
gradually expand to non-object regions as training iterations continue, which
significantly decreases the quality of the produced attention maps. To tackle
such an issue as well as promote the quality of object attention, we introduce
a simple yet effective Self-Erasing Network (SeeNet) to prohibit attentions
from spreading to unexpected background regions. In particular, SeeNet
leverages two self-erasing strategies to encourage networks to use reliable
object and background cues for learning to attention. In this way, integral
object regions can be effectively highlighted without including much more
background regions. To test the quality of the generated attention maps, we
employ the mined object regions as heuristic cues for learning semantic
segmentation models. Experiments on Pascal VOC well demonstrate the superiority
of our SeeNet over other state-of-the-art methods."
"While the use of bottom-up local operators in convolutional neural networks
(CNNs) matches well some of the statistics of natural images, it may also
prevent such models from capturing contextual long-range feature interactions.
In this work, we propose a simple, lightweight approach for better context
exploitation in CNNs. We do so by introducing a pair of operators: gather,
which efficiently aggregates feature responses from a large spatial extent, and
excite, which redistributes the pooled information to local features. The
operators are cheap, both in terms of number of added parameters and
computational complexity, and can be integrated directly in existing
architectures to improve their performance. Experiments on several datasets
show that gather-excite can bring benefits comparable to increasing the depth
of a CNN at a fraction of the cost. For example, we find ResNet-50 with
gather-excite operators is able to outperform its 101-layer counterpart on
ImageNet with no additional learnable parameters. We also propose a parametric
gather-excite operator pair which yields further performance gains, relate it
to the recently-introduced Squeeze-and-Excitation Networks, and analyse the
effects of these changes to the CNN feature activation statistics."
"Deep models are state-of-the-art for many computer vision tasks including
image classification and object detection. However, it has been shown that deep
models are vulnerable to adversarial examples. We highlight how one-hot
encoding directly contributes to this vulnerability and propose breaking away
from this widely-used, but highly-vulnerable mapping. We demonstrate that by
leveraging a different output encoding, multi-way encoding, we decorrelate
source and target models, making target models more secure. Our approach makes
it more difficult for adversaries to find useful gradients for generating
adversarial attacks. We present robustness for black-box and white-box attacks
on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. The strength
of our approach is also presented in the form of an attack for model
watermarking, raising challenges in detecting stolen models."
"Manipulation actions transform objects from an initial state into a final
state. In this paper, we report on the use of object state transitions as a
mean for recognizing manipulation actions. Our method is inspired by the
intuition that object states are visually more apparent than actions from a
still frame and thus provide information that is complementary to
spatio-temporal action recognition. We start by defining a state transition
matrix that maps action labels into a pre-state and a post-state. From each
keyframe, we learn appearance models of objects and their states. Manipulation
actions can then be recognized from the state transition matrix. We report
results on the EPIC kitchen action recognition challenge."
"We investigate privacy-preserving, video-based action recognition in deep
learning, a problem with growing importance in smart camera applications. A
novel adversarial training framework is formulated to learn an anonymization
transform for input videos such that the trade-off between target utility task
performance and the associated privacy budgets is explicitly optimized on the
anonymized videos. Notably, the privacy budget, often defined and measured in
task-driven contexts, cannot be reliably indicated using any single model
performance because strong protection of privacy should sustain against any
malicious model that tries to steal private information. To tackle this
problem, we propose two new optimization strategies of model restarting and
model ensemble to achieve stronger universal privacy protection against any
attacker models. Extensive experiments have been carried out and analyzed. On
the other hand, given few public datasets available with both utility and
privacy labels, the data-driven (supervised) learning cannot exert its full
power on this task. We first discuss an innovative heuristic of cross-dataset
training and evaluation, enabling the use of multiple single-task datasets (one
with target task labels and the other with privacy labels) in our problem. To
further address this dataset challenge, we have constructed a new dataset,
termed PA-HMDB51, with both target task labels (action) and selected privacy
attributes (skin color, face, gender, nudity, and relationship) annotated on a
per-frame basis. This first-of-its-kind video dataset and evaluation protocol
can greatly facilitate visual privacy research and open up other opportunities.
Our codes, models, and the PA-HMDB51 dataset are available at
https://github.com/VITA-Group/PA-HMDB51."
"Learning powerful deep generative models for 3D shape synthesis is largely
hindered by the difficulty in ensuring plausibility encompassing correct
topology and reasonable geometry. Indeed, learning the distribution of
plausible 3D shapes seems a daunting task for the holistic approaches, given
the significant topological variations of 3D objects even within the same
category. Enlightened by the fact that 3D shape structure is characterized as
part composition and placement, we propose to model 3D shape variations with a
part-aware deep generative network, coined as PAGENet. The network is composed
of an array of per-part VAE-GANs, generating semantic parts composing a
complete shape, followed by a part assembly module that estimates a
transformation for each part to correlate and assemble them into a plausible
structure. Through delegating the learning of part composition and part
placement into separate networks, the difficulty of modeling structural
variations of 3D shapes is greatly reduced. We demonstrate through both
qualitative and quantitative evaluations that PAGENet generates 3D shapes with
plausible, diverse and detailed structure, and show two applications, i.e.,
semantic shape segmentation and part-based shape editing."
"POLSAR image has an advantage over optical image because it can be acquired
independently of cloud cover and solar illumination. PolSAR image
classification is a hot and valuable topic for the interpretation of POLSAR
image. In this paper, a novel POLSAR image classification method is proposed
based on polarimetric scattering coding and sparse support matrix machine.
First, we transform the original POLSAR data to get a real value matrix by the
polarimetric scattering coding, which is called polarimetric scattering matrix
and is a sparse matrix. Second, the sparse support matrix machine is used to
classify the sparse polarimetric scattering matrix and get the classification
map. The combination of these two steps takes full account of the
characteristics of POLSAR. The experimental results show that the proposed
method can get better results and is an effective classification method."
"Generating a photorealistic image with intended human pose is a promising yet
challenging research topic for many applications such as smart photo editing,
movie making, virtual try-on, and fashion display. In this paper, we present a
novel deep generative model to transfer an image of a person from a given pose
to a new pose while keeping fashion item consistent. In order to formulate the
framework, we employ one generator and two discriminators for image synthesis.
The generator includes an image encoder, a pose encoder and a decoder. The two
encoders provide good representation of visual and geometrical context which
will be utilized by the decoder in order to generate a photorealistic image.
Unlike existing pose-guided image generation models, we exploit two
discriminators to guide the synthesis process where one discriminator
differentiates between generated image and real images (training samples), and
another discriminator verifies the consistency of appearance between a target
pose and a generated image. We perform end-to-end training of the network to
learn the parameters through back-propagation given ground-truth images. The
proposed generative model is capable of synthesizing a photorealistic image of
a person given a target pose. We have demonstrated our results by conducting
rigorous experiments on two data sets, both quantitatively and qualitatively."
"Middle-echo, which covers one or a few corresponding points, is a specific
type of 3D point cloud acquired by a multi-echo laser scanner. In this paper,
we propose a novel approach for automatic segmentation of trees that leverages
middle-echo information from LiDAR point clouds. First, using a convolution
classification method, the proposed type of point clouds reflected by the
middle echoes are identified from all point clouds. The middle-echo point
clouds are distinguished from the first and last echoes. Hence, the crown
positions of the trees are quickly detected from the huge number of point
clouds. Second, to accurately extract trees from all point clouds, we propose a
3D deep learning network, PointNLM, to semantically segment tree crowns.
PointNLM captures the long-range relationship between the point clouds via a
non-local branch and extracts high-level features via max-pooling applied to
unordered points. The whole framework is evaluated using the Semantic 3D
reduced-test set. The IoU of tree point cloud segmentation reached 0.864. In
addition, the semantic segmentation network was tested using the Paris-Lille-3D
dataset. The average IoU outperformed several other popular methods. The
experimental results indicate that the proposed algorithm provides an excellent
solution for vegetation segmentation from LiDAR point clouds."
"Human visual system can selectively attend to parts of a scene for quick
perception, a biological mechanism known as Human attention. Inspired by this,
recent deep learning models encode attention mechanisms to focus on the most
task-relevant parts of the input signal for further processing, which is called
Machine/Neural/Artificial attention. Understanding the relation between human
and machine attention is important for interpreting and designing neural
networks. Many works claim that the attention mechanism offers an extra
dimension of interpretability by explaining where the neural networks look.
However, recent studies demonstrate that artificial attention maps do not
always coincide with common intuition. In view of these conflicting evidence,
here we make a systematic study on using artificial attention and human
attention in neural network design. With three example computer vision tasks,
diverse representative backbones, and famous architectures, corresponding real
human gaze data, and systematically conducted large-scale quantitative studies,
we quantify the consistency between artificial attention and human visual
attention and offer novel insights into existing artificial attention
mechanisms by giving preliminary answers to several key questions related to
human and artificial attention mechanisms. Overall results demonstrate that
human attention can benchmark the meaningful `ground-truth' in attention-driven
tasks, where the more the artificial attention is close to human attention, the
better the performance; for higher-level vision tasks, it is case-by-case. It
would be advisable for attention-driven tasks to explicitly force a better
alignment between artificial and human attention to boost the performance; such
alignment would also improve the network explainability for higher-level
computer vision tasks."
"3D object detection from LiDAR point cloud is a challenging problem in 3D
scene understanding and has many practical applications. In this paper, we
extend our preliminary work PointRCNN to a novel and strong point-cloud-based
3D object detection framework, the part-aware and aggregation neural network
(Part-$A^2$ net). The whole framework consists of the part-aware stage and the
part-aggregation stage. Firstly, the part-aware stage for the first time fully
utilizes free-of-charge part supervisions derived from 3D ground-truth boxes to
simultaneously predict high quality 3D proposals and accurate intra-object part
locations. The predicted intra-object part locations within the same proposal
are grouped by our new-designed RoI-aware point cloud pooling module, which
results in an effective representation to encode the geometry-specific features
of each 3D proposal. Then the part-aggregation stage learns to re-score the box
and refine the box location by exploring the spatial relationship of the pooled
intra-object part locations. Extensive experiments are conducted to demonstrate
the performance improvements from each component of our proposed framework. Our
Part-$A^2$ net outperforms all existing 3D detection methods and achieves new
state-of-the-art on KITTI 3D object detection dataset by utilizing only the
LiDAR point cloud data. Code is available at
https://github.com/sshaoshuai/PointCloudDet3D."
"Although Deep neural networks (DNNs) are being pervasively used in
vision-based autonomous driving systems, they are found vulnerable to
adversarial attacks where small-magnitude perturbations into the inputs during
test time cause dramatic changes to the outputs. While most of the recent
attack methods target at digital-world adversarial scenarios, it is unclear how
they perform in the physical world, and more importantly, the generated
perturbations under such methods would cover a whole driving scene including
those fixed background imagery such as the sky, making them inapplicable to
physical world implementation. We present PhysGAN, which generates
physical-world-resilient adversarial examples for mislead-ing autonomous
driving systems in a continuous manner. We show the effectiveness and
robustness of PhysGAN via extensive digital and real-world evaluations. Digital
experiments show that PhysGAN is effective for various steer-ing models and
scenes, which misleads the average steer-ing angle by up to 23.06 degrees under
various scenarios. The real-world studies further demonstrate that PhysGAN is
sufficiently resilient in practice, which misleads the average steering angle
by up to 19.17 degrees. We compare PhysGAN with a set of state-of-the-art
baseline methods including several of our self-designed ones, which further
demonstrate the robustness and efficacy of our approach. We also show that
PhysGAN outperforms state-of-the-art baseline methods To the best of our
knowledge, PhysGANis probably the first technique of generating realistic and
physical-world-resilient adversarial examples for attacking common autonomous
driving scenarios."
"With a growing demand for the search by image, many works have studied the
task of fashion instance-level image retrieval (FIR). Furthermore, the recent
works introduce a concept of fashion attribute manipulation (FAM) which
manipulates a specific attribute (e.g color) of a fashion item while
maintaining the rest of the attributes (e.g shape, and pattern). In this way,
users can search not only ""the same"" items but also ""similar"" items with the
desired attributes. FAM is a challenging task in that the attributes are hard
to define, and the unique characteristics of a query are hard to be preserved.
Although both FIR and FAM are important in real-life applications, most of the
previous studies have focused on only one of these problem. In this study, we
aim to achieve competitive performance on both FIR and FAM. To do so, we
propose a novel method that converts a query into a representation with the
desired attributes. We introduce a new idea of attribute manipulation at the
feature level, by matching the distribution of manipulated features with real
features. In this fashion, the attribute manipulation can be done independently
from learning a representation from the image. By introducing the feature-level
attribute manipulation, the previous methods for FIR can perform attribute
manipulation without sacrificing their retrieval performance."
"We worked with Nestle SHIELD (Skin Health, Innovation, Education, and
Longevity Development, NSH) to develop a deep learning model that is able to
assess acne severity from selfie images as accurate as dermatologists. The
model was deployed as a mobile application, providing patients an easy way to
assess and track the progress of their acne treatment. NSH acquired 4,700
selfie images for this study and recruited 11 internal dermatologists to label
them in five categories: 1-Clear, 2- Almost Clear, 3-Mild, 4-Moderate,
5-Severe. Using OpenCV to detect facial landmarks we cut specific skin patches
from the selfie images in order to minimize irrelevant background. We then
applied a transfer learning approach by extracting features from the patches
using a ResNet 152 pre-trained model, followed by a fully connected layer
trained to approximate the desired severity rating. To address the problem of
spatial sensitivity of CNN models, we introduce a new image rolling data
augmentation approach, effectively causing acne lesions appeared in more
locations in the training images. Our results demonstrate that this approach
improved the generalization of the CNN model, outperforming more than half of
the panel of human dermatologists on test images. To our knowledge, this is the
first deep learning-based solution for acne assessment using selfie images."
"While supervised object detection methods achieve impressive accuracy, they
generalize poorly to images whose appearance significantly differs from the
data they have been trained on. To address this in scenarios where annotating
data is prohibitively expensive, we introduce a self-supervised approach to
object detection and segmentation, able to work with monocular images captured
with a moving camera. At the heart of our approach lies the observation that
segmentation and background reconstruction are linked tasks, and the idea that,
because we observe a structured scene, background regions can be re-synthesized
from their surroundings, whereas regions depicting the object cannot. We
therefore encode this intuition as a self-supervised loss function that we
exploit to train a proposal-based segmentation network. To account for the
discrete nature of object proposals, we develop a Monte Carlo-based training
strategy that allows us to explore the large space of object proposals. Our
experiments demonstrate that our approach yields accurate detections and
segmentations in images that visually depart from those of standard benchmarks,
outperforming existing self-supervised methods and approaching weakly
supervised ones that exploit large annotated datasets."
"We propose a Dynamic Graph-Based Spatial-Temporal Attention (DG-STA) method
for hand gesture recognition. The key idea is to first construct a
fully-connected graph from a hand skeleton, where the node features and edges
are then automatically learned via a self-attention mechanism that performs in
both spatial and temporal domains. We further propose to leverage the
spatial-temporal cues of joint positions to guarantee robust recognition in
challenging conditions. In addition, a novel spatial-temporal mask is applied
to significantly cut down the computational cost by 99%. We carry out extensive
experiments on benchmarks (DHG-14/28 and SHREC'17) and prove the superior
performance of our method compared with the state-of-the-art methods. The
source code can be found at https://github.com/yuxiaochen1103/DG-STA."
"Most state-of-the-art person re-identification (re-id) methods depend on
supervised model learning with a large set of cross-view identity labelled
training data. Even worse, such trained models are limited to only the
same-domain deployment with significantly degraded cross-domain generalization
capability, i.e. ""domain specific"". To solve this limitation, there are a
number of recent unsupervised domain adaptation and unsupervised learning
methods that leverage unlabelled target domain training data. However, these
methods need to train a separate model for each target domain as supervised
learning methods. This conventional ""{\em train once, run once}"" pattern is
unscalable to a large number of target domains typically encountered in
real-world deployments. We address this problem by presenting a ""train once,
run everywhere"" pattern industry-scale systems are desperate for. We formulate
a ""universal model learning' approach enabling domain-generic person re-id
using only limited training data of a ""{\em single}"" seed domain. Specifically,
we train a universal re-id deep model to discriminate between a set of
transformed person identity classes. Each of such classes is formed by applying
a variety of random appearance transformations to the images of that class,
where the transformations simulate the camera viewing conditions of any domains
for making the model training domain generic. Extensive evaluations show the
superiority of our method for universal person re-id over a wide variety of
state-of-the-art unsupervised domain adaptation and unsupervised learning re-id
methods on five standard benchmarks: Market-1501, DukeMTMC, CUHK03, MSMT17, and
VIPeR."
"The horizon line is an important geometric feature for many image processing
and scene understanding tasks in computer vision. For instance, in navigation
of autonomous vehicles or driver assistance, it can be used to improve 3D
reconstruction as well as for semantic interpretation of dynamic environments.
While both algorithms and datasets exist for single images, the problem of
horizon line estimation from video sequences has not gained attention. In this
paper, we show how convolutional neural networks are able to utilise the
temporal consistency imposed by video sequences in order to increase the
accuracy and reduce the variance of horizon line estimates. A novel CNN
architecture with an improved residual convolutional LSTM is presented for
temporally consistent horizon line estimation. We propose an adaptive loss
function that ensures stable training as well as accurate results. Furthermore,
we introduce an extension of the KITTI dataset which contains precise horizon
line labels for 43699 images across 72 video sequences. A comprehensive
evaluation shows that the proposed approach consistently achieves superior
performance compared with existing methods."
"We address the challenging problem of generating facial attributes using a
single image in an unconstrained pose. In contrast to prior works that largely
consider generation on 2D near-frontal images, we propose a GAN-based framework
to generate attributes directly on a dense 3D representation given by UV
texture and position maps, resulting in photorealistic,
geometrically-consistent and identity-preserving outputs. Starting from a
self-occluded UV texture map obtained by applying an off-the-shelf 3D
reconstruction method, we propose two novel components. First, a texture
completion generative adversarial network (TC-GAN) completes the partial UV
texture map. Second, a 3D attribute generation GAN (3DA-GAN) synthesizes the
target attribute while obtaining an appearance consistent with 3D face geometry
and preserving identity. Extensive experiments on CelebA, LFW and IJB-A show
that our method achieves consistently better attribute generation accuracy than
prior methods, a higher degree of qualitative photorealism and preserves face
identity information."
"Background subtraction is a basic task in computer vision and video
processing often applied as a pre-processing step for object tracking, people
recognition, etc. Recently, a number of successful background-subtraction
algorithms have been proposed, however nearly all of the top-performing ones
are supervised. Crucially, their success relies upon the availability of some
annotated frames of the test video during training. Consequently, their
performance on completely ""unseen"" videos is undocumented in the literature. In
this work, we propose a new, supervised, background-subtraction algorithm for
unseen videos (BSUV-Net) based on a fully-convolutional neural network. The
input to our network consists of the current frame and two background frames
captured at different time scales along with their semantic segmentation maps.
In order to reduce the chance of overfitting, we also introduce a new
data-augmentation technique which mitigates the impact of illumination
difference between the background frames and the current frame. On the
CDNet-2014 dataset, BSUV-Net outperforms state-of-the-art algorithms evaluated
on unseen videos in terms of several metrics including F-measure, recall and
precision."
"Most of current Convolution Neural Network (CNN) based methods for optical
flow estimation focus on learning optical flow on synthetic datasets with
groundtruth, which is not practical. In this paper, we propose an unsupervised
optical flow estimation framework named PCLNet. It uses pyramid Convolution
LSTM (ConvLSTM) with the constraint of adjacent frame reconstruction, which
allows flexibly estimating multi-frame optical flows from any video clip.
Besides, by decoupling motion feature learning and optical flow representation,
our method avoids complex short-cut connections used in existing frameworks
while improving accuracy of optical flow estimation. Moreover, different from
those methods using specialized CNN architectures for capturing motion, our
framework directly learns optical flow from the features of generic CNNs and
thus can be easily embedded in any CNN based frameworks for other tasks.
Extensive experiments have verified that our method not only estimates optical
flow effectively and accurately, but also obtains comparable performance on
action recognition."
"Iris recognition is one of the most important biometric recognition method.
This is because the iris texture provides many features such as freckles,
coronas, stripes, furrows, crypts, etc. Those features are unique for different
people and distinguishable. Such unique features in the anatomical structure of
the iris make it possible the differentiation among individuals. So during last
years huge number of people have been trying to improve its performance. In
this article first different common steps for the Iris recognition system is
explained. Then a special type of neural network is used for recognition part.
Experimental results show high accuracy can be obtained especially when the
primary steps are done well."
"Computerized registration between maxillofacial cone-beam computed tomography
(CT) images and a scanned dental model is an essential prerequisite in surgical
planning for dental implants or orthognathic surgery. We propose a novel method
that performs fully automatic registration between a cone-beam CT image and an
optically scanned model. To build a robust and automatic initial registration
method, our method applies deep-pose regression neural networks in a reduced
domain (i.e., 2-dimensional image). Subsequently, fine registration is
performed via optimal clusters. Majority voting system achieves globally
optimal transformations while each cluster attempts to optimize local
transformation parameters. The coherency of clusters determines their candidacy
for the optimal cluster set. The outlying regions in the iso-surface are
effectively removed based on the consensus among the optimal clusters. The
accuracy of registration was evaluated by the Euclidean distance of 10
landmarks on a scanned model which were annotated by the experts in the field.
The experiments show that the proposed method's registration accuracy, measured
in landmark distance, outperforms other existing methods by 30.77% to 70%. In
addition to achieving high accuracy, our proposed method requires neither
human-interactions nor priors (e.g., iso-surface extraction). The main
significance of our study is twofold: 1) the employment of light-weighted
neural networks which indicates the applicability of neural network in
extracting pose cues that can be easily obtained and 2) the introduction of an
optimal cluster-based registration method that can avoid metal artifacts during
the matching procedures."
"In order to achieve better performance for point cloud analysis, many
researchers apply deeper neural networks using stacked Multi-Layer-Perceptron
(MLP) convolutions over irregular point cloud. However, applying dense MLP
convolutions over large amount of points (e.g. autonomous driving application)
leads to inefficiency in memory and computation. To achieve high performance
but less complexity, we propose a deep-wide neural network, called
ShufflePointNet, to exploit fine-grained local features and reduce redundancy
in parallel using group convolution and channel shuffle operation. Unlike
conventional operation that directly applies MLPs on high-dimensional features
of point cloud, our model goes wider by splitting features into groups in
advance, and each group with certain smaller depth is only responsible for
respective MLP operation, which can reduce complexity and allows to encode more
useful information. Meanwhile, we connect communication between groups by
shuffling groups in feature channel to capture fine-grained features. We claim
that, multi-branch method for wider neural networks is also beneficial to
feature extraction for point cloud. We present extensive experiments for shape
classification task on ModelNet40 dataset and semantic segmentation task on
large scale datasets ShapeNet part, S3DIS and KITTI. We further perform
ablation study and compare our model to other state-of-the-art algorithms in
terms of complexity and accuracy."
"Model-based human pose estimation is currently approached through two
different paradigms. Optimization-based methods fit a parametric body model to
2D observations in an iterative manner, leading to accurate image-model
alignments, but are often slow and sensitive to the initialization. In
contrast, regression-based methods, that use a deep network to directly
estimate the model parameters from pixels, tend to provide reasonable, but not
pixel accurate, results while requiring huge amounts of supervision. In this
work, instead of investigating which approach is better, our key insight is
that the two paradigms can form a strong collaboration. A reasonable, directly
regressed estimate from the network can initialize the iterative optimization
making the fitting faster and more accurate. Similarly, a pixel accurate fit
from iterative optimization can act as strong supervision for the network. This
is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The
deep network initializes an iterative optimization routine that fits the body
model to 2D joints within the training loop, and the fitted estimate is
subsequently used to supervise the network. Our approach is self-improving by
nature, since better network estimates can lead the optimization to better
solutions, while more accurate optimization fits provide better supervision for
the network. We demonstrate the effectiveness of our approach in different
settings, where 3D ground truth is scarce, or not available, and we
consistently outperform the state-of-the-art model-based pose estimation
approaches by significant margins. The project website with videos, results,
and code can be found at https://seas.upenn.edu/~nkolot/projects/spin."
"Real-time single-stage object detectors based on deep learning still remain
less accurate than more complex ones. The trade-off between model performance
and computational speed is a major challenge. In this paper, we propose a new
way to efficiently learn a single-shot detector which offers a very good
compromise between these two objectives. To this end, we introduce LapNet, an
anchor based detector, trained end-to-end without any sampling strategy. Our
approach aims to overcome two important problems encountered in training an
anchor based detector: (1) ambiguity in the assignment of anchor to ground
truth and (2) class and object size imbalance. To address the first limitation,
we propose a soft positive/negative anchor assignment procedure based on a new
overlapping function called ""Per-Object Normalized Overlap"" (PONO). This soft
assignment can be self-corrected by the network itself to avoid ambiguity
between close objects. To cope with the second limitation, we propose to learn
additional weights, that are not used at inference, to efficiently manage
sample imbalance. These two contributions make the detector learning more
generic whatever the training dataset. Various experiments show the
effectiveness of the proposed approach."
"Segmentation of optic disc (OD) and optic cup (OC) is critical in automated
fundus image analysis system. Existing state-of-the-arts focus on designing
deep neural networks with one or multiple dense prediction branches. Such kind
of designs ignore connections among prediction branches and their learning
capacity is limited. To build connections among prediction branches, this paper
introduces gradient boosting framework to deep classification model and
proposes a gradient boosting network called BoostNet. Specifically, deformable
side-output unit and aggregation unit with deep supervisions are proposed to
learn base functions and expansion coefficients in gradient boosting framework.
By stacking aggregation units in a deep-to-shallow manner, models' performances
are gradually boosted along deep to shallow stages. BoostNet achieves superior
results to existing deep OD and OC segmentation networks on the public dataset
ORIGA."
"The ROI (region-of-interest) based pooling method performs pooling operations
on the cropped ROI regions for various samples and has shown great success in
the object detection methods. It compresses the model size while preserving the
localization accuracy, thus it is useful in the visual tracking field. Though
being effective, the ROI-based pooling operation is not yet considered in the
correlation filter formula. In this paper, we propose a novel ROI pooled
correlation filter (RPCF) algorithm for robust visual tracking. Through
mathematical derivations, we show that the ROI-based pooling can be
equivalently achieved by enforcing additional constraints on the learned filter
weights, which makes the ROI-based pooling feasible on the virtual circular
samples. Besides, we develop an efficient joint training formula for the
proposed correlation filter algorithm, and derive the Fourier solvers for
efficient model training. Finally, we evaluate our RPCF tracker on OTB-2013,
OTB-2015 and VOT-2017 benchmark datasets. Experimental results show that our
tracker performs favourably against other state-of-the-art trackers."
"Channel pruning is one of the important methods for deep model compression.
Most of existing pruning methods mainly focus on classification. Few of them
conduct systematic research on object detection. However, object detection is
different from classification, which requires not only semantic information but
also localization information. In this paper, based on discrimination-aware
channel pruning (DCP) which is state-of-the-art pruning method for
classification, we propose a localization-aware auxiliary network to find out
the channels with key information for classification and regression so that we
can conduct channel pruning directly for object detection, which saves lots of
time and computing resources. In order to capture the localization information,
we first design the auxiliary network with a contextual ROIAlign layer which
can obtain precise localization information of the default boxes by pixel
alignment and enlarges the receptive fields of the default boxes when pruning
shallow layers. Then, we construct a loss function for object detection task
which tends to keep the channels that contain the key information for
classification and regression. Extensive experiments demonstrate the
effectiveness of our method. On MS COCO, we prune 70\% parameters of the SSD
based on ResNet-50 with modest accuracy drop, which outperforms
the-state-of-art method."
"We propose a compact framework with guided attention for multi-label
classification in the fashion domain. Our visual semantic attention model
(VSAM) is supervised by automatic pose extraction creating a discriminative
feature space. VSAM outperforms the state of the art for an in-house dataset
and performs on par with previous works on the DeepFashion dataset, even
without using any landmark annotations. Additionally, we show that our semantic
attention module brings robustness to large quantities of wrong annotations and
provides more interpretable results."
"Despite the recent works on knowledge distillation (KD) have achieved a
further improvement through elaborately modeling the decision boundary as the
posterior knowledge, their performance is still dependent on the hypothesis
that the target network has a powerful capacity (representation ability). In
this paper, we propose a knowledge representing (KR) framework mainly focusing
on modeling the parameters distribution as prior knowledge. Firstly, we suggest
a knowledge aggregation scheme in order to answer how to represent the prior
knowledge from teacher network. Through aggregating the parameters distribution
from teacher network into more abstract level, the scheme is able to alleviate
the phenomenon of residual accumulation in the deeper layers. Secondly, as the
critical issue of what the most important prior knowledge is for better
distilling, we design a sparse recoding penalty for constraining the student
network to learn with the penalized gradients. With the proposed penalty, the
student network can effectively avoid the over-regularization during knowledge
distilling and converge faster. The quantitative experiments exhibit that the
proposed framework achieves the state-ofthe-arts performance, even though the
target network does not have the expected capacity. Moreover, the framework is
flexible enough for combining with other KD methods based on the posterior
knowledge."
"We tackle the tasks of: 1) predicting a Canonical Surface Mapping (CSM) that
indicates the mapping from 2D pixels to corresponding points on a canonical
template shape, and 2) inferring the articulation and pose of the template
corresponding to the input image. While previous approaches rely on keypoint
supervision for learning, we present an approach that can learn without such
annotations. Our key insight is that these tasks are geometrically related, and
we can obtain supervisory signal via enforcing consistency among the
predictions. We present results across a diverse set of animal object
categories, showing that our method can learn articulation and CSM prediction
from image collections using only foreground mask labels for training. We
empirically show that allowing articulation helps learn more accurate CSM
prediction, and that enforcing the consistency with predicted CSM is similarly
critical for learning meaningful articulation."
"We present a lightweight solution to recover 3D pose from multi-view images
captured with spatially calibrated cameras. Building upon recent advances in
interpretable representation learning, we exploit 3D geometry to fuse input
images into a unified latent representation of pose, which is disentangled from
camera view-points. This allows us to reason effectively about 3D pose across
different views without using compute-intensive volumetric grids. Our
architecture then conditions the learned representation on camera projection
operators to produce accurate per-view 2d detections, that can be simply lifted
to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do
it efficiently, we propose a novel implementation of DLT that is orders of
magnitude faster on GPU architectures than standard SVD-based triangulation
methods. We evaluate our approach on two large-scale human pose datasets (H36M
and Total Capture): our method outperforms or performs comparably to the
state-of-the-art volumetric methods, while, unlike them, yielding real-time
performance."
"Monocular estimation of 3d human pose has attracted increased attention with
the availability of large ground-truth motion capture datasets. However, the
diversity of training data available is limited and it is not clear to what
extent methods generalize outside the specific datasets they are trained on. In
this work we carry out a systematic study of the diversity and biases present
in specific datasets and its effect on cross-dataset generalization across a
compendium of 5 pose datasets. We specifically focus on systematic differences
in the distribution of camera viewpoints relative to a body-centered coordinate
frame. Based on this observation, we propose an auxiliary task of predicting
the camera viewpoint in addition to pose. We find that models trained to
jointly predict viewpoint and pose systematically show significantly improved
cross-dataset generalization."
"Predicting the mutation status of genes in tumors is of great clinical
significance. Recent studies have suggested that certain mutations may be
noninvasively predicted by studying image features of the tumors from Computed
Tomography (CT) data. Currently, this kind of image feature identification
method mainly relies on manual processing to extract generalized image features
alone or machine processing without considering the morphological differences
of the tumor itself, which makes it difficult to achieve further breakthroughs.
In this paper, we propose a pyramid focusing network (PFNet) for mutation
prediction and classification based on CT images. Firstly, we use Space Pyramid
Pooling to collect semantic cues in feature maps from multiple scales according
to the observation that the shape and size of the tumors are varied.Secondly,
we improve the loss function based on the consideration that the features
required for proper mutation detection are often not obvious in cross-sections
of tumor edges, which raises more attention to these hard examples in the
network. Finally, we devise a training scheme based on data augmentation to
enhance the generalization ability of networks. Extensively verified on
clinical gastric CT datasets of 20 testing volumes with 63648 CT images, our
method achieves the accuracy of 94.90% in predicting the HER-2 genes mutation
status of at the CT image."
"Cross-modal hashing facilitates mapping of heterogeneous multimedia data into
a common Hamming space, which can beutilized for fast and flexible retrieval
across different modalities. In this paper, we propose a novel cross-modal
hashingarchitecture-deep neural decoder cross-modal hashing (DNDCMH), which
uses a binary vector specifying the presence of certainfacial attributes as an
input query to retrieve relevant face images from a database. The DNDCMH
network consists of two separatecomponents: an attribute-based deep cross-modal
hashing (ADCMH) module, which uses a margin (m)-based loss function
toefficiently learn compact binary codes to preserve similarity between
modalities in the Hamming space, and a neural error correctingdecoder (NECD),
which is an error correcting decoder implemented with a neural network. The
goal of NECD network in DNDCMH isto error correct the hash codes generated by
ADCMH to improve the retrieval efficiency. The NECD network is trained such
that it hasan error correcting capability greater than or equal to the margin
(m) of the margin-based loss function. This results in NECD cancorrect the
corrupted hash codes generated by ADCMH up to the Hamming distance of m. We
have evaluated and comparedDNDCMH with state-of-the-art cross-modal hashing
methods on standard datasets to demonstrate the superiority of our method."
"Adversarial examples have been well known as a serious threat to deep neural
networks (DNNs). In this work, we study the detection of adversarial examples,
based on the assumption that the output and internal responses of one DNN model
for both adversarial and benign examples follow the generalized Gaussian
distribution (GGD), but with different parameters (i.e., shape factor, mean,
and variance). GGD is a general distribution family to cover many popular
distributions (e.g., Laplacian, Gaussian, or uniform). It is more likely to
approximate the intrinsic distributions of internal responses than any specific
distribution. Besides, since the shape factor is more robust to different
databases rather than the other two parameters, we propose to construct
discriminative features via the shape factor for adversarial detection,
employing the magnitude of Benford-Fourier coefficients (MBF), which can be
easily estimated using responses. Finally, a support vector machine is trained
as the adversarial detector through leveraging the MBF features. Extensive
experiments in terms of image classification demonstrate that the proposed
detector is much more effective and robust on detecting adversarial examples of
different crafting methods and different sources, compared to state-of-the-art
adversarial detection methods."
"In this paper, we propose to detect forged videos, of faces, in online
videos. To facilitate this detection, we propose to use smaller (fewer
parameters to learn) convolutional neural networks (CNN), for a data-driven
approach to forged video detection. To validate our approach, we investigate
the FaceForensics public dataset detailing both frame-based and video-based
results. The proposed method is shown to outperform current state of the art.
We also perform an ablation study, analyzing the impact of batch size, number
of filters, and number of network layers on the accuracy of detecting forged
videos."
"Predicting future human motion plays a significant role in human-machine
interactions for various real-life applications. A unified formulation and
multi-order modeling are two critical perspectives for analyzing and
representing human motion. In contrast to prior works, we improve the
multi-order modeling ability of human motion systems for more accurate
predictions by building a deep state-space model (DeepSSM). The DeepSSM
utilizes the advantages of both the state-space theory and the deep network.
Specifically, we formulate the human motion system as the state-space model of
a dynamic system and model the motion system by the state-space theory,
offering a unified formulation for diverse human motion systems. Moreover, a
novel deep network is designed to parameterize this system, which jointly
models the state-state transition and state-observation transition processes.
In this way, the state of a system is updated by the multi-order information of
a time-varying human motion sequence. Multiple future poses are recursively
predicted via the state-observation transition. To further improve the model
ability of the system, a novel loss, WT-MPJPE (Weighted Temporal Mean Per Joint
Position Error), is introduced to optimize the model. The proposed loss
encourages the system to achieve more accurate predictions by increasing
weights to the early time steps. The experiments on two benchmark datasets
(i.e., Human3.6M and 3DPW) confirm that our method achieves state-of-the-art
performance with improved accuracy of at least 2.2mm per joint. The code will
be available at: \url{https://github.com/lily2lab/DeepSSM.git}."
"We tackle the image reassembly problem with wide space between the fragments,
in such a way that the patterns and colors continuity is mostly unusable. The
spacing emulates the erosion of which the archaeological fragments suffer. We
crop-square the fragments borders to compel our algorithm to learn from the
content of the fragments. We also complicate the image reassembly by removing
fragments and adding pieces from other sources. We use a two-step method to
obtain the reassemblies: 1) a neural network predicts the positions of the
fragments despite the gaps between them; 2) a graph that leads to the best
reassemblies is made from these predictions. In this paper, we notably
investigate the effect of branch-cut in the graph of reassemblies. We also
provide a comparison with the literature, solve complex images reassemblies,
explore at length the dataset, and propose a new metric that suits its
specificities.
  Keywords: image reassembly, jigsaw puzzle, deep learning, graph, branch-cut,
cultural heritage"
"The problem of open-set recognition is considered. While previous approaches
only consider this problem in the context of large-scale classifier training,
we seek a unified solution for this and the low-shot classification setting. It
is argued that the classic softmax classifier is a poor solution for open-set
recognition, since it tends to overfit on the training classes. Randomization
is then proposed as a solution to this problem. This suggests the use of
meta-learning techniques, commonly used for few-shot classification, for the
solution of open-set recognition. A new oPen sEt mEta LEaRning (PEELER)
algorithm is then introduced. This combines the random selection of a set of
novel classes per episode, a loss that maximizes the posterior entropy for
examples of those classes, and a new metric learning formulation based on the
Mahalanobis distance. Experimental results show that PEELER achieves state of
the art open set recognition performance for both few-shot and large-scale
recognition. On CIFAR and miniImageNet, it achieves substantial gains in
seen/unseen class detection AUROC for a given seen-class classification
accuracy."
"Towards 3D object tracking in point clouds, a novel point-to-box network
termed P2B is proposed in an end-to-end learning manner. Our main idea is to
first localize potential target centers in 3D search area embedded with target
information. Then point-driven 3D target proposal and verification are executed
jointly. In this way, the time-consuming 3D exhaustive search can be avoided.
Specifically, we first sample seeds from the point clouds in template and
search area respectively. Then, we execute permutation-invariant feature
augmentation to embed target clues from template into search area seeds and
represent them with target-specific features. Consequently, the augmented
search area seeds regress the potential target centers via Hough voting. The
centers are further strengthened with seed-wise targetness scores. Finally,
each center clusters its neighbors to leverage the ensemble power for joint 3D
target proposal and verification. We apply PointNet++ as our backbone and
experiments on KITTI tracking dataset demonstrate P2B's superiority (~10%'s
improvement over state-of-the-art). Note that P2B can run with 40FPS on a
single NVIDIA 1080Ti GPU. Our code and model are available at
https://github.com/HaozheQi/P2B."
"Localizing thoracic diseases on chest X-ray plays a critical role in clinical
practices such as diagnosis and treatment planning. However, current deep
learning based approaches often require strong supervision, e.g. annotated
bounding boxes, for training such systems, which is infeasible to harvest in
large-scale. We present Probabilistic Class Activation Map (PCAM) pooling, a
novel global pooling operation for lesion localization with only image-level
supervision. PCAM pooling explicitly leverages the excellent localization
ability of CAM during training in a probabilistic fashion. Experiments on the
ChestX-ray14 dataset show a ResNet-34 model trained with PCAM pooling
outperforms state-of-the-art baselines on both the classification task and the
localization task. Visual examination on the probability maps generated by PCAM
pooling shows clear and sharp boundaries around lesion regions compared to the
localization heatmaps generated by CAM. PCAM pooling is open sourced at
https://github.com/jfhealthcare/Chexpert."
"Explainability of machine learning (ML) techniques in digital pathology (DP)
is of great significance to facilitate their wide adoption in clinics.
Recently, graph techniques encoding relevant biological entities have been
employed to represent and assess DP images. Such paradigm shift from pixel-wise
to entity-wise analysis provides more control over concept representation. In
this paper, we introduce a post-hoc explainer to derive compact per-instance
explanations emphasizing diagnostically important entities in the graph.
Although we focus our analyses to cells and cellular interactions in breast
cancer subtyping, the proposed explainer is generic enough to be extended to
other topological representations in DP. Qualitative and quantitative analyses
demonstrate the efficacy of the explainer in generating comprehensive and
compact explanations."
"We present RELATE, a model that learns to generate physically plausible
scenes and videos of multiple interacting objects. Similar to other generative
approaches, RELATE is trained end-to-end on raw, unlabeled data. RELATE
combines an object-centric GAN formulation with a model that explicitly
accounts for correlations between individual objects. This allows the model to
generate realistic scenes and videos from a physically-interpretable
parameterization. Furthermore, we show that modeling the object correlation is
necessary to learn to disentangle object positions and identity. We find that
RELATE is also amenable to physically realistic scene editing and that it
significantly outperforms prior art in object-centric scene generation in both
synthetic (CLEVR, ShapeStacks) and real-world data (cars). In addition, in
contrast to state-of-the-art methods in object-centric generative modeling,
RELATE also extends naturally to dynamic scenes and generates videos of high
visual fidelity. Source code, datasets and more results are available at
http://geometry.cs.ucl.ac.uk/projects/2020/relate/."
"Deep learning techniques have been successfully deployed for automating plant
stress identification and quantification. In recent years, there is a growing
push towards training models that are interpretable -i.e. that justify their
classification decisions by visually highlighting image features that were
crucial for classification decisions. The expectation is that trained network
models utilize image features that mimic visual cues used by plant
pathologists. In this work, we compare some of the most popular
interpretability methods: Saliency Maps, SmoothGrad, Guided Backpropogation,
Deep Taylor Decomposition, Integrated Gradients, Layer-wise Relevance
Propagation and Gradient times Input, for interpreting the deep learning model.
We train a DenseNet-121 network for the classification of eight different
soybean stresses (biotic and abiotic). Using a dataset consisting of 16,573 RGB
images of healthy and stressed soybean leaflets captured under controlled
conditions, we obtained an overall classification accuracy of 95.05 \%. For a
diverse subset of the test data, we compared the important features with those
identified by a human expert. We observed that most interpretability methods
identify the infected regions of the leaf as important features for some -- but
not all -- of the correctly classified images. For some images, the output of
the interpretability methods indicated that spurious feature correlations may
have been used to correctly classify them. Although the output explanation maps
of these interpretability methods may be different from each other for a given
image, we advocate the use of these interpretability methods as `hypothesis
generation' mechanisms that can drive scientific insight."
"Predicting consumers' purchasing behaviors is critical for targeted
advertisement and sales promotion in e-commerce. Human faces are an invaluable
source of information for gaining insights into consumer personality and
behavioral traits. However, consumer's faces are largely unexplored in previous
research, and the existing face-related studies focus on high-level features
such as personality traits while neglecting the business significance of
learning from facial data. We propose to predict consumers' purchases based on
their facial features and purchasing histories. We design a semi-supervised
model based on a hierarchical embedding network to extract high-level features
of consumers and to predict the top-$N$ purchase destinations of a consumer.
Our experimental results on a real-world dataset demonstrate the positive
effect of incorporating facial information in predicting consumers' purchasing
behaviors."
"Human beings are fundamentally sociable -- that we generally organize our
social lives in terms of relations with other people. Understanding social
relations from an image has great potential for intelligent systems such as
social chatbots and personal assistants. In this paper, we propose a simpler,
faster, and more accurate method named graph relational reasoning network
(GR2N) for social relation recognition. Different from existing methods which
process all social relations on an image independently, our method considers
the paradigm of jointly inferring the relations by constructing a social
relation graph. Furthermore, the proposed GR2N constructs several virtual
relation graphs to explicitly grasp the strong logical constraints among
different types of social relations. Experimental results illustrate that our
method generates a reasonable and consistent social relation graph and improves
the performance in both accuracy and efficiency."
"Despite the recent success of convolutional neural networks in texture
recognition, model-based descriptors are still competitive, especially when we
do not have access to large amounts of annotated data for training and the
interpretation of the model is an important issue. Among the model-based
approaches, fractal geometry has been one of the most popular, especially in
biological applications. Nevertheless, fractals are part of a much broader
family of models, which are the non-linear operators, studied in chaos theory.
In this context, we propose here a chaos-based local descriptor for texture
recognition. More specifically, we map the image into the three-dimensional
Euclidean space, iterate a chaotic map over this three-dimensional structure
and convert it back to the original image. From such chaos-transformed image at
each iteration we collect local descriptors (here we use local binary patters)
and those descriptors compose the feature representation of the texture. The
performance of our method was verified on the classification of benchmark
databases and in the identification of Brazilian plant species based on the
texture of the leaf surface. The achieved results confirmed our expectation of
a competitive performance, even when compared with some learning-based modern
approaches in the literature."
"Despite the achievements of recent binarization methods on reducing the
performance degradation of Binary Neural Networks (BNNs), gradient mismatching
caused by the Straight-Through-Estimator (STE) still dominates quantized
networks. This paper proposes a meta-based quantizer named QuantNet, which
utilizes a differentiable sub-network to directly binarize the full-precision
weights without resorting to STE and any learnable gradient estimators. Our
method not only solves the problem of gradient mismatching, but also reduces
the impact of discretization errors, caused by the binarizing operation in the
deployment, on performance. Generally, the proposed algorithm is implemented
within a fully differentiable framework, and is easily extended to the general
network quantization with any bits. The quantitative experiments on CIFAR-100
and ImageNet demonstrate that QuantNet achieves the signifficant improvements
comparing with previous binarization methods, and even bridges gaps of
accuracies between binarized models and full-precision models."
"In this paper, we describe our study on how humans allocate their attention
during visual crowd counting. Using an eye tracker, we collect gaze behavior of
human participants who are tasked with counting the number of people in crowd
images. Analyzing the collected gaze behavior of ten human participants on
thirty crowd images, we observe some common approaches for visual counting. For
an image of a small crowd, the approach is to enumerate over all people or
groups of people in the crowd, and this explains the high level of similarity
between the fixation density maps of different human participants. For an image
of a large crowd, our participants tend to focus on one section of the image,
count the number of people in that section, and then extrapolate to the other
sections. In terms of count accuracy, our human participants are not as good at
the counting task, compared to the performance of the current state-of-the-art
computer algorithms. Interestingly, there is a tendency to under count the
number of people in all crowd images. Gaze behavior data and images can be
downloaded from
https://www3.cs.stonybrook.edu/~minhhoai/projects/crowd_counting_gaze/."
"Popular network pruning algorithms reduce redundant information by optimizing
hand-crafted models, and may cause suboptimal performance and long time in
selecting filters. We innovatively introduce adaptive exemplar filters to
simplify the algorithm design, resulting in an automatic and efficient pruning
approach called EPruner. Inspired by the face recognition community, we use a
message passing algorithm Affinity Propagation on the weight matrices to obtain
an adaptive number of exemplars, which then act as the preserved filters.
EPruner breaks the dependency on the training data in determining the
""important"" filters and allows the CPU implementation in seconds, an order of
magnitude faster than GPU based SOTAs. Moreover, we show that the weights of
exemplars provide a better initialization for the fine-tuning. On VGGNet-16,
EPruner achieves a 76.34%-FLOPs reduction by removing 88.80% parameters, with
0.06% accuracy improvement on CIFAR-10. In ResNet-152, EPruner achieves a
65.12%-FLOPs reduction by removing 64.18% parameters, with only 0.71% top-5
accuracy loss on ILSVRC-2012. Our code can be available at
https://github.com/lmbxmu/EPruner."
"Since collecting and annotating data for spatio-temporal action detection is
very expensive, there is a need to learn approaches with less supervision.
Weakly supervised approaches do not require any bounding box annotations and
can be trained only from labels that indicate whether an action occurs in a
video clip. Current approaches, however, cannot handle the case when there are
multiple persons in a video that perform multiple actions at the same time. In
this work, we address this very challenging task for the first time. We propose
a baseline based on multi-instance and multi-label learning. Furthermore, we
propose a novel approach that uses sets of actions as representation instead of
modeling individual action classes. Since computing, the probabilities for the
full power set becomes intractable as the number of action classes increases,
we assign an action set to each detected person under the constraint that the
assignment is consistent with the annotation of the video clip. We evaluate the
proposed approach on the challenging AVA dataset where the proposed approach
outperforms the MIML baseline and is competitive to fully supervised
approaches."
"In this paper, we present a robust and efficient Structure from Motion
pipeline for accurate 3D reconstruction under challenging environments by
leveraging the camera pose information from a visual-inertial odometry.
Specifically, we propose a geometric verification method to filter out
mismatches by considering the prior geometric configuration of candidate image
pairs. Furthermore, we introduce an efficient and scalable reconstruction
approach that relies on batched image registration and robust bundle
adjustment, both leveraging the reliable local odometry estimation. Extensive
experimental results show that our pipeline performs better than the
state-of-the-art SfM approaches in terms of reconstruction accuracy and
robustness for challenging sequential image collections."
"Junctions reflect the important geometrical structure information of the
image, and are of primary significance to applications such as image matching
and motion analysis. Previous event-based feature extraction methods are mainly
focused on corners, which mainly find their locations, however, ignoring the
geometrical structure information like orientations and scales of edges. This
paper adapts the frame-based a-contrario junction detector(ACJ) to event data,
proposing the event-based a-contrario junction detector(e-ACJ), which yields
junctions' locations while giving the scales and orientations of their
branches. The proposed method relies on an a-contrario model and can operate on
asynchronous events directly without generating synthesized event frames. We
evaluate the performance on public event datasets. The result shows our method
successfully finds the orientations and scales of branches, while maintaining
high accuracy in junction's location."
"Most modern multiple object tracking (MOT) systems follow the
tracking-by-detection paradigm, consisting of a detector followed by a method
for associating detections into tracks. There is a long history in tracking of
combining motion and appearance features to provide robustness to occlusions
and other challenges, but typically this comes with the trade-off of a more
complex and slower implementation. Recent successes on popular 2D tracking
benchmarks indicate that top-scores can be achieved using a state-of-the-art
detector and relatively simple associations relying on single-frame spatial
offsets -- notably outperforming contemporary methods that leverage learned
appearance features to help re-identify lost tracks. In this paper, we propose
an efficient joint detection and tracking model named DEFT, or ""Detection
Embeddings for Tracking."" Our approach relies on an appearance-based object
matching network jointly-learned with an underlying object detection network.
An LSTM is also added to capture motion constraints. DEFT has comparable
accuracy and speed to the top methods on 2D online tracking leaderboards while
having significant advantages in robustness when applied to more challenging
tracking data. DEFT raises the bar on the nuScenes monocular 3D tracking
challenge, more than doubling the performance of the previous top method. Code
is publicly available."
"The past decade has seen an increased interest in human activity recognition
based on sensor data. Most often, the sensor data come unannotated, creating
the need for fast labelling methods. For assessing the quality of the
labelling, an appropriate performance measure has to be chosen. Our main
contribution is a novel post-processing method for activity recognition. It
improves the accuracy of the classification methods by correcting for
unrealistic short activities in the estimate. We also propose a new performance
measure, the Locally Time-Shifted Measure (LTS measure), which addresses
uncertainty in the times of state changes. The effectiveness of the
post-processing method is evaluated, using the novel LTS measure, on the basis
of a simulated dataset and a real application on sensor data from football. The
simulation study is also used to discuss the choice of the parameters of the
post-processing method and the LTS measure."
"Coarse-to-fine models and cascade segmentation architectures are widely
adopted to solve the problem of large scale variations in medical image
segmentation. However, those methods have two primary limitations: the
first-stage segmentation becomes a performance bottleneck; the lack of overall
differentiability makes the training process of two stages asynchronous and
inconsistent. In this paper, we propose a differentiable two-stage network
architecture to tackle these problems. In the first stage, a localization
network (L-Net) locates Regions of Interest (RoIs) in a detection fashion; in
the second stage, a segmentation network (S-Net) performs fine segmentation on
the recalibrated RoIs; a RoI recalibration module between L-Net and S-Net
eliminating the inconsistencies. Experimental results on the public dataset
show that our method outperforms state-of-the-art coarse-to-fine models with
negligible computation overheads."
"Prior research on self-supervised learning has led to considerable progress
on image classification, but often with degraded transfer performance on object
detection. The objective of this paper is to advance self-supervised pretrained
models specifically for object detection. Based on the inherent difference
between classification and detection, we propose a new self-supervised pretext
task, called instance localization. Image instances are pasted at various
locations and scales onto background images. The pretext task is to predict the
instance category given the composited images as well as the foreground
bounding boxes. We show that integration of bounding boxes into pretraining
promotes better task alignment and architecture alignment for transfer
learning. In addition, we propose an augmentation method on the bounding boxes
to further enhance the feature alignment. As a result, our model becomes weaker
at Imagenet semantic classification but stronger at image patch localization,
with an overall stronger pretrained model for object detection. Experimental
results demonstrate that our approach yields state-of-the-art transfer learning
results for object detection on PASCAL VOC and MSCOCO."
"Most recent person re-identification approaches are based on the use of deep
convolutional neural networks (CNNs). These networks, although effective in
multiple tasks such as classification or object detection, tend to focus on the
most discriminative part of an object rather than retrieving all its relevant
features. This behavior penalizes the performance of a CNN for the
re-identification task, since it should identify diverse and fine grained
features. It is then essential to make the network learn a wide variety of
finer characteristics in order to make the re-identification process of people
effective and robust to finer changes. In this article, we introduce Deep
Miner, a method that allows CNNs to ""mine"" richer and more diverse features
about people for their re-identification. Deep Miner is specifically composed
of three types of branches: a Global branch (G-branch), a Local branch
(L-branch) and an Input-Erased branch (IE-branch). G-branch corresponds to the
initial backbone which predicts global characteristics, while L-branch
retrieves part level resolution features. The IE-branch for its part, receives
partially suppressed feature maps as input thereby allowing the network to
""mine"" new features (those ignored by G-branch) as output. For this special
purpose, a dedicated suppression procedure for identifying and removing
features within a given CNN is introduced. This suppression procedure has the
major benefit of being simple, while it produces a model that significantly
outperforms state-of-the-art (SOTA) re-identification methods. Specifically, we
conduct experiments on four standard person re-identification benchmarks and
witness an absolute performance gain up to 6.5% mAP compared to SOTA."
"Gait recognition is an appealing biometric modality which aims to identify
individuals based on the way they walk. Deep learning has reshaped the research
landscape in this area since 2015 through the ability to automatically learn
discriminative representations. Gait recognition methods based on deep learning
now dominate the state-of-the-art in the field and have fostered real-world
applications. In this paper, we present a comprehensive overview of
breakthroughs and recent developments in gait recognition with deep learning,
and cover broad topics including datasets, test protocols, state-of-the-art
solutions, challenges, and future research directions. We first review the
commonly used gait datasets along with the principles designed for evaluating
them. We then propose a novel taxonomy made up of four separate dimensions
namely body representation, temporal representation, feature representation,
and neural architecture, to help characterize and organize the research
landscape and literature in this area. Following our proposed taxonomy, a
comprehensive survey of gait recognition methods using deep learning is
presented with discussions on their performances, characteristics, advantages,
and limitations. We conclude this survey with a discussion on current
challenges and mention a number of promising directions for future research in
gait recognition."
"Deep neural networks have been proved that they are vulnerable to adversarial
examples, which are generated by adding human-imperceptible perturbations to
images. To defend these adversarial examples, various detection based methods
have been proposed. However, most of them perform poorly on detecting
adversarial examples with extremely slight perturbations. By exploring these
adversarial examples, we find that there exists compliance between
perturbations and prediction confidence, which guides us to detect
few-perturbation attacks from the aspect of prediction confidence. To detect
both few-perturbation attacks and large-perturbation attacks, we propose a
method beyond image space by a two-stream architecture, in which the image
stream focuses on the pixel artifacts and the gradient stream copes with the
confidence artifacts. The experimental results show that the proposed method
outperforms the existing methods under oblivious attacks and is verified
effective to defend omniscient attacks as well."
"Recent methods for long-tailed instance segmentation still struggle on rare
object classes with few training data. We propose a simple yet effective
method, Feature Augmentation and Sampling Adaptation (FASA), that addresses the
data scarcity issue by augmenting the feature space especially for rare
classes. Both the Feature Augmentation (FA) and feature sampling components are
adaptive to the actual training status -- FA is informed by the feature mean
and variance of observed real samples from past iterations, and we sample the
generated virtual features in a loss-adapted manner to avoid over-fitting. FASA
does not require any elaborate loss design, and removes the need for
inter-class transfer learning that often involves large cost and
manually-defined head/tail class groups. We show FASA is a fast, generic method
that can be easily plugged into standard or long-tailed segmentation
frameworks, with consistent performance gains and little added cost. FASA is
also applicable to other tasks like long-tailed classification with
state-of-the-art performance."
"The ultimate goal for an inference model is to be robust and functional in
real life applications. However, training vs. test data domain gaps often
negatively affect model performance. This issue is especially critical for the
monocular 3D human pose estimation problem, in which 3D human data is often
collected in a controlled lab setting. In this paper, we focus on alleviating
the negative effect of domain shift in both appearance and pose space for 3D
human pose estimation by presenting our adapted human pose (AHuP) approach.
AHuP is built upon two key components: (1) semantically aware adaptation (SAA)
for the cross-domain feature space adaptation, and (2) skeletal pose adaptation
(SPA) for the pose space adaptation which takes only limited information from
the target domain. By using zero real 3D human pose data, one of our adapted
synthetic models shows comparable performance with the SOTA pose estimation
models trained with large scale real 3D human datasets. The proposed SPA can be
also employed independently as a light-weighted head to improve existing SOTA
models in a novel context. A new 3D scan-based synthetic human dataset called
ScanAva+ is also going to be publicly released with this work."
"Book covers are intentionally designed and provide an introduction to a book.
However, they typically require professional skills to design and produce the
cover images. Thus, we propose a generative neural network that can produce
book covers based on an easy-to-use layout graph. The layout graph contains
objects such as text, natural scene objects, and solid color spaces. This
layout graph is embedded using a graph convolutional neural network and then
used with a mask proposal generator and a bounding-box generator and filled
using an object proposal generator. Next, the objects are compiled into a
single image and the entire network is trained using a combination of
adversarial training, perceptual training, and reconstruction. Finally, a Style
Retention Network (SRNet) is used to transfer the learned font style onto the
desired text. Using the proposed method allows for easily controlled and unique
book covers."
"Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of
great importance for various applications such as robotics, inter-human and
human-object interaction in augmented reality. However, 360{\deg} video SHD has
been seldom discussed in the computer vision community due to a lack of
datasets with large-scale omnidirectional videos and rich annotations. To this
end, we propose SHD360, the first 360{\deg} video SHD dataset which contains
various real-life daily scenes. Since so far there is no method proposed for
360{\deg} image/video SHD, we systematically benchmark 11 representative
state-of-the-art salient object detection (SOD) approaches on our SHD360, and
explore key issues derived from extensive experimenting results. We hope our
proposed dataset and benchmark could serve as a good starting point for
advancing human-centric researches towards 360{\deg} panoramic data. The
dataset is available at https://github.com/PanoAsh/SHD360."
"Convolutional neural networks (CNNs) are highly successful for
super-resolution (SR) but often require sophisticated architectures with heavy
memory cost and computational overhead, significantly restricts their practical
deployments on resource-limited devices. In this paper, we proposed a novel
contrastive self-distillation (CSD) framework to simultaneously compress and
accelerate various off-the-shelf SR models. In particular, a channel-splitting
super-resolution network can first be constructed from a target teacher network
as a compact student network. Then, we propose a novel contrastive loss to
improve the quality of SR images and PSNR/SSIM via explicit knowledge transfer.
Extensive experiments demonstrate that the proposed CSD scheme effectively
compresses and accelerates several standard SR models such as EDSR, RCAN and
CARN. Code is available at https://github.com/Booooooooooo/CSD."
"Synthetic data has been applied in many deep learning based computer vision
tasks. Limited performance of algorithms trained solely on synthetic data has
been approached with domain adaptation techniques such as the ones based on
generative adversarial framework. We demonstrate how adversarial training alone
can introduce semantic inconsistencies in translated images. To tackle this
issue we propose density prematching strategy using KLIEP-based density ratio
estimation procedure. Finally, we show that aforementioned strategy improves
quality of translated images of underlying method and their usability for the
semantic segmentation task in the context of autonomous driving."
"Estimating geometric elements such as depth, camera motion, and optical flow
from images is an important part of the robot's visual perception. We use a
joint self-supervised method to estimate the three geometric elements. Depth
network, optical flow network and camera motion network are independent of each
other but are jointly optimized during training phase. Compared with
independent training, joint training can make full use of the geometric
relationship between geometric elements and provide dynamic and static
information of the scene. In this paper, we improve the joint self-supervision
method from three aspects: network structure, dynamic object segmentation, and
geometric constraints. In terms of network structure, we apply the attention
mechanism to the camera motion network, which helps to take advantage of the
similarity of camera movement between frames. And according to attention
mechanism in Transformer, we propose a plug-and-play convolutional attention
module. In terms of dynamic object, according to the different influences of
dynamic objects in the optical flow self-supervised framework and the
depth-pose self-supervised framework, we propose a threshold algorithm to
detect dynamic regions, and mask that in the loss function respectively. In
terms of geometric constraints, we use traditional methods to estimate the
fundamental matrix from the corresponding points to constrain the camera motion
network. We demonstrate the effectiveness of our method on the KITTI dataset.
Compared with other joint self-supervised methods, our method achieves
state-of-the-art performance in the estimation of pose and optical flow, and
the depth estimation has also achieved competitive results. Code will be
available https://github.com/jianfenglihg/Unsupervised_geometry."
"Image classification is an important task in various machine learning
applications. In recent years, a number of classification methods based on
quantum machine learning and different quantum image encoding techniques have
been proposed. In this paper, we study the effect of three different quantum
image encoding approaches on the performance of a convolution-inspired hybrid
quantum-classical image classification algorithm called quanvolutional neural
network (QNN). We furthermore examine the effect of variational - i.e.
trainable - quantum circuits on the classification results. Our experiments
indicate that some image encodings are better suited for variational circuits.
However, our experiments show as well that there is not one best image
encoding, but that the choice of the encoding depends on the specific
constraints of the application."
"Benefit from large-scale training data, recent advances in Siamese-based
object tracking have achieved compelling results on the normal sequences.
Whilst Siamese-based trackers assume training and test data follow an identical
distribution. Suppose there is a set of foggy or rainy test sequences, it
cannot be guaranteed that the trackers trained on the normal images perform
well on the data belonging to other domains. The problem of domain shift among
training and test data has already been discussed in object detection and
semantic segmentation areas, which, however, has not been investigated for
visual tracking. To this end, based on SiamRPN++, we introduce a Domain
Adaptive SiamRPN++, namely DASiamRPN++, to improve the cross-domain
transferability and robustness of a tracker. Inspired by A-distance theory, we
present two domain adaptive modules, Pixel Domain Adaptation (PDA) and Semantic
Domain Adaptation (SDA). The PDA module aligns the feature maps of template and
search region images to eliminate the pixel-level domain shift caused by
weather, illumination, etc. The SDA module aligns the feature representations
of the tracking target's appearance to eliminate the semantic-level domain
shift. PDA and SDA modules reduce the domain disparity by learning domain
classifiers in an adversarial training manner. The domain classifiers enforce
the network to learn domain-invariant feature representations. Extensive
experiments are performed on the standard datasets of two different domains,
including synthetic foggy and TIR sequences, which demonstrate the
transferability and domain adaptability of the proposed tracker."
"We propose an effective two-stage approach to tackle the problem of
language-based Human-centric Spatio-Temporal Video Grounding (HC-STVG) task. In
the first stage, we propose an Augmented 2D Temporal Adjacent Network
(Augmented 2D-TAN) to temporally ground the target moment corresponding to the
given description. Primarily, we improve the original 2D-TAN from two aspects:
First, a temporal context-aware Bi-LSTM Aggregation Module is developed to
aggregate clip-level representations, replacing the original max-pooling.
Second, we propose to employ Random Concatenation Augmentation (RCA) mechanism
during the training phase. In the second stage, we use pretrained MDETR model
to generate per-frame bounding boxes via language query, and design a set of
hand-crafted rules to select the best matching bounding box outputted by MDETR
for each frame within the grounded moment."
"Semantic segmentation is a challenging problem due to difficulties in
modeling context in complex scenes and class confusions along boundaries. Most
literature either focuses on context modeling or boundary refinement, which is
less generalizable in open-world scenarios. In this work, we advocate a unified
framework(UN-EPT) to segment objects by considering both context information
and boundary artifacts. We first adapt a sparse sampling strategy to
incorporate the transformer-based attention mechanism for efficient context
modeling. In addition, a separate spatial branch is introduced to capture image
details for boundary refinement. The whole model can be trained in an
end-to-end manner. We demonstrate promising performance on three popular
benchmarks for semantic segmentation with low memory footprint. Code will be
released soon."
"Deep convolutional neural networks (DCNNs) and the ventral visual pathway
share vast architectural and functional similarities in visual challenges such
as object recognition. Recent insights have demonstrated that both hierarchical
cascades can be compared in terms of both exerted behavior and underlying
activation. However, these approaches ignore key differences in spatial
priorities of information processing. In this proof-of-concept study, we
demonstrate a comparison of human observers (N = 45) and three feedforward
DCNNs through eye tracking and saliency maps. The results reveal fundamentally
different resolutions in both visualization methods that need to be considered
for an insightful comparison. Moreover, we provide evidence that a DCNN with
biologically plausible receptive field sizes called vNet reveals higher
agreement with human viewing behavior as contrasted with a standard ResNet
architecture. We find that image-specific factors such as category, animacy,
arousal, and valence have a direct link to the agreement of spatial object
recognition priorities in humans and DCNNs, while other measures such as
difficulty and general image properties do not. With this approach, we try to
open up new perspectives at the intersection of biological and computer vision
research."
"Feature pyramids have been proven powerful in image understanding tasks that
require multi-scale features. State-of-the-art methods for multi-scale feature
learning focus on performing feature interactions across space and scales using
neural networks with a fixed topology. In this paper, we propose graph feature
pyramid networks that are capable of adapting their topological structures to
varying intrinsic image structures and supporting simultaneous feature
interactions across all scales. We first define an image-specific superpixel
hierarchy for each input image to represent its intrinsic image structures. The
graph feature pyramid network inherits its structure from this superpixel
hierarchy. Contextual and hierarchical layers are designed to achieve feature
interactions within the same scale and across different scales. To make these
layers more powerful, we introduce two types of local channel attention for
graph neural networks by generalizing global channel attention for
convolutional neural networks. The proposed graph feature pyramid network can
enhance the multiscale features from a convolutional feature pyramid network.
We evaluate our graph feature pyramid network in the object detection task by
integrating it into the Faster R-CNN algorithm. The modified algorithm
outperforms not only previous state-of-the-art feature pyramid-based methods
with a clear margin but also other popular detection methods on both MS-COCO
2017 validation and test datasets."
"Online hashing methods usually learn the hash functions online, aiming to
efficiently adapt to the data variations in the streaming environment. However,
when the hash functions are updated, the binary codes for the whole database
have to be updated to be consistent with the hash functions, resulting in the
inefficiency in the online image retrieval process. In this paper, we propose a
novel online hashing framework without updating binary codes. In the proposed
framework, the hash functions are fixed and a parametric similarity function
for the binary codes is learnt online to adapt to the streaming data.
Specifically, a parametric similarity function that has a bilinear form is
adopted and a metric learning algorithm is proposed to learn the similarity
function online based on the characteristics of the hashing methods. The
experiments on two multi-label image datasets show that our method is
competitive or outperforms the state-of-the-art online hashing methods in terms
of both accuracy and efficiency for multi-label image retrieval."
"Recent unsupervised domain adaptation methods have utilized vicinal space
between the source and target domains. However, the equilibrium collapse of
labels, a problem where the source labels are dominant over the target labels
in the predictions of vicinal instances, has never been addressed. In this
paper, we propose an instance-wise minimax strategy that minimizes the entropy
of high uncertainty instances in the vicinal space to tackle the stated
problem. We divide the vicinal space into two subspaces through the solution of
the minimax problem: contrastive space and consensus space. In the contrastive
space, inter-domain discrepancy is mitigated by constraining instances to have
contrastive views and labels, and the consensus space reduces the confusion
between intra-domain categories. The effectiveness of our method is
demonstrated on public benchmarks, including Office-31, Office-Home, and
VisDA-C, achieving state-of-the-art performances. We further show that our
method outperforms the current state-of-the-art methods on PACS, which
indicates that our instance-wise approach works well for multi-source domain
adaptation as well. Code is available at https://github.com/NaJaeMin92/CoVi."
"Learning-based optical flow estimation has been dominated with the pipeline
of cost volume with convolutions for flow regression, which is inherently
limited to local correlations and thus is hard to address the long-standing
challenge of large displacements. To alleviate this, the state-of-the-art
framework RAFT gradually improves its prediction quality by using a large
number of iterative refinements, achieving remarkable performance but
introducing linearly increasing inference time. To enable both high accuracy
and efficiency, we completely revamp the dominant flow regression pipeline by
reformulating optical flow as a global matching problem, which identifies the
correspondences by directly comparing feature similarities. Specifically, we
propose a GMFlow framework, which consists of three main components: a
customized Transformer for feature enhancement, a correlation and softmax layer
for global feature matching, and a self-attention layer for flow propagation.
We further introduce a refinement step that reuses GMFlow at higher feature
resolution for residual flow prediction. Our new framework outperforms
31-refinements RAFT on the challenging Sintel benchmark, while using only one
refinement and running faster, suggesting a new paradigm for accurate and
efficient optical flow estimation. Code is available at
https://github.com/haofeixu/gmflow."
"Convolution and self-attention are two powerful techniques for representation
learning, and they are usually considered as two peer approaches that are
distinct from each other. In this paper, we show that there exists a strong
underlying relation between them, in the sense that the bulk of computations of
these two paradigms are in fact done with the same operation. Specifically, we
first show that a traditional convolution with kernel size k x k can be
decomposed into k^2 individual 1x1 convolutions, followed by shift and
summation operations. Then, we interpret the projections of queries, keys, and
values in self-attention module as multiple 1x1 convolutions, followed by the
computation of attention weights and aggregation of the values. Therefore, the
first stage of both two modules comprises the similar operation. More
importantly, the first stage contributes a dominant computation complexity
(square of the channel size) comparing to the second stage. This observation
naturally leads to an elegant integration of these two seemingly distinct
paradigms, i.e., a mixed model that enjoys the benefit of both self-Attention
and Convolution (ACmix), while having minimum computational overhead compared
to the pure convolution or self-attention counterpart. Extensive experiments
show that our model achieves consistently improved results over competitive
baselines on image recognition and downstream tasks. Code and pre-trained
models will be released at https://github.com/LeapLabTHU/ACmix and
https://gitee.com/mindspore/models."
"Multi-person pose estimation methods generally follow top-down and bottom-up
paradigms, both of which can be considered as two-stage approaches thus leading
to the high computation cost and low efficiency. Towards a compact and
efficient pipeline for multi-person pose estimation task, in this paper, we
propose to represent the human parts as points and present a novel body
representation, which leverages an adaptive point set including the human
center and seven human-part related points to represent the human instance in a
more fine-grained manner. The novel representation is more capable of capturing
the various pose deformation and adaptively factorizes the long-range
center-to-joint displacement thus delivers a single-stage differentiable
network to more precisely regress multi-person pose, termed as AdaptivePose.
For inference, our proposed network eliminates the grouping as well as
refinements and only needs a single-step disentangling process to form
multi-person pose. Without any bells and whistles, we achieve the best
speed-accuracy trade-offs of 67.4% AP / 29.4 fps with DLA-34 and 71.3% AP / 9.1
fps with HRNet-W48 on COCO test-dev dataset."
"Thanks to the rapid advances in deep learning techniques and the wide
availability of large-scale training sets, the performance of video saliency
detection models has been improving steadily and significantly. However, deep
learning-based visualaudio fixation prediction is still in its infancy. At
present, only a few visual-audio sequences have been furnished, with real
fixations being recorded in real visual-audio environments. Hence, it would be
neither efficient nor necessary to recollect real fixations under the same
visual-audio circumstances. To address this problem, this paper promotes a
novel approach in a weakly supervised manner to alleviate the demand of
large-scale training sets for visual-audio model training. By using only the
video category tags, we propose the selective class activation mapping (SCAM)
and its upgrade (SCAM+). In the spatial-temporal-audio circumstance, the former
follows a coarse-to-fine strategy to select the most discriminative regions,
and these regions are usually capable of exhibiting high consistency with the
real human-eye fixations. The latter equips the SCAM with an additional
multi-granularity perception mechanism, making the whole process more
consistent with that of the real human visual system. Moreover, we distill
knowledge from these regions to obtain complete new spatial-temporal-audio
(STA) fixation prediction (FP) networks, enabling broad applications in cases
where video tags are not available. Without resorting to any real human-eye
fixation, the performances of these STA FP networks are comparable to those of
fully supervised networks. The code and results are publicly available at
https://github.com/guotaowang/STANet."
"Clustering-based unsupervised domain adaptive (UDA) person re-identification
(ReID) reduces exhaustive annotations. However, owing to unsatisfactory feature
embedding and imperfect clustering, pseudo labels for target domain data
inherently contain an unknown proportion of wrong ones, which would mislead
feature learning. In this paper, we propose an approach named probabilistic
uncertainty guided progressive label refinery (P$^2$LR) for domain adaptive
person re-identification. First, we propose to model the labeling uncertainty
with the probabilistic distance along with ideal single-peak distributions. A
quantitative criterion is established to measure the uncertainty of pseudo
labels and facilitate the network training. Second, we explore a progressive
strategy for refining pseudo labels. With the uncertainty-guided alternative
optimization, we balance between the exploration of target domain data and the
negative effects of noisy labeling. On top of a strong baseline, we obtain
significant improvements and achieve the state-of-the-art performance on four
UDA ReID benchmarks. Specifically, our method outperforms the baseline by 6.5%
mAP on the Duke2Market task, while surpassing the state-of-the-art method by
2.5% mAP on the Market2MSMT task."
"In this research, an attention-based depthwise separable neural network with
Bayesian optimization (ADSNN-BO) is proposed to detect and classify rice
disease from rice leaf images. Rice diseases frequently result in 20 to 40 \%
corp production loss in yield and is highly related to the global economy.
Rapid disease identification is critical to plan treatment promptly and reduce
the corp losses. Rice disease diagnosis is still mainly performed manually. To
achieve AI assisted rapid and accurate disease detection, we proposed the
ADSNN-BO model based on MobileNet structure and augmented attention mechanism.
Moreover, Bayesian optimization method is applied to tune hyper-parameters of
the model. Cross-validated classification experiments are conducted based on a
public rice disease dataset with four categories in total. The experimental
results demonstrate that our mobile compatible ADSNN-BO model achieves a test
accuracy of 94.65\%, which outperforms all of the state-of-the-art models
tested. To check the interpretability of our proposed model, feature analysis
including activation map and filters visualization approach are also conducted.
Results show that our proposed attention-based mechanism can more effectively
guide the ADSNN-BO model to learn informative features. The outcome of this
research will promote the implementation of artificial intelligence for fast
plant disease diagnosis and control in the agricultural field."
"Transfer learning is a powerful way to adapt existing deep learning models to
new emerging use-cases in remote sensing. Starting from a neural network
already trained for semantic segmentation, we propose to modify its label space
to swiftly adapt it to new classes under weak supervision. To alleviate the
background shift and the catastrophic forgetting problems inherent to this form
of continual learning, we compare different regularization terms and leverage a
pseudo-label strategy. We experimentally show the relevance of our approach on
three public remote sensing datasets. Code is open-source and released in this
repository:
https://github.com/alteia-ai/ICSS}{https://github.com/alteia-ai/ICSS."
"Scene-text recognition is remarkably better in Latin languages than the
non-Latin languages due to several factors like multiple fonts, simplistic
vocabulary statistics, updated data generation tools, and writing systems. This
paper examines the possible reasons for low accuracy by comparing English
datasets with non-Latin languages. We compare various features like the size
(width and height) of the word images and word length statistics. Over the last
decade, generating synthetic datasets with powerful deep learning techniques
has tremendously improved scene-text recognition. Several controlled
experiments are performed on English, by varying the number of (i) fonts to
create the synthetic data and (ii) created word images. We discover that these
factors are critical for the scene-text recognition systems. The English
synthetic datasets utilize over 1400 fonts while Arabic and other non-Latin
datasets utilize less than 100 fonts for data generation. Since some of these
languages are a part of different regions, we garner additional fonts through a
region-based search to improve the scene-text recognition models in Arabic and
Devanagari. We improve the Word Recognition Rates (WRRs) on Arabic MLT-17 and
MLT-19 datasets by 24.54% and 2.32% compared to previous works or baselines. We
achieve WRR gains of 7.88% and 3.72% for IIIT-ILST and MLT-19 Devanagari
datasets."
"Motion, as the most distinct phenomenon in a video to involve the changes
over time, has been unique and critical to the development of video
representation learning. In this paper, we ask the question: how important is
the motion particularly for self-supervised video representation learning. To
this end, we compose a duet of exploiting the motion for data augmentation and
feature learning in the regime of contrastive learning. Specifically, we
present a Motion-focused Contrastive Learning (MCL) method that regards such
duet as the foundation. On one hand, MCL capitalizes on optical flow of each
frame in a video to temporally and spatially sample the tubelets (i.e.,
sequences of associated frame patches across time) as data augmentations. On
the other hand, MCL further aligns gradient maps of the convolutional layers to
optical flow maps from spatial, temporal and spatio-temporal perspectives, in
order to ground motion information in feature learning. Extensive experiments
conducted on R(2+1)D backbone demonstrate the effectiveness of our MCL. On
UCF101, the linear classifier trained on the representations learnt by MCL
achieves 81.91% top-1 accuracy, outperforming ImageNet supervised pre-training
by 6.78%. On Kinetics-400, MCL achieves 66.62% top-1 accuracy under the linear
protocol. Code is available at
https://github.com/YihengZhang-CV/MCL-Motion-Focused-Contrastive-Learning."
"Reference-based super-resolution (RefSR) has made significant progress in
producing realistic textures using an external reference (Ref) image. However,
existing RefSR methods obtain high-quality correspondence matchings consuming
quadratic computation resources with respect to the input size, limiting its
application. Moreover, these approaches usually suffer from scale misalignments
between the low-resolution (LR) image and Ref image. In this paper, we propose
an Accelerated Multi-Scale Aggregation network (AMSA) for Reference-based
Super-Resolution, including Coarse-to-Fine Embedded PatchMatch (CFE-PatchMatch)
and Multi-Scale Dynamic Aggregation (MSDA) module. To improve matching
efficiency, we design a novel Embedded PatchMacth scheme with random samples
propagation, which involves end-to-end training with asymptotic linear
computational cost to the input size. To further reduce computational cost and
speed up convergence, we apply the coarse-to-fine strategy on Embedded
PatchMacth constituting CFE-PatchMatch. To fully leverage reference information
across multiple scales and enhance robustness to scale misalignment, we develop
the MSDA module consisting of Dynamic Aggregation and Multi-Scale Aggregation.
The Dynamic Aggregation corrects minor scale misalignment by dynamically
aggregating features, and the Multi-Scale Aggregation brings robustness to
large scale misalignment by fusing multi-scale information. Experimental
results show that the proposed AMSA achieves superior performance over
state-of-the-art approaches on both quantitative and qualitative evaluations."
"Human gait is considered a unique biometric identifier which can be acquired
in a covert manner at a distance. However, models trained on existing public
domain gait datasets which are captured in controlled scenarios lead to drastic
performance decline when applied to real-world unconstrained gait data. On the
other hand, video person re-identification techniques have achieved promising
performance on large-scale publicly available datasets. Given the diversity of
clothing characteristics, clothing cue is not reliable for person recognition
in general. So, it is actually not clear why the state-of-the-art person
re-identification methods work as well as they do. In this paper, we construct
a new gait dataset by extracting silhouettes from an existing video person
re-identification challenge which consists of 1,404 persons walking in an
unconstrained manner. Based on this dataset, a consistent and comparative study
between gait recognition and person re-identification can be carried out. Given
that our experimental results show that current gait recognition approaches
designed under data collected in controlled scenarios are inappropriate for
real surveillance scenarios, we propose a novel gait recognition method, called
RealGait. Our results suggest that recognizing people by their gait in real
surveillance scenarios is feasible and the underlying gait pattern is probably
the true reason why video person re-idenfification works in practice."
"Learning from different data views by exploring the underlying complementary
information among them can endow the representation with stronger expressive
ability. However, high-dimensional features tend to contain noise, and
furthermore, the quality of data usually varies for different samples (even for
different views), i.e., one view may be informative for one sample but not the
case for another. Therefore, it is quite challenging to integrate multi-view
noisy data under unsupervised setting. Traditional multi-view methods either
simply treat each view with equal importance or tune the weights of different
views to fixed values, which are insufficient to capture the dynamic noise in
multi-view data. In this work, we devise a novel unsupervised multi-view
learning approach, termed as Dynamic Uncertainty-Aware Networks (DUA-Nets).
Guided by the uncertainty of data estimated from the generation perspective,
intrinsic information from multiple views is integrated to obtain noise-free
representations. Under the help of uncertainty, DUA-Nets weigh each view of
individual sample according to data quality so that the high-quality samples
(or views) can be fully exploited while the effects from the noisy samples (or
views) will be alleviated. Our model achieves superior performance in extensive
experiments and shows the robustness to noisy data."
"Deep learning-based models have been shown to improve the accuracy of
fingerprint recognition. While these algorithms show exceptional performance,
they require large-scale fingerprint datasets for training and evaluation. In
this work, we propose a novel fingerprint synthesis and reconstruction
framework based on the StyleGan2 architecture, to address the privacy issues
related to the acquisition of such large-scale datasets. We also derive a
computational approach to modify the attributes of the generated fingerprint
while preserving their identity. This allows synthesizing multiple different
fingerprint images per finger. In particular, we introduce the SynFing
synthetic fingerprints dataset consisting of 100K image pairs, each pair
corresponding to the same identity. The proposed framework was experimentally
shown to outperform contemporary state-of-the-art approaches for both
fingerprint synthesis and reconstruction. It significantly improved the realism
of the generated fingerprints, both visually and in terms of their ability to
spoof fingerprint-based verification systems. The code and fingerprints dataset
are publicly available: https://github.com/rafaelbou/fingerprint_generator."
"This paper describes a system prepared at Brno University of Technology for
ICDAR 2021 Competition on Historical Document Classification, experiments
leading to its design, and the main findings. The solved tasks include script
and font classification, document origin localization, and dating. We combined
patch-level and line-level approaches, where the line-level system utilizes an
existing, publicly available page layout analysis engine. In both systems,
neural networks provide local predictions which are combined into page-level
decisions, and the results of both systems are fused using linear or log-linear
interpolation. We propose loss functions suitable for weakly supervised
classification problem where multiple possible labels are provided, and we
propose loss functions suitable for interval regression in the dating task. The
line-level system significantly improves results in script and font
classification and in the dating task. The full system achieved 98.48 %, 88.84
%, and 79.69 % accuracy in the font, script, and location classification tasks
respectively. In the dating task, our system achieved a mean absolute error of
21.91 years. Our system achieved the best results in all tasks and became the
overall winner of the competition."
"Image deblurring is a classic problem in low-level computer vision with the
aim to recover a sharp image from a blurred input image. Advances in deep
learning have led to significant progress in solving this problem, and a large
number of deblurring networks have been proposed. This paper presents a
comprehensive and timely survey of recently published deep-learning based image
deblurring approaches, aiming to serve the community as a useful literature
review. We start by discussing common causes of image blur, introduce benchmark
datasets and performance metrics, and summarize different problem formulations.
Next, we present a taxonomy of methods using convolutional neural networks
(CNN) based on architecture, loss function, and application, offering a
detailed review and comparison. In addition, we discuss some domain-specific
deblurring applications including face images, text, and stereo image pairs. We
conclude by discussing key challenges and future research directions."
"The goal of person search is to localize a target person from a gallery set
of scene images, which is extremely challenging due to large scale variations,
pose/viewpoint changes, and occlusions. In this paper, we propose the Cascade
Occluded Attention Transformer (COAT) for end-to-end person search. Our
three-stage cascade design focuses on detecting people in the first stage,
while later stages simultaneously and progressively refine the representation
for person detection and re-identification. At each stage the occluded
attention transformer applies tighter intersection over union thresholds,
forcing the network to learn coarse-to-fine pose/scale invariant features.
Meanwhile, we calculate each detection's occluded attention to differentiate a
person's tokens from other people or the background. In this way, we simulate
the effect of other objects occluding a person of interest at the token-level.
Through comprehensive experiments, we demonstrate the benefits of our method by
achieving state-of-the-art performance on two benchmark datasets."
"Recently self-supervised representation learning has drawn considerable
attention from the scene text recognition community. Different from previous
studies using contrastive learning, we tackle the issue from an alternative
perspective, i.e., by formulating the representation learning scheme in a
generative manner. Typically, the neighboring image patches among one text line
tend to have similar styles, including the strokes, textures, colors, etc.
Motivated by this common sense, we augment one image patch and use its
neighboring patch as guidance to recover itself. Specifically, we propose a
Similarity-Aware Normalization (SimAN) module to identify the different
patterns and align the corresponding styles from the guiding patch. In this
way, the network gains representation capability for distinguishing complex
patterns such as messy strokes and cluttered backgrounds. Experiments show that
the proposed SimAN significantly improves the representation quality and
achieves promising performance. Moreover, we surprisingly find that our
self-supervised generative network has impressive potential for data synthesis,
text image editing, and font interpolation, which suggests that the proposed
SimAN has a wide range of practical applications."
"With the advent of convolutional neural networks, stereo matching algorithms
have recently gained tremendous progress. However, it remains a great challenge
to accurately extract disparities from real-world image pairs taken by
consumer-level devices like smartphones, due to practical complicating factors
such as thin structures, non-ideal rectification, camera module inconsistencies
and various hard-case scenes. In this paper, we propose a set of innovative
designs to tackle the problem of practical stereo matching: 1) to better
recover fine depth details, we design a hierarchical network with recurrent
refinement to update disparities in a coarse-to-fine manner, as well as a
stacked cascaded architecture for inference; 2) we propose an adaptive group
correlation layer to mitigate the impact of erroneous rectification; 3) we
introduce a new synthetic dataset with special attention to difficult cases for
better generalizing to real-world scenes. Our results not only rank 1st on both
Middlebury and ETH3D benchmarks, outperforming existing state-of-the-art
methods by a notable margin, but also exhibit high-quality details for
real-life photos, which clearly demonstrates the efficacy of our contributions."
"Semantic face editing has achieved substantial progress in recent years.
Known as a growingly popular method, latent space manipulation performs face
editing by changing the latent code of an input face to liberate users from
painting skills. However, previous latent space manipulation methods usually
encode an entire face into a single low-dimensional embedding, which constrains
the reconstruction capacity and the control flexibility of facial components,
such as eyes and nose. This paper proposes IA-FaceS as a bidirectional method
for disentangled face attribute manipulation as well as flexible, controllable
component editing without the need for segmentation masks or sketches in the
original image. To strike a balance between the reconstruction capacity and the
control flexibility, the encoder is designed as a multi-head structure to yield
embeddings for reconstruction and control, respectively: a high-dimensional
tensor with spatial properties for consistent reconstruction and four
low-dimensional facial component embeddings for semantic face editing.
Manipulating the separate component embeddings can help achieve disentangled
attribute manipulation and flexible control of facial components. To further
disentangle the highly-correlated components, a component adaptive modulation
(CAM) module is proposed for the decoder. The semantic single-eye editing is
developed for the first time without any input visual guidance, such as
segmentation masks or sketches. According to the experimental results, IA-FaceS
establishes a good balance between maintaining image details and performing
flexible face manipulation. Both quantitative and qualitative results indicate
that the proposed method outperforms the other techniques in reconstruction,
face attribute manipulation, and component transfer."
"Class-Incremental Learning (CIL) struggles with catastrophic forgetting when
learning new knowledge, and Data-Free CIL (DFCIL) is even more challenging
without access to the training data of previously learned classes. Though
recent DFCIL works introduce techniques such as model inversion to synthesize
data for previous classes, they fail to overcome forgetting due to the severe
domain gap between the synthetic and real data. To address this issue, this
paper proposes relation-guided representation learning (RRL) for DFCIL, dubbed
R-DFCIL. In RRL, we introduce relational knowledge distillation to flexibly
transfer the structural relation of new data from the old model to the current
model. Our RRL-boosted DFCIL can guide the current model to learn
representations of new classes better compatible with representations of
previous classes, which greatly reduces forgetting while improving plasticity.
To avoid the mutual interference between representation and classifier
learning, we employ local rather than global classification loss during RRL.
After RRL, the classification head is refined with global class-balanced
classification loss to address the data imbalance issue as well as learn the
decision boundaries between new and previous classes. Extensive experiments on
CIFAR100, Tiny-ImageNet200, and ImageNet100 demonstrate that our R-DFCIL
significantly surpasses previous approaches and achieves a new state-of-the-art
performance for DFCIL. Code is available at
https://github.com/jianzhangcs/R-DFCIL"
"Facial expression recognition (FER) is still one challenging research due to
the small inter-class discrepancy in the facial expression data. In view of the
significance of facial crucial regions for FER, many existing researches
utilize the prior information from some annotated crucial points to improve the
performance of FER. However, it is complicated and time-consuming to manually
annotate facial crucial points, especially for vast wild expression images.
Based on this, a local non-local joint network is proposed to adaptively light
up the facial crucial regions in feature learning of FER in this paper. In the
proposed method, two parts are constructed based on facial local and non-local
information respectively, where an ensemble of multiple local networks are
proposed to extract local features corresponding to multiple facial local
regions and a non-local attention network is addressed to explore the
significance of each local region. Especially, the attention weights obtained
by the non-local network is fed into the local part to achieve the interactive
feedback between the facial global and local information. Interestingly, the
non-local weights corresponding to local regions are gradually updated and
higher weights are given to more crucial regions. Moreover, U-Net is employed
to extract the integrated features of deep semantic information and low
hierarchical detail information of expression images. Finally, experimental
results illustrate that the proposed method achieves more competitive
performance compared with several state-of-the art methods on five benchmark
datasets. Noticeably, the analyses of the non-local weights corresponding to
local regions demonstrate that the proposed method can automatically enhance
some crucial regions in the process of feature learning without any facial
landmark information."
"We address the task of predicting what parts of an object can open and how
they move when they do so. The input is a single image of an object, and as
output we detect what parts of the object can open, and the motion parameters
describing the articulation of each openable part. To tackle this task, we
create two datasets of 3D objects: OPDSynth based on existing synthetic
objects, and OPDReal based on RGBD reconstructions of real objects. We then
design OPDRCNN, a neural architecture that detects openable parts and predicts
their motion parameters. Our experiments show that this is a challenging task
especially when considering generalization across object categories, and the
limited amount of information in a single image. Our architecture outperforms
baselines and prior work especially for RGB image inputs. Short video summary
at https://www.youtube.com/watch?v=P85iCaD0rfc"
"Recent advances show that Generative Adversarial Networks (GANs) can
synthesize images with smooth variations along semantically meaningful latent
directions, such as pose, expression, layout, etc. While this indicates that
GANs implicitly learn pixel-level correspondences across images, few studies
explored how to extract them explicitly. In this work, we introduce Coordinate
GAN (CoordGAN), a structure-texture disentangled GAN that learns a dense
correspondence map for each generated image. We represent the correspondence
maps of different images as warped coordinate frames transformed from a
canonical coordinate frame, i.e., the correspondence map, which describes the
structure (e.g., the shape of a face), is controlled via a transformation.
Hence, finding correspondences boils down to locating the same coordinate in
different correspondence maps. In CoordGAN, we sample a transformation to
represent the structure of a synthesized instance, while an independent texture
branch is responsible for rendering appearance details orthogonal to the
structure. Our approach can also extract dense correspondence maps for real
images by adding an encoder on top of the generator. We quantitatively
demonstrate the quality of the learned dense correspondences through
segmentation mask transfer on multiple datasets. We also show that the proposed
generator achieves better structure and texture disentanglement compared to
existing approaches. Project page: https://jitengmu.github.io/CoordGAN/"
"Since the rise of vision-language navigation (VLN), great progress has been
made in instruction following -- building a follower to navigate environments
under the guidance of instructions. However, far less attention has been paid
to the inverse task: instruction generation -- learning a speaker~to generate
grounded descriptions for navigation routes. Existing VLN methods train a
speaker independently and often treat it as a data augmentation tool to
strengthen the follower while ignoring rich cross-task relations. Here we
describe an approach that learns the two tasks simultaneously and exploits
their intrinsic correlations to boost the training of each: the follower judges
whether the speaker-created instruction explains the original navigation route
correctly, and vice versa. Without the need of aligned instruction-path pairs,
such cycle-consistent learning scheme is complementary to task-specific
training targets defined on labeled data, and can also be applied over
unlabeled paths (sampled without paired instructions). Another agent,
called~creator is added to generate counterfactual environments. It greatly
changes current scenes yet leaves novel items -- which are vital for the
execution of original instructions -- unchanged. Thus more informative training
scenes are synthesized and the three agents compose a powerful VLN learning
system. Extensive experiments on a standard benchmark show that our approach
improves the performance of various follower models and produces accurate
navigation instructions."
"Recently, video frame interpolation using a combination of frame- and
event-based cameras has surpassed traditional image-based methods both in terms
of performance and memory efficiency. However, current methods still suffer
from (i) brittle image-level fusion of complementary interpolation results,
that fails in the presence of artifacts in the fused image, (ii) potentially
temporally inconsistent and inefficient motion estimation procedures, that run
for every inserted frame and (iii) low contrast regions that do not trigger
events, and thus cause events-only motion estimation to generate artifacts.
Moreover, previous methods were only tested on datasets consisting of planar
and faraway scenes, which do not capture the full complexity of the real world.
In this work, we address the above problems by introducing multi-scale
feature-level fusion and computing one-shot non-linear inter-frame motion from
events and images, which can be efficiently sampled for image warping. We also
collect the first large-scale events and frames dataset consisting of more than
100 challenging scenes with depth variations, captured with a new experimental
setup based on a beamsplitter. We show that our method improves the
reconstruction quality by up to 0.2 dB in terms of PSNR and up to 15% in LPIPS
score."
"In this work we introduce Sen4AgriNet, a Sentinel-2 based time series multi
country benchmark dataset, tailored for agricultural monitoring applications
with Machine and Deep Learning. Sen4AgriNet dataset is annotated from farmer
declarations collected via the Land Parcel Identification System (LPIS) for
harmonizing country wide labels. These declarations have only recently been
made available as open data, allowing for the first time the labeling of
satellite imagery from ground truth data. We proceed to propose and standardise
a new crop type taxonomy across Europe that address Common Agriculture Policy
(CAP) needs, based on the Food and Agriculture Organization (FAO) Indicative
Crop Classification scheme. Sen4AgriNet is the only multi-country, multi-year
dataset that includes all spectral information. It is constructed to cover the
period 2016-2020 for Catalonia and France, while it can be extended to include
additional countries. Currently, it contains 42.5 million parcels, which makes
it significantly larger than other available archives. We extract two
sub-datasets to highlight its value for diverse Deep Learning applications; the
Object Aggregated Dataset (OAD) and the Patches Assembled Dataset (PAD). OAD
capitalizes zonal statistics of each parcel, thus creating a powerful
label-to-features instance for classification algorithms. On the other hand,
PAD structure generalizes the classification problem to parcel extraction and
semantic segmentation and labeling. The PAD and OAD are examined under three
different scenarios to showcase and model the effects of spatial and temporal
variability across different years and different countries."
"State-of-the-art machine learning models, and especially deep learning ones,
are significantly data-hungry; they require vast amounts of manually labeled
samples to function correctly. However, in most medical imaging fields,
obtaining said data can be challenging. Not only the volume of data is a
problem, but also the imbalances within its classes; it is common to have many
more images of healthy patients than of those with pathology. Computer-aided
diagnostic systems suffer from these issues, usually over-designing their
models to perform accurately. This work proposes using self-supervised learning
for wireless endoscopy videos by introducing a custom-tailored method that does
not initially need labels or appropriate balance. We prove that using the
inferred inherent structure learned by our method, extracted from the temporal
axis, improves the detection rate on several domain-specific applications even
under severe imbalance."
"Biometric authentication using gait has become a promising field due to its
unobtrusive nature. Recent approaches in model-based gait recognition
techniques utilize spatio-temporal graphs for the elegant extraction of gait
features. However, existing methods often rely on multi-scale operators for
extracting long-range relationships among joints resulting in biased weighting.
In this paper, we present HEATGait, a gait recognition system that improves the
existing multi-scale graph convolution by efficient hop-extraction technique to
alleviate the issue. Combined with preprocessing and augmentation techniques,
we propose a powerful feature extractor that utilizes ResGCN to achieve
state-of-the-art performance in model-based gait recognition on the CASIA-B
gait dataset."
"High-quality point clouds have practical significance for point-based
rendering, semantic understanding, and surface reconstruction. Upsampling
sparse, noisy and nonuniform point clouds for a denser and more regular
approximation of target objects is a desirable but challenging task. Most
existing methods duplicate point features for upsampling, constraining the
upsampling scales at a fixed rate. In this work, the flexible upsampling rates
are achieved via edge vector based affine combinations, and a novel design of
Edge Vector based Approximation for Flexible-scale Point clouds Upsampling
(PU-EVA) is proposed. The edge vector based approximation encodes the
neighboring connectivity via affine combinations based on edge vectors, and
restricts the approximation error within the second-order term of Taylor's
Expansion. The EVA upsampling decouples the upsampling scales with network
architecture, achieving the flexible upsampling rates in one-time training.
Qualitative and quantitative evaluations demonstrate that the proposed PU-EVA
outperforms the state-of-the-art in terms of proximity-to-surface, distribution
uniformity, and geometric details preservation."
"Analysis of indoor spaces requires topological information. In this paper, we
propose to extract topological information from room attributes using what we
call Iterative and adaptive graph Topology Learning (ITL). ITL progressively
predicts multiple relations between rooms; at each iteration, it improves node
embeddings, which in turn facilitates generation of a better topological graph
structure. This notion of iterative improvement of node embeddings and
topological graph structure is in the same spirit as \cite{chen2020iterative}.
However, while \cite{chen2020iterative} computes the adjacency matrix based on
node similarity, we learn the graph metric using a relational decoder to
extract room correlations. Experiments using a new challenging indoor dataset
validate our proposed method. Qualitative and quantitative evaluation for
layout topology prediction and floorplan generation applications also
demonstrate the effectiveness of ITL."
"The purpose of signal extrapolation is to estimate unknown signal parts from
known samples. This task is especially important for error concealment in image
and video communication. For obtaining a high quality reconstruction,
assumptions have to be made about the underlying signal in order to solve this
underdetermined problem. Among existent reconstruction algorithms, frequency
selective extrapolation (FSE) achieves high performance by assuming that image
signals can be sparsely represented in the frequency domain. However, FSE does
not take into account the low-pass behaviour of natural images. In this paper,
we propose a modified FSE that takes this prior knowledge into account for the
modelling, yielding significant PSNR gains."
"Inspired from the assets of handcrafted and deep learning approaches, we
proposed a RARITYNet: RARITY guided affective emotion learning framework to
learn the appearance features and identify the emotion class of facial
expressions. The RARITYNet framework is designed by combining the shallow
(RARITY) and deep (AffEmoNet) features to recognize the facial expressions from
challenging images as spontaneous expressions, pose variations, ethnicity
changes, and illumination conditions. The RARITY is proposed to encode the
inter-radial transitional patterns in the local neighbourhood. The AffEmoNet:
affective emotion learning network is proposed by incorporating three feature
streams: high boost edge filtering (HBSEF) stream, to extract the edge
information of highly affected facial expressive regions, multi-scale
sophisticated edge cumulative (MSSEC) stream is to learns the sophisticated
edge information from multi-receptive fields and RARITY uplift complementary
context feature (RUCCF) stream refines the RARITY-encoded features and aid the
MSSEC stream features to enrich the learning ability of RARITYNet."
"We introduce UViM, a unified approach capable of modeling a wide range of
computer vision tasks. In contrast to previous models, UViM has the same
functional form for all tasks; it requires no task-specific modifications which
require extensive human expertise. The approach involves two components: (I) a
base model (feed-forward) which is trained to directly predict raw vision
outputs, guided by a learned discrete code and (II) a language model
(autoregressive) that is trained to generate the guiding code. These components
complement each other: the language model is well-suited to modeling structured
interdependent data, while the base model is efficient at dealing with
high-dimensional outputs. We demonstrate the effectiveness of UViM on three
diverse and challenging vision tasks: panoptic segmentation, depth prediction
and image colorization, where we achieve competitive and near state-of-the-art
results. Our experimental results suggest that UViM is a promising candidate
for a unified modeling approach in computer vision."
"Active contour models have achieved prominent success in the area of image
segmentation, allowing complex objects to be segmented from the background for
further analysis. Existing models can be divided into region-based active
contour models and edge-based active contour models. However, both models use
direct image data to achieve segmentation and face many challenging problems in
terms of the initial contour position, noise sensitivity, local minima and
inefficiency owing to the in-homogeneity of image intensities. The saliency map
of an image changes the image representation, making it more visual and
meaningful. In this study, we propose a novel model that uses the advantages of
a saliency map with local image information (LIF) and overcomes the drawbacks
of previous models. The proposed model is driven by a saliency map of an image
and the local image information to enhance the progress of the active contour
models. In this model, the saliency map of an image is first computed to find
the saliency driven local fitting energy. Then, the saliency-driven local
fitting energy is combined with the LIF model, resulting in a final novel
energy functional. This final energy functional is formulated through a level
set formulation, and regulation terms are added to evolve the contour more
precisely across the object boundaries. The quality of the proposed method was
verified on different synthetic images, real images and publicly available
datasets, including medical images. The image segmentation results, and
quantitative comparisons confirmed the contour initialization independence,
noise insensitivity, and superior segmentation accuracy of the proposed model
in comparison to the other segmentation models."
"Scene segmentation and classification (SSC) serve as a critical step towards
the field of video structuring analysis. Intuitively, jointly learning of these
two tasks can promote each other by sharing common information. However, scene
segmentation concerns more on the local difference between adjacent shots while
classification needs the global representation of scene segments, which
probably leads to the model dominated by one of the two tasks in the training
phase. In this paper, from an alternate perspective to overcome the above
challenges, we unite these two tasks into one task by a new form of predicting
shots link: a link connects two adjacent shots, indicating that they belong to
the same scene or category. To the end, we propose a general One Stage
Multimodal Sequential Link Framework (OS-MSL) to both distinguish and leverage
the two-fold semantics by reforming the two learning tasks into a unified one.
Furthermore, we tailor a specific module called DiffCorrNet to explicitly
extract the information of differences and correlations among shots. Extensive
experiments on a brand-new large scale dataset collected from real-world
applications, and MovieScenes are conducted. Both the results demonstrate the
effectiveness of our proposed method against strong baselines."
"We present OSFormer, the first one-stage transformer framework for
camouflaged instance segmentation (CIS). OSFormer is based on two key designs.
First, we design a location-sensing transformer (LST) to obtain the location
label and instance-aware parameters by introducing the location-guided queries
and the blend-convolution feedforward network. Second, we develop a
coarse-to-fine fusion (CFF) to merge diverse context information from the LST
encoder and CNN backbone. Coupling these two components enables OSFormer to
efficiently blend local features and long-range context dependencies for
predicting camouflaged instances. Compared with two-stage frameworks, our
OSFormer reaches 41% AP and achieves good convergence efficiency without
requiring enormous training data, i.e., only 3,040 samples under 60 epochs.
Code link: https://github.com/PJLallen/OSFormer."
"As two fundamental representation modalities of 3D objects, 3D point clouds
and multi-view 2D images record shape information from different domains of
geometric structures and visual appearances. In the current deep learning era,
remarkable progress in processing such two data modalities has been achieved
through respectively customizing compatible 3D and 2D network architectures.
However, unlike multi-view image-based 2D visual modeling paradigms, which have
shown leading performance in several common 3D shape recognition benchmarks,
point cloud-based 3D geometric modeling paradigms are still highly limited by
insufficient learning capacity, due to the difficulty of extracting
discriminative features from irregular geometric signals. In this paper, we
explore the possibility of boosting deep 3D point cloud encoders by
transferring visual knowledge extracted from deep 2D image encoders under a
standard teacher-student distillation workflow. Generally, we propose PointMCD,
a unified multi-view cross-modal distillation architecture, including a
pretrained deep image encoder as the teacher and a deep point encoder as the
student. To perform heterogeneous feature alignment between 2D visual and 3D
geometric domains, we further investigate visibility-aware feature projection
(VAFP), by which point-wise embeddings are reasonably aggregated into
view-specific geometric descriptors. By pair-wisely aligning multi-view visual
and geometric descriptors, we can obtain more powerful deep point encoders
without exhausting and complicated network modification. Experiments on 3D
shape classification, part segmentation, and unsupervised learning strongly
validate the effectiveness of our method. The code and data will be publicly
available at https://github.com/keeganhk/PointMCD."
"With the development of computer graphics technology, the images synthesized
by computer software become more and more closer to the photographs. While
computer graphics technology brings us a grand visual feast in the field of
games and movies, it may also be utilized by someone with bad intentions to
guide public opinions and cause political crisis or social unrest. Therefore,
how to distinguish the computer-generated graphics (CG) from the photographs
(PG) has become an important topic in the field of digital image forensics.
This paper proposes a dual stream convolutional neural network based on channel
joint and softpool. The proposed network architecture includes a residual
module for extracting image noise information and a joint channel information
extraction module for capturing the shallow semantic information of image. In
addition, we also design a residual structure to enhance feature extraction and
reduce the loss of information in residual flow. The joint channel information
extraction module can obtain the shallow semantic information of the input
image which can be used as the information supplement block of the residual
module. The whole network uses SoftPool to reduce the information loss of
down-sampling for image. Finally, we fuse the two flows to get the
classification results. Experiments on SPL2018 and DsTok show that the proposed
method outperforms existing methods, especially on the DsTok dataset. For
example, the performance of our model surpasses the state-of-the-art by a large
margin of 3%."
"Image registration is a research field in which images must be compared and
aligned independently of the point of view or camera characteristics. In some
applications (such as forensic biometrics, satellite photography or outdoor
scene identification) classical image registration systems fail due to one of
the images compared represents a tiny piece of the other image. For instance,
in forensics palmprint recognition, it is usual to find only a small piece of
the palmprint, but in the database, the whole palmprint has been enrolled. The
main reason of the poor behaviour of classical image registration methods is
the gap between the amounts of salient points of both images, which is related
to the number of points to be considered as outliers. Usually, the difficulty
of finding a good match increases when the image that represents the tiny part
of the scene has been drastically rotated. Again, in the case of palmprint
forensics, it is difficult to decide a priori the orientation of the found tiny
palmprint image. We present a rotation invariant registration method that
explicitly considers that the image to be matched is a small piece of a larger
image. We have experimentally validated our method in two different scenarios;
palmprint identification and outdoor image registration."
"Recently, automatically extracting information from visually rich documents
(e.g., tickets and resumes) has become a hot and vital research topic due to
its widespread commercial value. Most existing methods divide this task into
two subparts: the text reading part for obtaining the plain text from the
original document images and the information extraction part for extracting key
contents. These methods mainly focus on improving the second, while neglecting
that the two parts are highly correlated. This paper proposes a unified
end-to-end information extraction framework from visually rich documents, where
text reading and information extraction can reinforce each other via a
well-designed multi-modal context block. Specifically, the text reading part
provides multi-modal features like visual, textual and layout features. The
multi-modal context block is developed to fuse the generated multi-modal
features and even the prior knowledge from the pre-trained language model for
better semantic representation. The information extraction part is responsible
for generating key contents with the fused context features. The framework can
be trained in an end-to-end trainable manner, achieving global optimization.
What is more, we define and group visually rich documents into four categories
across two dimensions, the layout and text type. For each document category, we
provide or recommend the corresponding benchmarks, experimental settings and
strong baselines for remedying the problem that this research area lacks the
uniform evaluation standard. Extensive experiments on four kinds of benchmarks
(from fixed layout to variable layout, from full-structured text to
semi-unstructured text) are reported, demonstrating the proposed method's
effectiveness. Data, source code and models are available."
"Reliable and stable 6D pose estimation of uncooperative space objects plays
an essential role in on-orbit servicing and debris removal missions.
Considering that the pose estimator is sensitive to background interference,
this paper proposes a counterfactual analysis framework named CASpaceNet to
complete robust 6D pose estimation of the spaceborne targets under complicated
background. Specifically, conventional methods are adopted to extract the
features of the whole image in the factual case. In the counterfactual case, a
non-existent image without the target but only the background is imagined. Side
effect caused by background interference is reduced by counterfactual analysis,
which leads to unbiased prediction in final results. In addition, we also carry
out lowbit-width quantization for CA-SpaceNet and deploy part of the framework
to a Processing-In-Memory (PIM) accelerator on FPGA. Qualitative and
quantitative results demonstrate the effectiveness and efficiency of our
proposed method. To our best knowledge, this paper applies causal inference and
network quantization to the 6D pose estimation of space-borne targets for the
first time. The code is available at
https://github.com/Shunli-Wang/CA-SpaceNet."
"Surface defect detection is an extremely crucial step to ensure the quality
of industrial products. Nowadays, convolutional neural networks (CNNs) based on
encoder-decoder architecture have achieved tremendous success in various defect
detection tasks. However, due to the intrinsic locality of convolution, they
commonly exhibit a limitation in explicitly modeling long-range interactions,
critical for pixel-wise defect detection in complex cases, e.g., cluttered
background and illegible pseudo-defects. Recent transformers are especially
skilled at learning global image dependencies but with limited local structural
information necessary for detailed defect location. To overcome the above
limitations, we propose an efficient hybrid transformer architecture, termed
Defect Transformer (DefT), for surface defect detection, which incorporates CNN
and transformer into a unified model to capture local and non-local
relationships collaboratively. Specifically, in the encoder module, a
convolutional stem block is firstly adopted to retain more detailed spatial
information. Then, the patch aggregation blocks are used to generate
multi-scale representation with four hierarchies, each of them is followed by a
series of DefT blocks, which respectively include a locally position-aware
block for local position encoding, a lightweight multi-pooling self-attention
to model multi-scale global contextual relationships with good computational
efficiency, and a convolutional feed-forward network for feature transformation
and further location information learning. Finally, a simple but effective
decoder module is proposed to gradually recover spatial details from the skip
connections in the encoder. Extensive experiments on three datasets demonstrate
the superiority and efficiency of our method compared with other CNN- and
transformer-based networks."
"This paper presents OmniCity, a new dataset for omnipotent city understanding
from multi-level and multi-view images. More precisely, the OmniCity contains
multi-view satellite images as well as street-level panorama and mono-view
images, constituting over 100K pixel-wise annotated images that are
well-aligned and collected from 25K geo-locations in New York City. To
alleviate the substantial pixel-wise annotation efforts, we propose an
efficient street-view image annotation pipeline that leverages the existing
label maps of satellite view and the transformation relations between different
views (satellite, panorama, and mono-view). With the new OmniCity dataset, we
provide benchmarks for a variety of tasks including building footprint
extraction, height estimation, and building plane/instance/fine-grained
segmentation. Compared with the existing multi-level and multi-view benchmarks,
OmniCity contains a larger number of images with richer annotation types and
more views, provides more benchmark results of state-of-the-art models, and
introduces a novel task for fine-grained building instance segmentation on
street-level panorama images. Moreover, OmniCity provides new problem settings
for existing tasks, such as cross-view image matching, synthesis, segmentation,
detection, etc., and facilitates the developing of new methods for large-scale
city understanding, reconstruction, and simulation. The OmniCity dataset as
well as the benchmarks will be available at
https://city-super.github.io/omnicity."
"A Cleft lip is a congenital abnormality requiring surgical repair by a
specialist. The surgeon must have extensive experience and theoretical
knowledge to perform surgery, and Artificial Intelligence (AI) method has been
proposed to guide surgeons in improving surgical outcomes. If AI can be used to
predict what a repaired cleft lip would look like, surgeons could use it as an
adjunct to adjust their surgical technique and improve results. To explore the
feasibility of this idea while protecting patient privacy, we propose a deep
learning-based image inpainting method that is capable of covering a cleft lip
and generating a lip and nose without a cleft. Our experiments are conducted on
two real-world cleft lip datasets and are assessed by expert cleft lip surgeons
to demonstrate the feasibility of the proposed method."
"Out-Of-Distribution generalization (OOD) is all about learning invariance
against environmental changes. If the context in every class is evenly
distributed, OOD would be trivial because the context can be easily removed due
to an underlying principle: class is invariant to context. However, collecting
such a balanced dataset is impractical. Learning on imbalanced data makes the
model bias to context and thus hurts OOD. Therefore, the key to OOD is context
balance. We argue that the widely adopted assumption in prior work, the context
bias can be directly annotated or estimated from biased class prediction,
renders the context incomplete or even incorrect. In contrast, we point out the
everoverlooked other side of the above principle: context is also invariant to
class, which motivates us to consider the classes (which are already labeled)
as the varying environments to resolve context bias (without context labels).
We implement this idea by minimizing the contrastive loss of intra-class sample
similarity while assuring this similarity to be invariant across all classes.
On benchmarks with various context biases and domain gaps, we show that a
simple re-weighting based classifier equipped with our context estimation
achieves state-of-the-art performance. We provide the theoretical
justifications in Appendix and codes on
https://github.com/simpleshinobu/IRMCon."
"With the ever-increasing electrification of the vehicle showing no sign of
retreating, electronic systems deployed in automotive applications are subject
to more stringent Electromagnetic Immunity compliance constraints than ever
before, to ensure the proximity of nearby electronic systems will not affect
their operation. The EMI compliance testing of an analog camera link requires
video quality to be monitored and assessed to validate such compliance, which
up to now, has been a manual task. Due to the nature of human interpretation,
this is open to inconsistency. Here, we propose a solution using deep learning
models that analyse, and grade video content derived from an EMI compliance
test. These models are trained using a dataset built entirely from real test
image data to ensure the accuracy of the resultant model(s) is maximised.
Starting with the standard AlexNet, we propose four models to classify the EMI
noise level"
"Recognizing 3D part instances from a 3D point cloud is crucial for 3D
structure and scene understanding. Several learning-based approaches use
semantic segmentation and instance center prediction as training tasks and fail
to further exploit the inherent relationship between shape semantics and part
instances. In this paper, we present a new method for 3D part instance
segmentation. Our method exploits semantic segmentation to fuse nonlocal
instance features, such as center prediction, and further enhances the fusion
scheme in a multi- and cross-level way. We also propose a semantic region
center prediction task to train and leverage the prediction results to improve
the clustering of instance points. Our method outperforms existing methods with
a large-margin improvement in the PartNet benchmark. We also demonstrate that
our feature fusion scheme can be applied to other existing methods to improve
their performance in indoor scene instance segmentation tasks."
"Recent works on language-guided image manipulation have shown great power of
language in providing rich semantics, especially for face images. However, the
other natural information, motions, in language is less explored. In this
paper, we leverage the motion information and study a novel task,
language-guided face animation, that aims to animate a static face image with
the help of languages. To better utilize both semantics and motions from
languages, we propose a simple yet effective framework. Specifically, we
propose a recurrent motion generator to extract a series of semantic and motion
information from the language and feed it along with visual information to a
pre-trained StyleGAN to generate high-quality frames. To optimize the proposed
framework, three carefully designed loss functions are proposed including a
regularization loss to keep the face identity, a path length regularization
loss to ensure motion smoothness, and a contrastive loss to enable video
synthesis with various language guidance in one single model. Extensive
experiments with both qualitative and quantitative evaluations on diverse
domains (\textit{e.g.,} human face, anime face, and dog face) demonstrate the
superiority of our model in generating high-quality and realistic videos from
one still image with the guidance of language. Code will be available at
https://github.com/TiankaiHang/language-guided-animation.git."
"The lack of out-of-domain generalization is a critical weakness of deep
networks for semantic segmentation. Previous studies relied on the assumption
of a static model, i. e., once the training process is complete, model
parameters remain fixed at test time. In this work, we challenge this premise
with a self-adaptive approach for semantic segmentation that adjusts the
inference process to each input sample. Self-adaptation operates on two levels.
First, it fine-tunes the parameters of convolutional layers to the input image
using consistency regularization. Second, in Batch Normalization layers,
self-adaptation interpolates between the training and the reference
distribution derived from a single test sample. Despite both techniques being
well known in the literature, their combination sets new state-of-the-art
accuracy on synthetic-to-real generalization benchmarks. Our empirical study
suggests that self-adaptation may complement the established practice of model
regularization at training time for improving deep network generalization to
out-of-domain data. Our code and pre-trained models are available at
https://github.com/visinf/self-adaptive."
"Domain generalization (DG) aims at learning a model on source domains to well
generalize on the unseen target domain. Although it has achieved great success,
most of existing methods require the label information for all training samples
in source domains, which is time-consuming and expensive in the real-world
application. In this paper, we resort to solving the semi-supervised domain
generalization (SSDG) task, where there are a few label information in each
source domain. To address the task, we first analyze the theory of the
multi-domain learning, which highlights that 1) mitigating the impact of domain
gap and 2) exploiting all samples to train the model can effectively reduce the
generalization error in each source domain so as to improve the quality of
pseudo-labels. According to the analysis, we propose MultiMatch, i.e.,
extending FixMatch to the multi-task learning framework, producing the
high-quality pseudo-label for SSDG. To be specific, we consider each training
domain as a single task (i.e., local task) and combine all training domains
together (i.e., global task) to train an extra task for the unseen test domain.
In the multi-task framework, we utilize the independent BN and classifier for
each task, which can effectively alleviate the interference from different
domains during pseudo-labeling. Also, most of parameters in the framework are
shared, which can be trained by all training samples sufficiently. Moreover, to
further boost the pseudo-label accuracy and the model's generalization, we fuse
the predictions from the global task and local task during training and
testing, respectively. A series of experiments validate the effectiveness of
the proposed method, and it outperforms the existing semi-supervised methods
and the SSDG method on several benchmark DG datasets."
"We present a novel approach to OCR(Optical Character Recognition) of Korean
character, Hangul. As a phonogram, Hangul can represent 11,172 different
characters with only 52 graphemes, by describing each character with a
combination of the graphemes. As the total number of the characters could
overwhelm the capacity of a neural network, the existing OCR encoding methods
pre-define a smaller set of characters that are frequently used. This design
choice naturally compromises the performance on long-tailed characters in the
distribution. In this work, we demonstrate that grapheme encoding is not only
efficient but also performant for Hangul OCR. Benchmark tests show that our
approach resolves two main problems of Hangul OCR: class imbalance and target
class selection."
"Diabetic Retinopathy (DR) has become one of the leading causes of vision
impairment in working-aged people and is a severe problem worldwide. However,
most of the works ignored the ordinal information of labels. In this project,
we propose a novel design MTCSNN, a Multi-task Clinical Siamese Neural Network
for Diabetic Retinopathy severity prediction task. The novelty of this project
is to utilize the ordinal information among labels and add a new regression
task, which can help the model learn more discriminative feature embedding for
fine-grained classification tasks. We perform comprehensive experiments over
the RetinaMNIST, comparing MTCSNN with other models like ResNet-18, 34, 50. Our
results indicate that MTCSNN outperforms the benchmark models in terms of AUC
and accuracy on the test dataset."
"This paper studies the problem of holistic 3D wireframe perception (HoW-3D),
a new task of perceiving both the visible 3D wireframes and the invisible ones
from single-view 2D images. As the non-front surfaces of an object cannot be
directly observed in a single view, estimating the non-line-of-sight (NLOS)
geometries in HoW-3D is a fundamentally challenging problem and remains open in
computer vision. We study the problem of HoW-3D by proposing an ABC-HoW
benchmark, which is created on top of CAD models sourced from the ABC-dataset
with 12k single-view images and the corresponding holistic 3D wireframe models.
With our large-scale ABC-HoW benchmark available, we present a novel Deep
Spatial Gestalt (DSG) model to learn the visible junctions and line segments as
the basis and then infer the NLOS 3D structures from the visible cues by
following the Gestalt principles of human vision systems. In our experiments,
we demonstrate that our DSG model performs very well in inferring the holistic
3D wireframes from single-view images. Compared with the strong baseline
methods, our DSG model outperforms the previous wireframe detectors in
detecting the invisible line geometry in single-view images and is even very
competitive with prior arts that take high-fidelity PointCloud as inputs on
reconstructing 3D wireframes."
"Semantic segmentation is a key technique involved in automatic interpretation
of high-resolution remote sensing (HRS) imagery and has drawn much attention in
the remote sensing community. Deep convolutional neural networks (DCNNs) have
been successfully applied to the HRS imagery semantic segmentation task due to
their hierarchical representation ability. However, the heavy dependency on a
large number of training data with dense annotation and the sensitiveness to
the variation of data distribution severely restrict the potential application
of DCNNs for the semantic segmentation of HRS imagery. This study proposes a
novel unsupervised domain adaptation semantic segmentation network
(MemoryAdaptNet) for the semantic segmentation of HRS imagery. MemoryAdaptNet
constructs an output space adversarial learning scheme to bridge the domain
distribution discrepancy between source domain and target domain and to narrow
the influence of domain shift. Specifically, we embed an invariant feature
memory module to store invariant domain-level context information because the
features obtained from adversarial learning only tend to represent the variant
feature of current limited inputs. This module is integrated by a category
attention-driven invariant domain-level context aggregation module to current
pseudo invariant feature for further augmenting the pixel representations. An
entropy-based pseudo label filtering strategy is used to update the memory
module with high-confident pseudo invariant feature of current target images.
Extensive experiments under three cross-domain tasks indicate that our proposed
MemoryAdaptNet is remarkably superior to the state-of-the-art methods."
"Facial pose estimation refers to the task of predicting face orientation from
a single RGB image. It is an important research topic with a wide range of
applications in computer vision. Label distribution learning (LDL) based
methods have been recently proposed for facial pose estimation, which achieve
promising results. However, there are two major issues in existing LDL methods.
First, the expectations of label distributions are biased, leading to a biased
pose estimation. Second, fixed distribution parameters are applied for all
learning samples, severely limiting the model capability. In this paper, we
propose an Anisotropic Spherical Gaussian (ASG)-based LDL approach for facial
pose estimation. In particular, our approach adopts the spherical Gaussian
distribution on a unit sphere which constantly generates unbiased expectation.
Meanwhile, we introduce a new loss function that allows the network to learn
the distribution parameter for each learning sample flexibly. Extensive
experimental results show that our method sets new state-of-the-art records on
AFLW2000 and BIWI datasets."
"Theory of convolutional neural networks suggests the property of shift
equivariance, i.e., that a shifted input causes an equally shifted output. In
practice, however, this is not always the case. This poses a great problem for
scene text detection for which a consistent spatial response is crucial,
irrespective of the position of the text in the scene.
  Using a simple synthetic experiment, we demonstrate the inherent shift
variance of a state-of-the-art fully convolutional text detector. Furthermore,
using the same experimental setting, we show how small architectural changes
can lead to an improved shift equivariance and less variation of the detector
output. We validate the synthetic results using a real-world training schedule
on the text detection network. To quantify the amount of shift variability, we
propose a metric based on well-established text detection benchmarks.
  While the proposed architectural changes are not able to fully recover shift
equivariance, adding smoothing filters can substantially improve shift
consistency on common text datasets. Considering the potentially large impact
of small shifts, we propose to extend the commonly used text detection metrics
by the metric described in this work, in order to be able to quantify the
consistency of text detectors."
"The prosperity of deep learning contributes to the rapid progress in scene
text detection. Among all the methods with convolutional networks,
segmentation-based ones have drawn extensive attention due to their superiority
in detecting text instances of arbitrary shapes and extreme aspect ratios.
However, the bottom-up methods are limited to the performance of their
segmentation models. In this paper, we propose DPTNet (Dual-Path Transformer
Network), a simple yet effective architecture to model the global and local
information for the scene text detection task. We further propose a parallel
design that integrates the convolutional network with a powerful self-attention
mechanism to provide complementary clues between the attention path and
convolutional path. Moreover, a bi-directional interaction module across the
two paths is developed to provide complementary clues in the channel and
spatial dimensions. We also upgrade the concentration operation by adding an
extra multi-head attention layer to it. Our DPTNet achieves state-of-the-art
results on the MSRA-TD500 dataset, and provides competitive results on other
standard benchmarks in terms of both detection accuracy and speed."
"Biases inherent in both data and algorithms make the fairness of widespread
machine learning (ML)-based decision-making systems less than optimal. To
improve the trustfulness of such ML decision systems, it is crucial to be aware
of the inherent biases in these solutions and to make them more transparent to
the public and developers. In this work, we aim at providing a set of
explainability tool that analyse the difference in the face recognition models'
behaviors when processing different demographic groups. We do that by
leveraging higher-order statistical information based on activation maps to
build explainability tools that link the FR models' behavior differences to
certain facial regions. The experimental results on two datasets and two face
recognition models pointed out certain areas of the face where the FR models
react differently for certain demographic groups compared to reference groups.
The outcome of these analyses interestingly aligns well with the results of
studies that analyzed the anthropometric differences and the human judgment
differences on the faces of different demographic groups. This is thus the
first study that specifically tries to explain the biased behavior of FR models
on different demographic groups and link it directly to the spatial facial
features. The code is publicly available here."
"In this paper, we consider the problem of generalised visual object counting,
with the goal of developing a computational model for counting the number of
objects from arbitrary semantic categories, using arbitrary number of
""exemplars"", i.e. zero-shot or few-shot counting. To this end, we make the
following four contributions: (1) We introduce a novel transformer-based
architecture for generalised visual object counting, termed as Counting
Transformer (CounTR), which explicitly capture the similarity between image
patches or with given ""exemplars"" with the attention mechanism;(2) We adopt a
two-stage training regime, that first pre-trains the model with self-supervised
learning, and followed by supervised fine-tuning;(3) We propose a simple,
scalable pipeline for synthesizing training images with a large number of
instances or that from different semantic categories, explicitly forcing the
model to make use of the given ""exemplars"";(4) We conduct thorough ablation
studies on the large-scale counting benchmark, e.g. FSC-147, and demonstrate
state-of-the-art performance on both zero and few-shot settings."
"With the rapid development of self-supervised learning (e.g., contrastive
learning), the importance of having large-scale images (even without
annotations) for training a more generalizable AI model has been widely
recognized in medical image analysis. However, collecting large-scale
task-specific unannotated data at scale can be challenging for individual labs.
Existing online resources, such as digital books, publications, and search
engines, provide a new resource for obtaining large-scale images. However,
published images in healthcare (e.g., radiology and pathology) consist of a
considerable amount of compound figures with subplots. In order to extract and
separate compound figures into usable individual images for downstream
learning, we propose a simple compound figure separation (SimCFS) framework
without using the traditionally required detection bounding box annotations,
with a new loss function and a hard case simulation. Our technical contribution
is four-fold: (1) we introduce a simulation-based training framework that
minimizes the need for resource extensive bounding box annotations; (2) we
propose a new side loss that is optimized for compound figure separation; (3)
we propose an intra-class image augmentation method to simulate hard cases; and
(4) to the best of our knowledge, this is the first study that evaluates the
efficacy of leveraging self-supervised learning with compound image separation.
From the results, the proposed SimCFS achieved state-of-the-art performance on
the ImageCLEF 2016 Compound Figure Separation Database. The pretrained
self-supervised learning model using large-scale mined figures improved the
accuracy of downstream image classification tasks with a contrastive learning
algorithm. The source code of SimCFS is made publicly available at
https://github.com/hrlblab/ImageSeperation."
"Visual Question Answering (VQA) is a multi-modal task that involves answering
questions from an input image, semantically understanding the contents of the
image and answering it in natural language. Using VQA for disaster management
is an important line of research due to the scope of problems that are answered
by the VQA system. However, the main challenge is the delay caused by the
generation of labels in the assessment of the affected areas. To tackle this,
we deployed pre-trained CLIP model, which is trained on visual-image pairs.
however, we empirically see that the model has poor zero-shot performance.
Thus, we instead use pre-trained embeddings of text and image from this model
for our supervised training and surpass previous state-of-the-art results on
the FloodNet dataset. We expand this to a continual setting, which is a more
real-life scenario. We tackle the problem of catastrophic forgetting using
various experience replay methods. Our training runs are available at:
https://wandb.ai/compyle/continual_vqa_final. Our code is available at
https://github.com/AdityaKane2001/continual_vqa."
"Modeling sparse and dense image matching within a unified functional
correspondence model has recently attracted increasing research interest.
However, existing efforts mainly focus on improving matching accuracy while
ignoring its efficiency, which is crucial for realworld applications. In this
paper, we propose an efficient structure named Efficient Correspondence
Transformer (ECO-TR) by finding correspondences in a coarse-to-fine manner,
which significantly improves the efficiency of functional correspondence model.
To achieve this, multiple transformer blocks are stage-wisely connected to
gradually refine the predicted coordinates upon a shared multi-scale feature
extraction network. Given a pair of images and for arbitrary query coordinates,
all the correspondences are predicted within a single feed-forward pass. We
further propose an adaptive query-clustering strategy and an uncertainty-based
outlier detection module to cooperate with the proposed framework for faster
and better predictions. Experiments on various sparse and dense matching tasks
demonstrate the superiority of our method in both efficiency and effectiveness
against existing state-of-the-arts."
"We introduce a new approach to image forensics: placing physical refractive
objects, which we call totems, into a scene so as to protect any photograph
taken of that scene. Totems bend and redirect light rays, thus providing
multiple, albeit distorted, views of the scene within a single image. A
defender can use these distorted totem pixels to detect if an image has been
manipulated. Our approach unscrambles the light rays passing through the totems
by estimating their positions in the scene and using their known geometric and
material properties. To verify a totem-protected image, we detect
inconsistencies between the scene reconstructed from totem viewpoints and the
scene's appearance from the camera viewpoint. Such an approach makes the
adversarial manipulation task more difficult, as the adversary must modify both
the totem and image pixels in a geometrically consistent manner without knowing
the physical properties of the totem. Unlike prior learning-based approaches,
our method does not require training on datasets of specific manipulations, and
instead uses physical properties of the scene and camera to solve the forensics
problem."
"With the demand for standardized large-scale livestock farming and the
development of artificial intelligence technology, a lot of research in area of
animal face recognition were carried on pigs, cattle, sheep and other
livestock. Face recognition consists of three sub-task: face detection, face
normalizing and face identification. Most of animal face recognition study
focuses on face detection and face identification. Animals are often
uncooperative when taking photos, so the collected animal face images are often
in arbitrary directions. The use of non-standard images may significantly
reduce the performance of face recognition system. However, there is no study
on normalizing of the animal face image with arbitrary directions. In this
study, we developed a light-weight angle detection and region-based
convolutional network (LAD-RCNN) containing a new rotation angle coding method
that can detect the rotation angle and the location of animal face in
one-stage. LAD-RCNN has a frame rate of 72.74 FPS (including all steps) on a
single GeForce RTX 2080 Ti GPU. LAD-RCNN has been evaluated on multiple dataset
including goat dataset and gaot infrared image. Evaluation result show that the
AP of face detection was more than 95% and the deviation between the detected
rotation angle and the ground-truth rotation angle were less than 0.036 (i.e.
6.48{\deg}) on all the test dataset. This shows that LAD-RCNN has excellent
performance on livestock face and its direction detection, and therefore it is
very suitable for livestock face detection and Normalizing. Code is available
at https://github.com/SheepBreedingLab-HZAU/LAD-RCNN/"
"Video super-resolution (VSR) is a task that aims to reconstruct
high-resolution (HR) frames from the low-resolution (LR) reference frame and
multiple neighboring frames. The vital operation is to utilize the relative
misaligned frames for the current frame reconstruction and preserve the
consistency of the results. Existing methods generally explore information
propagation and frame alignment to improve the performance of VSR. However, few
studies focus on the temporal consistency of inter-frames. In this paper, we
propose a Temporal Consistency learning Network (TCNet) for VSR in an
end-to-end manner, to enhance the consistency of the reconstructed videos. A
spatio-temporal stability module is designed to learn the self-alignment from
inter-frames. Especially, the correlative matching is employed to exploit the
spatial dependency from each frame to maintain structural stability. Moreover,
a self-attention mechanism is utilized to learn the temporal correspondence to
implement an adaptive warping operation for temporal consistency among
multi-frames. Besides, a hybrid recurrent architecture is designed to leverage
short-term and long-term information. We further present a progressive fusion
module to perform a multistage fusion of spatio-temporal features. And the
final reconstructed frames are refined by these fused features. Objective and
subjective results of various experiments demonstrate that TCNet has superior
performance on different benchmark datasets, compared to several
state-of-the-art methods."
"Although Deep Neural Networks (DNNs) have been widely applied in various
real-world scenarios, they are vulnerable to adversarial examples. The current
adversarial attacks in computer vision can be divided into digital attacks and
physical attacks according to their different attack forms. Compared with
digital attacks, which generate perturbations in the digital pixels, physical
attacks are more practical in the real world. Owing to the serious security
problem caused by physically adversarial examples, many works have been
proposed to evaluate the physically adversarial robustness of DNNs in the past
years. In this paper, we summarize a survey versus the current physically
adversarial attacks and physically adversarial defenses in computer vision. To
establish a taxonomy, we organize the current physical attacks from attack
tasks, attack forms, and attack methods, respectively. Thus, readers can have a
systematic knowledge of this topic from different aspects. For the physical
defenses, we establish the taxonomy from pre-processing, in-processing, and
post-processing for the DNN models to achieve full coverage of the adversarial
defenses. Based on the above survey, we finally discuss the challenges of this
research field and further outlook on the future direction."
"Besides the complex nature of colonoscopy frames with intrinsic frame
formation artefacts such as light reflections and the diversity of polyp
types/shapes, the publicly available polyp segmentation training datasets are
limited, small and imbalanced. In this case, the automated polyp segmentation
using a deep neural network remains an open challenge due to the overfitting of
training on small datasets. We proposed a simple yet effective polyp
segmentation pipeline that couples the segmentation (FCN) and classification
(CNN) tasks. We find the effectiveness of interactive weight transfer between
dense and coarse vision tasks that mitigates the overfitting in learning. And
It motivates us to design a new training scheme within our segmentation
pipeline. Our method is evaluated on CVC-EndoSceneStill and Kvasir-SEG
datasets. It achieves 4.34% and 5.70% Polyp-IoU improvements compared to the
state-of-the-art methods on the EndoSceneStill and Kvasir-SEG datasets,
respectively."
"We present a novel clustering algorithm, visClust, that is based on lower
dimensional data representations and visual interpretation. Thereto, we design
a transformation that allows the data to be represented by a binary integer
array enabling the use of image processing methods to select a partition.
Qualitative and quantitative analyses measured in accuracy and an adjusted
Rand-Index show that the algorithm performs well while requiring low runtime
and RAM. We compare the results to 6 state-of-the-art algorithms with available
code, confirming the quality of visClust by superior performance in most
experiments. Moreover, the algorithm asks for just one obligatory input
parameter while allowing optimization via optional parameters. The code is made
available on GitHub and straightforward to use."
"Single-image 3D human reconstruction aims to reconstruct the 3D textured
surface of the human body given a single image. While implicit function-based
methods recently achieved reasonable reconstruction performance, they still
bear limitations showing degraded quality in both surface geometry and texture
from an unobserved view. In response, to generate a realistic textured surface,
we propose ReFu, a coarse-to-fine approach that refines the projected backside
view image and fuses the refined image to predict the final human body. To
suppress the diffused occupancy that causes noise in projection images and
reconstructed meshes, we propose to train occupancy probability by
simultaneously utilizing 2D and 3D supervisions with occupancy-based volume
rendering. We also introduce a refinement architecture that generates
detail-preserving backside-view images with front-to-back warping. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
in 3D human reconstruction from a single image, showing enhanced geometry and
texture quality from an unobserved view."
"Scene text removal aims to remove the text and fill the regions with
perceptually plausible background information in natural images. It has
attracted increasing attention due to its various applications in privacy
protection, scene text retrieval, and text editing. With the development of
deep learning, the previous methods have achieved significant improvements.
However, most of the existing methods seem to ignore the large perceptive
fields and global information. The pioneer method can get significant
improvements by only changing training data from the cropped image to the full
image. In this paper, we present a single-stage multi-scale network MSLKANet
for scene text removal in full images. For obtaining large perceptive fields
and global information, we propose multi-scale large kernel attention (MSLKA)
to obtain long-range dependencies between the text regions and the backgrounds
at various granularity levels. Furthermore, we combine the large kernel
decomposition mechanism and atrous spatial pyramid pooling to build a large
kernel spatial pyramid pooling (LKSPP), which can perceive more valid pixels in
the spatial dimension while maintaining large receptive fields and low cost of
computation. Extensive experimental results indicate that the proposed method
achieves state-of-the-art performance on both synthetic and real-world datasets
and the effectiveness of the proposed components MSLKA and LKSPP."
"We propose a post-processor, called NeighborTrack, that leverages neighbor
information of the tracking target to validate and improve single-object
tracking (SOT) results. It requires no additional data or retraining. Instead,
it uses the confidence score predicted by the backbone SOT network to
automatically derive neighbor information and then uses this information to
improve the tracking results. When tracking an occluded target, its appearance
features are untrustworthy. However, a general siamese network often cannot
tell whether the tracked object is occluded by reading the confidence score
alone, because it could be misled by neighbors with high confidence scores. Our
proposed NeighborTrack takes advantage of unoccluded neighbors' information to
reconfirm the tracking target and reduces false tracking when the target is
occluded. It not only reduces the impact caused by occlusion, but also fixes
tracking problems caused by object appearance changes. NeighborTrack is
agnostic to SOT networks and post-processing methods. For the VOT challenge
dataset commonly used in short-term object tracking, we improve three famous
SOT networks, Ocean, TransT, and OSTrack, by an average of ${1.92\%}$ EAO and
${2.11\%}$ robustness. For the mid- and long-term tracking experiments based on
OSTrack, we achieve state-of-the-art ${72.25\%}$ AUC on LaSOT and ${75.7\%}$ AO
on GOT-10K. Code duplication can be found in
https://github.com/franktpmvu/NeighborTrack."
"The method of neural radiance fields (NeRF) has been developed in recent
years, and this technology has promising applications for synthesizing novel
views of complex scenes. However, NeRF requires dense input views, typically
numbering in the hundreds, for generating high-quality images. With a decrease
in the number of input views, the rendering quality of NeRF for unseen
viewpoints tends to degenerate drastically. To overcome this challenge, we
propose pseudo-view augmentation of NeRF, a scheme that expands a sufficient
amount of data by considering the geometry of few-shot inputs. We first
initialized the NeRF network by leveraging the expanded pseudo-views, which
efficiently minimizes uncertainty when rendering unseen views. Subsequently, we
fine-tuned the network by utilizing sparse-view inputs containing precise
geometry and color information. Through experiments under various settings, we
verified that our model faithfully synthesizes novel-view images of superior
quality and outperforms existing methods for multi-view datasets."
"Traditional methods of reconstructing 3D human pose and mesh from single
images rely on paired image-mesh datasets, which can be difficult and expensive
to obtain. Due to this limitation, model scalability is constrained as well as
reconstruction performance. Towards addressing the challenge, we introduce Mesh
Pre-Training (MPT), an effective pre-training strategy that leverages large
amounts of MoCap data to effectively perform pre-training at scale. We
introduce the use of MoCap-generated heatmaps as input representations to the
mesh regression transformer and propose a Masked Heatmap Modeling approach for
improving pre-training performance. This study demonstrates that pre-training
using the proposed MPT allows our models to perform effective inference without
requiring fine-tuning. We further show that fine-tuning the pre-trained MPT
model considerably improves the accuracy of human mesh reconstruction from
single images. Experimental results show that MPT outperforms previous
state-of-the-art methods on Human3.6M and 3DPW datasets. As a further
application, we benchmark and study MPT on the task of 3D hand reconstruction,
showing that our generic pre-training scheme generalizes well to hand pose
estimation and achieves promising reconstruction performance."
"Deep learning has enabled realistic face manipulation (i.e., deepfake), which
poses significant concerns over the integrity of the media in circulation. Most
existing deep learning techniques for deepfake detection can achieve promising
performance in the intra-dataset evaluation setting (i.e., training and testing
on the same dataset), but are unable to perform satisfactorily in the
inter-dataset evaluation setting (i.e., training on one dataset and testing on
another). Most of the previous methods use the backbone network to extract
global features for making predictions and only employ binary supervision
(i.e., indicating whether the training instances are fake or authentic) to
train the network. Classification merely based on the learning of global
features leads often leads to weak generalizability to unseen manipulation
methods. In addition, the reconstruction task can improve the learned
representations. In this paper, we introduce a novel approach for deepfake
detection, which considers the reconstruction and classification tasks
simultaneously to address these problems. This method shares the information
learned by one task with the other, which focuses on a different aspect other
existing works rarely consider and hence boosts the overall performance. In
particular, we design a two-branch Convolutional AutoEncoder (CAE), in which
the Convolutional Encoder used to compress the feature map into the latent
representation is shared by both branches. Then the latent representation of
the input data is fed to a simple classifier and the unsupervised
reconstruction component simultaneously. Our network is trained end-to-end.
Experiments demonstrate that our method achieves state-of-the-art performance
on three commonly-used datasets, particularly in the cross-dataset evaluation
setting."
"Signal-quality awareness has been found to increase recognition rates and to
support decisions in multisensor environments significantly. Nevertheless,
automatic quality assessment is still an open issue. Here, we study the
orientation tensor of fingerprint images to quantify signal impairments, such
as noise, lack of structure, blur, with the help of symmetry descriptors. A
strongly reduced reference is especially favorable in biometrics, but less
information is not sufficient for the approach. This is also supported by
numerous experiments involving a simpler quality estimator, a trained method
(NFIQ), as well as the human perception of fingerprint quality on several
public databases. Furthermore, quality measurements are extensively reused to
adapt fusion parameters in a monomodal multialgorithm fingerprint recognition
environment. In this study, several trained and nontrained score-level fusion
schemes are investigated. A Bayes-based strategy for incorporating experts past
performances and current quality conditions, a novel cascaded scheme for
computational efficiency, besides simple fusion rules, is presented. The
quantitative results favor quality awareness under all aspects, boosting
recognition rates and fusing differently skilled experts efficiently as well as
effectively (by training)."
"Unsupervised generation of 3D-aware clothed humans with various appearances
and controllable geometries is important for creating virtual human avatars and
other AR/VR applications. Existing methods are either limited to rigid object
modeling, or not generative and thus unable to generate high-quality virtual
humans and animate them. In this work, we propose AvatarGen, the first method
that enables not only geometry-aware clothed human synthesis with high-fidelity
appearances but also disentangled human animation controllability, while only
requiring 2D images for training. Specifically, we decompose the generative 3D
human synthesis into pose-guided mapping and canonical representation with
predefined human pose and shape, such that the canonical representation can be
explicitly driven to different poses and shapes with the guidance of a 3D
parametric human model SMPL. AvatarGen further introduces a deformation network
to learn non-rigid deformations for modeling fine-grained geometric details and
pose-dependent dynamics. To improve the geometry quality of the generated human
avatars, it leverages the signed distance field as geometric proxy, which
allows more direct regularization from the 3D geometric priors of SMPL.
Benefiting from these designs, our method can generate animatable 3D human
avatars with high-quality appearance and geometry modeling, significantly
outperforming previous 3D GANs. Furthermore, it is competent for many
applications, e.g., single-view reconstruction, re-animation, and text-guided
synthesis/editing. Code and pre-trained model will be available at
http://jeff95.me/projects/avatargen.html."
"Occluded person re-identification (ReID) is a challenging problem due to
contamination from occluders. Existing approaches address the issue with prior
knowledge cues, such as human body key points and semantic segmentations, which
easily fail in the presence of heavy occlusion and other humans as occluders.
In this paper, we propose a feature pruning and consolidation (FPC) framework
to circumvent explicit human structure parsing. The framework mainly consists
of a sparse encoder, a multi-view feature mathcing module, and a feature
consolidation decoder. Specifically, the sparse encoder drops less important
image tokens, mostly related to background noise and occluders, solely based on
correlation within the class token attention. Subsequently, the matching stage
relies on the preserved tokens produced by the sparse encoder to identify
k-nearest neighbors in the gallery by measuring the image and patch-level
combined similarity. Finally, we use the feature consolidation module to
compensate pruned features using identified neighbors for recovering essential
information while disregarding disturbance from noise and occlusion.
Experimental results demonstrate the effectiveness of our proposed framework on
occluded, partial, and holistic Re-ID datasets. In particular, our method
outperforms state-of-the-art results by at least 8.6\% mAP and 6.0\% Rank-1
accuracy on the challenging Occluded-Duke dataset."
"This technical report introduces CyberLoc, an image-based visual localization
pipeline for robust and accurate long-term pose estimation under challenging
conditions. The proposed method comprises four modules connected in a sequence.
First, a mapping module is applied to build accurate 3D maps of the scene, one
map for each reference sequence if there exist multiple reference sequences
under different conditions. Second, a single-image-based localization pipeline
(retrieval--matching--PnP) is performed to estimate 6-DoF camera poses for each
query image, one for each 3D map. Third, a consensus set maximization module is
proposed to filter out outlier 6-DoF camera poses, and outputs one 6-DoF camera
pose for a query. Finally, a robust pose refinement module is proposed to
optimize 6-DoF query poses, taking candidate global 6-DoF camera poses and
their corresponding global 2D-3D matches, sparse 2D-2D feature matches between
consecutive query images and SLAM poses of the query sequence as input.
Experiments on the 4seasons dataset show that our method achieves high accuracy
and robustness. In particular, our approach wins the localization challenge of
ECCV 2022 workshop on Map-based Localization for Autonomous Driving
(MLAD-ECCV2022)."
"Existing deep learning-based shadow removal methods still produce images with
shadow remnants. These shadow remnants typically exist in homogeneous regions
with low-intensity values, making them untraceable in the existing
image-to-image mapping paradigm. We observe that shadows mainly degrade images
at the image-structure level (in which humans perceive object shapes and
continuous colors). Hence, in this paper, we propose to remove shadows at the
image structure level. Based on this idea, we propose a novel
structure-informed shadow removal network (StructNet) to leverage the
image-structure information to address the shadow remnant problem.
Specifically, StructNet first reconstructs the structure information of the
input image without shadows and then uses the restored shadow-free structure
prior to guiding the image-level shadow removal. StructNet contains two main
novel modules: (1) a mask-guided shadow-free extraction (MSFE) module to
extract image structural features in a non-shadow-to-shadow directional manner,
and (2) a multi-scale feature & residual aggregation (MFRA) module to leverage
the shadow-free structure information to regularize feature consistency. In
addition, we also propose to extend StructNet to exploit multi-level structure
information (MStructNet), to further boost the shadow removal performance with
minimum computational overheads. Extensive experiments on three shadow removal
benchmarks demonstrate that our method outperforms existing shadow removal
methods, and our StructNet can be integrated with existing methods to improve
them further."
"To date, the majority of positioning systems have been designed to operate
within environments that have long-term stable macro-structure with potential
small-scale dynamics. These assumptions allow the existing positioning systems
to produce and utilize stable maps. However, in highly dynamic industrial
settings these assumptions are no longer valid and the task of tracking people
is more challenging due to the rapid large-scale changes in structure. In this
paper we propose a novel positioning system for tracking people in highly
dynamic industrial environments, such as construction sites. The proposed
system leverages the existing CCTV camera infrastructure found in many
industrial settings along with radio and inertial sensors within each worker's
mobile phone to accurately track multiple people. This multi-target
multi-sensor tracking framework also allows our system to use cross-modality
training in order to deal with the environment dynamics. In particular, we show
how our system uses cross-modality training in order to automatically keep
track environmental changes (i.e. new walls) by utilizing occlusion maps. In
addition, we show how these maps can be used in conjunction with social forces
to accurately predict human motion and increase the tracking accuracy. We have
conducted extensive real-world experiments in a construction site showing
significant accuracy improvement via cross-modality training and the use of
social forces."
"The latest developments in Artificial Intelligence include diffusion
generative models, quite popular tools which can produce original images both
unconditionally and, in some cases, conditioned by some inputs provided by the
user. Apart from implementation details, which are outside the scope of this
work, all of the main models used to generate images are substantially based on
a common theory which restores a new image from a completely degraded one. In
this work we explain how this is possible by focusing on the mathematical
theory behind them, i.e. without analyzing in detail the specific
implementations and related methods. The aim of this work is to clarify to the
interested reader what all this means mathematically and intuitively."
"We present vMAP, an object-level dense SLAM system using neural field
representations. Each object is represented by a small MLP, enabling efficient,
watertight object modelling without the need for 3D priors. As an RGB-D camera
browses a scene with no prior information, vMAP detects object instances
on-the-fly, and dynamically adds them to its map. Specifically, thanks to the
power of vectorised training, vMAP can optimise as many as 50 individual
objects in a single scene, with an extremely efficient training speed of 5Hz
map update. We experimentally demonstrate significantly improved scene-level
and object-level reconstruction quality compared to prior neural field SLAM
systems. Project page: https://kxhit.github.io/vMAP."
"Currently, most existing person re-identification methods use Instance-Level
features, which are extracted only from a single image. However, these
Instance-Level features can easily ignore the discriminative information due to
the appearance of each identity varies greatly in different images. Thus, it is
necessary to exploit Identity-Level features, which can be shared across
different images of each identity. In this paper, we propose to promote
Instance-Level features to Identity-Level features by employing cross-attention
to incorporate information from one image to another of the same identity, thus
more unified and discriminative pedestrian information can be obtained. We
propose a novel training framework named X-ReID. Specifically, a Cross
Intra-Identity Instances module (IntraX) fuses different intra-identity
instances to transfer Identity-Level knowledge and make Instance-Level features
more compact. A Cross Inter-Identity Instances module (InterX) involves hard
positive and hard negative instances to improve the attention response to the
same identity instead of different identity, which minimizes intra-identity
variation and maximizes inter-identity variation. Extensive experiments on
benchmark datasets show the superiority of our method over existing works.
Particularly, on the challenging MSMT17, our proposed method gains 1.1% mAP
improvements when compared to the second place."
"Convolutional neural networks (CNN) have been broadly studied on images,
videos, graphs, and triangular meshes. However, it has seldom been studied on
tetrahedral meshes. Given the merits of using volumetric meshes in applications
like brain image analysis, we introduce a novel interpretable graph CNN
framework for the tetrahedral mesh structure. Inspired by ChebyNet, our model
exploits the volumetric Laplace-Beltrami Operator (LBO) to define filters over
commonly used graph Laplacian which lacks the Riemannian metric information of
3D manifolds. For pooling adaptation, we introduce new objective functions for
localized minimum cuts in the Graclus algorithm based on the LBO. We employ a
piece-wise constant approximation scheme that uses the clustering assignment
matrix to estimate the LBO on sampled meshes after each pooling. Finally,
adapting the Gradient-weighted Class Activation Mapping algorithm for
tetrahedral meshes, we use the obtained heatmaps to visualize discovered
regions-of-interest as biomarkers. We demonstrate the effectiveness of our
model on cortical tetrahedral meshes from patients with Alzheimer's disease, as
there is scientific evidence showing the correlation of cortical thickness to
neurodegenerative disease progression. Our results show the superiority of our
LBO-based convolution layer and adapted pooling over the conventionally used
unitary cortical thickness, graph Laplacian, and point cloud representation."
"Guided depth map super-resolution (GDSR), which aims to reconstruct a
high-resolution (HR) depth map from a low-resolution (LR) observation with the
help of a paired HR color image, is a longstanding and fundamental problem, it
has attracted considerable attention from computer vision and image processing
communities. A myriad of novel and effective approaches have been proposed
recently, especially with powerful deep learning techniques. This survey is an
effort to present a comprehensive survey of recent progress in GDSR. We start
by summarizing the problem of GDSR and explaining why it is challenging. Next,
we introduce some commonly used datasets and image quality assessment methods.
In addition, we roughly classify existing GDSR methods into three categories,
i.e., filtering-based methods, prior-based methods, and learning-based methods.
In each category, we introduce the general description of the published
algorithms and design principles, summarize the representative methods, and
discuss their highlights and limitations. Moreover, the depth related
applications are introduced. Furthermore, we conduct experiments to evaluate
the performance of some representative methods based on unified experimental
configurations, so as to offer a systematic and fair performance evaluation to
readers. Finally, we conclude this survey with possible directions and open
problems for further research. All the related materials can be found at
\url{https://github.com/zhwzhong/Guided-Depth-Map-Super-resolution-A-Survey}."
"Stereo matching is a fundamental task for 3D scene reconstruction. Recently,
deep learning based methods have proven effective on some benchmark datasets,
such as KITTI and Scene Flow. UAVs (Unmanned Aerial Vehicles) are commonly
utilized for surface observation, and their captured images are frequently used
for detailed 3D reconstruction due to high resolution and low-altitude
acquisition. At present, the mainstream supervised learning network requires a
significant amount of training data with ground-truth labels to learn model
parameters. However, due to the scarcity of UAV stereo matching datasets, the
learning-based network cannot be applied to UAV images. To facilitate further
research, this paper proposes a novel pipeline to generate accurate and dense
disparity maps using detailed meshes reconstructed by UAV images and LiDAR
point clouds. Through the proposed pipeline, this paper constructs a
multi-resolution UAV scenario dataset, called UAVStereo, with over 34k stereo
image pairs covering 3 typical scenes. As far as we know, UAVStereo is the
first stereo matching dataset of UAV low-altitude scenarios. The dataset
includes synthetic and real stereo pairs to enable generalization from the
synthetic domain to the real domain. Furthermore, our UAVStereo dataset
provides multi-resolution and multi-scene images pairs to accommodate a variety
of sensors and environments. In this paper, we evaluate traditional and
state-of-the-art deep learning methods, highlighting their limitations in
addressing challenges in UAV scenarios and offering suggestions for future
research. The dataset is available at
https://github.com/rebecca0011/UAVStereo.git"
"Weakly Supervised Object Detection (WSOD) enables the training of object
detection models using only image-level annotations. State-of-the-art WSOD
detectors commonly rely on multi-instance learning (MIL) as the backbone of
their detectors and assume that the bounding box proposals of an image are
independent of each other. However, since such approaches only utilize the
highest score proposal and discard the potentially useful information from
other proposals, their independent MIL backbone often limits models to salient
parts of an object or causes them to detect only one object per class. To solve
the above problems, we propose a novel backbone for WSOD based on our tailored
Vision Transformer named Weakly Supervised Transformer Detection Network
(WSTDN). Our algorithm is not only the first to demonstrate that self-attention
modules that consider inter-instance relationships are effective backbones for
WSOD, but also we introduce a novel bounding box mining method (BBM) integrated
with a memory transfer refinement (MTR) procedure to utilize the instance
dependencies for facilitating instance refinements. Experimental results on
PASCAL VOC2007 and VOC2012 benchmarks demonstrate the effectiveness of our
proposed WSTDN and modified instance refinement modules."
"Human-Object Interaction (HOI) detection aims to localize human-object pairs
and recognize their interactions. Recently, Contrastive Language-Image
Pre-training (CLIP) has shown great potential in providing interaction prior
for HOI detectors via knowledge distillation. However, such approaches often
rely on large-scale training data and suffer from inferior performance under
few/zero-shot scenarios. In this paper, we propose a novel HOI detection
framework that efficiently extracts prior knowledge from CLIP and achieves
better generalization. In detail, we first introduce a novel interaction
decoder to extract informative regions in the visual feature map of CLIP via a
cross-attention mechanism, which is then fused with the detection backbone by a
knowledge integration block for more accurate human-object pair detection. In
addition, prior knowledge in CLIP text encoder is leveraged to generate a
classifier by embedding HOI descriptions. To distinguish fine-grained
interactions, we build a verb classifier from training data via visual semantic
arithmetic and a lightweight verb representation adapter. Furthermore, we
propose a training-free enhancement to exploit global HOI predictions from
CLIP. Extensive experiments demonstrate that our method outperforms the state
of the art by a large margin on various settings, e.g. +4.04 mAP on HICO-Det.
The source code is available in https://github.com/Artanic30/HOICLIP."
"Learning semantic segmentation requires pixel-wise annotations, which can be
time-consuming and expensive. To reduce the annotation cost, we propose a
superpixel-based active learning (AL) framework, which collects a dominant
label per superpixel instead. To be specific, it consists of adaptive
superpixel and sieving mechanisms, fully dedicated to AL. At each round of AL,
we adaptively merge neighboring pixels of similar learned features into
superpixels. We then query a selected subset of these superpixels using an
acquisition function assuming no uniform superpixel size. This approach is more
efficient than existing methods, which rely only on innate features such as RGB
color and assume uniform superpixel sizes. Obtaining a dominant label per
superpixel drastically reduces annotators' burden as it requires fewer clicks.
However, it inevitably introduces noisy annotations due to mismatches between
superpixel and ground truth segmentation. To address this issue, we further
devise a sieving mechanism that identifies and excludes potentially noisy
annotations from learning. Our experiments on both Cityscapes and PASCAL VOC
datasets demonstrate the efficacy of adaptive superpixel and sieving
mechanisms."
"Unsupervised domain adaptation (UDA) approaches focus on adapting models
trained on a labeled source domain to an unlabeled target domain. UDA methods
have a strong assumption that the source data is accessible during adaptation,
which may not be feasible in many real-world scenarios due to privacy concerns
and resource constraints of devices. In this regard, source-free domain
adaptation (SFDA) excels as access to source data is no longer required during
adaptation. Recent state-of-the-art (SOTA) methods on SFDA mostly focus on
pseudo-label refinement based self-training which generally suffers from two
issues: i) inevitable occurrence of noisy pseudo-labels that could lead to
early training time memorization, ii) refinement process requires maintaining a
memory bank which creates a significant burden in resource constraint
scenarios. To address these concerns, we propose C-SFDA, a curriculum learning
aided self-training framework for SFDA that adapts efficiently and reliably to
changes across domains based on selective pseudo-labeling. Specifically, we
employ a curriculum learning scheme to promote learning from a restricted
amount of pseudo labels selected based on their reliabilities. This simple yet
effective step successfully prevents label noise propagation during different
stages of adaptation and eliminates the need for costly memory-bank based label
refinement. Our extensive experimental evaluations on both image recognition
and semantic segmentation tasks confirm the effectiveness of our method. C-SFDA
is readily applicable to online test-time domain adaptation and also
outperforms previous SOTA methods in this task."
"We present a framework to use recently introduced Capsule Networks for
solving the problem of Optical Flow, one of the fundamental computer vision
tasks. Most of the existing state of the art deep architectures either uses a
correlation oepration to match features from them. While correlation layer is
sensitive to the choice of hyperparameters and does not put a prior on the
underlying structure of the object, spatio temporal features will be limited by
the network's receptive field. Also, we as humans look at moving objects as
whole, something which cannot be encoded by correlation or spatio temporal
features. Capsules, on the other hand, are specialized to model seperate
entities and their pose as a continuous matrix. Thus, we show that a simpler
linear operation over poses of the objects detected by the capsules in enough
to model flow. We show reslts on a small toy dataset where we outperform
FlowNetC and PWC-Net models."
"Deep neural networks have made huge progress in the last few decades.
However, as the real-world data often exhibits a long-tailed distribution,
vanilla deep models tend to be heavily biased toward the majority classes. To
address this problem, state-of-the-art methods usually adopt a mixture of
experts (MoE) to focus on different parts of the long-tailed distribution.
Experts in these methods are with the same model depth, which neglects the fact
that different classes may have different preferences to be fit by models with
different depths. To this end, we propose a novel MoE-based method called
Self-Heterogeneous Integration with Knowledge Excavation (SHIKE). We first
propose Depth-wise Knowledge Fusion (DKF) to fuse features between different
shallow parts and the deep part in one network for each expert, which makes
experts more diverse in terms of representation. Based on DKF, we further
propose Dynamic Knowledge Transfer (DKT) to reduce the influence of the hardest
negative class that has a non-negligible impact on the tail classes in our MoE
framework. As a result, the classification accuracy of long-tailed data can be
significantly improved, especially for the tail classes. SHIKE achieves the
state-of-the-art performance of 56.3%, 60.3%, 75.4%, and 41.9% on CIFAR100-LT
(IF100), ImageNet-LT, iNaturalist 2018, and Places-LT, respectively."
"Ethylene leakage detection has become one of the most important research
directions in the field of target detection due to the fact that ethylene
leakage in the petrochemical industry is closely related to production safety
and environmental pollution. Under infrared conditions, there are many factors
that affect the texture characteristics of ethylene, such as ethylene
concentration, background, and so on. We find that the detection criteria used
in infrared imaging ethylene leakage detection research cannot fully reflect
real-world production conditions, which is not conducive to evaluate the
performance of current image-based target detection methods. Therefore, we
create a new infrared image dataset of ethylene leakage with different
concentrations and backgrounds, including 54275 images. We use the proposed
dataset benchmark to evaluate seven advanced image-based target detection
algorithms. Experimental results demonstrate the performance and limitations of
existing algorithms, and the dataset benchmark has good versatility and
effectiveness."
"Line segments are powerful features complementary to points. They offer
structural cues, robust to drastic viewpoint and illumination changes, and can
be present even in texture-less areas. However, describing and matching them is
more challenging compared to points due to partial occlusions, lack of texture,
or repetitiveness. This paper introduces a new matching paradigm, where points,
lines, and their descriptors are unified into a single wireframe structure. We
propose GlueStick, a deep matching Graph Neural Network (GNN) that takes two
wireframes from different images and leverages the connectivity information
between nodes to better glue them together. In addition to the increased
efficiency brought by the joint matching, we also demonstrate a large boost of
performance when leveraging the complementary nature of these two features in a
single architecture. We show that our matching strategy outperforms the
state-of-the-art approaches independently matching line segments and points for
a wide variety of datasets and tasks. The code is available at
https://github.com/cvg/GlueStick."
"Popular benchmarks for self-supervised LiDAR scene flow (stereoKITTI, and
FlyingThings3D) have unrealistic rates of dynamic motion, unrealistic
correspondences, and unrealistic sampling patterns. As a result, progress on
these benchmarks is misleading and may cause researchers to focus on the wrong
problems. We evaluate a suite of top methods on a suite of real-world datasets
(Argoverse 2.0, Waymo, and NuScenes) and report several conclusions. First, we
find that performance on stereoKITTI is negatively correlated with performance
on real-world data. Second, we find that one of this task's key components --
removing the dominant ego-motion -- is better solved by classic ICP than any
tested method. Finally, we show that despite the emphasis placed on learning,
most performance gains are caused by pre- and post-processing steps:
piecewise-rigid refinement and ground removal. We demonstrate this through a
baseline method that combines these processing steps with a learning-free
test-time flow optimization. This baseline outperforms every evaluated method."
"This paper tackles spectral reflectance recovery (SRR) from RGB images. Since
capturing ground-truth spectral reflectance and camera spectral sensitivity are
challenging and costly, most existing approaches are trained on synthetic
images and utilize the same parameters for all unseen testing images, which are
suboptimal especially when the trained models are tested on real images because
they never exploit the internal information of the testing images. To address
this issue, we adopt a self-supervised meta-auxiliary learning (MAXL) strategy
that fine-tunes the well-trained network parameters with each testing image to
combine external with internal information. To the best of our knowledge, this
is the first work that successfully adapts the MAXL strategy to this problem.
Instead of relying on naive end-to-end training, we also propose a novel
architecture that integrates the physical relationship between the spectral
reflectance and the corresponding RGB images into the network based on our
mathematical analysis. Besides, since the spectral reflectance of a scene is
independent to its illumination while the corresponding RGB images are not, we
recover the spectral reflectance of a scene from its RGB images captured under
multiple illuminations to further reduce the unknown. Qualitative and
quantitative evaluations demonstrate the effectiveness of our proposed network
and of the MAXL. Our code and data are available at
https://github.com/Dong-Huo/SRR-MAXL."
"Cross-modal retrieval methods are the preferred tool to search databases for
the text that best matches a query image and vice versa. However, image-text
retrieval models commonly learn to memorize spurious correlations in the
training data, such as frequent object co-occurrence, instead of looking at the
actual underlying reasons for the prediction in the image. For image-text
retrieval, this manifests in retrieved sentences that mention objects that are
not present in the query image. In this work, we introduce ODmAP@k, an object
decorrelation metric that measures a model's robustness to spurious
correlations in the training data. We use automatic image and text
manipulations to control the presence of such object correlations in designated
test data. Additionally, our data synthesis technique is used to tackle model
biases due to spurious correlations of semantically unrelated objects in the
training data. We apply our proposed pipeline, which involves the finetuning of
image-text retrieval frameworks on carefully designed synthetic data, to three
state-of-the-art models for image-text retrieval. This results in significant
improvements for all three models, both in terms of the standard retrieval
performance and in terms of our object decorrelation metric. The code is
available at https://github.com/ExplainableML/Spurious_CM_Retrieval."
"Product Retrieval (PR) and Grounding (PG), aiming to seek image and
object-level products respectively according to a textual query, have attracted
great interest recently for better shopping experience. Owing to the lack of
relevant datasets, we collect two large-scale benchmark datasets from Taobao
Mall and Live domains with about 474k and 101k image-query pairs for PR, and
manually annotate the object bounding boxes in each image for PG. As annotating
boxes is expensive and time-consuming, we attempt to transfer knowledge from
annotated domain to unannotated for PG to achieve un-supervised Domain
Adaptation (PG-DA). We propose a {\bf D}omain {\bf A}daptive Produc{\bf t}
S{\bf e}eker ({\bf DATE}) framework, regarding PR and PG as Product Seeking
problem at different levels, to assist the query {\bf date} the product.
Concretely, we first design a semantics-aggregated feature extractor for each
modality to obtain concentrated and comprehensive features for following
efficient retrieval and fine-grained grounding tasks. Then, we present two
cooperative seekers to simultaneously search the image for PR and localize the
product for PG. Besides, we devise a domain aligner for PG-DA to alleviate
uni-modal marginal and multi-modal conditional distribution shift between
source and target domains, and design a pseudo box generator to dynamically
select reliable instances and generate bounding boxes for further knowledge
transfer. Extensive experiments show that our DATE achieves satisfactory
performance in fully-supervised PR, PG and un-supervised PG-DA. Our
desensitized datasets will be publicly available
here\footnote{\url{https://github.com/Taobao-live/Product-Seeking}}."
"Referring expression segmentation aims to segment an object described by a
language expression from an image. Despite the recent progress on this task,
existing models tackling this task may not be able to fully capture semantics
and visual representations of individual concepts, which limits their
generalization capability, especially when handling novel compositions of
learned concepts. In this work, through the lens of meta learning, we propose a
Meta Compositional Referring Expression Segmentation (MCRES) framework to
enhance model compositional generalization performance. Specifically, to handle
various levels of novel compositions, our framework first uses training data to
construct a virtual training set and multiple virtual testing sets, where data
samples in each virtual testing set contain a level of novel compositions
w.r.t. the virtual training set. Then, following a novel meta optimization
scheme to optimize the model to obtain good testing performance on the virtual
testing sets after training on the virtual training set, our framework can
effectively drive the model to better capture semantics and visual
representations of individual concepts, and thus obtain robust generalization
performance even when handling novel compositions. Extensive experiments on
three benchmark datasets demonstrate the effectiveness of our framework."
"DeepFake involves the use of deep learning and artificial intelligence
techniques to produce or change video and image contents typically generated by
GANs. Moreover, it can be misused and leads to fictitious news, ethical and
financial crimes, and also affects the performance of facial recognition
systems. Thus, detection of real or fake images is significant specially to
authenticate originality of people's images or videos. One of the most
important challenges in this topic is obstruction that decreases the system
precision. In this study, we present a deep learning approach using the entire
face and face patches to distinguish real/fake images in the presence of
obstruction with a three-path decision: first entire-face reasoning, second a
decision based on the concatenation of feature vectors of face patches, and
third a majority vote decision based on these features. To test our approach,
new datasets including real and fake images are created. For producing fake
images, StyleGAN and StyleGAN2 are trained by FFHQ images and also StarGAN and
PGGAN are trained by CelebA images. The CelebA and FFHQ datasets are used as
real images. The proposed approach reaches higher results in early epochs than
other methods and increases the SoTA results by 0.4\%-7.9\% in the different
built data-sets. Also, we have shown in experimental results that weighing the
patches may improve accuracy."
"Video Panoptic Segmentation (VPS) aims to achieve comprehensive pixel-level
scene understanding by segmenting all pixels and associating objects in a
video. Current solutions can be categorized into online and near-online
approaches. Evolving over the time, each category has its own specialized
designs, making it nontrivial to adapt models between different categories. To
alleviate the discrepancy, in this work, we propose a unified approach for
online and near-online VPS. The meta architecture of the proposed Video-kMaX
consists of two components: within clip segmenter (for clip-level segmentation)
and cross-clip associater (for association beyond clips). We propose clip-kMaX
(clip k-means mask transformer) and HiLA-MB (Hierarchical Location-Aware Memory
Buffer) to instantiate the segmenter and associater, respectively. Our general
formulation includes the online scenario as a special case by adopting clip
length of one. Without bells and whistles, Video-kMaX sets a new
state-of-the-art on KITTI-STEP and VIPSeg for video panoptic segmentation, and
VSPW for video semantic segmentation. Code will be made publicly available."
"Deep normal estimators have made great strides on synthetic benchmarks.
Unfortunately, their performance dramatically drops on the real scan data since
they are supervised only on synthetic datasets. The point-wise annotation of
ground truth normals is vulnerable to inefficiency and inaccuracies, which
totally makes it impossible to build perfect real datasets for supervised deep
learning. To overcome the challenge, we propose a multi-sample consensus
paradigm for unsupervised normal estimation. The paradigm consists of
multi-candidate sampling, candidate rejection, and mode determination. The
latter two are driven by neighbor point consensus and candidate consensus
respectively. Two primary implementations of the paradigm, MSUNE and MSUNE-Net,
are proposed. MSUNE minimizes a candidate consensus loss in mode determination.
As a robust optimization method, it outperforms the cutting-edge supervised
deep learning methods on real data at the cost of longer runtime for sampling
enough candidate normals for each query point. MSUNE-Net, the first
unsupervised deep normal estimator as far as we know, significantly promotes
the multi-sample consensus further. It transfers the three online stages of
MSUNE to offline training. Thereby its inference time is 100 times faster.
Besides that, more accurate inference is achieved, since the candidates of
query points from similar patches can form a sufficiently large candidate set
implicitly in MSUNE-Net. Comprehensive experiments demonstrate that the two
proposed unsupervised methods are noticeably superior to some supervised deep
normal estimators on the most common synthetic dataset. More importantly, they
show better generalization ability and outperform all the SOTA conventional and
deep methods on three real datasets: NYUV2, KITTI, and a dataset from PCV [1]."
"Self-supervised image denoising implies restoring the signal from a noisy
image without access to the ground truth. State-of-the-art solutions for this
task rely on predicting masked pixels with a fully-convolutional neural
network. This most often requires multiple forward passes, information about
the noise model, or intricate regularization functions. In this paper, we
propose a Swin Transformer-based Image Autoencoder (SwinIA), the first
fully-transformer architecture for self-supervised denoising. The flexibility
of the attention mechanism helps to fulfill the blind-spot property that
convolutional counterparts normally approximate. SwinIA can be trained
end-to-end with a simple mean squared error loss without masking and does not
require any prior knowledge about clean data or noise distribution. Simple to
use, SwinIA establishes the state of the art on several common benchmarks."
"Dynamics prediction, which is the problem of predicting future states of
scene objects based on current and prior states, is drawing increasing
attention as an instance of learning physics. To solve this problem, Region
Proposal Convolutional Interaction Network (RPCIN), a vision-based model, was
proposed and achieved state-of-the-art performance in long-term prediction.
RPCIN only takes raw images and simple object descriptions, such as the
bounding box and segmentation mask of each object, as input. However, despite
its success, the model's capability can be compromised under conditions of
environment misalignment. In this paper, we investigate two challenging
conditions for environment misalignment: Cross-Domain and Cross-Context by
proposing four datasets that are designed for these challenges: SimB-Border,
SimB-Split, BlenB-Border, and BlenB-Split. The datasets cover two domains and
two contexts. Using RPCIN as a probe, experiments conducted on the combinations
of the proposed datasets reveal potential weaknesses of the vision-based
long-term dynamics prediction model. Furthermore, we propose a promising
direction to mitigate the Cross-Domain challenge and provide concrete evidence
supporting such a direction, which provides dramatic alleviation of the
challenge on the proposed datasets."
"We propose a pipeline for combined multi-class object geolocation and height
estimation from street level RGB imagery, which is considered as a single
available input data modality. Our solution is formulated via Markov Random
Field optimization with deterministic output. The proposed technique uses image
metadata along with coordinates of objects detected in the image plane as found
by a custom-trained Convolutional Neural Network. Computing the object height
using our methodology, in addition to object geolocation, has negligible effect
on the overall computational cost. Accuracy is demonstrated experimentally for
water drains and road signs on which we achieve average elevation estimation
error lower than 20cm."
"We present an approach to reconstruct humans and track them over time. At the
core of our approach, we propose a fully ""transformerized"" version of a network
for human mesh recovery. This network, HMR 2.0, advances the state of the art
and shows the capability to analyze unusual poses that have in the past been
difficult to reconstruct from single images. To analyze video, we use 3D
reconstructions from HMR 2.0 as input to a tracking system that operates in 3D.
This enables us to deal with multiple people and maintain identities through
occlusion events. Our complete approach, 4DHumans, achieves state-of-the-art
results for tracking people from monocular video. Furthermore, we demonstrate
the effectiveness of HMR 2.0 on the downstream task of action recognition,
achieving significant improvements over previous pose-based action recognition
approaches. Our code and models are available on the project website:
https://shubham-goel.github.io/4dhumans/."
"Modern deep learning approaches usually utilize modality-specific processing.
For example, the most common deep learning approach to image classification
involves decoding image file bytes into an RGB tensor which is passed into a
neural network. Instead, we investigate modality-independent representation
learning by performing classification directly on file bytes, without the need
for decoding files at inference time. This enables models to operate on various
modalities without any hand-designed, modality-specific processing. Our model,
ByteFormer, improves ImageNet Top-1 classification accuracy by $5\%$ (from
$72.2\%$ to $77.33\%$) relative to DeIT models of similar size. Compared to
Perceiver IO, our model requires absolutely no modality-specific processing at
inference time, and uses an order of magnitude fewer parameters at equivalent
accuracy on ImageNet. We demonstrate that the same ByteFormer architecture can
perform audio classification without modifications or modality-specific
preprocessing. We achieve $95.42\%$ classification accuracy on the Speech
Commands V2 dataset (comparable to the state-of-the-art accuracy of $98.7\%$).
Additionally, we demonstrate that ByteFormer can operate jointly on images and
audio, handling joint classification without explicit knowledge of the input
modality. We release our code at
https://github.com/apple/corenet/tree/main/projects/byteformer."
"Diffusion models have achieved promising results in image restoration tasks,
yet suffer from time-consuming, excessive computational resource consumption,
and unstable restoration. To address these issues, we propose a robust and
efficient Diffusion-based Low-Light image enhancement approach, dubbed DiffLL.
Specifically, we present a wavelet-based conditional diffusion model (WCDM)
that leverages the generative power of diffusion models to produce results with
satisfactory perceptual fidelity. Additionally, it also takes advantage of the
strengths of wavelet transformation to greatly accelerate inference and reduce
computational resource usage without sacrificing information. To avoid chaotic
content and diversity, we perform both forward diffusion and denoising in the
training phase of WCDM, enabling the model to achieve stable denoising and
reduce randomness during inference. Moreover, we further design a
high-frequency restoration module (HFRM) that utilizes the vertical and
horizontal details of the image to complement the diagonal information for
better fine-grained restoration. Extensive experiments on publicly available
real-world benchmarks demonstrate that our method outperforms the existing
state-of-the-art methods both quantitatively and visually, and it achieves
remarkable improvements in efficiency compared to previous diffusion-based
methods. In addition, we empirically show that the application for low-light
face detection also reveals the latent practical values of our method. Code is
available at https://github.com/JianghaiSCU/Diffusion-Low-Light."
"Existing deepfake detection methods fail to generalize well to unseen or
degraded samples, which can be attributed to the over-fitting of low-level
forgery patterns. Here we argue that high-level semantics are also
indispensable recipes for generalizable forgery detection. Recently, large
pre-trained Vision Transformers (ViTs) have shown promising generalization
capability. In this paper, we propose the first parameter-efficient tuning
approach for deepfake detection, namely DeepFake-Adapter, to effectively and
efficiently adapt the generalizable high-level semantics from large pre-trained
ViTs to aid deepfake detection. Given large pre-trained models but limited
deepfake data, DeepFake-Adapter introduces lightweight yet dedicated dual-level
adapter modules to a ViT while keeping the model backbone frozen. Specifically,
to guide the adaptation process to be aware of both global and local forgery
cues of deepfake data, 1) we not only insert Globally-aware Bottleneck Adapters
in parallel to MLP layers of ViT, 2) but also actively cross-attend
Locally-aware Spatial Adapters with features from ViT. Unlike existing deepfake
detection methods merely focusing on low-level forgery patterns, the forgery
detection process of our model can be regularized by generalizable high-level
semantics from a pre-trained ViT and adapted by global and local low-level
forgeries of deepfake data. Extensive experiments on several standard deepfake
detection benchmarks validate the effectiveness of our approach. Notably,
DeepFake-Adapter demonstrates a convincing advantage under cross-dataset and
cross-manipulation settings. The code has been released at
https://github.com/rshaojimmy/DeepFake-Adapter."
"Scene text removal (STR) aims at replacing text strokes in natural scenes
with visually coherent backgrounds. Recent STR approaches rely on iterative
refinements or explicit text masks, resulting in high complexity and
sensitivity to the accuracy of text localization. Moreover, most existing STR
methods adopt convolutional architectures while the potential of vision
Transformers (ViTs) remains largely unexplored. In this paper, we propose a
simple-yet-effective ViT-based text eraser, dubbed ViTEraser. Following a
concise encoder-decoder framework, ViTEraser can easily incorporate various
ViTs to enhance long-range modeling. Specifically, the encoder hierarchically
maps the input image into the hidden space through ViT blocks and patch
embedding layers, while the decoder gradually upsamples the hidden features to
the text-erased image with ViT blocks and patch splitting layers. As ViTEraser
implicitly integrates text localization and inpainting, we propose a novel
end-to-end pretraining method, termed SegMIM, which focuses the encoder and
decoder on the text box segmentation and masked image modeling tasks,
respectively. Experimental results demonstrate that ViTEraser with SegMIM
achieves state-of-the-art performance on STR by a substantial margin and
exhibits strong generalization ability when extended to other tasks,
\textit{e.g.}, tampered scene text detection. Furthermore, we comprehensively
explore the architecture, pretraining, and scalability of the ViT-based
encoder-decoder for STR, which provides deep insights into the application of
ViT to the STR field. Code is available at
https://github.com/shannanyinxiang/ViTEraser."
"Diplomatics, the analysis of medieval charters, is a major field of research
in which paleography is applied. Annotating data, if performed by laymen, needs
validation and correction by experts. In this paper, we propose an effective
and efficient annotation approach for charter segmentation, essentially
reducing it to object detection. This approach allows for a much more efficient
use of the paleographer's time and produces results that can compete and even
outperform pixel-level segmentation in some use cases. Further experiments shed
light on how to design a class ontology in order to make the best use of
annotators' time and effort. Exploiting the presence of calibration cards in
the image, we further annotate the data with the physical length in pixels and
train regression neural networks to predict it from image patches."
"Recognizing characters from low-resolution (LR) text images poses a
significant challenge due to the information deficiency as well as the noise
and blur in low-quality images. Current solutions for low-resolution text
recognition (LTR) typically rely on a two-stage pipeline that involves
super-resolution as the first stage followed by the second-stage recognition.
Although this pipeline is straightforward and intuitive, it has to use an
additional super-resolution network, which causes inefficiencies during
training and testing. Moreover, the recognition accuracy of the second stage
heavily depends on the reconstruction quality of the first stage, causing
ineffectiveness. In this work, we attempt to address these challenges from a
novel perspective: adapting the recognizer to low-resolution inputs by
transferring the knowledge from the high-resolution. Guided by this idea, we
propose an efficient and effective knowledge distillation framework to achieve
multi-level knowledge transfer. Specifically, the visual focus loss is proposed
to extract the character position knowledge with resolution gap reduction and
character region focus, the semantic contrastive loss is employed to exploit
the contextual semantic knowledge with contrastive learning, and the soft
logits loss facilitates both local word-level and global sequence-level
learning from the soft teacher label. Extensive experiments show that the
proposed one-stage pipeline significantly outperforms super-resolution based
two-stage frameworks in terms of effectiveness and efficiency, accompanied by
favorable robustness. Code is available at https://github.com/csguoh/KD-LTR."
"Objects in videos are typically characterized by continuous smooth motion. We
exploit continuous smooth motion in three ways. 1) Improved accuracy by using
object motion as an additional source of supervision, which we obtain by
anticipating object locations from a static keyframe. 2) Improved efficiency by
only doing the expensive feature computations on a small subset of all frames.
Because neighboring video frames are often redundant, we only compute features
for a single static keyframe and predict object locations in subsequent frames.
3) Reduced annotation cost, where we only annotate the keyframe and use smooth
pseudo-motion between keyframes. We demonstrate computational efficiency,
annotation efficiency, and improved mean average precision compared to the
state-of-the-art on four datasets: ImageNet VID, EPIC KITCHENS-55,
YouTube-BoundingBoxes, and Waymo Open dataset. Our source code is available at
https://github.com/L-KID/Videoobject-detection-by-location-anticipation."
"In this paper, we propose a new key-based defense focusing on both efficiency
and robustness. Although the previous key-based defense seems effective in
defending against adversarial examples, carefully designed adaptive attacks can
bypass the previous defense, and it is difficult to train the previous defense
on large datasets like ImageNet. We build upon the previous defense with two
major improvements: (1) efficient training and (2) optional randomization. The
proposed defense utilizes one or more secret patch embeddings and classifier
heads with a pre-trained isotropic network. When more than one secret
embeddings are used, the proposed defense enables randomization on inference.
Experiments were carried out on the ImageNet dataset, and the proposed defense
was evaluated against an arsenal of state-of-the-art attacks, including
adaptive ones. The results show that the proposed defense achieves a high
robust accuracy and a comparable clean accuracy compared to the previous
key-based defense."
"Image restoration aims to recover the high-quality images from their degraded
observations. Since most existing methods have been dedicated into single
degradation removal, they may not yield optimal results on other types of
degradations, which do not satisfy the applications in real world scenarios. In
this paper, we propose a novel data ingredient-oriented approach that leverages
prompt-based learning to enable a single model to efficiently tackle multiple
image degradation tasks. Specifically, we utilize a encoder to capture features
and introduce prompts with degradation-specific information to guide the
decoder in adaptively recovering images affected by various degradations. In
order to model the local invariant properties and non-local information for
high-quality image restoration, we combined CNNs operations and Transformers.
Simultaneously, we made several key designs in the Transformer blocks
(multi-head rearranged attention with prompts and simple-gate feed-forward
network) to reduce computational requirements and selectively determines what
information should be persevered to facilitate efficient recovery of
potentially sharp images. Furthermore, we incorporate a feature fusion
mechanism further explores the multi-scale information to improve the
aggregated features. The resulting tightly interlinked hierarchy architecture,
named as CAPTNet, extensive experiments demonstrate that our method performs
competitively to the state-of-the-art."
"Recently, sparsity-based algorithms are proposed for super-resolution
spectrum estimation. However, to achieve adequately high resolution in
real-world signal analysis, the dictionary atoms have to be close to each other
in frequency, thereby resulting in a coherent design. The popular convex
compressed sensing methods break down in presence of high coherence and large
noise. We propose a new regularization approach to handle model collinearity
and obtain parsimonious frequency selection simultaneously. It takes advantage
of the pairing structure of sine and cosine atoms in the frequency dictionary.
A probabilistic spectrum screening is also developed for fast computation in
high dimensions. A data-resampling version of high-dimensional Bayesian
Information Criterion is used to determine the regularization parameters.
Experiments show the efficacy and efficiency of the proposed algorithms in
challenging situations with small sample size, high frequency resolution, and
low signal-to-noise ratio."
"Visual rendering of graphs is a key task in the mapping of complex network
data. Although most graph drawing algorithms emphasize aesthetic appeal,
certain applications such as travel-time maps place more importance on
visualization of structural network properties. The present paper advocates two
graph embedding approaches with centrality considerations to comply with node
hierarchy. The problem is formulated first as one of constrained
multi-dimensional scaling (MDS), and it is solved via block coordinate descent
iterations with successive approximations and guaranteed convergence to a KKT
point. In addition, a regularization term enforcing graph smoothness is
incorporated with the goal of reducing edge crossings. A second approach
leverages the locally-linear embedding (LLE) algorithm which assumes that the
graph encodes data sampled from a low-dimensional manifold. Closed-form
solutions to the resulting centrality-constrained optimization problems are
determined yielding meaningful embeddings. Experimental results demonstrate the
efficacy of both approaches, especially for visualizing large networks on the
order of thousands of nodes."
"In this tutorial we explain the inference procedures developed for the sparse
Gaussian process (GP) regression and Gaussian process latent variable model
(GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias &
Lawrence (2010) is brief, hence getting a full picture of it requires
collecting results from several different sources and a substantial amount of
algebra to fill-in the gaps. Our main goal is thus to collect all the results
and full derivations into one place to help speed up understanding this work.
In doing so we present a re-parametrisation of the inference that allows it to
be carried out in parallel. A secondary goal for this document is, therefore,
to accompany our paper and open-source implementation of the parallel inference
scheme for the models. We hope that this document will bridge the gap between
the equations as implemented in code and those published in the original
papers, in order to make it easier to extend existing work. We assume prior
knowledge of Gaussian processes and variational inference, but we also include
references for further reading where appropriate."
"Linear dimensionality reduction methods are a cornerstone of analyzing high
dimensional data, due to their simple geometric interpretations and typically
attractive computational properties. These methods capture many data features
of interest, such as covariance, dynamical structure, correlation between data
sets, input-output relationships, and margin between data classes. Methods have
been developed with a variety of names and motivations in many fields, and
perhaps as a result the connections between all these methods have not been
highlighted. Here we survey methods from this disparate literature as
optimization programs over matrix manifolds. We discuss principal component
analysis, factor analysis, linear multidimensional scaling, Fisher's linear
discriminant analysis, canonical correlations analysis, maximum autocorrelation
factors, slow feature analysis, sufficient dimensionality reduction,
undercomplete independent component analysis, linear regression, distance
metric learning, and more. This optimization framework gives insight to some
rarely discussed shortcomings of well-known methods, such as the suboptimality
of certain eigenvector solutions. Modern techniques for optimization over
matrix manifolds enable a generic linear dimensionality reduction solver, which
accepts as input data and an objective to be optimized, and returns, as output,
an optimal low-dimensional projection of the data. This simple optimization
framework further allows straightforward generalizations and novel variants of
classical methods, which we demonstrate here by creating an
orthogonal-projection canonical correlations analysis. More broadly, this
survey and generic solver suggest that linear dimensionality reduction can move
toward becoming a blackbox, objective-agnostic numerical technology."
"Matrix factorization (MF) has become a common approach to collaborative
filtering, due to ease of implementation and scalability to large data sets.
Two existing drawbacks of the basic model is that it does not incorporate side
information on either users or items, and assumes a common variance for all
users. We extend the work of constrained probabilistic matrix factorization by
deriving the Gibbs updates for the side feature vectors for items
(Salakhutdinov and Minh, 2008). We show that this Bayesian treatment to the
constrained PMF model outperforms simple MAP estimation. We also consider
extensions to heteroskedastic precision introduced in the literature
(Lakshminarayanan, Bouchard, and Archambeau, 2011). We show that this tends
result in overfitting for deterministic approximation algorithms (ex:
Variational inference) when the observed entries in the user / item matrix are
distributed in an non-uniform manner. In light of this, we propose a truncated
precision model. Our experimental results suggest that this model tends to
delay overfitting."
"We derive a new discrepancy statistic for measuring differences between two
probability distributions based on combining Stein's identity with the
reproducing kernel Hilbert space theory. We apply our result to test how well a
probabilistic model fits a set of observations, and derive a new class of
powerful goodness-of-fit tests that are widely applicable for complex and high
dimensional distributions, even for those with computationally intractable
normalization constants. Both theoretical and empirical properties of our
methods are studied thoroughly."
"To the best of our knowledge, there are no general well-founded robust
methods for statistical unsupervised learning. Most of the unsupervised methods
explicitly or implicitly depend on the kernel covariance operator (kernel CO)
or kernel cross-covariance operator (kernel CCO). They are sensitive to
contaminated data, even when using bounded positive definite kernels. First, we
propose robust kernel covariance operator (robust kernel CO) and robust kernel
crosscovariance operator (robust kernel CCO) based on a generalized loss
function instead of the quadratic loss function. Second, we propose influence
function of classical kernel canonical correlation analysis (classical kernel
CCA). Third, using this influence function, we propose a visualization method
to detect influential observations from two sets of data. Finally, we propose a
method based on robust kernel CO and robust kernel CCO, called robust kernel
CCA, which is designed for contaminated data and less sensitive to noise than
classical kernel CCA. The principles we describe also apply to many kernel
methods which must deal with the issue of kernel CO or kernel CCO. Experiments
on synthesized and imaging genetics analysis demonstrate that the proposed
visualization and robust kernel CCA can be applied effectively to both ideal
data and contaminated data. The robust methods show the superior performance
over the state-of-the-art methods."
"Rapid overlay of chemical structures (ROCS) is a standard tool for the
calculation of 3D shape and chemical (""color"") similarity. ROCS uses unweighted
sums to combine many aspects of similarity, yielding parameter-free models for
virtual screening. In this report, we decompose the ROCS color force field into
""color components"" and ""color atom overlaps"", novel color similarity features
that can be weighted in a system-specific manner by machine learning
algorithms. In cross-validation experiments, these additional features
significantly improve virtual screening performance (ROC AUC scores) relative
to standard ROCS."
"In many applications (in particular information systems, such as pattern
recognition, machine learning, cheminformatics, bioinformatics to name but a
few) the assessment of uncertainty is essential - i.e., the estimation of the
underlying probability distribution function. More often than not, the form of
this function is unknown and it becomes necessary to non-parametrically
construct/estimate it from a given sample. One of the methods of choice to
non-parametrically estimate the unknown probability distribution function for a
given random variable (defined on binary space) has been the expansion of the
estimation function in Rademacher-Walsh Polynomial basis functions. In this
paper we demonstrate that the expansion of the probability distribution
function estimation in Rademacher-Walsh Polynomial basis functions is
equivalent to the expansion of the function estimation in a set of ""Dirac
kernel"" functions. The latter approach can ameliorate the computational
bottleneck and notational awkwardness often associated with the
Rademacher-Walsh Polynomial basis functions approach, in particular when the
binary input space is large."
"We propose a probabilistic model to aggregate the answers of respondents
answering multiple-choice questions. The model does not assume that everyone
has access to the same information, and so does not assume that the consensus
answer is correct. Instead, it infers the most probable world state, even if
only a minority vote for it. Each respondent is modeled as receiving a signal
contingent on the actual world state, and as using this signal to both
determine their own answer and predict the answers given by others. By
incorporating respondent's predictions of others' answers, the model infers
latent parameters corresponding to the prior over world states and the
probability of different signals being received in all possible world states,
including counterfactual ones. Unlike other probabilistic models for
aggregation, our model applies to both single and multiple questions, in which
case it estimates each respondent's expertise. The model shows good
performance, compared to a number of other probabilistic models, on data from
seven studies covering different types of expertise."
"Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD),
the resulting distance between distributions, are useful tools for fully
nonparametric two-sample testing and learning on distributions. However, it is
rarely that all possible differences between samples are of interest --
discovered differences can be due to different types of measurement noise, data
collection artefacts or other irrelevant sources of variability. We propose
distances between distributions which encode invariance to additive symmetric
noise, aimed at testing whether the assumed true underlying processes differ.
Moreover, we construct invariant features of distributions, leading to learning
algorithms robust to the impairment of the input distributions with symmetric
additive noise."
"Many applied settings in empirical economics involve simultaneous estimation
of a large number of parameters. In particular, applied economists are often
interested in estimating the effects of many-valued treatments (like teacher
effects or location effects), treatment effects for many groups, and prediction
models with many regressors. In these settings, machine learning methods that
combine regularized estimation and data-driven choices of regularization
parameters are useful to avoid over-fitting. In this article, we analyze the
performance of a class of machine learning estimators that includes ridge,
lasso and pretest in contexts that require simultaneous estimation of many
parameters. Our analysis aims to provide guidance to applied researchers on (i)
the choice between regularized estimators in practice and (ii) data-driven
selection of regularization parameters. To address (i), we characterize the
risk (mean squared error) of regularized estimators and derive their relative
performance as a function of simple features of the data generating process. To
address (ii), we show that data-driven choices of regularization parameters,
based on Stein's unbiased risk estimate or on cross-validation, yield
estimators with risk uniformly close to the risk attained under the optimal
(unfeasible) choice of regularization parameters. We use data from recent
examples in the empirical economics literature to illustrate the practical
applicability of our results."
"We give convergence guarantees for estimating the coefficients of a symmetric
mixture of two linear regressions by expectation maximization (EM). In
particular, we show that the empirical EM iterates converge to the target
parameter vector at the parametric rate, provided the algorithm is initialized
in an unbounded cone. In particular, if the initial guess has a sufficiently
large cosine angle with the target parameter vector, a sample-splitting version
of the EM algorithm converges to the true coefficient vector with high
probability. Interestingly, our analysis borrows from tools used in the problem
of estimating the centers of a symmetric mixture of two Gaussians by EM. We
also show that the population EM operator for mixtures of two regressions is
anti-contractive from the target parameter vector if the cosine angle between
the input vector and the target parameter vector is too small, thereby
establishing the necessity of our conic condition. Finally, we give empirical
evidence supporting this theoretical observation, which suggests that the
sample based EM algorithm performs poorly when initial guesses are drawn
accordingly. Our simulation study also suggests that the EM algorithm performs
well even under model misspecification (i.e., when the covariate and error
distributions violate the model assumptions)."
"In this paper a new Bayesian model for sparse linear regression with a
spatio-temporal structure is proposed. It incorporates the structural
assumptions based on a hierarchical Gaussian process prior for spike and slab
coefficients. We design an inference algorithm based on Expectation Propagation
and evaluate the model over the real data."
"Tick is a statistical learning library for Python~3, with a particular
emphasis on time-dependent models, such as point processes, and tools for
generalized linear models and survival analysis. The core of the library is an
optimization module providing model computational classes, solvers and proximal
operators for regularization. tick relies on a C++ implementation and
state-of-the-art optimization algorithms to provide very fast computations in a
single node multi-core setting. Source code and documentation can be downloaded
from https://github.com/X-DataInitiative/tick"
"Nowadays, with the unprecedented penetration of renewable distributed energy
resources (DERs), the necessity of an efficient energy forecasting model is
more demanding than before. Generally, forecasting models are trained using
observed weather data while the trained models are applied for energy
forecasting using forecasted weather data. In this study, the performance of
several commonly used forecasting methods in the presence of weather predictors
with uncertainty is assessed and compared. Accordingly, both observed and
forecasted weather data are collected, then the influential predictors for
solar PV generation forecasting model are selected using several measures.
Using observed and forecasted weather data, an analysis on the uncertainty of
weather variables is represented by MAE and bootstrapping. The energy
forecasting model is trained using observed weather data, and finally, the
performance of several commonly used forecasting methods in solar energy
forecasting is simulated and compared for a real case study."
"The emergence of mobile games has caused a paradigm shift in the video-game
industry. Game developers now have at their disposal a plethora of information
on their players, and thus can take advantage of reliable models that can
accurately predict player behavior and scale to huge datasets. Churn
prediction, a challenge common to a variety of sectors, is particularly
relevant for the mobile game industry, as player retention is crucial for the
successful monetization of a game. In this article, we present an approach to
predicting game abandon based on survival ensembles. Our method provides
accurate predictions on both the level at which each player will leave the game
and their accumulated playtime until that moment. Further, it is robust to
different data distributions and applicable to a wide range of response
variables, while also allowing for efficient parallelization of the algorithm.
This makes our model well suited to perform real-time analyses of churners,
even for games with millions of daily active users."
"The modified Cholesky decomposition is commonly used for precision matrix
estimation given a specified order of random variables. However, the order of
variables is often not available or cannot be pre-determined. In this work, we
propose to address the variable order issue in the modified Cholesky
decomposition for sparse precision matrix estimation. The key idea is to
effectively combine a set of estimates obtained from multiple permutations of
variable orders, and to efficiently encourage the sparse structure for the
resultant estimate by the thresholding technique on the ensemble Cholesky
factor matrix. The consistent property of the proposed estimate is established
under some weak regularity conditions. Simulation studies are conducted to
evaluate the performance of the proposed method in comparison with several
existing approaches. The proposed method is also applied into linear
discriminant analysis of real data for classification."
"We present semiparametric spectral modeling of the complete larval Drosophila
mushroom body connectome. Motivated by a thorough exploratory data analysis of
the network via Gaussian mixture modeling (GMM) in the adjacency spectral
embedding (ASE) representation space, we introduce the latent structure model
(LSM) for network modeling and inference. LSM is a generalization of the
stochastic block model (SBM) and a special case of the random dot product graph
(RDPG) latent position model, and is amenable to semiparametric GMM in the ASE
representation space. The resulting connectome code derived via semiparametric
GMM composed with ASE captures latent connectome structure and elucidates
biologically relevant neuronal properties."
"We consider the problem of estimation of a low-rank matrix from a limited
number of noisy rank-one projections. In particular, we propose two fast,
non-convex \emph{proper} algorithms for matrix recovery and support them with
rigorous theoretical analysis. We show that the proposed algorithms enjoy
linear convergence and that their sample complexity is independent of the
condition number of the unknown true low-rank matrix. By leveraging recent
advances in low-rank matrix approximation techniques, we show that our
algorithms achieve computational speed-ups over existing methods. Finally, we
complement our theory with some numerical experiments."
"Exponential family distributions are highly useful in machine learning since
their calculation can be performed efficiently through natural parameters. The
exponential family has recently been extended to the t-exponential family,
which contains Student-t distributions as family members and thus allows us to
handle noisy data well. However, since the t-exponential family is denied by
the deformed exponential, we cannot derive an efficient learning algorithm for
the t-exponential family such as expectation propagation (EP). In this paper,
we borrow the mathematical tools of q-algebra from statistical physics and show
that the pseudo additivity of distributions allows us to perform calculation of
t-exponential family distributions through natural parameters. We then develop
an expectation propagation (EP) algorithm for the t-exponential family, which
provides a deterministic approximation to the posterior or predictive
distribution with simple moment matching. We finally apply the proposed EP
algorithm to the Bayes point machine and Student-t process classication, and
demonstrate their performance numerically."
"Stochastic gradient Markov Chain Monte Carlo (SG-MCMC) has been developed as
a flexible family of scalable Bayesian sampling algorithms. However, there has
been little theoretical analysis of the impact of minibatch size to the
algorithm's convergence rate. In this paper, we prove that under a limited
computational budget/time, a larger minibatch size leads to a faster decrease
of the mean squared error bound (thus the fastest one corresponds to using full
gradients), which motivates the necessity of variance reduction in SG-MCMC.
Consequently, by borrowing ideas from stochastic optimization, we propose a
practical variance-reduction technique for SG-MCMC, that is efficient in both
computation and storage. We develop theory to prove that our algorithm induces
a faster convergence rate than standard SG-MCMC. A number of large-scale
experiments, ranging from Bayesian learning of logistic regression to deep
neural networks, validate the theory and demonstrate the superiority of the
proposed variance-reduction SG-MCMC framework."
"Automatic Chemical Design is a framework for generating novel molecules with
optimized properties. The original scheme, featuring Bayesian optimization over
the latent space of a variational autoencoder, suffers from the pathology that
it tends to produce invalid molecular structures. First, we demonstrate
empirically that this pathology arises when the Bayesian optimization scheme
queries latent points far away from the data on which the variational
autoencoder has been trained. Secondly, by reformulating the search procedure
as a constrained Bayesian optimization problem, we show that the effects of
this pathology can be mitigated, yielding marked improvements in the validity
of the generated molecules. We posit that constrained Bayesian optimization is
a good approach for solving this class of training set mismatch in many
generative tasks involving Bayesian optimization over the latent space of a
variational autoencoder."
"We provide a theoretical foundation for non-parametric estimation of
functions of random variables using kernel mean embeddings. We show that for
any continuous function $f$, consistent estimators of the mean embedding of a
random variable $X$ lead to consistent estimators of the mean embedding of
$f(X)$. For Mat\'ern kernels and sufficiently smooth functions we also provide
rates of convergence. Our results extend to functions of multiple random
variables. If the variables are dependent, we require an estimator of the mean
embedding of their joint distribution as a starting point; if they are
independent, it is sufficient to have separate estimators of the mean
embeddings of their marginal distributions. In either case, our results cover
both mean embeddings based on i.i.d. samples as well as ""reduced set""
expansions in terms of dependent expansion points. The latter serves as a
justification for using such expansions to limit memory resources when applying
the approach as a basis for probabilistic programming."
"Effective and accurate model selection is an important problem in modern data
analysis. One of the major challenges is the computational burden required to
handle large data sets that cannot be stored or processed on one machine.
Another challenge one may encounter is the presence of outliers and
contaminations that damage the inference quality. The parallel ""divide and
conquer"" model selection strategy divides the observations of the full data set
into roughly equal subsets and perform inference and model selection
independently on each subset. After local subset inference, this method
aggregates the posterior model probabilities or other model/variable selection
criteria to obtain a final model by using the notion of geometric median. This
approach leads to improved concentration in finding the ""correct"" model and
model parameters and also is provably robust to outliers and data
contamination."
"Gradient matching with Gaussian processes is a promising tool for learning
parameters of ordinary differential equations (ODE's). The essence of gradient
matching is to model the prior over state variables as a Gaussian process which
implies that the joint distribution given the ODE's and GP kernels is also
Gaussian distributed. The state-derivatives are integrated out analytically
since they are modelled as latent variables. However, the state variables
themselves are also latent variables because they are contaminated by noise.
Previous work sampled the state variables since integrating them out is
\textit{not} analytically tractable. In this paper we use mean-field
approximation to establish tight variational lower bounds that decouple state
variables and are therefore, in contrast to the integral over state variables,
analytically tractable and even concave for a restricted family of ODE's,
including nonlinear and periodic ODE's. Such variational lower bounds
facilitate ""hill climbing"" to determine the maximum a posteriori estimate of
ODE parameters. An additional advantage of our approach over sampling methods
is the determination of a proxy to the intractable posterior distribution over
state variables given observations and the ODE's."
"Canonical correlation analysis (CCA) is a multivariate statistical technique
for finding the linear relationship between two sets of variables. The kernel
generalization of CCA named kernel CCA has been proposed to find nonlinear
relations between datasets. Despite their wide usage, they have one common
limitation that is the lack of sparsity in their solution. In this paper, we
consider sparse kernel CCA and propose a novel sparse kernel CCA algorithm
(SKCCA). Our algorithm is based on a relationship between kernel CCA and least
squares. Sparsity of the dual transformations is introduced by penalizing the
$\ell_{1}$-norm of dual vectors. Experiments demonstrate that our algorithm not
only performs well in computing sparse dual transformations but also can
alleviate the over-fitting problem of kernel CCA."
"We consider the learning of multi-agent Hawkes processes, a model containing
multiple Hawkes processes with shared endogenous impact functions and different
exogenous intensities. In the framework of stochastic maximum likelihood
estimation, we explore the associated risk bound. Further, we consider the
superposition of Hawkes processes within the model, and demonstrate that under
certain conditions such an operation is beneficial for tightening the risk
bound. Accordingly, we propose a stochastic optimization algorithm assisted
with a diversity-driven superposition strategy, achieving better learning
results with improved convergence properties. The effectiveness of the proposed
method is verified on synthetic data, and its potential to solve the cold-start
problem of sequential recommendation systems is demonstrated on real-world
data."
"Measuring divergence between two distributions is essential in machine
learning and statistics and has various applications including binary
classification, change point detection, and two-sample test. Furthermore, in
the era of big data, designing divergence measure that is interpretable and can
handle high-dimensional and complex data becomes extremely important. In the
paper, we propose a post selection inference (PSI) framework for divergence
measure, which can select a set of statistically significant features that
discriminate two distributions. Specifically, we employ an additive variant of
maximum mean discrepancy (MMD) for features and introduce a general hypothesis
test for PSI. A novel MMD estimator using the incomplete U-statistics, which
has an asymptotically Normal distribution (under mild assumptions) and gives
high detection power in PSI, is also proposed and analyzed theoretically.
Through synthetic and real-world feature selection experiments, we show that
the proposed framework can successfully detect statistically significant
features. Last, we propose a sample selection framework for analyzing different
members in the Generative Adversarial Networks (GANs) family."
"Semi-supervised learning is a powerful technique for leveraging unlabeled
data to improve machine learning models, but it can be affected by the presence
of ``informative'' labels, which occur when some classes are more likely to be
labeled than others. In the missing data literature, such labels are called
missing not at random. In this paper, we propose a novel approach to address
this issue by estimating the missing-data mechanism and using inverse
propensity weighting to debias any SSL algorithm, including those using data
augmentation. We also propose a likelihood ratio test to assess whether or not
labels are indeed informative. Finally, we demonstrate the performance of the
proposed methods on different datasets, in particular on two medical datasets
for which we design pseudo-realistic missing data scenarios."
"This paper applies machine learning techniques to student modeling. It
presents a method for discovering high-level student behaviors from a very
large set of low-level traces corresponding to problem-solving actions in a
learning environment. Basic actions are encoded into sets of domain-dependent
attribute-value patterns called cases. Then a domain-independent hierarchical
clustering identifies what we call general attitudes, yielding automatic
diagnosis expressed in natural language, addressed in principle to teachers.
The method can be applied to individual students or to entire groups, like a
class. We exhibit examples of this system applied to thousands of students'
actions in the domain of algebraic transformations."
"There is growing body of learning problems for which it is natural to
organize the parameters into matrix, so as to appropriately regularize the
parameters under some matrix norm (in order to impose some more sophisticated
prior knowledge). This work describes and analyzes a systematic method for
constructing such matrix-based, regularization methods. In particular, we focus
on how the underlying statistical properties of a given problem can help us
decide which regularization function is appropriate.
  Our methodology is based on the known duality fact: that a function is
strongly convex with respect to some norm if and only if its conjugate function
is strongly smooth with respect to the dual norm. This result has already been
found to be a key component in deriving and analyzing several learning
algorithms. We demonstrate the potential of this framework by deriving novel
generalization and regret bounds for multi-task learning, multi-class learning,
and kernel learning."
"Objective: Modelling the associations from high-throughput experimental
molecular data has provided unprecedented insights into biological pathways and
signalling mechanisms. Graphical models and networks have especially proven to
be useful abstractions in this regard. Ad-hoc thresholds are often used in
conjunction with structure learning algorithms to determine significant
associations. The present study overcomes this limitation by proposing a
statistically-motivated approach for identifying significant associations in a
network.
  Methods and Materials: A new method that identifies significant associations
in graphical models by estimating the threshold minimising the $L_{\mathrm{1}}$
norm between the cumulative distribution function (CDF) of the observed edge
confidences and those of its asymptotic counterpart is proposed. The
effectiveness of the proposed method is demonstrated on popular synthetic data
sets as well as publicly available experimental molecular data corresponding to
gene and protein expression profiles.
  Results: The improved performance of the proposed approach is demonstrated
across the synthetic data sets using sensitivity, specificity and accuracy as
performance metrics. The results are also demonstrated across varying sample
sizes and three different structure learning algorithms with widely varying
assumptions. In all cases, the proposed approach has specificity and accuracy
close to 1, while sensitivity increases linearly in the logarithm of the sample
size. The estimated threshold systematically outperforms common ad-hoc ones in
terms of sensitivity while maintaining comparable levels of specificity and
accuracy. Networks from experimental data sets are reconstructed accurately
with respect to the results from the original papers."
"Structural reliability methods aim at computing the probability of failure of
systems with respect to some prescribed performance functions. In modern
engineering such functions usually resort to running an expensive-to-evaluate
computational model (e.g. a finite element model). In this respect simulation
methods, which may require $10^{3-6}$ runs cannot be used directly. Surrogate
models such as quadratic response surfaces, polynomial chaos expansions or
kriging (which are built from a limited number of runs of the original model)
are then introduced as a substitute of the original model to cope with the
computational cost. In practice it is almost impossible to quantify the error
made by this substitution though. In this paper we propose to use a kriging
surrogate of the performance function as a means to build a quasi-optimal
importance sampling density. The probability of failure is eventually obtained
as the product of an augmented probability computed by substituting the
meta-model for the original performance function and a correction term which
ensures that there is no bias in the estimation even if the meta-model is not
fully accurate. The approach is applied to analytical and finite element
reliability problems and proves efficient up to 100 random variables."
"Elucidating the genetic basis of human diseases is a central goal of genetics
and molecular biology. While traditional linkage analysis and modern
high-throughput techniques often provide long lists of tens or hundreds of
disease gene candidates, the identification of disease genes among the
candidates remains time-consuming and expensive. Efficient computational
methods are therefore needed to prioritize genes within the list of candidates,
by exploiting the wealth of information available about the genes in various
databases. Here we propose ProDiGe, a novel algorithm for Prioritization of
Disease Genes. ProDiGe implements a novel machine learning strategy based on
learning from positive and unlabeled examples, which allows to integrate
various sources of information about the genes, to share information about
known disease genes across diseases, and to perform genome-wide searches for
new disease genes. Experiments on real data show that ProDiGe outperforms
state-of-the-art methods for the prioritization of genes in human diseases."
"We propose a hybrid algorithmic strategy for complex stochastic optimization
problems, which combines the use of scenario trees from multistage stochastic
programming with machine learning techniques for learning a policy in the form
of a statistical model, in the context of constrained vector-valued decisions.
Such a policy allows one to run out-of-sample simulations over a large number
of independent scenarios, and obtain a signal on the quality of the
approximation scheme used to solve the multistage stochastic program. We
propose to apply this fast simulation technique to choose the best tree from a
set of scenario trees. A solution scheme is introduced, where several scenario
trees with random branching structure are solved in parallel, and where the
tree from which the best policy for the true problem could be learned is
ultimately retained. Numerical tests show that excellent trade-offs can be
achieved between run times and solution quality."
"Conditional independence testing is an important problem, especially in
Bayesian network learning and causal discovery. Due to the curse of
dimensionality, testing for conditional independence of continuous variables is
particularly challenging. We propose a Kernel-based Conditional Independence
test (KCI-test), by constructing an appropriate test statistic and deriving its
asymptotic distribution under the null hypothesis of conditional independence.
The proposed method is computationally efficient and easy to implement.
Experimental results show that it outperforms other methods, especially when
the conditioning set is large or the sample size is not very large, in which
case other methods encounter difficulties."
"The problem of topic modeling can be seen as a generalization of the
clustering problem, in that it posits that observations are generated due to
multiple latent factors (e.g., the words in each document are generated as a
mixture of several active topics, as opposed to just one). This increased
representational power comes at the cost of a more challenging unsupervised
learning problem of estimating the topic probability vectors (the distributions
over words for each topic), when only the words are observed and the
corresponding topics are hidden.
  We provide a simple and efficient learning procedure that is guaranteed to
recover the parameters for a wide class of mixture models, including the
popular latent Dirichlet allocation (LDA) model. For LDA, the procedure
correctly recovers both the topic probability vectors and the prior over the
topics, using only trigram statistics (i.e., third order moments, which may be
estimated with documents containing just three words). The method, termed
Excess Correlation Analysis (ECA), is based on a spectral decomposition of low
order moments (third and fourth order) via two singular value decompositions
(SVDs). Moreover, the algorithm is scalable since the SVD operations are
carried out on $k\times k$ matrices, where $k$ is the number of latent factors
(e.g. the number of topics), rather than in the $d$-dimensional observed space
(typically $d \gg k$)."
"Products of Hidden Markov Models(PoHMMs) are an interesting class of
generative models which have received little attention since their
introduction. This maybe in part due to their more computationally expensive
gradient-based learning algorithm,and the intractability of computing the log
likelihood of sequences under the model. In this paper, we demonstrate how the
partition function can be estimated reliably via Annealed Importance Sampling.
We perform experiments using contrastive divergence learning on rainfall data
and data captured from pairs of people dancing. Our results suggest that
advances in learning and evaluation for undirected graphical models and recent
increases in available computing power make PoHMMs worth considering for
complex time-series modeling tasks."
"Traditionally, multitask learning (MTL) assumes that all the tasks are
related. This can lead to negative transfer when tasks are indeed incoherent.
Recently, a number of approaches have been proposed that alleviate this problem
by discovering the underlying task clusters or relationships. However, they are
limited to modeling these relationships at the task level, which may be
restrictive in some applications. In this paper, we propose a novel MTL
formulation that captures task relationships at the feature-level. Depending on
the interactions among tasks and features, the proposed method construct
different task clusters for different features, without even the need of
pre-specifying the number of clusters. Computationally, the proposed
formulation is strongly convex, and can be efficiently solved by accelerated
proximal methods. Experiments are performed on a number of synthetic and
real-world data sets. Under various degrees of task relationships, the accuracy
of the proposed method is consistently among the best. Moreover, the
feature-specific task clusters obtained agree with the known/plausible task
structures of the data."
"We propose a novel interpretation of the collapsed variational Bayes
inference with a zero-order Taylor expansion approximation, called CVB0
inference, for latent Dirichlet allocation (LDA). We clarify the properties of
the CVB0 inference by using the alpha-divergence. We show that the CVB0
inference is composed of two different divergence projections: alpha=1 and -1.
This interpretation will help shed light on CVB0 works."
"This is an index to the papers that appear in the Proceedings of the 29th
International Conference on Machine Learning (ICML-12). The conference was held
in Edinburgh, Scotland, June 27th - July 3rd, 2012."
"Active learning is a powerful approach to analyzing data effectively. We show
that the feasibility of active learning depends crucially on the choice of
measure with respect to which the query is being optimized. The standard
information gain, for example, does not permit an accurate evaluation with a
small committee, a representative subset of the model space. We propose a
surrogate measure requiring only a small committee and discuss the properties
of this new measure. We devise, in addition, a bootstrap approach for committee
selection. The advantages of this approach are illustrated in the context of
recovering (regulatory) network models."
"The task of clustering a set of objects based on multiple sources of data
arises in several modern applications. We propose an integrative statistical
model that permits a separate clustering of the objects for each data source.
These separate clusterings adhere loosely to an overall consensus clustering,
and hence they are not independent. We describe a computationally scalable
Bayesian framework for simultaneous estimation of both the consensus clustering
and the source-specific clusterings. We demonstrate that this flexible approach
is more robust than joint clustering of all data sources, and is more powerful
than clustering each data source separately. This work is motivated by the
integrated analysis of heterogeneous biomedical data, and we present an
application to subtype identification of breast cancer tumor samples using
publicly available data from The Cancer Genome Atlas. Software is available at
http://people.duke.edu/~el113/software.html."
"During the past years there has been an explosion of interest in learning
methods based on sparsity regularization. In this paper, we discuss a general
class of such methods, in which the regularizer can be expressed as the
composition of a convex function $\omega$ with a linear function. This setting
includes several methods such the group Lasso, the Fused Lasso, multi-task
learning and many more. We present a general approach for solving
regularization problems of this kind, under the assumption that the proximity
operator of the function $\omega$ is available. Furthermore, we comment on the
application of this approach to support vector machines, a technique pioneered
by the groundbreaking work of Vladimir Vapnik."
"In multiple instance learning, objects are sets (bags) of feature vectors
(instances) rather than individual feature vectors. In this paper we address
the problem of how these bags can best be represented. Two standard approaches
are to use (dis)similarities between bags and prototype bags, or between bags
and prototype instances. The first approach results in a relatively
low-dimensional representation determined by the number of training bags, while
the second approach results in a relatively high-dimensional representation,
determined by the total number of instances in the training set. In this paper
a third, intermediate approach is proposed, which links the two approaches and
combines their strengths. Our classifier is inspired by a random subspace
ensemble, and considers subspaces of the dissimilarity space, defined by
subsets of instances, as prototypes. We provide guidelines for using such an
ensemble, and show state-of-the-art performances on a range of multiple
instance learning problems."
"From concentration inequalities for the suprema of Gaussian or Rademacher
processes an inequality is derived. It is applied to sharpen existing and to
derive novel bounds on the empirical Rademacher complexities of unit balls in
various norms appearing in the context of structured sparsity and multitask
dictionary learning or matrix factorization. A key role is played by the
largest eigenvalue of the data covariance matrix."
"In this paper we study speaker linking (a.k.a.\ partitioning) given
constraints of the distribution of speaker identities over speech recordings.
Specifically, we show that the intractable partitioning problem becomes
tractable when the constraints pre-partition the data in smaller cliques with
non-overlapping speakers. The surprisingly common case where speakers in
telephone conversations are known, but the assignment of channels to identities
is unspecified, is treated in a Bayesian way. We show that for the Dutch CGN
database, where this channel assignment task is at hand, a lightweight speaker
recognition system can quite effectively solve the channel assignment problem,
with 93% of the cliques solved. We further show that the posterior distribution
over channel assignment configurations is well calibrated."
"Principal component analysis (PCA) is a mainstay of modern data analysis - a
black box that is widely used but (sometimes) poorly understood. The goal of
this paper is to dispel the magic behind this black box. This manuscript
focuses on building a solid intuition for how and why principal component
analysis works. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind PCA. This tutorial does not shy away
from explaining the ideas informally, nor does it shy away from the
mathematics. The hope is that by addressing both aspects, readers of all levels
will be able to gain a better understanding of PCA as well as the when, the how
and the why of applying this technique."
"In binary classification and regression problems, it is well understood that
Lipschitz continuity and smoothness of the loss function play key roles in
governing generalization error bounds for empirical risk minimization
algorithms. In this paper, we show how these two properties affect
generalization error bounds in the learning to rank problem. The learning to
rank problem involves vector valued predictions and therefore the choice of the
norm with respect to which Lipschitz continuity and smoothness are defined
becomes crucial. Choosing the $\ell_\infty$ norm in our definition of Lipschitz
continuity allows us to improve existing bounds. Furthermore, under smoothness
assumptions, our choice enables us to prove rates that interpolate between
$1/\sqrt{n}$ and $1/n$ rates. Application of our results to ListNet, a popular
learning to rank method, gives state-of-the-art performance guarantees."
"We consider the problem of minimizing the regret in stochastic multi-armed
bandit, when the measure of goodness of an arm is not the mean return, but some
general function of the mean and the variance.We characterize the conditions
under which learning is possible and present examples for which no natural
algorithm can achieve sublinear regret."
"Batch Reinforcement Learning (RL) algorithms attempt to choose a policy from
a designer-provided class of policies given a fixed set of training data.
Choosing the policy which maximizes an estimate of return often leads to
over-fitting when only limited data is available, due to the size of the policy
class in relation to the amount of data available. In this work, we focus on
learning policy classes that are appropriately sized to the amount of data
available. We accomplish this by using the principle of Structural Risk
Minimization, from Statistical Learning Theory, which uses Rademacher
complexity to identify a policy class that maximizes a bound on the return of
the best policy in the chosen policy class, given the available data. Unlike
similar batch RL approaches, our bound on return requires only extremely weak
assumptions on the true system."
"The stability of statistical analysis is an important indicator for
reproducibility, which is one main principle of scientific method. It entails
that similar statistical conclusions can be reached based on independent
samples from the same underlying population. In this paper, we introduce a
general measure of classification instability (CIS) to quantify the sampling
variability of the prediction made by a classification method. Interestingly,
the asymptotic CIS of any weighted nearest neighbor classifier turns out to be
proportional to the Euclidean norm of its weight vector. Based on this concise
form, we propose a stabilized nearest neighbor (SNN) classifier, which
distinguishes itself from other nearest neighbor classifiers, by taking the
stability into consideration. In theory, we prove that SNN attains the minimax
optimal convergence rate in risk, and a sharp convergence rate in CIS. The
latter rate result is established for general plug-in classifiers under a
low-noise condition. Extensive simulated and real examples demonstrate that SNN
achieves a considerable improvement in CIS over existing nearest neighbor
classifiers, with comparable classification accuracy. We implement the
algorithm in a publicly available R package snn."
"We develop a learning principle and an efficient algorithm for batch learning
from logged bandit feedback. This learning setting is ubiquitous in online
systems (e.g., ad placement, web search, recommendation), where an algorithm
makes a prediction (e.g., ad ranking) for a given input (e.g., query) and
observes bandit feedback (e.g., user clicks on presented ads). We first address
the counterfactual nature of the learning problem through propensity scoring.
Next, we prove generalization error bounds that account for the variance of the
propensity-weighted empirical risk estimator. These constructive bounds give
rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM
can be used to derive a new learning method -- called Policy Optimizer for
Exponential Models (POEM) -- for learning stochastic linear rules for
structured output prediction. We present a decomposition of the POEM objective
that enables efficient stochastic gradient optimization. POEM is evaluated on
several multi-label classification problems showing substantially improved
robustness and generalization performance compared to the state-of-the-art."
"We develop a projected Nesterov's proximal-gradient (PNPG) approach for
sparse signal reconstruction that combines adaptive step size with Nesterov's
momentum acceleration. The objective function that we wish to minimize is the
sum of a convex differentiable data-fidelity (negative log-likelihood (NLL))
term and a convex regularization term. We apply sparse signal regularization
where the signal belongs to a closed convex set within the closure of the
domain of the NLL; the convex-set constraint facilitates flexible NLL domains
and accurate signal recovery. Signal sparsity is imposed using the
$\ell_1$-norm penalty on the signal's linear transform coefficients or gradient
map, respectively. The PNPG approach employs projected Nesterov's acceleration
step with restart and an inner iteration to compute the proximal mapping. We
propose an adaptive step-size selection scheme to obtain a good local
majorizing function of the NLL and reduce the time spent backtracking. Thanks
to step-size adaptation, PNPG does not require Lipschitz continuity of the
gradient of the NLL. We present an integrated derivation of the momentum
acceleration and its $\mathcal{O}(k^{-2})$ convergence-rate and iterate
convergence proofs, which account for adaptive step-size selection, inexactness
of the iterative proximal mapping, and the convex-set constraint. The tuning of
PNPG is largely application-independent. Tomographic and compressed-sensing
reconstruction experiments with Poisson generalized linear and Gaussian linear
measurement models demonstrate the performance of the proposed approach."
"Mean field variational Bayes (MFVB) is a popular posterior approximation
method due to its fast runtime on large-scale data sets. However, it is well
known that a major failing of MFVB is that it underestimates the uncertainty of
model variables (sometimes severely) and provides no information about model
variable covariance. We develop a fast, general methodology for exponential
families that augments MFVB to deliver accurate uncertainty estimates for model
variables -- both for individual variables and coherently across variables.
MFVB for exponential families defines a fixed-point equation in the means of
the approximating posterior, and our approach yields a covariance estimate by
perturbing this fixed point. Inspired by linear response theory, we call our
method linear response variational Bayes (LRVB). We also show how LRVB can be
used to quickly calculate a measure of the influence of individual data points
on parameter point estimates. We demonstrate the accuracy and scalability of
our method by learning Gaussian mixture models for both simulated and real
data."
"We show that accelerated gradient descent, averaged gradient descent and the
heavy-ball method for non-strongly-convex problems may be reformulated as
constant parameter second-order difference equation algorithms, where stability
of the system is equivalent to convergence at rate O(1/n 2), where n is the
number of iterations. We provide a detailed analysis of the eigenvalues of the
corresponding linear dynamical system , showing various oscillatory and
non-oscillatory behaviors, together with a sharp stability result with explicit
constants. We also consider the situation where noisy gradients are available,
where we extend our general convergence result, which suggests an alternative
algorithm (i.e., with different step sizes) that exhibits the good aspects of
both averaging and acceleration."
"This paper describes an approach for automatic construction of dictionaries
for Named Entity Recognition (NER) using large amounts of unlabeled data and a
few seed examples. We use Canonical Correlation Analysis (CCA) to obtain lower
dimensional embeddings (representations) for candidate phrases and classify
these phrases using a small number of labeled examples. Our method achieves
16.5% and 11.3% F-1 score improvement over co-training on disease and virus NER
respectively. We also show that by adding candidate phrase embeddings as
features in a sequence tagger gives better performance compared to using word
embeddings."
"Tensor methods have emerged as a powerful paradigm for consistent learning of
many latent variable models such as topic models, independent component
analysis and dictionary learning. Model parameters are estimated via CP
decomposition of the observed higher order input moments. However, in many
domains, additional invariances such as shift invariances exist, enforced via
models such as convolutional dictionary learning. In this paper, we develop
novel tensor decomposition algorithms for parameter estimation of convolutional
models. Our algorithm is based on the popular alternating least squares method,
but with efficient projections onto the space of stacked circulant matrices.
Our method is embarrassingly parallel and consists of simple operations such as
fast Fourier transforms and matrix multiplications. Our algorithm converges to
the dictionary much faster and more accurately compared to the alternating
minimization over filters and activation maps."
"We present a framework for clustering with cluster-specific feature
selection. The framework, CRAFT, is derived from asymptotic log posterior
formulations of nonparametric MAP-based clustering models. CRAFT handles
assorted data, i.e., both numeric and categorical data, and the underlying
objective functions are intuitively appealing. The resulting algorithm is
simple to implement and scales nicely, requires minimal parameter tuning,
obviates the need to specify the number of clusters a priori, and compares
favorably with other methods on real datasets."
"Perceptron is a classic online algorithm for learning a classification
function. In this paper, we provide a novel extension of the perceptron
algorithm to the learning to rank problem in information retrieval. We consider
popular listwise performance measures such as Normalized Discounted Cumulative
Gain (NDCG) and Average Precision (AP). A modern perspective on perceptron for
classification is that it is simply an instance of online gradient descent
(OGD), during mistake rounds, using the hinge loss function. Motivated by this
interpretation, we propose a novel family of listwise, large margin ranking
surrogates. Members of this family can be thought of as analogs of the hinge
loss. Exploiting a certain self-bounding property of the proposed family, we
provide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our
perceptron-like algorithm. We show that, if there exists a perfect oracle
ranker which can correctly rank each instance in an online sequence of ranking
data, with some margin, the cumulative loss of perceptron algorithm on that
sequence is bounded by a constant, irrespective of the length of the sequence.
This result is reminiscent of Novikoff's convergence theorem for the
classification perceptron. Moreover, we prove a lower bound on the cumulative
loss achievable by any deterministic algorithm, under the assumption of
existence of perfect oracle ranker. The lower bound shows that our perceptron
bound is not tight, and we propose another, \emph{purely online}, algorithm
which achieves the lower bound. We provide empirical results on simulated and
large commercial datasets to corroborate our theoretical results."
"Mechanisms of human color vision are characterized by two phenomenological
aspects: the system is nonlinear and adaptive to changing environments.
Conventional attempts to derive these features from statistics use separate
arguments for each aspect. The few statistical approaches that do consider both
phenomena simultaneously follow parametric formulations based on empirical
models. Therefore, it may be argued that the behavior does not come directly
from the color statistics but from the convenient functional form adopted. In
addition, many times the whole statistical analysis is based on simplified
databases that disregard relevant physical effects in the input signal, as for
instance by assuming flat Lambertian surfaces. Here we address the simultaneous
statistical explanation of (i) the nonlinear behavior of achromatic and
chromatic mechanisms in a fixed adaptation state, and (ii) the change of such
behavior. Both phenomena emerge directly from the samples through a single
data-driven method: the Sequential Principal Curves Analysis (SPCA) with local
metric. SPCA is a new manifold learning technique to derive a set of sensors
adapted to the manifold using different optimality criteria. A new database of
colorimetrically calibrated images of natural objects under these illuminants
was collected. The results obtained by applying SPCA show that the
psychophysical behavior on color discrimination thresholds, discount of the
illuminant and corresponding pairs in asymmetric color matching, emerge
directly from realistic data regularities assuming no a priori functional form.
These results provide stronger evidence for the hypothesis of a statistically
driven organization of color sensors. Moreover, the obtained results suggest
that color perception at this low abstraction level may be guided by an error
minimization strategy rather than by the information maximization principle."
"Neural signals are characterized by rich temporal and spatiotemporal dynamics
that reflect the organization of cortical networks. Theoretical research has
shown how neural networks can operate at different dynamic ranges that
correspond to specific types of information processing. Here we present a data
analysis framework that uses a linearized model of these dynamic states in
order to decompose the measured neural signal into a series of components that
capture both rhythmic and non-rhythmic neural activity. The method is based on
stochastic differential equations and Gaussian process regression. Through
computer simulations and analysis of magnetoencephalographic data, we
demonstrate the efficacy of the method in identifying meaningful modulations of
oscillatory signals corrupted by structured temporal and spatiotemporal noise.
These results suggest that the method is particularly suitable for the analysis
and interpretation of complex temporal and spatiotemporal neural signals."
"The Dantzig selector has received popularity for many applications such as
compressed sensing and sparse modeling, thanks to its computational efficiency
as a linear programming problem and its nice sampling properties. Existing
results show that it can recover sparse signals mimicking the accuracy of the
ideal procedure, up to a logarithmic factor of the dimensionality. Such a
factor has been shown to hold for many regularization methods. An important
question is whether this factor can be reduced to a logarithmic factor of the
sample size in ultra-high dimensions under mild regularity conditions. To
provide an affirmative answer, in this paper we suggest the constrained Dantzig
selector, which has more flexible constraints and parameter space. We prove
that the suggested method can achieve convergence rates within a logarithmic
factor of the sample size of the oracle rates and improved sparsity, under a
fairly weak assumption on the signal strength. Such improvement is significant
in ultra-high dimensions. This method can be implemented efficiently through
sequential linear programming. Numerical studies confirm that the sample size
needed for a certain level of accuracy in these problems can be much reduced."
"Large-scale precision matrix estimation is of fundamental importance yet
challenging in many contemporary applications for recovering Gaussian graphical
models. In this paper, we suggest a new approach of innovated scalable
efficient estimation (ISEE) for estimating large precision matrix. Motivated by
the innovated transformation, we convert the original problem into that of
large covariance matrix estimation. The suggested method combines the strengths
of recent advances in high-dimensional sparse modeling and large covariance
matrix estimation. Compared to existing approaches, our method is scalable and
can deal with much larger precision matrices with simple tuning. Under mild
regularity conditions, we establish that this procedure can recover the
underlying graphical structure with significant probability and provide
efficient estimation of link strengths. Both computational and theoretical
advantages of the procedure are evidenced through simulation and real data
examples."
"Bayesian network structure learning is often performed in a Bayesian setting,
by evaluating candidate structures using their posterior probabilities for a
given data set. Score-based algorithms then use those posterior probabilities
as an objective function and return the maximum a posteriori network as the
learned model. For discrete Bayesian networks, the canonical choice for a
posterior score is the Bayesian Dirichlet equivalent uniform (BDeu) marginal
likelihood with a uniform (U) graph prior (Heckerman et al., 1995). Its
favourable theoretical properties descend from assuming a uniform prior both on
the space of the network structures and on the space of the parameters of the
network. In this paper, we revisit the limitations of these assumptions; and we
introduce an alternative set of assumptions and the resulting score: the
Bayesian Dirichlet sparse (BDs) empirical Bayes marginal likelihood with a
marginal uniform (MU) graph prior. We evaluate its performance in an extensive
simulation study, showing that MU+BDs is more accurate than U+BDeu both in
learning the structure of the network and in predicting new observations, while
not being computationally more complex to estimate."
"Markov chain Monte Carlo (MCMC) algorithms have become powerful tools for
Bayesian inference. However, they do not scale well to large-data problems.
Divide-and-conquer strategies, which split the data into batches and, for each
batch, run independent MCMC algorithms targeting the corresponding
subposterior, can spread the computational burden across a number of separate
workers. The challenge with such strategies is in recombining the subposteriors
to approximate the full posterior. By creating a Gaussian-process approximation
for each log-subposterior density we create a tractable approximation for the
full posterior. This approximation is exploited through three methodologies:
firstly a Hamiltonian Monte Carlo algorithm targeting the expectation of the
posterior density provides a sample from an approximation to the posterior;
secondly, evaluating the true posterior at the sampled points leads to an
importance sampler that, asymptotically, targets the true posterior
expectations; finally, an alternative importance sampler uses the full
Gaussian-process distribution of the approximation to the log-posterior density
to re-weight any initial sample and provide both an estimate of the posterior
expectation and a measure of the uncertainty in it."
"A Hilbert space embedding of a distribution---in short, a kernel mean
embedding---has recently emerged as a powerful tool for machine learning and
inference. The basic idea behind this framework is to map distributions into a
reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel
methods can be extended to probability measures. It can be viewed as a
generalization of the original ""feature map"" common to support vector machines
(SVMs) and other kernel methods. While initially closely associated with the
latter, it has meanwhile found application in fields ranging from kernel
machines and probabilistic modeling to statistical inference, causal discovery,
and deep learning. The goal of this survey is to give a comprehensive review of
existing work and recent advances in this research area, and to discuss the
most challenging issues and open problems that could lead to new research
directions. The survey begins with a brief introduction to the RKHS and
positive definite kernels which forms the backbone of this survey, followed by
a thorough discussion of the Hilbert space embedding of marginal distributions,
theoretical guarantees, and a review of its applications. The embedding of
distributions enables us to apply RKHS methods to probability measures which
prompts a wide range of applications such as kernel two-sample testing,
independent testing, and learning on distributional data. Next, we discuss the
Hilbert space embedding for conditional distributions, give theoretical
insights, and review some applications. The conditional mean embedding enables
us to perform sum, product, and Bayes' rules---which are ubiquitous in
graphical model, probabilistic inference, and reinforcement learning---in a
non-parametric way. We then discuss relationships between this framework and
other related areas. Lastly, we give some suggestions on future research
directions."
"A big challenge in algorithmic composition is to devise a model that is both
easily trainable and able to reproduce the long-range temporal dependencies
typical of music. Here we investigate how artificial neural networks can be
trained on a large corpus of melodies and turned into automated music composers
able to generate new melodies coherent with the style they have been trained
on. We employ gated recurrent unit networks that have been shown to be
particularly efficient in learning complex sequential activations with
arbitrary long time lags. Our model processes rhythm and melody in parallel
while modeling the relation between these two features. Using such an approach,
we were able to generate interesting complete melodies or suggest possible
continuations of a melody fragment that is coherent with the characteristics of
the fragment itself."
"Deep learning models' architectures, including depth and width, are key
factors influencing models' performance, such as test accuracy and computation
time. This paper solves two problems: given computation time budget, choose an
architecture to maximize accuracy, and given accuracy requirement, choose an
architecture to minimize computation time. We convert this architecture
optimization into a subset selection problem. With accuracy's submodularity and
computation time's supermodularity, we propose efficient greedy optimization
algorithms. The experiments demonstrate our algorithm's ability to find more
accurate models or faster models. By analyzing architecture evolution with
growing time budget, we discuss relationships among accuracy, time and
architecture, and give suggestions on neural network architecture design."
"We consider Blackwell approachability, a very powerful and geometric tool in
game theory, used for example to design strategies of the uninformed player in
repeated games with incomplete information. We extend this theory to
""generalized quitting games"" , a class of repeated stochastic games in which
each player may have quitting actions, such as the Big-Match. We provide three
simple geometric and strongly related conditions for the weak approachability
of a convex target set. The first is sufficient: it guarantees that, for any
fixed horizon, a player has a strategy ensuring that the expected time-average
payoff vector converges to the target set as horizon goes to infinity. The
third is necessary: if it is not satisfied, the opponent can weakly exclude the
target set. In the special case where only the approaching player can quit the
game (Big-Match of type I), the three conditions are equivalent and coincide
with Blackwell's condition. Consequently, we obtain a full characterization and
prove that the game is weakly determined-every convex set is either weakly
approachable or weakly excludable. In games where only the opponent can quit
(Big-Match of type II), none of our conditions is both sufficient and necessary
for weak approachability. We provide a continuous time sufficient condition
using techniques coming from differential games, and show its usefulness in
practice, in the spirit of Vieille's seminal work for weak
approachability.Finally, we study uniform approachability where the strategy
should not depend on the horizon and demonstrate that, in contrast with
classical Blackwell approacha-bility for convex sets, weak approachability does
not imply uniform approachability."
"Safety is a desirable property that can immensely increase the applicability
of learning algorithms in real-world decision-making problems. It is much
easier for a company to deploy an algorithm that is safe, i.e., guaranteed to
perform at least as well as a baseline. In this paper, we study the issue of
safety in contextual linear bandits that have application in many different
fields including personalized ad recommendation in online marketing. We
formulate a notion of safety for this class of algorithms. We develop a safe
contextual linear bandit algorithm, called conservative linear UCB (CLUCB),
that simultaneously minimizes its regret and satisfies the safety constraint,
i.e., maintains its performance above a fixed percentage of the performance of
a baseline strategy, uniformly over time. We prove an upper-bound on the regret
of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound
for the regret of the standard linear UCB algorithm that grows with the time
horizon and 2) a constant (does not grow with the time horizon) term that
accounts for the loss of being conservative in order to satisfy the safety
constraint. We empirically show that our algorithm is safe and validate our
theoretical analysis."
"Despite recent advances, the remaining bottlenecks in deep generative models
are necessity of extensive training and difficulties with generalization from
small number of training examples. We develop a new generative model called
Generative Matching Network which is inspired by the recently proposed matching
networks for one-shot learning in discriminative tasks. By conditioning on the
additional input dataset, our model can instantly learn new concepts that were
not available in the training data but conform to a similar generative process.
The proposed framework does not explicitly restrict diversity of the
conditioning data and also does not require an extensive inference procedure
for training or adaptation. Our experiments on the Omniglot dataset demonstrate
that Generative Matching Networks significantly improve predictive performance
on the fly as more additional data is available and outperform existing state
of the art conditional generative models."
"Monte Carlo algorithms simulate some prescribed number of samples, taking
some random real time to complete the computations necessary. This work
considers the converse: to impose a real-time budget on the computation, which
results in the number of samples simulated being random. To complicate matters,
the real time taken for each simulation may depend on the sample produced, so
that the samples themselves are not independent of their number, and a length
bias with respect to compute time is apparent. This is especially problematic
when a Markov chain Monte Carlo (MCMC) algorithm is used and the final state of
the Markov chain -- rather than an average over all states -- is required,
which is the case in parallel tempering implementations of MCMC. The length
bias does not diminish with the compute budget in this case. It also occurs in
sequential Monte Carlo (SMC) algorithms, which is the focus of this paper. We
propose an anytime framework to address the concern, using a continuous-time
Markov jump process to study the progress of the computation in real time. We
first show that for any MCMC algorithm, the length bias of the final state's
distribution due to the imposed real-time computing budget can be eliminated by
using a multiple chain construction. The utility of this construction is then
demonstrated on a large-scale SMC^2 implementation, using four billion
particles distributed across a cluster of 128 graphics processing units on the
Amazon EC2 service. The anytime framework imposes a real-time budget on the
MCMC move steps within the SMC$^{2}$ algorithm, ensuring that all processors
are simultaneously ready for the resampling step, demonstrably reducing
idleness to due waiting times and providing substantial control over the total
compute budget."
"As enjoying the closed form solution, least squares support vector machine
(LSSVM) has been widely used for classification and regression problems having
the comparable performance with other types of SVMs. However, LSSVM has two
drawbacks: sensitive to outliers and lacking sparseness. Robust LSSVM (R-LSSVM)
overcomes the first partly via nonconvex truncated loss function, but the
current algorithms for R-LSSVM with the dense solution are faced with the
second drawback and are inefficient for training large-scale problems. In this
paper, we interpret the robustness of R-LSSVM from a re-weighted viewpoint and
give a primal R-LSSVM by the representer theorem. The new model may have sparse
solution if the corresponding kernel matrix has low rank. Then approximating
the kernel matrix by a low-rank matrix and smoothing the loss function by
entropy penalty function, we propose a convergent sparse R-LSSVM (SR-LSSVM)
algorithm to achieve the sparse solution of primal R-LSSVM, which overcomes two
drawbacks of LSSVM simultaneously. The proposed algorithm has lower complexity
than the existing algorithms and is very efficient for training large-scale
problems. Many experimental results illustrate that SR-LSSVM can achieve better
or comparable performance with less training time than related algorithms,
especially for training large scale problems."
"Multi-label classification is a type of supervised learning where an instance
may belong to multiple labels simultaneously. Predicting each label
independently has been criticized for not exploiting any correlation between
labels. In this paper we propose a novel approach, Nearest Labelset using
Double Distances (NLDD), that predicts the labelset observed in the training
data that minimizes a weighted sum of the distances in both the feature space
and the label space to the new instance. The weights specify the relative
tradeoff between the two distances. The weights are estimated from a binomial
regression of the number of misclassified labels as a function of the two
distances. Model parameters are estimated by maximum likelihood. NLDD only
considers labelsets observed in the training data, thus implicitly taking into
account label dependencies. Experiments on benchmark multi-label data sets show
that the proposed method on average outperforms other well-known approaches in
terms of Hamming loss, 0/1 loss, and multi-label accuracy and ranks second
after ECC on the F-measure."
"Visualizing high-dimensional data has been a focus in data analysis
communities for decades, which has led to the design of many algorithms, some
of which are now considered references (such as t-SNE for example). In our era
of overwhelming data volumes, the scalability of such methods have become more
and more important. In this work, we present a method which allows to apply any
visualization or embedding algorithm on very large datasets by considering only
a fraction of the data as input and then extending the information to all data
points using a graph encoding its global similarity. We show that in most
cases, using only $\mathcal{O}(\log(N))$ samples is sufficient to diffuse the
information to all $N$ data points. In addition, we propose quantitative
methods to measure the quality of embeddings and demonstrate the validity of
our technique on both synthetic and real-world datasets."
"This paper provides estimation and inference methods for the best linear
predictor (approximation) of a structural function, such as conditional average
structural and treatment effects, and structural derivatives, based on modern
machine learning (ML) tools. We represent this structural function as a
conditional expectation of an unbiased signal that depends on a nuisance
parameter, which we estimate by modern machine learning techniques. We first
adjust the signal to make it insensitive (Neyman-orthogonal) with respect to
the first-stage regularization bias. We then project the signal onto a set of
basis functions, growing with sample size, which gives us the best linear
predictor of the structural function. We derive a complete set of results for
estimation and simultaneous inference on all parameters of the best linear
predictor, conducting inference by Gaussian bootstrap. When the structural
function is smooth and the basis is sufficiently rich, our estimation and
inference result automatically targets this function. When basis functions are
group indicators, the best linear predictor reduces to group average
treatment/structural effect, and our inference automatically targets these
parameters. We demonstrate our method by estimating uniform confidence bands
for the average price elasticity of gasoline demand conditional on income."
"In this paper we propose a synergistic melting of neural networks and
decision trees (DT) we call neural decision trees (NDT). NDT is an architecture
a la decision tree where each splitting node is an independent multilayer
perceptron allowing oblique decision functions or arbritrary nonlinear decision
function if more than one layer is used. This way, each MLP can be seen as a
node of the tree. We then show that with the weight sharing asumption among
those units, we end up with a Hashing Neural Network (HNN) which is a
multilayer perceptron with sigmoid activation function for the last layer as
opposed to the standard softmax. The output units then jointly represent the
probability to be in a particular region. The proposed framework allows for
global optimization as opposed to greedy in DT and differentiability w.r.t. all
parameters and the input, allowing easy integration in any learnable pipeline,
for example after CNNs for computer vision tasks. We also demonstrate the
modeling power of HNN allowing to learn union of disjoint regions for final
clustering or classification making it more general and powerful than standard
softmax MLP requiring linear separability thus reducing the need on the inner
layer to perform complex data transformations. We finally show experiments for
supervised, semi-suppervised and unsupervised tasks and compare results with
standard DTs and MLPs."
"McKernel introduces a framework to use kernel approximates in the mini-batch
setting with Stochastic Gradient Descent (SGD) as an alternative to Deep
Learning. Based on Random Kitchen Sinks [Rahimi and Recht 2007], we provide a
C++ library for Large-scale Machine Learning. It contains a CPU optimized
implementation of the algorithm in [Le et al. 2013], that allows the
computation of approximated kernel expansions in log-linear time. The algorithm
requires to compute the product of matrices Walsh Hadamard. A cache friendly
Fast Walsh Hadamard that achieves compelling speed and outperforms current
state-of-the-art methods has been developed. McKernel establishes the
foundation of a new architecture of learning that allows to obtain large-scale
non-linear classification combining lightning kernel expansions and a linear
classifier. It travails in the mini-batch setting working analogously to Neural
Networks. We show the validity of our method through extensive experiments on
MNIST and FASHION MNIST [Xiao et al. 2017]."
"Crowdsourcing platforms emerged as popular venues for purchasing human
intelligence at low cost for large volume of tasks. As many low-paid workers
are prone to give noisy answers, a common practice is to add redundancy by
assigning multiple workers to each task and then simply average out these
answers. However, to fully harness the wisdom of the crowd, one needs to learn
the heterogeneous quality of each worker. We resolve this fundamental challenge
in crowdsourced regression tasks, i.e., the answer takes continuous labels,
where identifying good or bad workers becomes much more non-trivial compared to
a classification setting of discrete labels. In particular, we introduce a
Bayesian iterative scheme and show that it provably achieves the optimal mean
squared error. Our evaluations on synthetic and real-world datasets support our
theoretical results and show the superiority of the proposed scheme."
"Informative Bayesian priors are often difficult to elicit, and when this is
the case, modelers usually turn to noninformative or objective priors. However,
objective priors such as the Jeffreys and reference priors are not tractable to
derive for many models of interest. We address this issue by proposing
techniques for learning reference prior approximations: we select a parametric
family and optimize a black-box lower bound on the reference prior objective to
find the member of the family that serves as a good approximation. We
experimentally demonstrate the method's effectiveness by recovering Jeffreys
priors and learning the Variational Autoencoder's reference prior."
"The Nonlinear autoregressive exogenous (NARX) model, which predicts the
current value of a time series based upon its previous values as well as the
current and past values of multiple driving (exogenous) series, has been
studied for decades. Despite the fact that various NARX models have been
developed, few of them can capture the long-term temporal dependencies
appropriately and select the relevant driving series to make predictions. In
this paper, we propose a dual-stage attention-based recurrent neural network
(DA-RNN) to address these two issues. In the first stage, we introduce an input
attention mechanism to adaptively extract relevant driving series (a.k.a.,
input features) at each time step by referring to the previous encoder hidden
state. In the second stage, we use a temporal attention mechanism to select
relevant encoder hidden states across all time steps. With this dual-stage
attention scheme, our model can not only make predictions effectively, but can
also be easily interpreted. Thorough empirical studies based upon the SML 2010
dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can
outperform state-of-the-art methods for time series prediction."
"This paper aims to decrease the time complexity of multi-output relevance
vector regression from O(VM^3) to O(V^3+M^3), where V is the number of output
dimensions, M is the number of basis functions, and V<M. The experimental
results demonstrate that the proposed method is more competitive than the
existing method, with regard to computation time. MATLAB codes are available at
http://www.mathworks.com/matlabcentral/fileexchange/49131."
"Emotion being a subjective thing, leveraging knowledge and science behind
labeled data and extracting the components that constitute it, has been a
challenging problem in the industry for many years. With the evolution of deep
learning in computer vision, emotion recognition has become a widely-tackled
research problem. In this work, we propose two independent methods for this
very task. The first method uses autoencoders to construct a unique
representation of each emotion, while the second method is an 8-layer
convolutional neural network (CNN). These methods were trained on the
posed-emotion dataset (JAFFE), and to test their robustness, both the models
were also tested on 100 random images from the Labeled Faces in the Wild (LFW)
dataset, which consists of images that are candid than posed. The results show
that with more fine-tuning and depth, our CNN model can outperform the
state-of-the-art methods for emotion recognition. We also propose some exciting
ideas for expanding the concept of representational autoencoders to improve
their performance."
"Learning the structure of dependencies among multiple random variables is a
problem of considerable theoretical and practical interest. Within the context
of Bayesian Networks, a practical and surprisingly successful solution to this
learning problem is achieved by adopting score-functions optimisation schema,
augmented with multiple restarts to avoid local optima. Yet, the conditions
under which such strategies work well are poorly understood, and there are also
some intrinsic limitations to learning the directionality of the interaction
among the variables. Following an early intuition of Friedman and Koller, we
propose to decouple the learning problem into two steps: first, we identify a
partial ordering among input variables which constrains the structural learning
problem, and then propose an effective bootstrap-based algorithm to simulate
augmented data sets, and select the most important dependencies among the
variables. By using several synthetic data sets, we show that our algorithm
yields better recovery performance than the state of the art, increasing the
chances of identifying a globally-optimal solution to the learning problem, and
solving also well-known identifiability issues that affect the standard
approach. We use our new algorithm to infer statistical dependencies between
cancer driver somatic mutations detected by high-throughput genome sequencing
data of multiple colorectal cancer patients. In this way, we also show how the
proposed methods can shade new insights about cancer initiation, and
progression. Code: https://github.com/caravagn/Bootstrap-based-Learning"
"Hyperparameter tuning is one of the most time-consuming workloads in deep
learning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam,
reduce this labor by adaptively tuning an individual learning rate for each
variable. Recently researchers have shown renewed interest in simpler methods
like momentum SGD as they may yield better test metrics. Motivated by this
trend, we ask: can simple adaptive methods based on SGD perform as well or
better? We revisit the momentum SGD algorithm and show that hand-tuning a
single learning rate and momentum makes it competitive with Adam. We then
analyze its robustness to learning rate misspecification and objective
curvature variation. Based on these insights, we design YellowFin, an automatic
tuner for momentum and learning rate in SGD. YellowFin optionally uses a
negative-feedback loop to compensate for the momentum dynamics in asynchronous
settings on the fly. We empirically show that YellowFin can converge in fewer
iterations than Adam on ResNets and LSTMs for image recognition, language
modeling and constituency parsing, with a speedup of up to 3.28x in synchronous
and up to 2.69x in asynchronous settings."
"Verification determines whether two samples belong to the same class or not,
and has important applications such as face and fingerprint verification, where
thousands or millions of categories are present but each category has scarce
labeled examples, presenting two major challenges for existing deep learning
models. We propose a deep semi-supervised model named SEmi-supervised
VErification Network (SEVEN) to address these challenges. The model consists of
two complementary components. The generative component addresses the lack of
supervision within each category by learning general salient structures from a
large amount of data across categories. The discriminative component exploits
the learned general features to mitigate the lack of supervision within
categories, and also directs the generative component to find more informative
structures of the whole data manifold. The two components are tied together in
SEVEN to allow an end-to-end training of the two components. Extensive
experiments on four verification tasks demonstrate that SEVEN significantly
outperforms other state-of-the-art deep semi-supervised techniques when labeled
data are in short supply. Furthermore, SEVEN is competitive with fully
supervised baselines trained with a larger amount of labeled data. It indicates
the importance of the generative component in SEVEN."
"To investigate whether training load monitoring data could be used to predict
injuries in elite Australian football players, data were collected from elite
athletes over 3 seasons at an Australian football club. Loads were quantified
using GPS devices, accelerometers and player perceived exertion ratings.
Absolute and relative training load metrics were calculated for each player
each day (rolling average, exponentially weighted moving average, acute:chronic
workload ratio, monotony and strain). Injury prediction models (regularised
logistic regression, generalised estimating equations, random forests and
support vector machines) were built for non-contact, non-contact time-loss and
hamstring specific injuries using the first two seasons of data. Injury
predictions were generated for the third season and evaluated using the area
under the receiver operator characteristic (AUC). Predictive performance was
only marginally better than chance for models of non-contact and non-contact
time-loss injuries (AUC$<$0.65). The best performing model was a multivariate
logistic regression for hamstring injuries (best AUC=0.76). Learning curves
suggested logistic regression was underfitting the load-injury relationship and
that using a more complex model or increasing the amount of model building data
may lead to future improvements. Injury prediction models built using training
load data from a single club showed poor ability to predict injuries when
tested on previously unseen data, suggesting they are limited as a daily
decision tool for practitioners. Focusing the modelling approach on specific
injury types and increasing the amount of training data may lead to the
development of improved predictive models for injury prevention."
"A statistical algorithm for categorizing different types of matches and fraud
in image databases is presented. The approach is based on a generative model of
a graph representing images and connections between pairs of identities,
trained using properties of a matching algorithm between images."
"Graph representations offer powerful and intuitive ways to describe data in a
multitude of application domains. Here, we consider stochastic processes
generating graphs and propose a methodology for detecting changes in
stationarity of such processes. The methodology is general and considers a
process generating attributed graphs with a variable number of vertices/edges,
without the need to assume one-to-one correspondence between vertices at
different time steps. The methodology acts by embedding every graph of the
stream into a vector domain, where a conventional multivariate change detection
procedure can be easily applied. We ground the soundness of our proposal by
proving several theoretical results. In addition, we provide a specific
implementation of the methodology and evaluate its effectiveness on several
detection problems involving attributed graphs representing biological
molecules and drawings. Experimental results are contrasted with respect to
suitable baseline methods, demonstrating the effectiveness of our approach."
"Dirichlet processes (DP) are widely applied in Bayesian nonparametric
modeling. However, in their basic form they do not directly integrate
dependency information among data arising from space and time. In this paper,
we propose location dependent Dirichlet processes (LDDP) which incorporate
nonparametric Gaussian processes in the DP modeling framework to model such
dependencies. We develop the LDDP in the context of mixture modeling, and
develop a mean field variational inference algorithm for this mixture model.
The effectiveness of the proposed modeling framework is shown on an image
segmentation task."
"Leasing is a popular channel to market new cars. Pricing a leasing contract
is complicated because the leasing rate embodies an expectation of the residual
value of the car after contract expiration. To aid lessors in their pricing
decisions, the paper develops resale price forecasting models. A peculiarity of
the leasing business is that forecast errors entail different costs.
Identifying effective ways to address this characteristic is the main objective
of the paper. More specifically, the paper contributes to the literature
through i) consolidating and integrating previous work in forecasting with
asymmetric cost of error functions, ii) systematically evaluating previous
approaches and comparing them to a new approach, and iii) demonstrating that
forecasting with asymmetric cost of error functions enhances the quality of
decision support in car leasing. For example, under the assumption that the
costs of overestimating resale prices is twice that of the opposite error,
incorporating corresponding cost asymmetry into forecast model development
reduces decision costs by about eight percent, compared to a standard
forecasting model. Higher asymmetry produces even larger improvements."
"In implicit models, one often interpolates between sampled points in latent
space. As we show in this paper, care needs to be taken to match-up the
distributional assumptions on code vectors with the geometry of the
interpolating paths. Otherwise, typical assumptions about the quality and
semantics of in-between points may not be justified. Based on our analysis we
propose to modify the prior code distribution to put significantly more
probability mass closer to the origin. As a result, linear interpolation paths
are not only shortest paths, but they are also guaranteed to pass through
high-density regions, irrespective of the dimensionality of the latent space.
Experiments on standard benchmark image datasets demonstrate clear visual
improvements in the quality of the generated samples and exhibit more
meaningful interpolation paths."
"Predicting epidemic dynamics is of great value in understanding and
controlling diffusion processes, such as infectious disease spread and
information propagation. This task is intractable, especially when surveillance
resources are very limited. To address the challenge, we study the problem of
active surveillance, i.e., how to identify a small portion of system components
as sentinels to effect monitoring, such that the epidemic dynamics of an entire
system can be readily predicted from the partial data collected by such
sentinels. We propose a novel measure, the gamma value, to identify the
sentinels by modeling a sentinel network with row sparsity structure. We design
a flexible group sparse Bayesian learning algorithm to mine the sentinel
network suitable for handling both linear and non-linear dynamical systems by
using the expectation maximization method and variational approximation. The
efficacy of the proposed algorithm is theoretically analyzed and empirically
validated using both synthetic and real-world data."
"Patient pain can be detected highly reliably from facial expressions using a
set of facial muscle-based action units (AUs) defined by the Facial Action
Coding System (FACS). A key characteristic of facial expression of pain is the
simultaneous occurrence of pain-related AU combinations, whose automated
detection would be highly beneficial for efficient and practical pain
monitoring. Existing general Automated Facial Expression Recognition (AFER)
systems prove inadequate when applied specifically for detecting pain as they
either focus on detecting individual pain-related AUs but not on combinations
or they seek to bypass AU detection by training a binary pain classifier
directly on pain intensity data but are limited by lack of enough labeled data
for satisfactory training. In this paper, we propose a new approach that mimics
the strategy of human coders of decoupling pain detection into two consecutive
tasks: one performed at the individual video-frame level and the other at
video-sequence level. Using state-of-the-art AFER tools to detect single AUs at
the frame level, we propose two novel data structures to encode AU combinations
from single AU scores. Two weakly supervised learning frameworks namely
multiple instance learning (MIL) and multiple clustered instance learning
(MCIL) are employed corresponding to each data structure to learn pain from
video sequences. Experimental results show an 87% pain recognition accuracy
with 0.94 AUC (Area Under Curve) on the UNBC-McMaster Shoulder Pain Expression
dataset. Tests on long videos in a lung cancer patient video dataset
demonstrates the potential value of the proposed system for pain monitoring in
clinical settings."
"The goal of the present study is to identify autism using machine learning
techniques and resting-state brain imaging data, leveraging the temporal
variability of the functional connections (FC) as the only information. We
estimated and compared the FC variability across brain regions between typical,
healthy subjects and autistic population by analyzing brain imaging data from a
world-wide multi-site database known as ABIDE (Autism Brain Imaging Data
Exchange). Our analysis revealed that patients diagnosed with autism spectrum
disorder (ASD) show increased FC variability in several brain regions that are
associated with low FC variability in the typical brain. We then used the
enhanced FC variability of brain regions as features for training machine
learning models for ASD classification and achieved 65% accuracy in
identification of ASD versus control subjects within the dataset. We also used
node strength estimated from number of functional connections per node averaged
over the whole scan as features for ASD classification.The results reveal that
the dynamic FC measures outperform or are comparable with the static FC
measures in predicting ASD."
"We address the problem of image translation between domains or modalities for
which no direct paired data is available (i.e. zero-pair translation). We
propose mix and match networks, based on multiple encoders and decoders aligned
in such a way that other encoder-decoder pairs can be composed at test time to
perform unseen image translation tasks between domains or modalities for which
explicit paired samples were not seen during training. We study the impact of
autoencoders, side information and losses in improving the alignment and
transferability of trained pairwise translation models to unseen translations.
We show our approach is scalable and can perform colorization and style
transfer between unseen combinations of domains. We evaluate our system in a
challenging cross-modal setting where semantic segmentation is estimated from
depth images, without explicit access to any depth-semantic segmentation
training pairs. Our model outperforms baselines based on pix2pix and CycleGAN
models."
"Crowdsourcing has become very popular among the machine learning community as
a way to obtain labels that allow a ground truth to be estimated for a given
dataset. In most of the approaches that use crowdsourced labels, annotators are
asked to provide, for each presented instance, a single class label. Such a
request could be inefficient, that is, considering that the labelers may not be
experts, that way to proceed could fail to take real advantage of the knowledge
of the labelers. In this paper, the use of candidate labeling for crowd
learning is proposed, where the annotators may provide more than a single label
per instance to try not to miss the real label. The main hypothesis is that, by
allowing candidate labeling, knowledge can be extracted from the labelers more
efficiently by than in the standard crowd learning scenario. Empirical evidence
which supports that hypothesis is presented."
"Most environmental phenomena, such as wind profiles, ozone concentration and
sunlight distribution under a forest canopy, exhibit nonstationary dynamics
i.e. phenomenon variation change depending on the location and time of
occurrence. Non-stationary dynamics pose both theoretical and practical
challenges to statistical machine learning algorithms aiming to accurately
capture the complexities governing the evolution of such processes. In this
paper, we address the sampling aspects of the problem of learning nonstationary
spatio-temporal models, and propose an efficient yet simple algorithm - LISAL.
The core idea in LISAL is to learn two models using Gaussian processes (GPs)
wherein the first is a nonstationary GP directly modeling the phenomenon. The
second model uses a stationary GP representing a latent space corresponding to
changes in dynamics, or the nonstationarity characteristics of the first model.
LISAL involves adaptively sampling the latent space dynamics using information
theory quantities to reduce the computational cost during the learning phase.
The relevance of LISAL is extensively validated using multiple real world
datasets."
"Adversarial learning is one of the most successful approaches to modelling
high-dimensional probability distributions from data. The quantum computing
community has recently begun to generalize this idea and to look for potential
applications. In this work, we derive an adversarial algorithm for the problem
of approximating an unknown quantum pure state. Although this could be done on
universal quantum computers, the adversarial formulation enables us to execute
the algorithm on near-term quantum computers. Two parametrized circuits are
optimized in tandem: One tries to approximate the target state, the other tries
to distinguish between target and approximated state. Supported by numerical
simulations, we show that resilient backpropagation algorithms perform
remarkably well in optimizing the two circuits. We use the bipartite
entanglement entropy to design an efficient heuristic for the stopping
criterion. Our approach may find application in quantum state tomography."
"Major advancements in building general-purpose and customized hardware have
been one of the key enablers of versatility and pervasiveness of machine
learning models such as deep neural networks. To sustain this ubiquitous
deployment of machine learning models and cope with their computational and
storage complexity, several solutions such as low-precision representation of
model parameters using fixed-point representation and deploying approximate
arithmetic operations have been employed. Studying the potency of such
solutions in different applications requires integrating them into existing
machine learning frameworks for high-level simulations as well as implementing
them in hardware to analyze their effects on power/energy dissipation,
throughput, and chip area. Lop is a library for design space exploration that
bridges the gap between machine learning and efficient hardware realization. It
comprises a Python module, which can be integrated with some of the existing
machine learning frameworks and implements various customizable data
representations including fixed-point and floating-point as well as approximate
arithmetic operations.Furthermore, it includes a highly-parameterized Scala
module, which allows synthesizing hardware based on the said data
representations and arithmetic operations. Lop allows researchers and designers
to quickly compare quality of their models using various data representations
and arithmetic operations in Python and contrast the hardware cost of viable
representations by synthesizing them on their target platforms (e.g., FPGA or
ASIC). To the best of our knowledge, Lop is the first library that allows both
software simulation and hardware realization using customized data
representations and approximate computing techniques."
"Several classification methods assume that the underlying distributions
follow tree-structured graphical models. Indeed, trees capture statistical
dependencies between pairs of variables, which may be crucial to attain low
classification errors. The resulting classifier is linear in the
log-transformed univariate and bivariate densities that correspond to the tree
edges. In practice, however, observed data may not be well approximated by
trees. Yet, motivated by the importance of pairwise dependencies for accurate
classification, here we propose to approximate the optimal decision boundary by
a sparse linear combination of the univariate and bivariate log-transformed
densities. Our proposed approach is semi-parametric in nature: we
non-parametrically estimate the univariate and bivariate densities, remove
pairs of variables that are nearly independent using the Hilbert-Schmidt
independence criteria, and finally construct a linear SVM on the retained
log-transformed densities. We demonstrate using both synthetic and real data
that our resulting classifier, denoted SLB (Sparse Log-Bivariate density), is
competitive with popular classification methods."
"Analyzing large-scale, multi-experiment studies requires scientists to test
each experimental outcome for statistical significance and then assess the
results as a whole. We present Black Box FDR (BB-FDR), an empirical-Bayes
method for analyzing multi-experiment studies when many covariates are gathered
per experiment. BB-FDR learns a series of black box predictive models to boost
power and control the false discovery rate (FDR) at two stages of study
analysis. In Stage 1, it uses a deep neural network prior to report which
experiments yielded significant outcomes. In Stage 2, a separate black box
model of each covariate is used to select features that have significant
predictive power across all experiments. In benchmarks, BB-FDR outperforms
competing state-of-the-art methods in both stages of analysis. We apply BB-FDR
to two real studies on cancer drug efficacy. For both studies, BB-FDR increases
the proportion of significant outcomes discovered and selects variables that
reveal key genomic drivers of drug sensitivity and resistance in cancer."
"The folding structure of the DNA molecule combined with helper molecules,
also referred to as the chromatin, is highly relevant for the functional
properties of DNA. The chromatin structure is largely determined by the
underlying primary DNA sequence, though the interaction is not yet fully
understood. In this paper we develop a convolutional neural network that takes
an image-representation of primary DNA sequence as its input, and predicts key
determinants of chromatin structure. The method is developed such that it is
capable of detecting interactions between distal elements in the DNA sequence,
which are known to be highly relevant. Our experiments show that the method
outperforms several existing methods both in terms of prediction accuracy and
training time."
"Centroid-based methods including k-means and fuzzy c-means are known as
effective and easy-to-implement approaches to clustering purposes in many
applications. However, these algorithms cannot be directly applied to
supervised tasks. This paper thus presents a generative model extending the
centroid-based clustering approach to be applicable to classification and
regression tasks. Given an arbitrary loss function, the proposed approach,
termed Supervised Fuzzy Partitioning (SFP), incorporates labels information
into its objective function through a surrogate term penalizing the empirical
risk. Entropy-based regularization is also employed to fuzzify the partition
and to weight features, enabling the method to capture more complex patterns,
identify significant features, and yield better performance facing
high-dimensional data. An iterative algorithm based on block coordinate descent
scheme is formulated to efficiently find a local optimum. Extensive
classification experiments on synthetic, real-world, and high-dimensional
datasets demonstrate that the predictive performance of SFP is competitive with
state-of-the-art algorithms such as SVM and random forest. SFP has a major
advantage over such methods, in that it not only leads to a flexible, nonlinear
model but also can exploit any convex loss function in the training phase
without compromising computational efficiency."
"A variety of methods have been proposed for interpreting nodes in deep neural
networks, which typically involve scoring nodes at lower layers with respect to
their effects on the output of higher-layer nodes (where lower and higher
layers are closer to the input and output layers, respectively). However, we
may be interested in picking out a prioritized collection of subsets of the
inputs across a range of scales according to their importance for an output
node, and not simply a prioritized ranking across the inputs as singletons.
Such a situation may arise in biological applications, for instance, where we
are interested in epistatic effects between groups of genes in determining a
trait of interest. Here, we outline a flexible framework which may be used to
generate multiscale network interpretations, using any previously defined
scoring function. We demonstrate the ability of our method to pick out
biologically important genes and gene sets in the domains of cancer and
psychiatric genomics."
"Thompson sampling, a Bayesian method for balancing exploration and
exploitation in bandit problems, has theoretical guarantees and exhibits strong
empirical performance in many domains. Traditional Thompson sampling, however,
assumes perfect compliance, where an agent's chosen action is treated as the
implemented action. This article introduces a stochastic noncompliance model
that relaxes this assumption. We prove that any noncompliance in a 2-armed
Bernoulli bandit increases existing regret bounds. With our noncompliance
model, we derive Thompson sampling variants that explicitly handle both
observed and latent noncompliance. With extensive empirical analysis, we
demonstrate that our algorithms either match or outperform traditional Thompson
sampling in both compliant and noncompliant environments."
"In our study, we demonstrate the synergy effect between convolutional neural
networks and the multiplicity of SMILES. The model we propose, the so-called
Convolutional Neural Fingerprint (CNF) model, reaches the accuracy of
traditional descriptors such as Dragon (Mauri et al. [22]), RDKit (Landrum
[18]), CDK2 (Willighagen et al. [43]) and PyDescriptor (Masand and Rastija
[20]). Moreover the CNF model generally performs better than highly fine-tuned
traditional descriptors, especially on small data sets, which is of great
interest for the chemical field where data sets are generally small due to
experimental costs, the availability of molecules or accessibility to private
databases. We evaluate the CNF model along with SMILES augmentation during both
training and testing. To the best of our knowledge, this is the first time that
such a methodology is presented. We show that using the multiplicity of SMILES
during training acts as a regulariser and therefore avoids overfitting and can
be seen as ensemble learning when considered for testing."
"In the Pioneer 100 (P100) Wellness Project (Price and others, 2017), multiple
types of data are collected on a single set of healthy participants at multiple
timepoints in order to characterize and optimize wellness. One way to do this
is to identify clusters, or subgroups, among the participants, and then to
tailor personalized health recommendations to each subgroup. It is tempting to
cluster the participants using all of the data types and timepoints, in order
to fully exploit the available information. However, clustering the
participants based on multiple data views implicitly assumes that a single
underlying clustering of the participants is shared across all data views. If
this assumption does not hold, then clustering the participants using multiple
data views may lead to spurious results. In this paper, we seek to evaluate the
assumption that there is some underlying relationship among the clusterings
from the different data views, by asking the question: are the clusters within
each data view dependent or independent? We develop a new test for answering
this question, which we then apply to clinical, proteomic, and metabolomic
data, across two distinct timepoints, from the P100 study. We find that while
the subgroups of the participants defined with respect to any single data type
seem to be dependent across time, the clustering among the participants based
on one data type (e.g. proteomic data) appears not to be associated with the
clustering based on another data type (e.g. clinical data)."
"We quantify the separation between the numbers of labeled examples required
to learn in two settings: Settings with and without the knowledge of the
distribution of the unlabeled data. More specifically, we prove a separation by
$\Theta(\log n)$ multiplicative factor for the class of projections over the
Boolean hypercube of dimension $n$. We prove that there is no separation for
the class of all functions on domain of any size.
  Learning with the knowledge of the distribution (a.k.a. fixed-distribution
learning) can be viewed as an idealized scenario of semi-supervised learning
where the number of unlabeled data points is so great that the unlabeled
distribution is known exactly. For this reason, we call the separation the
value of unlabeled data."
"We tackle the problem of algorithmic fairness, where the goal is to avoid the
unfairly influence of sensitive information, in the general context of
regression with possible continuous sensitive attributes. We extend the
framework of fair empirical risk minimization to this general scenario,
covering in this way the whole standard supervised learning setting. Our
generalized fairness measure reduces to well known notions of fairness
available in literature. We derive learning guarantees for our method, that
imply in particular its statistical consistency, both in terms of the risk and
the fairness measure. We then specialize our approach to kernel methods and
propose a convex fair estimator in that setting. We test the estimator on a
commonly used benchmark dataset (Communities and Crime) and on a new dataset
collected at the University of Genova, containing the information of the
academic career of five thousand students. The latter dataset provides a
challenging real case scenario of unfair behaviour of standard regression
methods that benefits from our methodology. The experimental results show that
our estimator is effective at mitigating the trade-off between accuracy and
fairness requirements."
"In the early history of positive-unlabeled (PU) learning, the sample
selection approach, which heuristically selects negative (N) data from U data,
was explored extensively. However, this approach was later dominated by the
importance reweighting approach, which carefully treats all U data as N data.
May there be a new sample selection method that can outperform the latest
importance reweighting method in the deep learning age? This paper is devoted
to answering this question affirmatively---we propose to label large-loss U
data as P, based on the memorization properties of deep networks. Since P data
selected in such a way are biased, we develop a novel learning objective that
can handle such biased P data properly. Experiments confirm the superiority of
the proposed method."
"The design of deep graph models still remains to be investigated and the
crucial part is how to explore and exploit the knowledge from different hops of
neighbors in an efficient way. In this paper, we propose a novel RNN-like deep
graph neural network architecture by incorporating AdaBoost into the
computation of network; and the proposed graph convolutional network called
AdaGCN~(Adaboosting Graph Convolutional Network) has the ability to efficiently
extract knowledge from high-order neighbors of current nodes and then
integrates knowledge from different hops of neighbors into the network in an
Adaboost way. Different from other graph neural networks that directly stack
many graph convolution layers, AdaGCN shares the same base neural network
architecture among all ``layers'' and is recursively optimized, which is
similar to an RNN. Besides, We also theoretically established the connection
between AdaGCN and existing graph convolutional methods, presenting the
benefits of our proposal. Finally, extensive experiments demonstrate the
consistent state-of-the-art prediction performance on graphs across different
label rates and the computational advantage of our approach
AdaGCN~\footnote{Code is available at \url{https://github.com/datake/AdaGCN}.}"
"In matrix factorization, available graph side-information may not be well
suited for the matrix completion problem, having edges that disagree with the
latent-feature relations learnt from the incomplete data matrix. We show that
removing these $\textit{contested}$ edges improves prediction accuracy and
scalability. We identify the contested edges through a highly-efficient
graphical lasso approximation. The identification and removal of contested
edges adds no computational complexity to state-of-the-art graph-regularized
matrix factorization, remaining linear with respect to the number of non-zeros.
Computational load even decreases proportional to the number of edges removed.
Formulating a probabilistic generative model and using expectation maximization
to extend graph-regularised alternating least squares (GRALS) guarantees
convergence. Rich simulated experiments illustrate the desired properties of
the resulting algorithm. On real data experiments we demonstrate improved
prediction accuracy with fewer graph edges (empirical evidence that graph
side-information is often inaccurate). A 300 thousand dimensional graph with
three million edges (Yahoo music side-information) can be analyzed in under ten
minutes on a standard laptop computer demonstrating the efficiency of our graph
update."
"AutoML systems are currently rising in popularity, as they can build powerful
models without human oversight. They often combine techniques from many
different sub-fields of machine learning in order to find a model or set of
models that optimize a user-supplied criterion, such as predictive performance.
The ultimate goal of such systems is to reduce the amount of time spent on
menial tasks, or tasks that can be solved better by algorithms while leaving
decisions that require human intelligence to the end-user. In recent years, the
importance of other criteria, such as fairness and interpretability, and many
others have become more and more apparent. Current AutoML frameworks either do
not allow to optimize such secondary criteria or only do so by limiting the
system's choice of models and preprocessing steps. We propose to optimize
additional criteria defined by the user directly to guide the search towards an
optimal machine learning pipeline. In order to demonstrate the need and
usefulness of our approach, we provide a simple multi-criteria AutoML system
and showcase an exemplary application."
"The field of mobile health aims to leverage recent advances in wearable
on-body sensing technology and smart phone computing capabilities to develop
systems that can monitor health states and deliver just-in-time adaptive
interventions. However, existing work has largely focused on analyzing
collected data in the off-line setting. In this paper, we propose a novel
approach to learning shallow detection cascades developed explicitly for use in
a real-time wearable-phone or wearable-phone-cloud systems. We apply our
approach to the problem of cigarette smoking detection from a combination of
wrist-worn actigraphy data and respiration chest band data using two and three
stage cascades."
"We provide a detailed study on the implicit bias of gradient descent when
optimizing loss functions with strictly monotone tails, such as the logistic
loss, over separable datasets. We look at two basic questions: (a) what are the
conditions on the tail of the loss function under which gradient descent
converges in the direction of the $L_2$ maximum-margin separator? (b) how does
the rate of margin convergence depend on the tail of the loss function and the
choice of the step size? We show that for a large family of super-polynomial
tailed losses, gradient descent iterates on linear networks of any depth
converge in the direction of $L_2$ maximum-margin solution, while this does not
hold for losses with heavier tails. Within this family, for simple linear
models we show that the optimal rates with fixed step size is indeed obtained
for the commonly used exponentially tailed losses such as logistic loss.
However, with a fixed step size the optimal convergence rate is extremely slow
as $1/\log(t)$, as also proved in Soudry et al. (2018). For linear models with
exponential loss, we further prove that the convergence rate could be improved
to $\log (t) /\sqrt{t}$ by using aggressive step sizes that compensates for the
rapidly vanishing gradients. Numerical results suggest this method might be
useful for deep networks."
"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert."
"The positive-unlabeled (PU) classification is a common scenario in real-world
applications such as healthcare, text classification, and bioinformatics, in
which we only observe a few samples labeled as ""positive"" together with a large
volume of ""unlabeled"" samples that may contain both positive and negative
samples. Building robust classifier for the PU problem is very challenging,
especially for complex data where the negative samples overwhelm and mislabeled
samples or corrupted features exist. To address these three issues, we propose
a robust learning framework that unifies AUC maximization (a robust metric for
biased labels), outlier detection (for excluding wrong labels), and feature
selection (for excluding corrupted features). The generalization error bounds
are provided for the proposed model that give valuable insight into the
theoretical performance of the method and lead to useful practical guidance,
e.g., to train a model, we find that the included unlabeled samples are
sufficient as long as the sample size is comparable to the number of positive
samples in the training process. Empirical comparisons and two real-world
applications on surgical site infection (SSI) and EEG seizure detection are
also conducted to show the effectiveness of the proposed model."
"Goldbach conjecture is one of the most famous open mathematical problems. It
states that every even number, bigger than two, can be presented as a sum of 2
prime numbers. % In this work we present a deep learning based model that
predicts the number of Goldbach partitions for a given even number.
Surprisingly, our model outperforms all state-of-the-art analytically derived
estimations for the number of couples, while not requiring prime factorization
of the given number. We believe that building a model that can accurately
predict the number of couples brings us one step closer to solving one of the
world most famous open problems. To the best of our knowledge, this is the
first attempt to consider machine learning based data-driven methods to
approximate open mathematical problems in the field of number theory, and hope
that this work will encourage such attempts."
"Vanishing and exploding gradients are two of the main obstacles in training
deep neural networks, especially in capturing long range dependencies in
recurrent neural networks~(RNNs). In this paper, we present an efficient
parametrization of the transition matrix of an RNN that allows us to stabilize
the gradients that arise in its training. Specifically, we parameterize the
transition matrix by its singular value decomposition(SVD), which allows us to
explicitly track and control its singular values. We attain efficiency by using
tools that are common in numerical linear algebra, namely Householder
reflectors for representing the orthogonal matrices that arise in the SVD. By
explicitly controlling the singular values, our proposed Spectral-RNN method
allows us to easily solve the exploding gradient problem and we observe that it
empirically solves the vanishing gradient issue to a large extent. We note that
the SVD parameterization can be used for any rectangular weight matrix, hence
it can be easily extended to any deep neural network, such as a multi-layer
perceptron. Theoretically, we demonstrate that our parameterization does not
lose any expressive power, and show how it controls generalization of RNN for
the classification task. %, and show how it potentially makes the optimization
process easier. Our extensive experimental results also demonstrate that the
proposed framework converges faster, and has good generalization, especially in
capturing long range dependencies, as shown on the synthetic addition and copy
tasks, as well as on MNIST and Penn Tree Bank data sets."
"Semi-supervised learning (SSL) has become important in current data analysis
applications, where the amount of unlabeled data is growing exponentially and
user input remains limited by logistics and expense. Constrained clustering, as
a subclass of SSL, makes use of user input in the form of relationships between
data points (e.g., pairs of data points belonging to the same class or
different classes) and can remarkably improve the performance of unsupervised
clustering in order to reflect user-defined knowledge of the relationships
between particular data points. Existing algorithms incorporate such user
input, heuristically, as either hard constraints or soft penalties, which are
separate from any generative or statistical aspect of the clustering model;
this results in formulations that are suboptimal and not sufficiently general.
In this paper, we propose a principled, generative approach to
probabilistically model, without ad hoc penalties, the joint distribution given
by user-defined pairwise relations. The proposed model accounts for general
underlying distributions without assuming a specific form and relies on
expectation-maximization for model fitting. For distributions in a standard
form, the proposed approach results in a closed-form solution for updated
parameters."
"While deep reinforcement learning has successfully solved many challenging
control tasks, its real-world applicability has been limited by the inability
to ensure the safety of learned policies. We propose an approach to verifiable
reinforcement learning by training decision tree policies, which can represent
complex policies (since they are nonparametric), yet can be efficiently
verified using existing techniques (since they are highly structured). The
challenge is that decision tree policies are difficult to train. We propose
VIPER, an algorithm that combines ideas from model compression and imitation
learning to learn decision tree policies guided by a DNN policy (called the
oracle) and its Q-function, and show that it substantially outperforms two
baselines. We use VIPER to (i) learn a provably robust decision tree policy for
a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree
policy for a toy game based on Pong that provably never loses, and (iii) learn
a provably stable decision tree policy for cart-pole. In each case, the
decision tree policy achieves performance equal to that of the original DNN
policy."
"As application demands for zeroth-order (gradient-free) optimization
accelerate, the need for variance reduced and faster converging approaches is
also intensifying. This paper addresses these challenges by presenting: a) a
comprehensive theoretical analysis of variance reduced zeroth-order (ZO)
optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c)
an experimental evaluation of our approach in the context of two compelling
applications, black-box chemical material classification and generation of
adversarial examples from black-box deep neural network models. Our theoretical
analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the
unbiased assumption on gradient estimates no longer holds. We prove that
compared to its first-order counterpart, ZO-SVRG with a two-point random
gradient estimator could suffer an additional error of order $O(1/b)$, where
$b$ is the mini-batch size. To mitigate this error, we propose two accelerated
versions of ZO-SVRG utilizing variance reduced gradient estimators, which
achieve the best rate known for ZO stochastic optimization (in terms of
iterations). Our extensive experimental results show that our approaches
outperform other state-of-the-art ZO algorithms, and strike a balance between
the convergence rate and the function query complexity."
"In this work, a unified framework for gradient-free Multidimensional Scaling
(MDS) based on Coordinate Search (CS) is proposed. This family of algorithms is
an instance of General Pattern Search (GPS) methods which avoid the explicit
computation of derivatives but instead evaluate the objective function while
searching on coordinate steps of the embedding space. The backbone element of
CSMDS framework is the corresponding probability matrix that correspond to how
likely is each corresponding coordinate to be evaluated. We propose a
Bootstrapped instance of CSMDS (BS CSMDS) which enhances the probability of the
direction that decreases the most the objective function while also reducing
the corresponding probability of all the other coordinates. BS CSMDS manages to
avoid unnecessary function evaluations and result to significant speedup over
other CSMDS alternatives while also obtaining the same error rate. Experiments
on both synthetic and real data reveal that BS CSMDS performs consistently
better than other CSMDS alternatives under various experimental setups."
"Partial label learning deals with the problem where each training instance is
assigned a set of candidate labels, only one of which is correct. This paper
provides the first attempt to leverage the idea of self-training for dealing
with partially labeled examples. Specifically, we propose a unified formulation
with proper constraints to train the desired model and perform pseudo-labeling
jointly. For pseudo-labeling, unlike traditional self-training that manually
differentiates the ground-truth label with enough high confidence, we introduce
the maximum infinity norm regularization on the modeling outputs to
automatically achieve this consideratum, which results in a convex-concave
optimization problem. We show that optimizing this convex-concave problem is
equivalent to solving a set of quadratic programming (QP) problems. By
proposing an upper-bound surrogate objective function, we turn to solving only
one QP problem for improving the optimization efficiency. Extensive experiments
on synthesized and real-world datasets demonstrate that the proposed approach
significantly outperforms the state-of-the-art partial label learning
approaches."
"There is a large body of work on convergence rates either in passive or
active learning. Here we outline some of the results that have been obtained,
more specifically in a nonparametric setting under assumptions about the
smoothness and the margin noise. We also discuss the relative merits of these
underlying assumptions by putting active learning in perspective with recent
work on passive learning. We provide a novel active learning algorithm with a
rate of convergence better than in passive learning, using a particular
smoothness assumption customized for $k$-nearest neighbors. This smoothness
assumption provides a dependence on the marginal distribution of the instance
space unlike other recent algorithms.
  Our algorithm thus avoids the strong density assumption that supposes the
existence of the density function of the marginal distribution of the instance
space and is therefore more generally applicable."
"Trajectory owner prediction is the basis for many applications such as
personalized recommendation, urban planning. Although much effort has been put
on this topic, the results archived are still not good enough. Existing methods
mainly employ RNNs to model trajectories semantically due to the inherent
sequential attribute of trajectories. However, these approaches are weak at
Point of Interest (POI) representation learning and trajectory feature
detection. Thus, the performance of existing solutions is far from the
requirements of practical applications. In this paper, we propose a novel
CNN-based Trajectory Owner Prediction (CNNTOP) method. Firstly, we connect all
POI according to trajectories from all users. The result is a connected graph
that can be used to generate more informative POI sequences than other
approaches. Secondly, we employ the Node2Vec algorithm to encode each POI into
a low-dimensional real value vector. Then, we transform each trajectory into a
fixed-dimensional matrix, which is similar to an image. Finally, a CNN is
designed to detect features and predict the owner of a given trajectory. The
CNN can extract informative features from the matrix representations of
trajectories by convolutional operations, Batch normalization, and $K$-max
pooling operations. Extensive experiments on real datasets demonstrate that
CNNTOP substantially outperforms existing solutions in terms of
macro-Precision, macro-Recall, macro-F1, and accuracy."
"Thompson Sampling is a well established approach to bandit and reinforcement
learning problems. However its use in continuum armed bandit problems has
received relatively little attention. We provide the first bounds on the regret
of Thompson Sampling for continuum armed bandits under weak conditions on the
function class containing the true function and sub-exponential observation
noise. Our bounds are realised by analysis of the eluder dimension, a recently
proposed measure of the complexity of a function class, which has been
demonstrated to be useful in bounding the Bayesian regret of Thompson Sampling
for simpler bandit problems under sub-Gaussian observation noise. We derive a
new bound on the eluder dimension for classes of functions with Lipschitz
derivatives, and generalise previous analyses in multiple regards."
"A variety of lifted inference algorithms, which exploit model symmetry to
reduce computational cost, have been proposed to render inference tractable in
probabilistic relational models. Most existing lifted inference algorithms
operate only over discrete domains or continuous domains with restricted
potential functions, e.g., Gaussian. We investigate two approximate lifted
variational approaches that are applicable to hybrid domains and expressive
enough to capture multi-modality. We demonstrate that the proposed variational
methods are both scalable and can take advantage of approximate model
symmetries, even in the presence of a large amount of continuous evidence. We
demonstrate that our approach compares favorably against existing
message-passing based approaches in a variety of settings. Finally, we present
a sufficient condition for the Bethe approximation to yield a non-trivial
estimate over the marginal polytope."
"Random forests are powerful non-parametric regression method but are severely
limited in their usage in the presence of randomly censored observations, and
naively applied can exhibit poor predictive performance due to the incurred
biases. Based on a local adaptive representation of random forests, we develop
its regression adjustment for randomly censored regression quantile models.
Regression adjustment is based on a new estimating equation that adapts to
censoring and leads to quantile score whenever the data do not exhibit
censoring. The proposed procedure named {\it censored quantile regression
forest}, allows us to estimate quantiles of time-to-event without any
parametric modeling assumption. We establish its consistency under mild model
specifications. Numerical studies showcase a clear advantage of the proposed
procedure."
"For feature selection and related problems, we introduce the notion of
classification game, a cooperative game, with features as players and hinge
loss based characteristic function and relate a feature's contribution to
Shapley value based error apportioning (SVEA) of total training error. Our
major contribution is ($\star$) to show that for any dataset the threshold 0 on
SVEA value identifies feature subset whose joint interactions for label
prediction is significant or those features that span a subspace where the data
is predominantly lying. In addition, our scheme ($\star$) identifies the
features on which Bayes classifier doesn't depend but any surrogate loss
function based finite sample classifier does; this contributes to the excess
$0$-$1$ risk of such a classifier, ($\star$) estimates unknown true hinge risk
of a feature, and ($\star$) relate the stability property of an allocation and
negative valued SVEA by designing the analogue of core of classification game.
Due to Shapley value's computationally expensive nature, we build on a known
Monte Carlo based approximation algorithm that computes characteristic function
(Linear Programs) only when needed. We address the potential sample bias
problem in feature selection by providing interval estimates for SVEA values
obtained from multiple sub-samples. We illustrate all the above aspects on
various synthetic and real datasets and show that our scheme achieves better
results than existing recursive feature elimination technique and ReliefF in
most cases. Our theoretically grounded classification game in terms of well
defined characteristic function offers interpretability (which we formalize in
terms of final task) and explainability of our framework, including
identification of important features."
"Given a publicly available pool of machine learning models constructed for
various tasks, when a user plans to build a model for her own machine learning
application, is it possible to build upon models in the pool such that the
previous efforts on these existing models can be reused rather than starting
from scratch? Here, a grand challenge is how to find models that are helpful
for the current application, without accessing the raw training data for the
models in the pool. In this paper, we present a two-phase framework. In the
upload phase, when a model is uploading into the pool, we construct a reduced
kernel mean embedding (RKME) as a specification for the model. Then in the
deployment phase, the relatedness of the current task and pre-trained models
will be measured based on the value of the RKME specification. Theoretical
results and extensive experiments validate the effectiveness of our approach."
"Anomaly detection algorithms find extensive use in various fields. This area
of research has recently made great advances thanks to deep learning. A recent
method, the deep Support Vector Data Description (deep SVDD), which is inspired
by the classic kernel-based Support Vector Data Description (SVDD), is capable
of simultaneously learning a feature representation of the data and a
data-enclosing hypersphere. The method has shown promising results in both
unsupervised and semi-supervised settings. However, deep SVDD suffers from
hypersphere collapse -- also known as mode collapse, if the architecture of the
model does not comply with certain architectural constraints, e.g. the removal
of bias terms. These constraints limit the adaptability of the model and in
some cases, may affect the model performance due to learning sub-optimal
features. In this work, we consider two regularizers to prevent hypersphere
collapse in deep SVDD. The first regularizer is based on injecting random noise
via the standard cross-entropy loss. The second regularizer penalizes the
minibatch variance when it becomes too small. Moreover, we introduce an
adaptive weighting scheme to control the amount of penalization between the
SVDD loss and the respective regularizer. Our proposed regularized variants of
deep SVDD show encouraging results and outperform a prominent state-of-the-art
method on a setup where the anomalies have no apparent geometrical structure."
"In this paper, we introduce a method for segmenting time series data using
tools from Bayesian nonparametrics. We consider the task of temporal
segmentation of a set of time series data into representative stationary
segments. We use Gaussian process (GP) priors to impose our knowledge about the
characteristics of the underlying stationary segments, and use a nonparametric
distribution to partition the sequences into such segments, formulated in terms
of a prior distribution on segment length. Given the segmentation, the model
can be viewed as a variant of a Gaussian mixture model where the mixture
components are described using the covariance function of a GP. We demonstrate
the effectiveness of our model on synthetic data as well as on real time-series
data of heartbeats where the task is to segment the indicative types of beats
and to classify the heartbeat recordings into classes that correspond to
healthy and abnormal heart sounds."
"The success of deep neural networks has inspired many to wonder whether other
learners could benefit from deep, layered architectures. We present a general
framework called forward thinking for deep learning that generalizes the
architectural flexibility and sophistication of deep neural networks while also
allowing for (i) different types of learning functions in the network, other
than neurons, and (ii) the ability to adaptively deepen the network as needed
to improve results. This is done by training one layer at a time, and once a
layer is trained, the input data are mapped forward through the layer to create
a new learning problem. The process is then repeated, transforming the data
through multiple layers, one at a time, rendering a new dataset, which is
expected to be better behaved, and on which a final output layer can achieve
good performance. In the case where the neurons of deep neural nets are
replaced with decision trees, we call the result a Forward Thinking Deep Random
Forest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the
MNIST dataset. We also provide a general mathematical formulation that allows
for other types of deep learning problems to be considered."
"We investigate the generalizability of deep learning based on the sensitivity
to input perturbation. We hypothesize that the high sensitivity to the
perturbation of data degrades the performance on it. To reduce the sensitivity
to perturbation, we propose a simple and effective regularization method,
referred to as spectral norm regularization, which penalizes the high spectral
norm of weight matrices in neural networks. We provide supportive evidence for
the abovementioned hypothesis by experimentally confirming that the models
trained using spectral norm regularization exhibit better generalizability than
other baseline methods."
"In this paper, we propose a novel method for projecting data from multiple
modalities to a new subspace optimized for one-class classification. The
proposed method iteratively transforms the data from the original feature space
of each modality to a new common feature space along with finding a joint
compact description of data coming from all the modalities. For data in each
modality, we define a separate transformation to map the data from the
corresponding feature space to the new optimized subspace by exploiting the
available information from the class of interest only. We also propose
different regularization strategies for the proposed method and provide both
linear and non-linear formulations. The proposed Multimodal Subspace Support
Vector Data Description outperforms all the competing methods using data from a
single modality or fusing data from all modalities in four out of five
datasets."
"Intricating cardiac complexities are the primary factor associated with
healthcare costs and the highest cause of death rate in the world. However,
preventive measures like the early detection of cardiac anomalies can prevent
severe cardiovascular arrests of varying complexities and can impose a
substantial impact on healthcare cost. Encountering such scenarios usually the
electrocardiogram (ECG or EKG) is the first diagnostic choice of a medical
practitioner or clinical staff to measure the electrical and muscular fitness
of an individual heart. This paper presents a system which is capable of
reading the recorded ECG and predict the cardiac anomalies without the
intervention of a human expert. The paper purpose an algorithm which read and
perform analysis on electrocardiogram datasets. The proposed architecture uses
the Discrete Wavelet Transform (DWT) at first place to perform preprocessing of
ECG data followed by undecimated Wavelet transform (UWT) to extract nine
relevant features which are of high interest to a cardiologist. The
probabilistic mode named Bayesian Network Classifier is trained using the
extracted nine parameters on UCL arrhythmia dataset. The proposed system
classifies a recorded heartbeat into four classes using Bayesian Network
classifier and Tukey's box analysis. The four classes for the prediction of a
heartbeat are (a) Normal Beat, (b) Premature Ventricular Contraction (PVC) (c)
Premature Atrial Contraction (PAC) and (d) Myocardial Infarction. The results
of experimental setup depict that the proposed system has achieved an average
accuracy of 96.6 for PAC\% 92.8\% for MI and 87\% for PVC, with an average
error rate of 3.3\% for PAC, 6\% for MI and 12.5\% for PVC on real
electrocardiogram datasets including Physionet and European ST-T Database
(EDB)."
"Learning Interpretable representation in medical applications is becoming
essential for adopting data-driven models into clinical practice. It has been
recently shown that learning a disentangled feature representation is important
for a more compact and explainable representation of the data. In this paper,
we introduce a novel adversarial variational autoencoder with a total
correlation constraint to enforce independence on the latent representation
while preserving the reconstruction fidelity. Our proposed method is validated
on a publicly available dataset showing that the learned disentangled
representation is not only interpretable, but also superior to the
state-of-the-art methods. We report a relative improvement of 81.50% in terms
of disentanglement, 11.60% in clustering, and 2% in supervised classification
with a few amounts of labeled data."
"Many expensive black-box optimisation problems are sensitive to their inputs.
In these problems it makes more sense to locate a region of good designs, than
a single-possibly fragile-optimal design. Expensive black-box functions can be
optimised effectively with Bayesian optimisation, where a Gaussian process is a
popular choice as a prior over the expensive function. We propose a method for
robust optimisation using Bayesian optimisation to find a region of design
space in which the expensive function's performance is relatively insensitive
to the inputs whilst retaining a good quality. This is achieved by sampling
realisations from a Gaussian process that is modelling the expensive function,
and evaluating the improvement for each realisation. The expectation of these
improvements can be optimised cheaply with an evolutionary algorithm to
determine the next location at which to evaluate the expensive function. We
describe an efficient process to locate the optimum expected improvement. We
show empirically that evaluating the expensive function at the location in the
candidate uncertainty region about which the model is most uncertain, or at
random, yield the best convergence in contrast to exploitative schemes. We
illustrate our method on six test functions in two, five, and ten dimensions,
and demonstrate that it is able to outperform two state-of-the-art approaches
from the literature. We also demonstrate our method one two real-world problems
in 4 and 8 dimensions, which involve training robot arms to push objects onto
targets."
"We present a preference learning framework for multiple criteria sorting. We
consider sorting procedures applying an additive value model with diverse types
of marginal value functions (including linear, piecewise-linear, splined, and
general monotone ones) under a unified analytical framework. Differently from
the existing sorting methods that infer a preference model from crisp decision
examples, where each reference alternative is assigned to a unique class, our
framework allows to consider valued assignment examples in which a reference
alternative can be classified into multiple classes with respective credibility
degrees. We propose an optimization model for constructing a preference model
from such valued examples by maximizing the credible consistency among
reference alternatives. To improve the predictive ability of the constructed
model on new instances, we employ the regularization techniques. Moreover, to
enhance the capability of addressing large-scale datasets, we introduce a
state-of-the-art algorithm that is widely used in the machine learning
community to solve the proposed optimization model in a computationally
efficient way. Using the constructed additive value model, we determine both
crisp and valued assignments for non-reference alternatives. Moreover, we allow
the Decision Maker to prioritize importance of classes and give the method a
flexibility to adjust classification performance across classes according to
the specified priorities. The practical usefulness of the analytical framework
is demonstrated on a real-world dataset by comparing it to several existing
sorting methods."
"Graph convolutional networks (GCNs) are vulnerable to perturbations of the
graph structure that are either random, or, adversarially designed. The
perturbed links modify the graph neighborhoods, which critically affects the
performance of GCNs in semi-supervised learning (SSL) tasks. Aiming at
robustifying GCNs conditioned on the perturbed graph, the present paper
generates multiple auxiliary graphs, each having its binary 0-1 edge weights
flip values with probabilities designed to enhance robustness. The resultant
edge-dithered auxiliary graphs are leveraged by an adaptive (A)GCN that
performs SSL. Robustness is enabled through learnable graph-combining weights
along with suitable regularizers. Relative to GCN, the novel AGCN achieves
markedly improved performance in tests with noisy inputs, graph perturbations,
and state-of-the-art adversarial attacks. Further experiments with protein
interaction networks showcase the competitive performance of AGCN for SSL over
multiple graphs."
"In this paper we explore the role of sample mean in building a neural network
for classification. This role is surprisingly extensive and includes: direct
computation of weights without training, performance monitoring for samples
without known classification, and self-training for unlabeled data.
Experimental computation on a CIFAR-10 data set provides promising empirical
evidence on the efficacy of a simple and widely applicable approach to some
difficult problems."
"A Bayesian treatment can mitigate overconfidence in ReLU nets around the
training data. But far away from them, ReLU Bayesian neural networks (BNNs) can
still underestimate uncertainty and thus be asymptotically overconfident. This
issue arises since the output variance of a BNN with finitely many features is
quadratic in the distance from the data region. Meanwhile, Bayesian linear
models with ReLU features converge, in the infinite-width limit, to a
particular Gaussian process (GP) with a variance that grows cubically so that
no asymptotic overconfidence can occur. While this may seem of mostly
theoretical interest, in this work, we show that it can be used in practice to
the benefit of BNNs. We extend finite ReLU BNNs with infinite ReLU features via
the GP and show that the resulting model is asymptotically maximally uncertain
far away from the data while the BNNs' predictive power is unaffected near the
data. Although the resulting model approximates a full GP posterior, thanks to
its structure, it can be applied \emph{post-hoc} to any pre-trained ReLU BNN at
a low cost."
"In this paper, the robustness and accuracy of the deep neural network (DNN)
was enhanced by introducing the $L_{2,\infty}$ normalization of the weight
matrices of the DNN with Relu as the activation function. It is proved that the
$L_{2,\infty}$ normalization leads to large dihedral angles between two
adjacent faces of the polyhedron graph of the DNN function and hence smoother
DNN functions, which reduces over-fitting. A measure is proposed for the
robustness of a classification DNN, which is the average radius of the maximal
robust spheres with the sample data as centers. A lower bound for the
robustness measure is given in terms of the $L_{2,\infty}$ norm. Finally, an
upper bound for the Rademacher complexity of DNN with $L_{2,\infty}$
normalization is given. An algorithm is given to train a DNN with the
$L_{2,\infty}$ normalization and experimental results are used to show that the
$L_{2,\infty}$ normalization is effective to improve the robustness and
accuracy."
"Anomaly detection suffers from unbalanced data since anomalies are quite
rare. Synthetically generated anomalies are a solution to such ill or not fully
defined data. However, synthesis requires an expressive representation to
guarantee the quality of the generated data. In this paper, we propose a
two-level hierarchical latent space representation that distills inliers'
feature-descriptors (through autoencoders) into more robust representations
based on a variational family of distributions (through a variational
autoencoder) for zero-shot anomaly generation. From the learned latent
distributions, we select those that lie on the outskirts of the training data
as synthetic-outlier generators. And, we synthesize from them, i.e., generate
negative samples without seen them before, to train binary classifiers. We
found that the use of the proposed hierarchical structure for feature
distillation and fusion creates robust and general representations that allow
us to synthesize pseudo outlier samples. And in turn, train robust binary
classifiers for true outlier detection (without the need for actual outliers
during training). We demonstrate the performance of our proposal on several
benchmarks for anomaly detection."
"The gradient boosting machine is one of the powerful tools for solving
regression problems. In order to cope with its shortcomings, an approach for
constructing ensembles of gradient boosting models is proposed. The main idea
behind the approach is to use the stacking algorithm in order to learn a
second-level meta-model which can be regarded as a model for implementing
various ensembles of gradient boosting models. First, the linear regression of
the gradient boosting models is considered as a simplest realization of the
meta-model under condition that the linear model is differentiable with respect
to its coefficients (weights). Then it is shown that the proposed approach can
be simply extended on arbitrary differentiable combination models, for example,
on neural networks which are differentiable and can implement arbitrary
functions of gradient boosting models. Various numerical examples illustrate
the proposed approach."
"A growing body of recent evidence has highlighted the limitations of natural
language processing (NLP) datasets and classifiers. These include the presence
of annotation artifacts in datasets, classifiers relying on shallow features
like a single word (e.g., if a movie review has the word ""romantic"", the review
tends to be positive), or unnecessary words (e.g., learning a proper noun to
classify a movie as positive or negative). The presence of such artifacts has
subsequently led to the development of challenging datasets to force the model
to generalize better. While a variety of heuristic strategies, such as
counterfactual examples and contrast sets, have been proposed, the theoretical
justification about what makes these examples difficult for the classifier is
often lacking or unclear. In this paper, using tools from information geometry,
we propose a theoretical way to quantify the difficulty of an example in NLP.
Using our approach, we explore difficult examples for several deep learning
architectures. We discover that both BERT, CNN and fasttext are susceptible to
word substitutions in high difficulty examples. These classifiers tend to
perform poorly on the FIM test set. (generated by sampling and perturbing
difficult examples, with accuracy dropping below 50%). We replicate our
experiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews,
YelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a
correlation coefficient of -0.4 between resilience to perturbations and the
difficulty score. Similarly we observe a correlation of 0.35 between the
difficulty score and the empirical success probability of random substitutions.
Our approach is simple, architecture agnostic and can be used to study the
fragilities of text classification models. All the code used will be made
publicly available, including a tool to explore the difficult examples for
other datasets."
"The collection of individuals' data has become commonplace in many
industries. Local differential privacy (LDP) offers a rigorous approach to
preserving privacy whereby the individual privatises their data locally,
allowing only their perturbed datum to leave their possession. LDP thus
provides a provable privacy guarantee to the individual against both
adversaries and database administrators. Existing LDP mechanisms have
successfully been applied to low-dimensional data, but in high dimensions the
privacy-inducing noise largely destroys the utility of the data. In this work,
our contributions are two-fold: first, by adapting state-of-the-art techniques
from representation learning, we introduce a novel approach to learning LDP
mechanisms. These mechanisms add noise to powerful representations on the
low-dimensional manifold underlying the data, thereby overcoming the
prohibitive noise requirements of LDP in high dimensions. Second, we introduce
a novel denoising approach for downstream model learning. The training of
performant machine learning models using collected LDP data is a common goal
for data collectors, and downstream model performance forms a proxy for the LDP
data utility. Our approach significantly outperforms current state-of-the-art
LDP mechanisms."
"We consider the problem of robust compressed sensing whose objective is to
recover a high-dimensional sparse signal from compressed measurements corrupted
by outliers. A new sparse Bayesian learning method is developed for robust
compressed sensing. The basic idea of the proposed method is to identify and
remove the outliers from sparse signal recovery. To automatically identify the
outliers, we employ a set of binary indicator hyperparameters to indicate which
observations are outliers. These indicator hyperparameters are treated as
random variables and assigned a beta process prior such that their values are
confined to be binary. In addition, a Gaussian-inverse Gamma prior is imposed
on the sparse signal to promote sparsity. Based on this hierarchical prior
model, we develop a variational Bayesian method to estimate the indicator
hyperparameters as well as the sparse signal. Simulation results show that the
proposed method achieves a substantial performance improvement over existing
robust compressed sensing techniques."
"The ability of a human being to extrapolate previously gained knowledge to
other domains inspired a new family of methods in machine learning called
transfer learning. Transfer learning is often based on the assumption that
objects in both target and source domains share some common feature and/or data
space. In this paper, we propose a simple and intuitive approach that minimizes
iteratively the distance between source and target task distributions by
optimizing the kernel target alignment (KTA). We show that this procedure is
suitable for transfer learning by relating it to Hilbert-Schmidt Independence
Criterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We run
our method on benchmark computer vision data sets and show that it can
outperform some state-of-art methods."
"The aim of this paper is to develop a general framework for training neural
networks (NNs) in a distributed environment, where training data is partitioned
over a set of agents that communicate with each other through a sparse,
possibly time-varying, connectivity pattern. In such distributed scenario, the
training problem can be formulated as the (regularized) optimization of a
non-convex social cost function, given by the sum of local (non-convex) costs,
where each agent contributes with a single error term defined with respect to
its local dataset. To devise a flexible and efficient solution, we customize a
recently proposed framework for non-convex optimization over networks, which
hinges on a (primal) convexification-decomposition technique to handle
non-convexity, and a dynamic consensus procedure to diffuse information among
the agents. Several typical choices for the training criterion (e.g., squared
loss, cross entropy, etc.) and regularization (e.g., $\ell_2$ norm, sparsity
inducing penalties, etc.) are included in the framework and explored along the
paper. Convergence to a stationary solution of the social non-convex problem is
guaranteed under mild assumptions. Additionally, we show a principled way
allowing each agent to exploit a possible multi-core architecture (e.g., a
local cloud) in order to parallelize its local optimization step, resulting in
strategies that are both distributed (across the agents) and parallel (inside
each agent) in nature. A comprehensive set of experimental results validate the
proposed approach."
"This paper studies systematic exploration for reinforcement learning with
rich observations and function approximation. We introduce a new model called
contextual decision processes, that unifies and generalizes most prior
settings. Our first contribution is a complexity measure, the Bellman rank,
that we show enables tractable learning of near-optimal behavior in these
processes and is naturally small for many well-studied reinforcement learning
settings. Our second contribution is a new reinforcement learning algorithm
that engages in systematic exploration to learn contextual decision processes
with low Bellman rank. Our algorithm provably learns near-optimal behavior with
a number of samples that is polynomial in all relevant parameters but
independent of the number of unique observations. The approach uses Bellman
error minimization with optimistic exploration and provides new insights into
efficient exploration for reinforcement learning with function approximation."
"Computation of moments of transformed random variables is a problem appearing
in many engineering applications. The current methods for moment transformation
are mostly based on the classical quadrature rules which cannot account for the
approximation errors. Our aim is to design a method for moment transformation
for Gaussian random variables which accounts for the error in the numerically
computed mean. We employ an instance of Bayesian quadrature, called Gaussian
process quadrature (GPQ), which allows us to treat the integral itself as a
random variable, where the integral variance informs about the incurred
integration error. Experiments on the coordinate transformation and nonlinear
filtering examples show that the proposed GPQ moment transform performs better
than the classical transforms."
"Recent work in distance metric learning has focused on learning
transformations of data that best align with specified pairwise similarity and
dissimilarity constraints, often supplied by a human observer. The learned
transformations lead to improved retrieval, classification, and clustering
algorithms due to the better adapted distance or similarity measures. Here, we
address the problem of learning these transformations when the underlying
constraint generation process is nonstationary. This nonstationarity can be due
to changes in either the ground-truth clustering used to generate constraints
or changes in the feature subspaces in which the class structure is apparent.
We propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD),
a general adaptive, online approach for learning and tracking optimal metrics
as they change over time that is highly robust to a variety of nonstationary
behaviors in the changing metric. We apply the OCELAD framework to an ensemble
of online learners. Specifically, we create a retro-initialized composite
objective mirror descent (COMID) ensemble (RICE) consisting of a set of
parallel COMID learners with different learning rates, and demonstrate
parameter-free RICE-OCELAD metric learning on both synthetic data and a highly
nonstationary Twitter dataset. We show significant performance improvements and
increased robustness to nonstationary effects relative to previously proposed
batch and online distance metric learning algorithms."
"Deep generative models are powerful tools that have produced impressive
results in recent years. These advances have been for the most part empirically
driven, making it essential that we use high quality evaluation metrics. In
this paper, we provide new insights into the Inception Score, a recently
proposed and widely used evaluation metric for generative models, and
demonstrate that it fails to provide useful guidance when comparing models. We
discuss both suboptimalities of the metric itself and issues with its
application. Finally, we call for researchers to be more systematic and careful
when evaluating and comparing generative models, as the advancement of the
field depends upon it."
"We propose a new technique that boosts the convergence of training generative
adversarial networks. Generally, the rate of training deep models reduces
severely after multiple iterations. A key reason for this phenomenon is that a
deep network is expressed using a highly non-convex finite-dimensional model,
and thus the parameter gets stuck in a local optimum. Because of this, methods
often suffer not only from degeneration of the convergence speed but also from
limitations in the representational power of the trained network. To overcome
this issue, we propose an additional layer called the gradient layer to seek a
descent direction in an infinite-dimensional space. Because the layer is
constructed in the infinite-dimensional space, we are not restricted by the
specific model structure of finite-dimensional models. As a result, we can get
out of the local optima in finite-dimensional models and move towards the
global optimal function more directly. In this paper, this phenomenon is
explained from the functional gradient method perspective of the gradient
layer. Interestingly, the optimization procedure using the gradient layer
naturally constructs the deep structure of the network. Moreover, we
demonstrate that this procedure can be regarded as a discretization method of
the gradient flow that naturally reduces the objective function. Finally, the
method is tested using several numerical experiments, which show its fast
convergence."
"Latent variable models can be used to probabilistically ""fill-in"" missing
data entries. The variational autoencoder architecture (Kingma and Welling,
2014; Rezende et al., 2014) includes a ""recognition"" or ""encoder"" network that
infers the latent variables given the data variables. However, it is not clear
how to handle missing data variables in this network. The factor analysis (FA)
model is a basic autoencoder, using linear encoder and decoder networks. We
show how to calculate exactly the latent posterior distribution for the factor
analysis (FA) model in the presence of missing data, and note that this
solution implies that a different encoder network is required for each pattern
of missingness. We also discuss various approximations to the exact solution.
Experiments compare the effectiveness of various approaches to filling in the
missing data."
"The identification of novel drug-target (DT) interactions is a substantial
part of the drug discovery process. Most of the computational methods that have
been proposed to predict DT interactions have focused on binary classification,
where the goal is to determine whether a DT pair interacts or not. However,
protein-ligand interactions assume a continuum of binding strength values, also
called binding affinity and predicting this value still remains a challenge.
The increase in the affinity data available in DT knowledge-bases allows the
use of advanced learning techniques such as deep learning architectures in the
prediction of binding affinities. In this study, we propose a deep-learning
based model that uses only sequence information of both targets and drugs to
predict DT interaction binding affinities. The few studies that focus on DT
binding affinity prediction use either 3D structures of protein-ligand
complexes or 2D features of compounds. One novel approach used in this work is
the modeling of protein sequences and compound 1D representations with
convolutional neural networks (CNNs). The results show that the proposed deep
learning based model that uses the 1D representations of targets and drugs is
an effective approach for drug target binding affinity prediction. The model in
which high-level representations of a drug and a target are constructed via
CNNs achieved the best Concordance Index (CI) performance in one of our larger
benchmark data sets, outperforming the KronRLS algorithm and SimBoost, a
state-of-the-art method for DT binding affinity prediction."
"We present PROPS, a lightweight transfer learning mechanism for sequential
data. PROPS learns probabilistic perturbations around the predictions of one or
more arbitrarily complex, pre-trained black box models (such as recurrent
neural networks). The technique pins the black-box prediction functions to
""source nodes"" of a hidden Markov model (HMM), and uses the remaining nodes as
""perturbation nodes"" for learning customized perturbations around those
predictions. In this paper, we describe the PROPS model, provide an algorithm
for online learning of its parameters, and demonstrate the consistency of this
estimation. We also explore the utility of PROPS in the context of personalized
language modeling. In particular, we construct a baseline language model by
training a LSTM on the entire Wikipedia corpus of 2.5 million articles (around
6.6 billion words), and then use PROPS to provide lightweight customization
into a personalized language model of President Donald J. Trump's tweeting. We
achieved good customization after only 2,000 additional words, and find that
the PROPS model, being fully probabilistic, provides insight into when
President Trump's speech departs from generic patterns in the Wikipedia corpus.
Python code (for both the PROPS training algorithm as well as experiment
reproducibility) is available at
https://github.com/cylance/perturbed-sequence-model."
"In this paper, we explore new approaches to combining information encoded
within the learned representations of auto-encoders. We explore models that are
capable of combining the attributes of multiple inputs such that a
resynthesised output is trained to fool an adversarial discriminator for real
versus synthesised data. Furthermore, we explore the use of such an
architecture in the context of semi-supervised learning, where we learn a
mixing function whose objective is to produce interpolations of hidden states,
or masked combinations of latent representations that are consistent with a
conditioned class label. We show quantitative and qualitative evidence that
such a formulation is an interesting avenue of research."
"In a discounted reward Markov Decision Process (MDP), the objective is to
find the optimal value function, i.e., the value function corresponding to an
optimal policy. This problem reduces to solving a functional equation known as
the Bellman equation and a fixed point iteration scheme known as the value
iteration is utilized to obtain the solution. In literature, a successive
over-relaxation based value iteration scheme is proposed to speed-up the
computation of the optimal value function. The speed-up is achieved by
constructing a modified Bellman equation that ensures faster convergence to the
optimal value function. However, in many practical applications, the model
information is not known and we resort to Reinforcement Learning (RL)
algorithms to obtain optimal policy and value function. One such popular
algorithm is Q-learning. In this paper, we propose Successive Over-Relaxation
(SOR) Q-learning. We first derive a modified fixed point iteration for SOR
Q-values and utilize stochastic approximation to derive a learning algorithm to
compute the optimal value function and an optimal policy. We then prove the
almost sure convergence of the SOR Q-learning to SOR Q-values. Finally, through
numerical experiments, we show that SOR Q-learning is faster compared to the
standard Q-learning algorithm."
"The present study provides a comparative assessment of non-invasive sensors
as means of estimating the microbial contamination and time-on-shelf (i.e.
storage time) of leafy green vegetables, using a novel unified spectra analysis
workflow. Two fresh ready-to-eat green salads were used in the context of this
study for the purpose of evaluating the efficiency and practical application of
the presented workflow: rocket and baby spinach salads. The employed analysis
workflow consisted of robust data normalization, powerful feature selection
based on random forests regression, and selection of the number of partial
least squares regression coefficients in the training process by estimating the
knee-point on the explained variance plot. Training processes were based on
microbiological and spectral data derived during storage of green salad samples
at isothermal conditions (4, 8 and 12C), whereas testing was performed on data
during storage under dynamic temperature conditions (simulating real-life
temperature fluctuations in the food supply chain). Since an increasing
interest in the use of non-invasive sensors in food quality assessment has been
made evident in recent years, the unified spectra analysis workflow described
herein, by being based on the creation/usage of limited sized featured sets,
could be very useful in food-specific low-cost sensor development."
"This article briefly introduced Arthur and Vassilvitshii's work on
\textbf{k-means++} algorithm and further generalized the center initialization
process. It is found that choosing the most distant sample point from the
nearest center as new center can mostly have the same effect as the center
initialization process in the \textbf{k-means++} algorithm."
"This paper is a tutorial for eigenvalue and generalized eigenvalue problems.
We first introduce eigenvalue problem, eigen-decomposition (spectral
decomposition), and generalized eigenvalue problem. Then, we mention the
optimization problems which yield to the eigenvalue and generalized eigenvalue
problems. We also provide examples from machine learning, including principal
component analysis, kernel supervised principal component analysis, and Fisher
discriminant analysis, which result in eigenvalue and generalized eigenvalue
problems. Finally, we introduce the solutions to both eigenvalue and
generalized eigenvalue problems."
"Kriging is the predominant method used for spatial prediction, but relies on
the assumption that predictions are linear combinations of the observations.
Kriging often also relies on additional assumptions such as normality and
stationarity. We propose a more flexible spatial prediction method based on the
Nearest-Neighbor Neural Network (4N) process that embeds deep learning into a
geostatistical model. We show that the 4N process is a valid stochastic process
and propose a series of new ways to construct features to be used as inputs to
the deep learning model based on neighboring information. Our model framework
outperforms some existing state-of-art geostatistical modelling methods for
simulated non-Gaussian data and is applied to a massive forestry dataset."
"Network regularization is an effective tool for incorporating structural
prior knowledge to learn coherent models over networks, and has yielded
provably accurate estimates in applications ranging from spatial economics to
neuroimaging studies. Recently, there has been an increasing interest in
extending network regularization to the spatio-temporal case to accommodate the
evolution of networks. However, in both static and spatio-temporal cases,
missing or corrupted edge weights can compromise the ability of network
regularization to discover desired solutions. To address these gaps, we propose
a novel approach---{\it discrepancy-aware network regularization} (DANR)---that
is robust to inadequate regularizations and effectively captures model
evolution and structural changes over spatio-temporal networks. We develop a
distributed and scalable algorithm based on the alternating direction method of
multipliers (ADMM) to solve the proposed problem with guaranteed convergence to
global optimum solutions. Experimental results on both synthetic and real-world
networks demonstrate that our approach achieves improved performance on various
tasks, and enables interpretation of model changes in evolving networks."
"The linear submodular bandit problem was proposed to simultaneously address
diversified retrieval and online learning in a recommender system. If there is
no uncertainty, this problem is equivalent to a submodular maximization problem
under a cardinality constraint. However, in some situations, recommendation
lists should satisfy additional constraints such as budget constraints, other
than a cardinality constraint. Thus, motivated by diversified retrieval
considering budget constraints, we introduce a submodular bandit problem under
the intersection of $l$ knapsacks and a $k$-system constraint. Here $k$-system
constraints form a very general class of constraints including cardinality
constraints and the intersection of $k$ matroid constraints. To solve this
problem, we propose a non-greedy algorithm that adaptively focuses on a
standard or modified upper-confidence bound. We provide a high-probability
upper bound of an approximation regret, where the approximation ratio matches
that of a fast offline algorithm. Moreover, we perform experiments under
various combinations of constraints using a synthetic and two real-world
datasets and demonstrate that our proposed methods outperform the existing
baselines."
"In this paper, we study Combinatorial Semi-Bandits (CSB) that is an extension
of classic Multi-Armed Bandits (MAB) under Differential Privacy (DP) and
stronger Local Differential Privacy (LDP) setting. Since the server receives
more information from users in CSB, it usually causes additional dependence on
the dimension of data, which is a notorious side-effect for privacy preserving
learning. However for CSB under two common smoothness assumptions
\cite{kveton2015tight,chen2016combinatorial}, we show it is possible to remove
this side-effect. In detail, for $B_{\infty}$-bounded smooth CSB under either
$\varepsilon$-LDP or $\varepsilon$-DP, we prove the optimal regret bound is
$\Theta(\frac{mB^2_{\infty}\ln T } {\Delta\epsilon^2})$ or
$\tilde{\Theta}(\frac{mB^2_{\infty}\ln T} { \Delta\epsilon})$ respectively,
where $T$ is time period, $\Delta$ is the gap of rewards and $m$ is the number
of base arms, by proposing novel algorithms and matching lower bounds. For
$B_1$-bounded smooth CSB under $\varepsilon$-DP, we also prove the optimal
regret bound is $\tilde{\Theta}(\frac{mKB^2_1\ln T} {\Delta\epsilon})$ with
both upper bound and lower bound, where $K$ is the maximum number of feedback
in each round. All above results nearly match corresponding non-private optimal
rates, which imply there is no additional price for (locally) differentially
private CSB in above common settings."
"Traditional multi-view learning methods often rely on two assumptions: ($i$)
the samples in different views are well-aligned, and ($ii$) their
representations in latent space obey the same distribution. Unfortunately,
these two assumptions may be questionable in practice, which limits the
application of multi-view learning. In this work, we propose a hierarchical
optimal transport (HOT) method to mitigate the dependency on these two
assumptions. Given unaligned multi-view data, the HOT method penalizes the
sliced Wasserstein distance between the distributions of different views. These
sliced Wasserstein distances are used as the ground distance to calculate the
entropic optimal transport across different views, which explicitly indicates
the clustering structure of the views. The HOT method is applicable to both
unsupervised and semi-supervised learning, and experimental results show that
it performs robustly on both synthetic and real-world tasks."
"The use of blackbox solvers inside neural networks is a relatively new area
which aims to improve neural network performance by including proven, efficient
solvers for complex problems. Existing work has created methods for learning
networks with these solvers as components while treating them as a blackbox.
This work attempts to improve upon existing techniques by optimizing not only
over the primary loss function, but also over the performance of the solver
itself by using Time-cost Regularization. Additionally, we propose a method to
learn blackbox parameters such as which blackbox solver to use or the heuristic
function for a particular solver. We do this by introducing the idea of a
hyper-blackbox which is a blackbox around one or more internal blackboxes."
"Receiver operating characteristic (ROC) curve is an informative tool in
binary classification and Area Under ROC Curve (AUC) is a popular metric for
reporting performance of binary classifiers. In this paper, first we present a
comprehensive review of ROC curve and AUC metric. Next, we propose a modified
version of AUC that takes confidence of the model into account and at the same
time, incorporates AUC into Binary Cross Entropy (BCE) loss used for training a
Convolutional neural Network for classification tasks. We demonstrate this on
three datasets: MNIST, prostate MRI, and brain MRI. Furthermore, we have
published GenuineAI, a new python library, which provides the functions for
conventional AUC and the proposed modified AUC along with metrics including
sensitivity, specificity, recall, precision, and F1 for each point of the ROC
curve."
"We investigate the Plackett-Luce (PL) model based listwise learning-to-rank
(LTR) on data with partitioned preference, where a set of items are sliced into
ordered and disjoint partitions, but the ranking of items within a partition is
unknown. Given $N$ items with $M$ partitions, calculating the likelihood of
data with partitioned preference under the PL model has a time complexity of
$O(N+S!)$, where $S$ is the maximum size of the top $M-1$ partitions. This
computational challenge restrains most existing PL-based listwise LTR methods
to a special case of partitioned preference, top-$K$ ranking, where the exact
order of the top $K$ items is known. In this paper, we exploit a random utility
model formulation of the PL model, and propose an efficient numerical
integration approach for calculating the likelihood and its gradients with a
time complexity $O(N+S^3)$. We demonstrate that the proposed method outperforms
well-known LTR baselines and remains scalable through both simulation
experiments and applications to real-world eXtreme Multi-Label classification
tasks."
"Improving the robustness of neural nets in regression tasks is key to their
application in multiple domains. Deep learning-based approaches aim to achieve
this goal either by improving their prediction of specific values (i.e., point
prediction), or by producing prediction intervals (PIs) that quantify
uncertainty. We present PIVEN, a deep neural network for producing both a PI
and a value prediction. Our loss function expresses the value prediction as a
function of the upper and lower bounds, thus ensuring that it falls within the
interval without increasing model complexity. Moreover, our approach makes no
assumptions regarding data distribution within the PI, making its value
prediction more effective for various real-world problems. Experiments and
ablation tests on known benchmarks show that our approach produces tighter
uncertainty bounds than the current state-of-the-art approaches for producing
PIs, while maintaining comparable performance to the state-of-the-art approach
for value-prediction. Additionally, we go beyond previous work and include
large image datasets in our evaluation, where PIVEN is combined with modern
neural nets."
"The goal of supervised representation learning is to construct effective data
representations for prediction. Among all the characteristics of an ideal
nonparametric representation of high-dimensional complex data, sufficiency, low
dimensionality and disentanglement are some of the most essential ones. We
propose a deep dimension reduction approach to learning representations with
these characteristics. The proposed approach is a nonparametric generalization
of the sufficient dimension reduction method. We formulate the ideal
representation learning task as that of finding a nonparametric representation
that minimizes an objective function characterizing conditional independence
and promoting disentanglement at the population level. We then estimate the
target representation at the sample level nonparametrically using deep neural
networks. We show that the estimated deep nonparametric representation is
consistent in the sense that its excess risk converges to zero. Our extensive
numerical experiments using simulated and real benchmark data demonstrate that
the proposed methods have better performance than several existing dimension
reduction methods and the standard deep learning models in the context of
classification and regression."
"We prove that finding all globally optimal two-layer ReLU neural networks can
be performed by solving a convex optimization program with cone constraints.
Our analysis is novel, characterizes all optimal solutions, and does not
leverage duality-based analysis which was recently used to lift neural network
training into convex spaces. Given the set of solutions of our convex
optimization program, we show how to construct exactly the entire set of
optimal neural networks. We provide a detailed characterization of this optimal
set and its invariant transformations. As additional consequences of our convex
perspective, (i) we establish that Clarke stationary points found by stochastic
gradient descent correspond to the global optimum of a subsampled convex
problem (ii) we provide a polynomial-time algorithm for checking if a neural
network is a global minimum of the training loss (iii) we provide an explicit
construction of a continuous path between any neural network and the global
minimum of its sublevel set and (iv) characterize the minimal size of the
hidden layer so that the neural network optimization landscape has no spurious
valleys. Overall, we provide a rich framework for studying the landscape of
neural network training loss through convexity."
"This paper studies minimax optimization problems $\min_x \max_y f(x,y)$,
where $f(x,y)$ is $m_x$-strongly convex with respect to $x$, $m_y$-strongly
concave with respect to $y$ and $(L_x,L_{xy},L_y)$-smooth. Zhang et al.
provided the following lower bound of the gradient complexity for any
first-order method: $\Omega\Bigl(\sqrt{\frac{L_x}{m_x}+\frac{L_{xy}^2}{m_x
m_y}+\frac{L_y}{m_y}}\ln(1/\epsilon)\Bigr).$ This paper proposes a new
algorithm with gradient complexity upper bound
$\tilde{O}\Bigl(\sqrt{\frac{L_x}{m_x}+\frac{L\cdot L_{xy}}{m_x
m_y}+\frac{L_y}{m_y}}\ln\left(1/\epsilon\right)\Bigr),$ where
$L=\max\{L_x,L_{xy},L_y\}$. This improves over the best known upper bound
$\tilde{O}\left(\sqrt{\frac{L^2}{m_x m_y}} \ln^3\left(1/\epsilon\right)\right)$
by Lin et al. Our bound achieves linear convergence rate and tighter dependency
on condition numbers, especially when $L_{xy}\ll L$ (i.e., when the interaction
between $x$ and $y$ is weak). Via reduction, our new bound also implies
improved bounds for strongly convex-concave and convex-concave minimax
optimization problems. When $f$ is quadratic, we can further improve the upper
bound, which matches the lower bound up to a small sub-polynomial factor."
"We study without-replacement SGD for solving finite-sum optimization
problems. Specifically, depending on how the indices of the finite-sum are
shuffled, we consider the RandomShuffle (shuffle at the beginning of each
epoch) and SingleShuffle (shuffle only once) algorithms. First, we establish
minimax optimal convergence rates of these algorithms up to poly-log factors.
Notably, our analysis is general enough to cover gradient dominated nonconvex
costs, and does not rely on the convexity of individual component functions
unlike existing optimal convergence results. Secondly, assuming convexity of
the individual components, we further sharpen the tight convergence results for
RandomShuffle by removing the drawbacks common to all prior arts: large number
of epochs required for the results to hold, and extra poly-log factor gaps to
the lower bound."
"We investigate the stochastic Thresholding Bandit problem (TBP) under several
shape constraints. On top of (i) the vanilla, unstructured TBP, we consider the
case where (ii) the sequence of arm's means $(\mu_k)_k$ is monotonically
increasing MTBP, (iii) the case where $(\mu_k)_k$ is unimodal UTBP and (iv) the
case where $(\mu_k)_k$ is concave CTBP. In the TBP problem the aim is to
output, at the end of the sequential game, the set of arms whose means are
above a given threshold. The regret is the highest gap between a misclassified
arm and the threshold. In the fixed budget setting, we provide problem
independent minimax rates for the expected regret in all settings, as well as
associated algorithms. We prove that the minimax rates for the regret are (i)
$\sqrt{\log(K)K/T}$ for TBP, (ii) $\sqrt{\log(K)/T}$ for MTBP, (iii)
$\sqrt{K/T}$ for UTBP and (iv) $\sqrt{\log\log K/T}$ for CTBP, where $K$ is the
number of arms and $T$ is the budget. These rates demonstrate that the
dependence on $K$ of the minimax regret varies significantly depending on the
shape constraint. This highlights the fact that the shape constraints modify
fundamentally the nature of the TBP."
"Graph Neural Networks (GNNs) have recently become increasingly popular due to
their ability to learn complex systems of relations or interactions arising in
a broad spectrum of problems ranging from biology and particle physics to
social networks and recommendation systems. Despite the plethora of different
models for deep learning on graphs, few approaches have been proposed thus far
for dealing with graphs that present some sort of dynamic nature (e.g. evolving
features or connectivity over time). In this paper, we present Temporal Graph
Networks (TGNs), a generic, efficient framework for deep learning on dynamic
graphs represented as sequences of timed events. Thanks to a novel combination
of memory modules and graph-based operators, TGNs are able to significantly
outperform previous approaches being at the same time more computationally
efficient. We furthermore show that several previous models for learning on
dynamic graphs can be cast as specific instances of our framework. We perform a
detailed ablation study of different components of our framework and devise the
best configuration that achieves state-of-the-art performance on several
transductive and inductive prediction tasks for dynamic graphs."
"The principle of optimism in the face of uncertainty is prevalent throughout
sequential decision making problems such as multi-armed bandits and
reinforcement learning (RL). To be successful, an optimistic RL algorithm must
over-estimate the true value function (optimism) but not by so much that it is
inaccurate (estimation error). In the tabular setting, many state-of-the-art
methods produce the required optimism through approaches which are intractable
when scaling to deep RL. We re-interpret these scalable optimistic model-based
algorithms as solving a tractable noise augmented MDP. This formulation
achieves a competitive regret bound: $\tilde{\mathcal{O}}(
|\mathcal{S}|H\sqrt{|\mathcal{A}| T } )$ when augmenting using Gaussian noise,
where $T$ is the total number of environment steps. We also explore how this
trade-off changes in the deep RL setting, where we show empirically that
estimation error is significantly more troublesome. However, we also show that
if this error is reduced, optimistic model-based RL algorithms can match
state-of-the-art performance in continuous control problems."
"Given multiple input signals, how can we infer node importance in a knowledge
graph (KG)? Node importance estimation is a crucial and challenging task that
can benefit a lot of applications including recommendation, search, and query
disambiguation. A key challenge towards this goal is how to effectively use
input from different sources. On the one hand, a KG is a rich source of
information, with multiple types of nodes and edges. On the other hand, there
are external input signals, such as the number of votes or pageviews, which can
directly tell us about the importance of entities in a KG. While several
methods have been developed to tackle this problem, their use of these external
signals has been limited as they are not designed to consider multiple signals
simultaneously. In this paper, we develop an end-to-end model MultiImport,
which infers latent node importance from multiple, potentially overlapping,
input signals. MultiImport is a latent variable model that captures the
relation between node importance and input signals, and effectively learns from
multiple signals with potential conflicts. Also, MultiImport provides an
effective estimator based on attentive graph neural networks. We ran
experiments on real-world KGs to show that MultiImport handles several
challenges involved with inferring node importance from multiple input signals,
and consistently outperforms existing methods, achieving up to 23.7% higher
NDCG@100 than the state-of-the-art method."
"In the absence of large labelled datasets, self-supervised learning
techniques can boost performance by learning useful representations from
unlabelled data, which is often more readily available. However, there is often
a domain shift between the unlabelled collection and the downstream target
problem data. We show that by learning Bayesian instance weights for the
unlabelled data, we can improve the downstream classification accuracy by
prioritising the most useful instances. Additionally, we show that the training
time can be reduced by discarding unnecessary datapoints. Our method,
BetaDataWeighter is evaluated using the popular self-supervised rotation
prediction task on STL-10 and Visual Decathlon. We compare to related instance
weighting schemes, both hand-designed heuristics and meta-learning, as well as
conventional self-supervised learning. BetaDataWeighter achieves both the
highest average accuracy and rank across datasets, and on STL-10 it prunes up
to 78% of unlabelled images without significant loss in accuracy, corresponding
to over 50% reduction in training time."
"In this paper, we study meta learning for support (i.e., the set of non-zero
entries) recovery in high-dimensional precision matrix estimation where we
reduce the sufficient sample complexity in a novel task with the information
learned from other auxiliary tasks. In our setup, each task has a different
random true precision matrix, each with a possibly different support. We assume
that the union of the supports of all the true precision matrices (i.e., the
true support union) is small in size. We propose to pool all the samples from
different tasks, and \emph{improperly} estimate a single precision matrix by
minimizing the $\ell_1$-regularized log-determinant Bregman divergence. We show
that with high probability, the support of the \emph{improperly} estimated
single precision matrix is equal to the true support union, provided a
sufficient number of samples per task $n \in O((\log N)/K)$, for
$N$-dimensional vectors and $K$ tasks. That is, one requires less samples per
task when more tasks are available. We prove a matching information-theoretic
lower bound for the necessary number of samples, which is $n \in \Omega((\log
N)/K)$, and thus, our algorithm is minimax optimal. Then for the novel task, we
prove that the minimization of the $\ell_1$-regularized log-determinant Bregman
divergence with the additional constraint that the support is a subset of the
estimated support union could reduce the sufficient sample complexity of
successful support recovery to $O(\log(|S_{\text{off}}|))$ where
$|S_{\text{off}}|$ is the number of off-diagonal elements in the support union
and is much less than $N$ for sparse matrices. We also prove a matching
information-theoretic lower bound of $\Omega(\log(|S_{\text{off}}|))$ for the
necessary number of samples. Synthetic experiments validate our theory."
"Dynamic time warping (DTW) is a useful method for aligning, comparing and
combining time series, but it requires them to live in comparable spaces. In
this work, we consider a setting in which time series live on different spaces
without a sensible ground metric, causing DTW to become ill-defined. To
alleviate this, we propose Gromov dynamic time warping (GDTW), a distance
between time series on potentially incomparable spaces that avoids the
comparability requirement by instead considering intra-relational geometry. We
demonstrate its effectiveness at aligning, combining and comparing time series
living on incomparable spaces. We further propose a smoothed version of GDTW as
a differentiable loss and assess its properties in a variety of settings,
including barycentric averaging, generative modeling and imitation learning."
"Inaccurate records of inventory occur frequently, and by some measures cost
retailers approximately 4% in annual sales. Detecting inventory inaccuracies
manually is cost-prohibitive, and existing algorithmic solutions rely almost
exclusively on learning from longitudinal data, which is insufficient in the
dynamic environment induced by modern retail operations. Instead, we propose a
solution based on cross-sectional data over stores and SKUs, observing that
detecting inventory inaccuracies can be viewed as a problem of identifying
anomalies in a (low-rank) Poisson matrix. State-of-the-art approaches to
anomaly detection in low-rank matrices apparently fall short. Specifically,
from a theoretical perspective, recovery guarantees for these approaches
require that non-anomalous entries be observed with vanishingly small noise
(which is not the case in our problem, and indeed in many applications).
  So motivated, we propose a conceptually simple entry-wise approach to anomaly
detection in low-rank Poisson matrices. Our approach accommodates a general
class of probabilistic anomaly models. We show that the cost incurred by our
algorithm approaches that of an optimal algorithm at a min-max optimal rate.
Using synthetic data and real data from a consumer goods retailer, we show that
our approach provides up to a 10x cost reduction over incumbent approaches to
anomaly detection. Along the way, we build on recent work that seeks entry-wise
error guarantees for matrix completion, establishing such guarantees for
sub-exponential matrices, a result of independent interest."
"Normalization Layers (NLs) are widely used in modern deep-learning
architectures. Despite their apparent simplicity, their effect on optimization
is not yet fully understood. This paper introduces a spherical framework to
study the optimization of neural networks with NLs from a geometric
perspective. Concretely, the radial invariance of groups of parameters, such as
filters for convolutional neural networks, allows to translate the optimization
steps on the $L_2$ unit hypersphere. This formulation and the associated
geometric interpretation shed new light on the training dynamics. Firstly, the
first effective learning rate expression of Adam is derived. Then the
demonstration that, in the presence of NLs, performing Stochastic Gradient
Descent (SGD) alone is actually equivalent to a variant of Adam constrained to
the unit hypersphere, stems from the framework. Finally, this analysis outlines
phenomena that previous variants of Adam act on and their importance in the
optimization process are experimentally validated."
"Recurrent neural networks (RNNs) are instrumental in modelling sequential and
time-series data. Yet, when using RNNs to inform decision-making, predictions
by themselves are not sufficient; we also need estimates of predictive
uncertainty. Existing approaches for uncertainty quantification in RNNs are
based predominantly on Bayesian methods; these are computationally prohibitive,
and require major alterations to the RNN architecture and training.
Capitalizing on ideas from classical jackknife resampling, we develop a
frequentist alternative that: (a) does not interfere with model training or
compromise its accuracy, (b) applies to any RNN architecture, and (c) provides
theoretical coverage guarantees on the estimated uncertainty intervals. Our
method derives predictive uncertainty from the variability of the (jackknife)
sampling distribution of the RNN outputs, which is estimated by repeatedly
deleting blocks of (temporally-correlated) training data, and collecting the
predictions of the RNN re-trained on the remaining data. To avoid exhaustive
re-training, we utilize influence functions to estimate the effect of removing
training data blocks on the learned RNN parameters. Using data from a critical
care setting, we demonstrate the utility of uncertainty quantification in
sequential decision-making."
"Autoencoders have been widely used for dimensional reduction and feature
extraction. Various types of autoencoders have been proposed by introducing
regularization terms. Most of these regularizations improve representation
learning by constraining the weights in the encoder part, which maps input into
hidden nodes and affects the generation of features. In this study, we show
that a constraint to the decoder can also significantly improve its performance
because the decoder determines how the latent variables contribute to the
reconstruction of input. Inspired by the structural modal analysis method in
mechanical engineering, a new modal autoencoder (MAE) is proposed by
othogonalising the columns of the readout weight matrix. The new regularization
helps to disentangle explanatory factors of variation and forces the MAE to
extract fundamental modes in data. The learned representations are functionally
independent in the reconstruction of input and perform better in consecutive
classification tasks. The results were validated on the MNIST variations and
USPS classification benchmark suite. Comparative experiments clearly show that
the new algorithm has a surprising advantage. The new MAE introduces a very
simple training principle for autoencoders and could be promising for the
pre-training of deep neural networks."
"Recent works have empirically shown that there exist adversarial examples
that can be hidden from neural network interpretability (namely, making network
interpretation maps visually similar), or interpretability is itself
susceptible to adversarial attacks. In this paper, we theoretically show that
with a proper measurement of interpretation, it is actually difficult to
prevent prediction-evasion adversarial attacks from causing interpretation
discrepancy, as confirmed by experiments on MNIST, CIFAR-10 and Restricted
ImageNet. Spurred by that, we develop an interpretability-aware defensive
scheme built only on promoting robust interpretation (without the need for
resorting to adversarial loss minimization). We show that our defense achieves
both robust classification and robust interpretation, outperforming
state-of-the-art adversarial training methods against attacks of large
perturbation in particular."
"It is well-known that information loss can occur in the classic and simple
Q-learning algorithm. Entropy-based policy search methods were introduced to
replace Q-learning and to design algorithms that are more robust against
information loss. We conjecture that the reduction in performance during
prolonged training sessions of Q-learning is caused by a loss of information,
which is non-transparent when only examining the cumulative reward without
changing the Q-learning algorithm itself. We introduce Differential Entropy of
Q-tables (DE-QT) as an external information loss detector to the Q-learning
algorithm. The behaviour of DE-QT over training episodes is analyzed to find an
appropriate stopping criterion during training. The results reveal that DE-QT
can detect the most appropriate stopping point, where a balance between a high
success rate and a high efficiency is met for classic Q-Learning algorithm."
"The Metropolis algorithm is arguably the most fundamental Markov chain Monte
Carlo (MCMC) method. But the algorithm is not guaranteed to converge to the
desired distribution in the case of multivariate binary distributions (e.g.,
Ising models or stochastic neural networks such as Boltzmann machines) if the
variables (sites or neurons) are updated in a fixed order, a setting commonly
used in practice. The reason is that the corresponding Markov chain may not be
irreducible. We propose a modified Metropolis transition operator that behaves
almost always identically to the standard Metropolis operator and prove that it
ensures irreducibility and convergence to the limiting distribution in the
multivariate binary case with fixed-order updates. The result provides an
explanation for the behaviour of Metropolis MCMC in that setting and closes a
long-standing theoretical gap. We experimentally studied the standard and
modified Metropolis operator for models were they actually behave differently.
If the standard algorithm also converges, the modified operator exhibits
similar (if not better) performance in terms of convergence speed."
"We propose Fuzzy Jaccard Index (FUJI) -- a scale-invariant score for
assessment of the similarity between two ranked/ordered lists. FUJI improves
upon the Jaccard index by incorporating a membership function which takes into
account the particular ranks, thus producing both more stable and more accurate
similarity estimates. We provide theoretical insights into the properties of
the FUJI score as well as propose an efficient algorithm for computing it. We
also present empirical evidence of its performance on different synthetic
scenarios. Finally, we demonstrate its utility in a typical machine learning
setting -- comparing feature ranking lists relevant to a given machine learning
task. In real-life, and in particular high-dimensional domains, where only a
small percentage of the whole feature space might be relevant, a robust and
confident feature ranking leads to interpretable findings as well as efficient
computation and good predictive performance. In such cases, FUJI correctly
distinguishes between existing feature ranking approaches, while being more
robust and efficient than the benchmark similarity scores."
"The application of data mining, machine learning and artificial intelligence
techniques in the field of diagnostics is not a new concept, and these
techniques have been very successfully applied in a variety of applications,
especially in dermatology and cancer research. But, in the case of medical
problems that involve tests resulting in true or false (binary classification),
the data generally has a class imbalance with samples majorly belonging to one
class (ex: a patient undergoes a regular test and the results are false). Such
disparity in data causes problems when trying to model predictive systems on
the data. In critical applications like diagnostics, this class imbalance
cannot be overlooked and must be given extra attention. In our research, we
depict how we can handle this class imbalance through neural networks using a
discriminative model and contrastive loss using a Siamese neural network
structure. Such a model does not work on a probability-based approach to
classify samples into labels. Instead it uses a distance-based approach to
differentiate between samples classified under different labels. The code is
available at https://tinyurl.com/DiscriminativeCHD/"
"Missing value problem in spatiotemporal traffic data has long been a
challenging topic, in particular for large-scale and high-dimensional data with
complex missing mechanisms and diverse degrees of missingness. Recent studies
based on tensor nuclear norm have demonstrated the superiority of tensor
learning in imputation tasks by effectively characterizing the complex
correlations/dependencies in spatiotemporal data. However, despite the
promising results, these approaches do not scale well to large data tensors. In
this paper, we focus on addressing the missing data imputation problem for
large-scale spatiotemporal traffic data. To achieve both high accuracy and
efficiency, we develop a scalable tensor learning model -- Low-Tubal-Rank
Smoothing Tensor Completion (LSTC-Tubal) -- based on the existing framework of
Low-Rank Tensor Completion, which is well-suited for spatiotemporal traffic
data that is characterized by multidimensional structure of location$\times$
time of day $\times$ day. In particular, the proposed LSTC-Tubal model involves
a scalable tensor nuclear norm minimization scheme by integrating linear
unitary transformation. Therefore, tensor nuclear norm minimization can be
solved by singular value thresholding on the transformed matrix of each day
while the day-to-day correlation can be effectively preserved by the unitary
transform matrix. We compare LSTC-Tubal with state-of-the-art baseline models,
and find that LSTC-Tubal can achieve competitive accuracy with a significantly
lower computational cost. In addition, the LSTC-Tubal will also benefit other
tasks in modeling large-scale spatiotemporal traffic data, such as
network-level traffic forecasting."
"Open set classification (OSC) tackles the problem of determining whether the
data are in-class or out-of-class during inference, when only provided with a
set of in-class examples at training time. Traditional OSC methods usually
train discriminative or generative models with in-class data, then utilize the
pre-trained models to classify test data directly. However, these methods
always suffer from embedding confusion problem, i.e., partial out-of-class
instances are mixed with in-class ones of similar semantics, making it
difficult to classify. To solve this problem, we unify semi-supervised learning
to develop a novel OSC algorithm, S2OSC, that incorporates out-of-class
instances filtering and model re-training in a transductive manner. In detail,
given a pool of newly coming test data, S2OSC firstly filters distinct
out-of-class instances using the pre-trained model, and annotates super-class
for them. Then, S2OSC trains a holistic classification model by combing
in-class and out-of-class labeled data and remaining unlabeled test data in
semi-supervised paradigm, which also integrates pre-trained model for knowledge
distillation to further separate mixed instances. Despite its simplicity, the
experimental results show that S2OSC achieves state-of-the-art performance
across a variety of OSC tasks, including 85.4% of F1 on CIFAR-10 with only 300
pseudo-labels. We also demonstrate how S2OSC can be expanded to incremental OSC
setting effectively with streaming data."
"In domains where data are sensitive or private, there is great value in
methods that can learn in a distributed manner without the data ever leaving
the local devices. In light of this need, federated learning has emerged as a
popular training paradigm. However, many federated learning approaches trade
transmitting data for communicating updated weight parameters for each local
device. Therefore, a successful breach that would have otherwise directly
compromised the data instead grants whitebox access to the local model, which
opens the door to a number of attacks, including exposing the very data
federated learning seeks to protect. Additionally, in distributed scenarios,
individual client devices commonly exhibit high statistical heterogeneity. Many
common federated approaches learn a single global model; while this may do well
on average, performance degrades when the i.i.d. assumption is violated,
underfitting individuals further from the mean, and raising questions of
fairness. To address these issues, we propose Weight Anonymized Factorization
for Federated Learning (WAFFLe), an approach that combines the Indian Buffet
Process with a shared dictionary of weight factors for neural networks.
Experiments on MNIST, FashionMNIST, and CIFAR-10 demonstrate WAFFLe's
significant improvement to local test performance and fairness while
simultaneously providing an extra layer of security."
"Recent work proposed the computation of so-called PI-explanations of Naive
Bayes Classifiers (NBCs). PI-explanations are subset-minimal sets of
feature-value pairs that are sufficient for the prediction, and have been
computed with state-of-the-art exact algorithms that are worst-case exponential
in time and space. In contrast, we show that the computation of one
PI-explanation for an NBC can be achieved in log-linear time, and that the same
result also applies to the more general class of linear classifiers.
Furthermore, we show that the enumeration of PI-explanations can be obtained
with polynomial delay. Experimental results demonstrate the performance gains
of the new algorithms when compared with earlier work. The experimental results
also investigate ways to measure the quality of heuristic explanations"
"Anomaly Detectors are trained on healthy operating condition data and raise
an alarm when the measured samples deviate from the training data distribution.
This means that the samples used to train the model should be sufficient in
quantity and representative of the healthy operating conditions. But for
industrial systems subject to changing operating conditions, acquiring such
comprehensive sets of samples requires a long collection period and delay the
point at which the anomaly detector can be trained and put in operation.
  A solution to this problem is to perform unsupervised transfer learning
(UTL), to transfer complementary data between different units. In the
literature however, UTL aims at finding common structure between the datasets,
to perform clustering or dimensionality reduction. Yet, the task of
transferring and combining complementary training data has not been studied.
  Our proposed framework is designed to transfer complementary operating
conditions between different units in a completely unsupervised way to train
more robust anomaly detectors. It differs, thereby, from other unsupervised
transfer learning works as it focuses on a one-class classification problem.
The proposed methodology enables to detect anomalies in operating conditions
only experienced by other units. The proposed end-to-end framework uses
adversarial deep learning to ensure alignment of the different units'
distributions. The framework introduces a new loss, inspired by a
dimensionality reduction tool, to enforce the conservation of the inherent
variability of each dataset, and uses state-of-the art once-class approach to
detect anomalies. We demonstrate the benefit of the proposed framework using
three open source datasets."
"The vulnerability of deep neural networks (DNNs) to adversarial attack, which
is an attack that can mislead state-of-the-art classifiers into making an
incorrect classification with high confidence by deliberately perturbing the
original inputs, raises concerns about the robustness of DNNs to such attacks.
Adversarial training, which is the main heuristic method for improving
adversarial robustness and the first line of defense against adversarial
attacks, requires many sample-by-sample calculations to increase training size
and is usually insufficiently strong for an entire network. This paper provides
a new perspective on the issue of adversarial robustness, one that shifts the
focus from the network as a whole to the critical part of the region close to
the decision boundary corresponding to a given class. From this perspective, we
propose a method to generate a single but image-agnostic adversarial
perturbation that carries the semantic information implying the directions to
the fragile parts on the decision boundary and causes inputs to be
misclassified as a specified target. We call the adversarial training based on
such perturbations ""region adversarial training"" (RAT), which resembles
classical adversarial training but is distinguished in that it reinforces the
semantic information missing in the relevant regions. Experimental results on
the MNIST and CIFAR-10 datasets show that this approach greatly improves
adversarial robustness even using a very small dataset from the training data;
moreover, it can defend against FGSM adversarial attacks that have a completely
different pattern from the model seen during retraining."
"Multi-task learning (MTL) can improve performance on a task by sharing
representations with one or more related auxiliary-tasks. Usually, MTL-networks
are trained on a composite loss function formed by a constant weighted
combination of the separate task losses. In practice, constant loss weights
lead to poor results for two reasons: (i) the relevance of the auxiliary tasks
can gradually drift throughout the learning process; (ii) for mini-batch based
optimisation, the optimal task weights vary significantly from one update to
the next depending on mini-batch sample composition. We introduce HydaLearn, an
intelligent weighting algorithm that connects main-task gain to the individual
task gradients, in order to inform dynamic loss weighting at the mini-batch
level, addressing i and ii. Using HydaLearn, we report performance increases on
synthetic data, as well as on two supervised learning domains."
"Understanding generalization in deep learning is arguably one of the most
important questions in deep learning. Deep learning has been successfully
adopted to a large number of problems ranging from pattern recognition to
complex decision making, but many recent researchers have raised many concerns
about deep learning, among which the most important is generalization. Despite
numerous attempts, conventional statistical learning approaches have yet been
able to provide a satisfactory explanation on why deep learning works. A recent
line of works aims to address the problem by trying to predict the
generalization performance through complexity measures. In this competition, we
invite the community to propose complexity measures that can accurately predict
generalization of models. A robust and general complexity measure would
potentially lead to a better understanding of deep learning's underlying
mechanism and behavior of deep models on unseen data, or shed light on better
generalization bounds. All these outcomes will be important for making deep
learning more robust and reliable."
"Asymmetric binary classification problems, in which the type I and II errors
have unequal severity, are ubiquitous in real-world applications. To handle
such asymmetry, researchers have developed the cost-sensitive and
Neyman-Pearson paradigms for training classifiers to control the more severe
type of classification error, say the type I error. The cost-sensitive paradigm
is widely used and has straightforward implementations that do not require
sample splitting; however, it demands an explicit specification of the costs of
the type I and II errors, and an open question is what specification can
guarantee a high-probability control on the population type I error. In
contrast, the Neyman-Pearson paradigm can train classifiers to achieve a
high-probability control of the population type I error, but it relies on
sample splitting that reduces the effective training sample size. Since the two
paradigms have complementary strengths, it is reasonable to combine their
strengths for classifier construction. In this work, we for the first time
study the methodological connections between the two paradigms, and we develop
the TUBE-CS algorithm to bridge the two paradigms from the perspective of
controlling the population type I error."
"Stochastic generative models enable us to capture the geometric structure of
a data manifold lying in a high dimensional space through a Riemannian metric
in the latent space. However, its practical use is rather limited mainly due to
inevitable complexity. In this work we propose a surrogate conformal Riemannian
metric in the latent space of a generative model that is simple, efficient and
robust. This metric is based on a learnable prior that we propose to learn
using a basic energy-based model. We theoretically analyze the behavior of the
proposed metric and show that it is sensible to use in practice. We demonstrate
experimentally the efficiency and robustness, as well as the behavior of the
new approximate metric. Also, we show the applicability of the proposed
methodology for data analysis in the life sciences."
"Being one of the most popular generative framework, variational
autoencoders(VAE) are known to suffer from a phenomenon termed posterior
collapse, i.e. the latent variational distributions collapse to the prior,
especially when a strong decoder network is used. In this work, we analyze the
latent representation of collapsed VAEs, and proposed a novel model, neighbor
embedding VAE(NE-VAE), which explicitly constraints the encoder to encode
inputs close in the input space to be close in the latent space. We observed
that for VAE variants that report similar ELBO, KL divergence or even mutual
information scores may still behave quite differently in the latent
organization. In our experiments, NE-VAE can produce qualitatively different
latent representations with majority of the latent dimensions remained active,
which may benefit downstream latent space optimization tasks. NE-VAE can
prevent posterior collapse to a much greater extent than it's predecessors, and
can be easily plugged into any autoencoder framework, without introducing
addition model components and complex training routines."
"Policy optimization methods remain a powerful workhorse in empirical
Reinforcement Learning (RL), with a focus on neural policies that can easily
reason over complex and continuous state and/or action spaces. Theoretical
understanding of strategic exploration in policy-based methods with non-linear
function approximation, however, is largely missing. In this paper, we address
this question by designing ENIAC, an actor-critic method that allows non-linear
function approximation in the critic. We show that under certain assumptions,
e.g., a bounded eluder dimension $d$ for the critic class, the learner finds a
near-optimal policy in $O(\poly(d))$ exploration rounds. The method is robust
to model misspecification and strictly extends existing works on linear
function approximation. We also develop some computational optimizations of our
approach with slightly worse statistical guarantees and an empirical adaptation
building on existing deep RL tools. We empirically evaluate this adaptation and
show that it outperforms prior heuristics inspired by linear methods,
establishing the value via correctly reasoning about the agent's uncertainty
under non-linear function approximation."
"Big spatio-temporal datasets, available through both open and administrative
data sources, offer significant potential for social science research. The
magnitude of the data allows for increased resolution and analysis at
individual level. While there are recent advances in forecasting techniques for
highly granular temporal data, little attention is given to segmenting the time
series and finding homogeneous patterns. In this paper, it is proposed to
estimate behavioral profiles of individuals' activities over time using
Gaussian Process-based models. In particular, the aim is to investigate how
individuals or groups may be clustered according to the model parameters. Such
a Bayesian non-parametric method is then tested by looking at the
predictability of the segments using a combination of models to fit different
parts of the temporal profiles. Model validity is then tested on a set of
holdout data. The dataset consists of half hourly energy consumption records
from smart meters from more than 100,000 households in the UK and covers the
period from 2015 to 2016. The methodological approach developed in the paper
may be easily applied to datasets of similar structure and granularity, for
example social media data, and may lead to improved accuracy in the prediction
of social dynamics and behavior."
"Increasingly many real world tasks involve data in multiple modalities or
views. This has motivated the development of many effective algorithms for
learning a common latent space to relate multiple domains. However, most
existing cross-view learning algorithms assume access to paired data for
training. Their applicability is thus limited as the paired data assumption is
often violated in practice: many tasks have only a small subset of data
available with pairing annotation, or even no paired data at all. In this paper
we introduce Deep Matching Autoencoders (DMAE), which learn a common latent
space and pairing from unpaired multi-modal data. Specifically we formulate
this as a cross-domain representation learning and object matching problem. We
simultaneously optimise parameters of representation learning auto-encoders and
the pairing of unpaired multi-modal data. This framework elegantly spans the
full regime from fully supervised, semi-supervised, and unsupervised (no paired
data) multi-modal learning. We show promising results in image captioning, and
on a new task that is uniquely enabled by our methodology: unsupervised
classifier learning."
"Facial attribute editing aims to manipulate single or multiple attributes of
a face image, i.e., to generate a new face with desired attributes while
preserving other details. Recently, generative adversarial net (GAN) and
encoder-decoder architecture are usually incorporated to handle this task with
promising results. Based on the encoder-decoder architecture, facial attribute
editing is achieved by decoding the latent representation of the given face
conditioned on the desired attributes. Some existing methods attempt to
establish an attribute-independent latent representation for further attribute
editing. However, such attribute-independent constraint on the latent
representation is excessive because it restricts the capacity of the latent
representation and may result in information loss, leading to over-smooth and
distorted generation. Instead of imposing constraints on the latent
representation, in this work we apply an attribute classification constraint to
the generated image to just guarantee the correct change of desired attributes,
i.e., to ""change what you want"". Meanwhile, the reconstruction learning is
introduced to preserve attribute-excluding details, in other words, to ""only
change what you want"". Besides, the adversarial learning is employed for
visually realistic editing. These three components cooperate with each other
forming an effective framework for high quality facial attribute editing,
referred as AttGAN. Furthermore, our method is also directly applicable for
attribute intensity control and can be naturally extended for attribute style
manipulation. Experiments on CelebA dataset show that our method outperforms
the state-of-the-arts on realistic attribute editing with facial details well
preserved."
"The gamma distribution arises frequently in Bayesian models, but there is not
an easy-to-use conjugate prior for the shape parameter of a gamma. This
inconvenience is usually dealt with by using either Metropolis-Hastings moves,
rejection sampling methods, or numerical integration. However, in models with a
large number of shape parameters, these existing methods are slower or more
complicated than one would like, making them burdensome in practice. It turns
out that the full conditional distribution of the gamma shape parameter is well
approximated by a gamma distribution, even for small sample sizes, when the
prior on the shape parameter is also a gamma distribution. This article
introduces a quick and easy algorithm for finding a gamma distribution that
approximates the full conditional distribution of the shape parameter. We
empirically demonstrate the speed and accuracy of the approximation across a
wide range of conditions. If exactness is required, the approximation can be
used as a proposal distribution for Metropolis-Hastings."
"The discovery of processes for the synthesis of new materials involves many
decisions about process design, operation, and material properties.
Experimentation is crucial but as complexity increases, exploration of
variables can become impractical using traditional combinatorial approaches. We
describe an iterative method which uses machine learning to optimise process
development, incorporating multiple qualitative and quantitative objectives. We
demonstrate the method with a novel fluid processing platform for synthesis of
short polymer fibers, and show how the synthesis process can be efficiently
directed to achieve material and process objectives."
"Finding optimal correction of errors in generic stabilizer codes is a
computationally hard problem, even for simple noise models. While this task can
be simplified for codes with some structure, such as topological stabilizer
codes, developing good and efficient decoders still remains a challenge. In our
work, we systematically study a very versatile class of decoders based on
feedforward neural networks. To demonstrate adaptability, we apply neural
decoders to the triangular color and toric codes under various noise models
with realistic features, such as spatially-correlated errors. We report that
neural decoders provide significant improvement over leading efficient decoders
in terms of the error-correction threshold. Using neural networks simplifies
the process of designing well-performing decoders, and does not require prior
knowledge of the underlying noise model."
"Exploratory analysis over network data is often limited by the ability to
efficiently calculate graph statistics, which can provide a model-free
understanding of the macroscopic properties of a network. We introduce a
framework for estimating the graphlet count---the number of occurrences of a
small subgraph motif (e.g. a wedge or a triangle) in the network. For massive
graphs, where accessing the whole graph is not possible, the only viable
algorithms are those that make a limited number of vertex neighborhood queries.
We introduce a Monte Carlo sampling technique for graphlet counts, called {\em
Lifting}, which can simultaneously sample all graphlets of size up to $k$
vertices for arbitrary $k$. This is the first graphlet sampling method that can
provably sample every graphlet with positive probability and can sample
graphlets of arbitrary size $k$. We outline variants of lifted graphlet counts,
including the ordered, unordered, and shotgun estimators, random walk starts,
and parallel vertex starts. We prove that our graphlet count updates are
unbiased for the true graphlet count and have a controlled variance for all
graphlets. We compare the experimental performance of lifted graphlet counts to
the state-of-the art graphlet sampling procedures: Waddling and the pairwise
subgraph random walk."
"Model discrimination identifies a mathematical model that usefully explains
and predicts a given system's behaviour. Researchers will often have several
models, i.e. hypotheses, about an underlying system mechanism, but insufficient
experimental data to discriminate between the models, i.e. discard inaccurate
models. Given rival mathematical models and an initial experimental data set,
optimal design of experiments suggests maximally informative experimental
observations that maximise a design criterion weighted by prediction
uncertainty. The model uncertainty requires gradients, which may not be readily
available for black-box models. This paper (i) proposes a new design criterion
using the Jensen-R\'enyi divergence, and (ii) develops a novel method replacing
black-box models with Gaussian process surrogates. Using the surrogates, we
marginalise out the model parameters with approximate inference. Results show
these contributions working well for both classical and new test instances. We
also (iii) introduce and discuss GPdoemd, the open-source implementation of the
Gaussian process surrogate method."
"In many situations, the choice of an adequate similarity measure or metric on
the feature space dramatically determines the performance of machine learning
methods. Building automatically such measures is the specific purpose of
metric/similarity learning. In Vogel et al. (2018), similarity learning is
formulated as a pairwise bipartite ranking problem: ideally, the larger the
probability that two observations in the feature space belong to the same class
(or share the same label), the higher the similarity measure between them. From
this perspective, the ROC curve is an appropriate performance criterion and it
is the goal of this article to extend recursive tree-based ROC optimization
techniques in order to propose efficient similarity learning algorithms. The
validity of such iterative partitioning procedures in the pairwise setting is
established by means of results pertaining to the theory of U-processes and
from a practical angle, it is discussed at length how to implement them by
means of splitting rules specifically tailored to the similarity learning task.
Beyond these theoretical/methodological contributions, numerical experiments
are displayed and provide strong empirical evidence of the performance of the
algorithmic approaches we propose."
"We introduce a new molecular dataset, named Alchemy, for developing machine
learning models useful in chemistry and material science. As of June 20th 2019,
the dataset comprises of 12 quantum mechanical properties of 119,487 organic
molecules with up to 14 heavy atoms, sampled from the GDB MedChem database. The
Alchemy dataset expands the volume and diversity of existing molecular
datasets. Our extensive benchmarks of the state-of-the-art graph neural network
models on Alchemy clearly manifest the usefulness of new data in validating and
developing machine learning models for chemistry and material science. We
further launch a contest to attract attentions from researchers in the related
fields. More details can be found on the contest website
\footnote{https://alchemy.tencent.com}. At the time of benchamrking experiment,
we have generated 119,487 molecules in our Alchemy dataset. More molecular
samples are generated since then. Hence, we provide a list of molecules used in
the reported benchmarks."
"This is a detailed tutorial paper which explains the Fisher discriminant
Analysis (FDA) and kernel FDA. We start with projection and reconstruction.
Then, one- and multi-dimensional FDA subspaces are covered. Scatters in two-
and then multi-classes are explained in FDA. Then, we discuss on the rank of
the scatters and the dimensionality of the subspace. A real-life example is
also provided for interpreting FDA. Then, possible singularity of the scatter
is discussed to introduce robust FDA. PCA and FDA directions are also compared.
We also prove that FDA and linear discriminant analysis are equivalent. Fisher
forest is also introduced as an ensemble of fisher subspaces useful for
handling data with different features and dimensionality. Afterwards, kernel
FDA is explained for both one- and multi-dimensional subspaces with both two-
and multi-classes. Finally, some simulations are performed on AT&T face dataset
to illustrate FDA and compare it with PCA."
"Gradient descent is a simple and widely used optimization method for machine
learning. For homogeneous linear classifiers applied to separable data,
gradient descent has been shown to converge to the maximal margin (or
equivalently, the minimal norm) solution for various smooth loss functions. The
previous theory does not, however, apply to non-smooth functions such as the
hinge loss which is widely used in practice. Here, we study the convergence of
a homotopic variant of gradient descent applied to the hinge loss and provide
explicit convergence rates to the max-margin solution for linearly separable
data."
"Dictionary based classifiers are a family of algorithms for time series
classification (TSC), that focus on capturing the frequency of pattern
occurrences in a time series. The ensemble based Bag of Symbolic Fourier
Approximation Symbols (BOSS) was found to be a top performing TSC algorithm in
a recent evaluation, as well as the best performing dictionary based
classifier. A recent addition to the category, the Word Extraction for Time
Series Classification (WEASEL), claims an improvement on this performance. Both
of these algorithms however have non-trivial scalability issues, taking a
considerable amount of build time and space on larger datasets. We evaluate
changes to the way BOSS chooses classifiers for its ensemble, replacing its
parameter search with random selection. This change allows for the easy
implementation of contracting, setting a build time limit for the classifier
and check-pointing, saving progress during the classifiers build. To
differentiate between the two BOSS ensemble methods we refer to our randomised
version as RBOSS. Additionally we test the application of common ensembling
techniques to help retain accuracy from the loss of the BOSS parameter search.
We achieve a significant reduction in build time without a significant change
in accuracy on average when compared to BOSS by creating a size $n$ weighted
ensemble selecting the best performers from $k$ randomly chosen parameter sets.
Our experiments are conducted on datasets from the recently expanded UCR time
series archive. We demonstrate the usability improvements to RBOSS with a case
study using a large whale acoustics dataset for which BOSS proved infeasible."
"The biological processes involved in a drug's mechanisms of action are
oftentimes dynamic, complex and difficult to discern. Time-course gene
expression data is a rich source of information that can be used to unravel
these complex processes, identify biomarkers of drug sensitivity and predict
the response to a drug. However, the majority of previous work has not fully
utilized this temporal dimension. In these studies, the gene expression data is
either considered at one time-point (before the administration of the drug) or
two timepoints (before and after the administration of the drug). This is
clearly inadequate in modeling dynamic gene-drug interactions, especially for
applications such as long-term drug therapy.
  In this work, we present a novel REcursive Prediction (REP) framework for
drug response prediction by taking advantage of time-course gene expression
data. Our goal is to predict drug response values at every stage of a long-term
treatment, given the expression levels of genes collected in the previous
time-points. To this end, REP employs a built-in recursive structure that
exploits the intrinsic time-course nature of the data and integrates past
values of drug responses for subsequent predictions. It also incorporates
tensor completion that can not only alleviate the impact of noise and missing
data, but also predict unseen gene expression levels (GELs). These advantages
enable REP to estimate drug response at any stage of a given treatment from
some GELs measured in the beginning of the treatment. Extensive experiments on
a dataset corresponding to 53 multiple sclerosis patients treated with
interferon are included to showcase the effectiveness of REP."
"Stochastic gradient descent (SGD) has been the dominant optimization method
for training deep neural networks due to its many desirable properties. One of
the more remarkable and least understood quality of SGD is that it generalizes
relatively well on unseen data even when the neural network has millions of
parameters. We hypothesize that in certain cases it is desirable to relax its
intrinsic generalization properties and introduce an extension of SGD called
deep gradient boosting (DGB). The key idea of DGB is that back-propagated
gradients inferred using the chain rule can be viewed as pseudo-residual
targets of a gradient boosting problem. Thus at each layer of a neural network
the weight update is calculated by solving the corresponding boosting problem
using a linear base learner. The resulting weight update formula can also be
viewed as a normalization procedure of the data that arrives at each layer
during the forward pass. When implemented as a separate input normalization
layer (INN) the new architecture shows improved performance on image
recognition tasks when compared to the same architecture without normalization
layers. As opposed to batch normalization (BN), INN has no learnable parameters
however it matches its performance on CIFAR10 and ImageNet classification
tasks."
"Item response theory (IRT) models are widely used in psychometrics and
educational measurement, being deployed in many high stakes tests such as the
GRE aptitude test. IRT has largely focused on estimation of a single latent
trait (e.g. ability) that remains static through the collection of item
responses. However, in contemporary settings where item responses are being
continuously collected, such as Massive Open Online Courses (MOOCs), interest
will naturally be on the dynamics of ability, thus complicating usage of
traditional IRT models. We propose DynAEsti, an augmentation of the traditional
IRT Expectation Maximization algorithm that allows ability to be a continuously
varying curve over time. In the process, we develop CurvFiFE, a novel
non-parametric continuous-time technique that handles the
curve-fitting/regression problem extended to address more general probabilistic
emissions (as opposed to simply noisy data points). Furthermore, to accomplish
this, we develop a novel technique called grafting, which can successfully
approximate distributions represented by graphical models when other popular
techniques like Loopy Belief Propogation (LBP) and Variational Inference (VI)
fail. The performance of DynAEsti is evaluated through simulation, where we
achieve results comparable to the optimal of what is observed in the static
ability scenario. Finally, DynAEsti is applied to a longitudinal performance
dataset (80-years of competitive golf at the 18-hole Masters Tournament) to
demonstrate its ability to recover key properties of human performance and the
heterogeneous characteristics of the different holes. Python code for CurvFiFE
and DynAEsti is publicly available at github.com/chausies/DynAEstiAndCurvFiFE.
This is the full version of our ICDM 2019 paper."
"A new meta-algorithm for estimating the conditional average treatment effects
is proposed in the paper. The main idea underlying the algorithm is to consider
a new dataset consisting of feature vectors produced by means of concatenation
of examples from control and treatment groups, which are close to each other.
Outcomes of new data are defined as the difference between outcomes of the
corresponding examples comprising new feature vectors. The second idea is based
on the assumption that the number of controls is rather large and the control
outcome function is precisely determined. This assumption allows us to augment
treatments by generating feature vectors which are closed to available
treatments. The outcome regression function constructed on the augmented set of
concatenated feature vectors can be viewed as an estimator of the conditional
average treatment effects. A simple modification of the Co-learner based on the
random subspace method or the feature bagging is also proposed. Various
numerical simulation experiments illustrate the proposed algorithm and show its
outperformance in comparison with the well-known T-learner and X-learner for
several types of the control and treatment outcome functions."
"Reinforcement learning has exceeded human-level performance in game playing
AI with deep learning methods according to the experiments from DeepMind on Go
and Atari games. Deep learning solves high dimension input problems which stop
the development of reinforcement for many years. This study uses both two
techniques to create several agents with different algorithms that successfully
learn to play T-rex Runner. Deep Q network algorithm and three types of
improvements are implemented to train the agent. The results from some of them
are far from satisfactory but others are better than human experts. Batch
normalization is a method to solve internal covariate shift problems in deep
neural network. The positive influence of this on reinforcement learning has
also been proved in this study."
"Graph Convolutional Neural Networks (GCNNs) extend classical CNNs to graph
data domain, such as brain networks, social networks and 3D point clouds. It is
critical to identify an appropriate graph for the subsequent graph convolution.
Existing methods manually construct or learn one fixed graph for all the layers
of a GCNN. In order to adapt to the underlying structure of node features in
different layers, we propose dynamic learning of graphs and node features
jointly in GCNNs. In particular, we cast the graph optimization problem as
distance metric learning to capture pairwise similarities of features in each
layer. We deploy the Mahalanobis distance metric and further decompose the
metric matrix into a low-dimensional matrix, which converts graph learning to
the optimization of a low-dimensional matrix for efficient implementation.
Extensive experiments on point clouds and citation network datasets demonstrate
the superiority of the proposed method in terms of both accuracies and
robustness."
"We present GraphMix, a regularization method for Graph Neural Network based
semi-supervised object classification, whereby we propose to train a
fully-connected network jointly with the graph neural network via parameter
sharing and interpolation-based regularization. Further, we provide a
theoretical analysis of how GraphMix improves the generalization bounds of the
underlying graph neural network, without making any assumptions about the
""aggregation"" layer or the depth of the graph neural networks. We
experimentally validate this analysis by applying GraphMix to various
architectures such as Graph Convolutional Networks, Graph Attention Networks
and Graph-U-Net. Despite its simplicity, we demonstrate that GraphMix can
consistently improve or closely match state-of-the-art performance using even
simpler architectures such as Graph Convolutional Networks, across three
established graph benchmarks: Cora, Citeseer and Pubmed citation network
datasets, as well as three newly proposed datasets: Cora-Full, Co-author-CS and
Co-author-Physics."
"Gradient-based temporal difference (GTD) algorithms are widely used in
off-policy learning scenarios. Among them, the two time-scale TD with gradient
correction (TDC) algorithm has been shown to have superior performance. In
contrast to previous studies that characterized the non-asymptotic convergence
rate of TDC only under identical and independently distributed (i.i.d.) data
samples, we provide the first non-asymptotic convergence analysis for two
time-scale TDC under a non-i.i.d.\ Markovian sample path and linear function
approximation. We show that the two time-scale TDC can converge as fast as
O(log t/(t^(2/3))) under diminishing stepsize, and can converge exponentially
fast under constant stepsize, but at the cost of a non-vanishing error. We
further propose a TDC algorithm with blockwisely diminishing stepsize, and show
that it asymptotically converges with an arbitrarily small error at a
blockwisely linear convergence rate. Our experiments demonstrate that such an
algorithm converges as fast as TDC under constant stepsize, and still enjoys
comparable accuracy as TDC under diminishing stepsize."
"In this paper we propose a novel observer-based method to improve the safety
and security of connected and automated vehicle (CAV) transportation. The
proposed method combines model-based signal filtering and anomaly detection
methods. Specifically, we use adaptive extended Kalman filter (AEKF) to smooth
sensor readings of a CAV based on a nonlinear car-following motion model. Under
the assumption of a car-following model, the subject vehicle utilizes its
leading vehicle's information to detect sensor anomalies by employing
previously-trained One Class Support Vector Machine (OCSVM) models. This
approach allows the AEKF to estimate the state of a vehicle not only based on
the vehicle's location and speed, but also by taking into account the state of
the surrounding traffic. A communication time delay factor is considered in the
car-following model to make it more suitable for real-world applications. Our
experiments show that compared with the AEKF with a traditional
$\chi^2$-detector, our proposed method achieves a better anomaly detection
performance. We also demonstrate that a larger time delay factor has a negative
impact on the overall detection performance."
"Although deep learning has been applied to successfully address many data
mining problems, relatively limited work has been done on deep learning for
anomaly detection. Existing deep anomaly detection methods, which focus on
learning new feature representations to enable downstream anomaly detection
methods, perform indirect optimization of anomaly scores, leading to
data-inefficient learning and suboptimal anomaly scoring. Also, they are
typically designed as unsupervised learning due to the lack of large-scale
labeled anomaly data. As a result, they are difficult to leverage prior
knowledge (e.g., a few labeled anomalies) when such information is available as
in many real-world anomaly detection applications.
  This paper introduces a novel anomaly detection framework and its
instantiation to address these problems. Instead of representation learning,
our method fulfills an end-to-end learning of anomaly scores by a neural
deviation learning, in which we leverage a few (e.g., multiple to dozens)
labeled anomalies and a prior probability to enforce statistically significant
deviations of the anomaly scores of anomalies from that of normal data objects
in the upper tail. Extensive results show that our method can be trained
substantially more data-efficiently and achieves significantly better anomaly
scoring than state-of-the-art competing methods."
"Balancing exploration and exploitation is a fundamental part of reinforcement
learning, yet most state-of-the-art algorithms use a naive exploration protocol
like $\epsilon$-greedy. This contributes to the problem of high sample
complexity, as the algorithm wastes effort by repeatedly visiting parts of the
state space that have already been explored. We introduce a novel method based
on Bayesian linear regression and latent space embedding to generate an
intrinsic reward signal that encourages the learning agent to seek out
unexplored parts of the state space. This method is computationally efficient,
simple to implement, and can extend any state-of-the-art reinforcement learning
algorithm. We evaluate the method on a range of algorithms and challenging
control tasks, on both simulated and physical robots, demonstrating how the
proposed method can significantly improve sample complexity."
"We present and study approximate notions of dimensional and margin
complexity, which correspond to the minimal dimension or norm of an embedding
required to approximate, rather then exactly represent, a given hypothesis
class. We show that such notions are not only sufficient for learning using
linear predictors or a kernel, but unlike the exact variants, are also
necessary. Thus they are better suited for discussing limitations of linear or
kernel methods."
"Matrix factorization is a very common machine learning technique in
recommender systems. Bayesian Matrix Factorization (BMF) algorithms would be
attractive because of their ability to quantify uncertainty in their
predictions and avoid over-fitting, combined with high prediction accuracy.
However, they have not been widely used on large-scale data because of their
prohibitive computational cost. In recent work, efforts have been made to
reduce the cost, both by improving the scalability of the BMF algorithm as well
as its implementation, but so far mainly separately. In this paper we show that
the state-of-the-art of both approaches to scalability can be combined. We
combine the recent highly-scalable Posterior Propagation algorithm for BMF,
which parallelizes computation of blocks of the matrix, with a distributed BMF
implementation that users asynchronous communication within each block. We show
that the combination of the two methods gives substantial improvements in the
scalability of BMF on web-scale datasets, when the goal is to reduce the
wall-clock time."
"I congratulate Profs. Binyan Jiang, Rui Song, Jialiang Li, and Donglin Zeng
(JSLZ) for an exciting development in conducting inferences on optimal dynamic
treatment regimes (DTRs) learned via empirical risk minimization using the
entropy loss as a surrogate. JSLZ's approach leverages a
rejection-and-importance-sampling estimate of the value of a given decision
rule based on inverse probability weighting (IPW) and its interpretation as a
weighted (or cost-sensitive) classification. Their use of smooth classification
surrogates enables their careful approach to analyzing asymptotic
distributions. However, even for evaluation purposes, the IPW estimate is
problematic as it leads to weights that discard most of the data and are
extremely variable on whatever remains. In this comment, I discuss an
optimization-based alternative to evaluating DTRs, review several connections,
and suggest directions forward. This extends the balanced policy evaluation
approach of Kallus (2018a) to the longitudinal setting."
"The COVID-19 can cause severe pneumonia and is estimated to have a high
impact on the healthcare system. The standard image diagnosis tests for
pneumonia are chest X-ray (CXR) and computed tomography (CT) scan. CXR are
useful in because it is cheaper, faster and more widespread than CT. This study
aims to identify pneumonia caused by COVID-19 from other types and also healthy
lungs using only CXR images. In order to achieve the objectives, we have
proposed a classification schema considering the multi-class and hierarchical
perspectives, since pneumonia can be structured as a hierarchy. Given the
natural data imbalance in this domain, we also proposed the use of resampling
algorithms in order to re-balance the classes distribution. Our classification
schema extract features using some well-known texture descriptors and also
using a pre-trained CNN model. We also explored early and late fusion
techniques in order to leverage the strength of multiple texture descriptors
and base classifiers at once. To evaluate the approach, we composed a database,
named RYDLS-20, containing CXR images of pneumonia caused by different
pathogens as well as CXR images of healthy lungs. The classes distribution
follows a real-world scenario in which some pathogens are more common than
others. The proposed approach achieved a macro-avg F1-Score of 0.65 using a
multi-class approach and a F1-Score of 0.89 for the COVID-19 identification in
the hierarchical classification scenario. As far as we know, we achieved the
best nominal rate obtained for COVID-19 identification in an unbalanced
environment with more than three classes. We must also highlight the novel
proposed hierarchical classification approach for this task, which considers
the types of pneumonia caused by the different pathogens and lead us to the
best COVID-19 recognition rate obtained here."
"In this paper, we consider the problem of sleeping bandits with stochastic
action sets and adversarial rewards. In this setting, in contrast to most work
in bandits, the actions may not be available at all times. For instance, some
products might be out of stock in item recommendation. The best existing
efficient (i.e., polynomial-time) algorithms for this problem only guarantee an
$O(T^{2/3})$ upper-bound on the regret. Yet, inefficient algorithms based on
EXP4 can achieve $O(\sqrt{T})$. In this paper, we provide a new computationally
efficient algorithm inspired by EXP3 satisfying a regret of order $O(\sqrt{T})$
when the availabilities of each action $i \in \cA$ are independent. We then
study the most general version of the problem where at each round available
sets are generated from some unknown arbitrary distribution (i.e., without the
independence assumption) and propose an efficient algorithm with $O(\sqrt {2^K
T})$ regret guarantee. Our theoretical results are corroborated with
experimental evaluations."
"Accurate prediction of postoperative complications can inform shared
decisions regarding prognosis, preoperative risk-reduction, and postoperative
resource use. We hypothesized that multi-task deep learning models would
outperform random forest models in predicting postoperative complications, and
that integrating high-resolution intraoperative physiological time series would
result in more granular and personalized health representations that would
improve prognostication compared to preoperative predictions. In a longitudinal
cohort study of 56,242 patients undergoing 67,481 inpatient surgical procedures
at a university medical center, we compared deep learning models with random
forests for predicting nine common postoperative complications using
preoperative, intraoperative, and perioperative patient data. Our study
indicated several significant results across experimental settings that suggest
the utility of deep learning for capturing more precise representations of
patient health for augmented surgical decision support. Multi-task learning
improved efficiency by reducing computational resources without compromising
predictive performance. Integrated gradients interpretability mechanisms
identified potentially modifiable risk factors for each complication. Monte
Carlo dropout methods provided a quantitative measure of prediction uncertainty
that has the potential to enhance clinical trust. Multi-task learning,
interpretability mechanisms, and uncertainty metrics demonstrated potential to
facilitate effective clinical implementation."
"Like k-means and Gaussian Mixture Model (GMM), fuzzy c-means (FCM) with soft
partition has also become a popular clustering algorithm and still is
extensively studied. However, these algorithms and their variants still suffer
from some difficulties such as determination of the optimal number of clusters
which is a key factor for clustering quality. A common approach for overcoming
this difficulty is to use the trial-and-validation strategy, i.e., traversing
every integer from large number like $\sqrt{n}$ to 2 until finding the optimal
number corresponding to the peak value of some cluster validity index. But it
is scarcely possible to naturally construct an adaptively agglomerative
hierarchical cluster structure as using the trial-and-validation strategy. Even
possible, existing different validity indices also lead to different number of
clusters. To effectively mitigate the problems while motivated by convex
clustering, in this paper we present a Centroid Auto-Fused Hierarchical Fuzzy
c-means method (CAF-HFCM) whose optimization procedure can automatically
agglomerate to form a cluster hierarchy, more importantly, yielding an optimal
number of clusters without resorting to any validity index. Although a
recently-proposed robust-learning fuzzy c-means (RL-FCM) can also automatically
obtain the best number of clusters without the help of any validity index,
so-involved 3 hyper-parameters need to adjust expensively, conversely, our
CAF-HFCM involves just 1 hyper-parameter which makes the corresponding
adjustment is relatively easier and more operational. Further, as an additional
benefit from our optimization objective, the CAF-HFCM effectively reduces the
sensitivity to the initialization of clustering performance. Moreover, our
proposed CAF-HFCM method is able to be straightforwardly extended to various
variants of FCM."
"The curse of dimensionality causes the well-known and widely discussed
problems for machine learning methods. There is a hypothesis that using of the
Manhattan distance and even fractional quasinorms lp (for p less than 1) can
help to overcome the curse of dimensionality in classification problems. In
this study, we systematically test this hypothesis. We confirm that fractional
quasinorms have a greater relative contrast or coefficient of variation than
the Euclidean norm l2, but we also demonstrate that the distance concentration
shows qualitatively the same behaviour for all tested norms and quasinorms and
the difference between them decays as dimension tends to infinity. Estimation
of classification quality for kNN based on different norms and quasinorms shows
that a greater relative contrast does not mean better classifier performance
and the worst performance for different databases was shown by different norms
(quasinorms). A systematic comparison shows that the difference of the
performance of kNN based on lp for p=2, 1, and 0.5 is statistically
insignificant."
"Random forest is a popular prediction approach for handling high dimensional
covariates. However, it often becomes infeasible to interpret the obtained high
dimensional and non-parametric model. Aiming for obtaining an interpretable
predictive model, we develop a forward variable selection method using the
continuous ranked probability score (CRPS) as the loss function. Our stepwise
procedure leads to a smallest set of variables that optimizes the CRPS risk by
performing at each step a hypothesis test on a significant decrease in CRPS
risk. We provide mathematical motivation for our method by proving that in
population sense the method attains the optimal set. Additionally, we show that
the test is consistent provided that the random forest estimator of a quantile
function is consistent.
  In a simulation study, we compare the performance of our method with an
existing variable selection method, for different sample sizes and different
correlation strength of covariates. Our method is observed to have a much lower
false positive rate. We also demonstrate an application of our method to
statistical post-processing of daily maximum temperature forecasts in the
Netherlands. Our method selects about 10% covariates while retaining the same
predictive power."
"A standard assumption adopted in the multi-armed bandit (MAB) framework is
that the mean rewards are constant over time. This assumption can be
restrictive in the business world as decision-makers often face an evolving
environment where the mean rewards are time-varying. In this paper, we consider
a non-stationary MAB model with $K$ arms whose mean rewards vary over time in a
periodic manner. The unknown periods can be different across arms and scale
with the length of the horizon $T$ polynomially. We propose a two-stage policy
that combines the Fourier analysis with a confidence-bound-based learning
procedure to learn the periods and minimize the regret. In stage one, the
policy correctly estimates the periods of all arms with high probability. In
stage two, the policy explores the periodic mean rewards of arms using the
periods estimated in stage one and exploits the optimal arm in the long run. We
show that our learning policy incurs a regret upper bound
$\tilde{O}(\sqrt{T\sum_{k=1}^K T_k})$ where $T_k$ is the period of arm $k$.
Moreover, we establish a general lower bound $\Omega(\sqrt{T\max_{k}\{ T_k\}})$
for any policy. Therefore, our policy is near-optimal up to a factor of
$\sqrt{K}$."
"This paper presents a novel meta learning framework for feature selection
(FS) based on fuzzy similarity. The proposed method aims to recommend the best
FS method from four candidate FS methods for any given dataset. This is
achieved by firstly constructing a large training data repository using data
synthesis. Six meta features that represent the characteristics of the training
dataset are then extracted. The best FS method for each of the training
datasets is used as the meta label. Both the meta features and the
corresponding meta labels are subsequently used to train a classification model
using a fuzzy similarity measure based framework. Finally the trained model is
used to recommend the most suitable FS method for a given unseen dataset. This
proposed method was evaluated based on eight public datasets of real-world
applications. It successfully recommended the best method for five datasets and
the second best method for one dataset, which outperformed any of the four
individual FS methods. Besides, the proposed method is computationally
efficient for algorithm selection, leading to negligible additional time for
the feature selection process. Thus, the paper contributes a novel method for
effectively recommending which feature selection method to use for any new
given dataset."
"Mixed integer linear programs are commonly solved by Branch and Bound
algorithms. A key factor of the efficiency of the most successful commercial
solvers is their fine-tuned heuristics. In this paper, we leverage patterns in
real-world instances to learn from scratch a new branching strategy optimised
for a given problem and compare it with a commercial solver. We propose FMSTS,
a novel Reinforcement Learning approach specifically designed for this task.
The strength of our method lies in the consistency between a local value
function and a global metric of interest. In addition, we provide insights for
adapting known RL techniques to the Branch and Bound setting, and present a new
neural network architecture inspired from the literature. To our knowledge, it
is the first time Reinforcement Learning has been used to fully optimise the
branching strategy. Computational experiments show that our method is
appropriate and able to generalise well to new instances."
"Tuning hyperparameters for machine learning algorithms is a tedious task, one
that is typically done manually. To enable automated hyperparameter tuning,
recent works have started to use techniques based on Bayesian optimization.
However, to practically enable automated tuning for large scale machine
learning training pipelines, significant gaps remain in existing libraries,
including lack of abstractions, fault tolerance, and flexibility to support
scheduling on any distributed computing framework. To address these challenges,
we present Mango, a Python library for parallel hyperparameter tuning. Mango
enables the use of any distributed scheduling framework, implements intelligent
parallel search strategies, and provides rich abstractions for defining complex
hyperparameter search spaces that are compatible with scikit-learn. Mango is
comparable in performance to Hyperopt, another widely used library. Mango is
available open-source and is currently used in production at Arm Research to
provide state-of-art hyperparameter tuning capabilities."
"In Bitcoin entity classification, results are strongly conditioned by the
ground-truth dataset, especially when applying supervised machine learning
approaches. However, these ground-truth datasets are frequently affected by
significant class imbalance as generally they contain much more information
regarding legal services (Exchange, Gambling), than regarding services that may
be related to illicit activities (Mixer, Service). Class imbalance increases
the complexity of applying machine learning techniques and reduces the quality
of classification results, especially for underrepresented, but critical
classes.
  In this paper, we propose to address this problem by using Generative
Adversarial Networks (GANs) for Bitcoin data augmentation as GANs recently have
shown promising results in the domain of image classification. However, there
is no ""one-fits-all"" GAN solution that works for every scenario. In fact,
setting GAN training parameters is non-trivial and heavily affects the quality
of the generated synthetic data. We therefore evaluate how GAN parameters such
as the optimization function, the size of the dataset and the chosen batch size
affect GAN implementation for one underrepresented entity class (Mining Pool)
and demonstrate how a ""good"" GAN configuration can be obtained that achieves
high similarity between synthetically generated and real Bitcoin address data.
To the best of our knowledge, this is the first study presenting GANs as a
valid tool for generating synthetic address data for data augmentation in
Bitcoin entity classification."
"A general fuzzy min-max (GFMM) neural network is one of the efficient
neuro-fuzzy systems for classification problems. However, a disadvantage of
most of the current learning algorithms for GFMM is that they can handle
effectively numerical valued features only. Therefore, this paper provides some
potential approaches to adapting GFMM learning algorithms for classification
problems with mixed-type or only categorical features as they are very common
in practical applications and often carry very useful information. We will
compare and assess three main methods of handling datasets with mixed features,
including the use of encoding methods, the combination of the GFMM model with
other classifiers, and employing the specific learning algorithms for both
types of features. The experimental results showed that the target and
James-Stein are appropriate categorical encoding methods for learning
algorithms of GFMM models, while the combination of GFMM neural networks and
decision trees is a flexible way to enhance the classification performance of
GFMM models on datasets with the mixed features. The learning algorithms with
the mixed-type feature abilities are potential approaches to deal with
mixed-attribute data in a natural way, but they need further improvement to
achieve a better classification accuracy. Based on the analysis, we also
identify the strong and weak points of different methods and propose potential
research directions."
"Real estate contributes significantly to all major economies around the
world. In particular, house prices have a direct impact on stakeholders,
ranging from house buyers to financing companies. Thus, a plethora of
techniques have been developed for real estate price prediction. Most of the
existing techniques rely on different house features to build a variety of
prediction models to predict house prices. Perceiving the effect of spatial
dependence on house prices, some later works focused on introducing spatial
regression models for improving prediction performance. However, they fail to
take into account the geo-spatial context of the neighborhood amenities such as
how close a house is to a train station, or a highly-ranked school, or a
shopping center. Such contextual information may play a vital role in users'
interests in a house and thereby has a direct influence on its price. In this
paper, we propose to leverage the concept of graph neural networks to capture
the geo-spatial context of the neighborhood of a house. In particular, we
present a novel method, the Geo-Spatial Network Embedding (GSNE), that learns
the embeddings of houses and various types of Points of Interest (POIs) in the
form of multipartite networks, where the houses and the POIs are represented as
attributed nodes and the relationships between them as edges. Extensive
experiments with a large number of regression techniques show that the
embeddings produced by our proposed GSNE technique consistently and
significantly improve the performance of the house price prediction task
regardless of the downstream regression model."
"In this technical report we describe some properties of f-divergences and
f-GAN training. We present an elementary derivation of the f-divergence lower
bounds which form the basis of f-GAN training. We derive informative but
perhaps underappreciated properties of f-divergences and f-GAN training,
including a gradient matching property and the fact that all f-divergences
agree up to an overall scale factor on the divergence between nearby
distributions. We provide detailed expressions for computing various common
f-divergences and their variational lower bounds. Finally, based on our
reformulation, we slightly generalize f-GAN training in a way that may improve
its stability."
"We propose an algorithm for Bayesian functional optimisation - that is,
finding the function to optimise a process - guided by experimenter beliefs and
intuitions regarding the expected characteristics (length-scale, smoothness,
cyclicity etc.) of the optimal solution encoded into the covariance function of
a Gaussian Process. Our algorithm generates a sequence of finite-dimensional
random subspaces of functional space spanned by a set of draws from the
experimenter's Gaussian Process. Standard Bayesian optimisation is applied on
each subspace, and the best solution found used as a starting point (origin)
for the next subspace. Using the concept of effective dimensionality, we
analyse the convergence of our algorithm and provide a regret bound to show
that our algorithm converges in sub-linear time provided a finite effective
dimension exists. We test our algorithm in simulated and real-world
experiments, namely blind function matching, finding the optimal
precipitation-strengthening function for an aluminium alloy, and learning rate
schedule optimisation for deep networks."
"Anomaly detectors are often designed to catch statistical anomalies.
End-users typically do not have interest in all of the detected outliers, but
only those relevant to their application. Given an existing black-box
sequential anomaly detector, this paper proposes a method to improve its user
relevancy using a small number of human feedback. As our first contribution,
the method is agnostic to the detector: it only assumes access to its anomaly
scores, without requirement on any additional information inside it. Inspired
by a fact that anomalies are of different types, our approach identifies these
types and utilizes user feedback to assign relevancy to types. This relevancy
score, as our second contribution, is used to adjust the subsequent anomaly
selection process. Empirical results on synthetic and real-world datasets show
that our approach yields significant improvements on precision and recall over
a range of anomaly detectors."
"As the number of installed meters in buildings increases, there is a growing
number of data time-series that could be used to develop data-driven models to
support and optimize building operation. However, building data sets are often
characterized by errors and missing values, which are considered, by the recent
research, among the main limiting factors on the performance of the proposed
models. Motivated by the need to address the problem of missing data in
building operation, this work presents a data-driven approach to fill these
gaps. In this study, three different autoencoder neural networks are trained to
reconstruct missing short-term indoor environment data time-series in a data
set collected in an office building in Aachen, Germany. This consisted of a
four year-long monitoring campaign in and between the years 2014 and 2017, of
84 different rooms. The models are applicable for different time-series
obtained from room automation, such as indoor air temperature, relative
humidity and $CO_{2}$ data streams. The results prove that the proposed methods
outperform classic numerical approaches and they result in reconstructing the
corresponding variables with average RMSEs of 0.42 {\deg}C, 1.30 % and 78.41
ppm, respectively."
"This paper studies the problem of learning the correlation structure of a set
of intervention functions defined on the directed acyclic graph (DAG) of a
causal model. This is useful when we are interested in jointly learning the
causal effects of interventions on different subsets of variables in a DAG,
which is common in field such as healthcare or operations research. We propose
the first multi-task causal Gaussian process (GP) model, which we call DAG-GP,
that allows for information sharing across continuous interventions and across
experiments on different variables. DAG-GP accommodates different assumptions
in terms of data availability and captures the correlation between functions
lying in input spaces of different dimensionality via a well-defined integral
operator. We give theoretical results detailing when and how the DAG-GP model
can be formulated depending on the DAG. We test both the quality of its
predictions and its calibrated uncertainties. Compared to single-task models,
DAG-GP achieves the best fitting performance in a variety of real and synthetic
settings. In addition, it helps to select optimal interventions faster than
competing approaches when used within sequential decision making frameworks,
like active learning or Bayesian optimization."
"This work targets the identification of a class of models for hybrid
dynamical systems characterized by nonlinear autoregressive exogenous (NARX)
components, with finite-dimensional polynomial expansions, and by a Markovian
switching mechanism. The estimation of the model parameters is performed under
a probabilistic framework via Expectation Maximization, including submodel
coefficients, hidden state values and transition probabilities. Discrete mode
classification and NARX regression tasks are disentangled within the
iterations. Soft-labels are assigned to latent states on the trajectories by
averaging over the state posteriors and updated using the parametrization
obtained from the previous maximization phase. Then, NARXs parameters are
repeatedly fitted by solving weighted regression subproblems through a cyclical
coordinate descent approach with coordinate-wise minimization. Moreover, we
investigate a two stage selection scheme, based on a l1-norm bridge estimation
followed by hard-thresholding, to achieve parsimonious models through selection
of the polynomial expansion. The proposed approach is demonstrated on a SMNARX
problem composed by three nonlinear sub-models with specific regressors."
"In the era of the big data, we create and collect lots of data from all
different kinds of sources: the Internet, the sensors, the consumer market, and
so on. Many of the data are coming sequentially, and would like to be processed
and understood quickly. One classic way of analyzing data is based on batch
processing, in which the data is stored and analyzed in an offline fashion.
However, when the volume of the data is too large, it is much more difficult
and time-consuming to do batch processing than sequential processing. What's
more, sequential data is usually changing dynamically, and needs to be
understood on-the-fly in order to capture the changes. Online Convex
Optimization (OCO) is a popular framework that matches the above sequential
data processing requirement. Applications using OCO include online routing,
online auctions, online classification and regression, as well as online
resource allocation. Due to the general applicability of OCO to the sequential
data and the rigorous theoretical guarantee, it has attracted lots of
researchers to develop useful algorithms to fulfill different needs. In this
thesis, we show our contributions to OCO's development by designing algorithms
to adapt to changing environments."
"In this paper, we propose a new method to perform data augmentation in a
reliable way in the High Dimensional Low Sample Size (HDLSS) setting using a
geometry-based variational autoencoder. Our approach combines a proper latent
space modeling of the VAE seen as a Riemannian manifold with a new generation
scheme which produces more meaningful samples especially in the context of
small data sets. The proposed method is tested through a wide experimental
study where its robustness to data sets, classifiers and training samples size
is stressed. It is also validated on a medical imaging classification task on
the challenging ADNI database where a small number of 3D brain MRIs are
considered and augmented using the proposed VAE framework. In each case, the
proposed method allows for a significant and reliable gain in the
classification metrics. For instance, balanced accuracy jumps from 66.3% to
74.3% for a state-of-the-art CNN classifier trained with 50 MRIs of cognitively
normal (CN) and 50 Alzheimer disease (AD) patients and from 77.7% to 86.3% when
trained with 243 CN and 210 AD while improving greatly sensitivity and
specificity metrics."
"Uplift is a particular case of conditional treatment effect modeling. Such
models deal with cause-and-effect inference for a specific factor, such as a
marketing intervention or a medical treatment. In practice, these models are
built on individual data from randomized clinical trials where the goal is to
partition the participants into heterogeneous groups depending on the uplift.
Most existing approaches are adaptations of random forests for the uplift case.
Several split criteria have been proposed in the literature, all relying on
maximizing heterogeneity. However, in practice, these approaches are prone to
overfitting. In this work, we bring a new vision to uplift modeling. We propose
a new loss function defined by leveraging a connection with the Bayesian
interpretation of the relative risk. Our solution is developed for a specific
twin neural network architecture allowing to jointly optimize the marginal
probabilities of success for treated and control individuals. We show that this
model is a generalization of the uplift logistic interaction model. We modify
the stochastic gradient descent algorithm to allow for structured sparse
solutions. This helps training our uplift models to a great extent. We show our
proposed method is competitive with the state-of-the-art in simulation setting
and on real data from large scale randomized experiments."
"In this paper we show how using satellite images can improve the accuracy of
housing price estimation models. Using Los Angeles County's property assessment
dataset, by transferring learning from an Inception-v3 model pretrained on
ImageNet, we could achieve an improvement of ~10% in R-squared score compared
to two baseline models that only use non-image features of the house."
"Kernel PCA is a powerful feature extractor which recently has seen a
reformulation in the context of Restricted Kernel Machines (RKMs). These RKMs
allow for a representation of kernel PCA in terms of hidden and visible units
similar to Restricted Boltzmann Machines. This connection has led to insights
on how to use kernel PCA in a generative procedure, called generative kernel
PCA. In this paper, the use of generative kernel PCA for exploring latent
spaces of datasets is investigated. New points can be generated by gradually
moving in the latent space, which allows for an interpretation of the
components. Firstly, examples of this feature space exploration on three
datasets are shown with one of them leading to an interpretable representation
of ECG signals. Afterwards, the use of the tool in combination with novelty
detection is shown, where the latent space around novel patterns in the data is
explored. This helps in the interpretation of why certain points are considered
as novel."
"Instance-based interpretation methods have been widely studied for supervised
learning methods as they help explain how black box neural networks predict.
However, instance-based interpretations remain ill-understood in the context of
unsupervised learning. In this paper, we investigate influence functions [Koh
and Liang, 2017], a popular instance-based interpretation method, for a class
of deep generative models called variational auto-encoders (VAE). We formally
frame the counter-factual question answered by influence functions in this
setting, and through theoretical analysis, examine what they reveal about the
impact of training samples on classical unsupervised learning methods. We then
introduce VAE- TracIn, a computationally efficient and theoretically sound
solution based on Pruthi et al. [2020], for VAEs. Finally, we evaluate
VAE-TracIn on several real world datasets with extensive quantitative and
qualitative analysis."
"Modeling the time evolution of discrete sets of items (e.g., genetic
mutations) is a fundamental problem in many biomedical applications. We
approach this problem through the lens of continuous-time Markov chains, and
show that the resulting learning task is generally underspecified in the usual
setting of cross-sectional data. We explore a perhaps surprising remedy:
including a number of additional independent items can help determine time
order, and hence resolve underspecification. This is in sharp contrast to the
common practice of limiting the analysis to a small subset of relevant items,
which is followed largely due to poor scaling of existing methods. To put our
theoretical insight into practice, we develop an approximate likelihood
maximization method for learning continuous-time Markov chains, which can scale
to hundreds of items and is orders of magnitude faster than previous methods.
We demonstrate the effectiveness of our approach on synthetic and real cancer
data."
"In this work, we propose an ensemble forecasting approach based on randomized
neural networks. Improved randomized learning streamlines the fitting abilities
of individual learners by generating network parameters in accordance with the
data and target function features. A pattern-based representation of time
series makes the proposed approach suitable for forecasting time series with
multiple seasonality. We propose six strategies for controlling the diversity
of ensemble members. Case studies conducted on four real-world forecasting
problems verified the effectiveness and superior performance of the proposed
ensemble forecasting approach. It outperformed statistical models as well as
state-of-the-art machine learning models in terms of forecasting accuracy. The
proposed approach has several advantages: fast and easy training, simple
architecture, ease of implementation, high accuracy and the ability to deal
with nonstationarity and multiple seasonality in time series."
"As they have a vital effect on social decision-making, AI algorithms should
be not only accurate but also fair. Among various algorithms for fairness AI,
learning fair representation (LFR), whose goal is to find a fair representation
with respect to sensitive variables such as gender and race, has received much
attention. For LFR, the adversarial training scheme is popularly employed as is
done in the generative adversarial network type algorithms. The choice of a
discriminator, however, is done heuristically without justification. In this
paper, we propose a new adversarial training scheme for LFR, where the integral
probability metric (IPM) with a specific parametric family of discriminators is
used. The most notable result of the proposed LFR algorithm is its theoretical
guarantee about the fairness of the final prediction model, which has not been
considered yet. That is, we derive theoretical relations between the fairness
of representation and the fairness of the prediction model built on the top of
the representation (i.e., using the representation as the input). Moreover, by
numerical experiments, we show that our proposed LFR algorithm is
computationally lighter and more stable, and the final prediction model is
competitive or superior to other LFR algorithms using more complex
discriminators."
"Sample-efficiency guarantees for offline reinforcement learning (RL) often
rely on strong assumptions on both the function classes (e.g.,
Bellman-completeness) and the data coverage (e.g., all-policy concentrability).
Despite the recent efforts on relaxing these assumptions, existing works are
only able to relax one of the two factors, leaving the strong assumption on the
other factor intact. As an important open problem, can we achieve
sample-efficient offline RL with weak assumptions on both factors?
  In this paper we answer the question in the positive. We analyze a simple
algorithm based on the primal-dual formulation of MDPs, where the dual
variables (discounted occupancy) are modeled using a density-ratio function
against offline data. With proper regularization, we show that the algorithm
enjoys polynomial sample complexity, under only realizability and single-policy
concentrability. We also provide alternative analyses based on different
assumptions to shed light on the nature of primal-dual algorithms for offline
RL."
"Since the celebrated works of Russo and Zou (2016,2019) and Xu and Raginsky
(2017), it has been well known that the generalization error of supervised
learning algorithms can be bounded in terms of the mutual information between
their input and the output, given that the loss of any fixed hypothesis has a
subgaussian tail. In this work, we generalize this result beyond the standard
choice of Shannon's mutual information to measure the dependence between the
input and the output. Our main result shows that it is indeed possible to
replace the mutual information by any strongly convex function of the joint
input-output distribution, with the subgaussianity condition on the losses
replaced by a bound on an appropriately chosen norm capturing the geometry of
the dependence measure. This allows us to derive a range of generalization
bounds that are either entirely new or strengthen previously known ones.
Examples include bounds stated in terms of $p$-norm divergences and the
Wasserstein-2 distance, which are respectively applicable for heavy-tailed loss
distributions and highly smooth loss functions. Our analysis is entirely based
on elementary tools from convex analysis by tracking the growth of a potential
function associated with the dependence measure and the loss function."
"Catastrophic forgetting (CF) happens whenever a neural network overwrites
past knowledge while being trained on new tasks. Common techniques to handle CF
include regularization of the weights (using, e.g., their importance on past
tasks), and rehearsal strategies, where the network is constantly re-trained on
past data. Generative models have also been applied for the latter, in order to
have endless sources of data. In this paper, we propose a novel method that
combines the strengths of regularization and generative-based rehearsal
approaches. Our generative model consists of a normalizing flow (NF), a
probabilistic and invertible neural network, trained on the internal embeddings
of the network. By keeping a single NF throughout the training process, we show
that our memory overhead remains constant. In addition, exploiting the
invertibility of the NF, we propose a simple approach to regularize the
network's embeddings with respect to past tasks. We show that our method
performs favorably with respect to state-of-the-art approaches in the
literature, with bounded computational power and memory overheads."
"In a motorway network, correlations between parts or, more precisely, between
the sections of (different) motorways, are of considerable interest. Knowledge
of flows and velocities on individual motorways is not sufficient, rather,
their correlations determine or reflect, respectively, the functionality of and
the dynamics on the network. These correlations are time-dependent as the
dynamics on the network is highly non-stationary. Apart from the conceptual
importance, correlations are also indispensable to detect risks of failure in a
traffic network. Here, we proceed with revealing a certain hierarchy of
correlations in traffic networks that is due to the presence and to the extent
of collectivity. In a previous study, we focused on the collectivity motion
present in the entire traffic network, i.e. the collectivity of the system as a
whole. Here, we manage to subtract this dominant effect from the data and
identify the subdominant collectivities which affect different, large parts of
the traffic network. To this end, we employ a spectral analysis of the
correlation matrix for the whole system. We thereby extract information from
the virtual network induced by the correlations and map it on the true
topology, i.e. on the real motorway network. The uncovered subdominant
collectivities provide a new characterization of the traffic network. We carry
out our study for the large motorway network of North Rhine-Westphalia (NRW),
Germany."
"We study the problem of learning causal models from observational data
through the lens of interpolation and its counterpart -- regularization. A
large volume of recent theoretical, as well as empirical work, suggests that,
in highly complex model classes, interpolating estimators can have good
statistical generalization properties and can even be optimal for statistical
learning. Motivated by an analogy between statistical and causal learning
recently highlighted by Janzing (2019), we investigate whether interpolating
estimators can also learn good causal models. To this end, we consider a simple
linearly confounded model and derive precise asymptotics for the *causal risk*
of the min-norm interpolator and ridge-regularized regressors in the
high-dimensional regime. Under the principle of independent causal mechanisms,
a standard assumption in causal learning, we find that interpolators cannot be
optimal and causal learning requires stronger regularization than statistical
learning. This resolves a recent conjecture in Janzing (2019). Beyond this
assumption, we find a larger range of behavior that can be precisely
characterized with a new measure of *confounding strength*. If the confounding
strength is negative, causal learning requires weaker regularization than
statistical learning, interpolators can be optimal, and the optimal
regularization can even be negative. If the confounding strength is large, the
optimal regularization is infinite, and learning from observational data is
actively harmful."
"We propose Multivariate Quantile Function Forecaster (MQF$^2$), a global
probabilistic forecasting method constructed using a multivariate quantile
function and investigate its application to multi-horizon forecasting. Prior
approaches are either autoregressive, implicitly capturing the dependency
structure across time but exhibiting error accumulation with increasing
forecast horizons, or multi-horizon sequence-to-sequence models, which do not
exhibit error accumulation, but also do typically not model the dependency
structure across time steps. MQF$^2$ combines the benefits of both approaches,
by directly making predictions in the form of a multivariate quantile function,
defined as the gradient of a convex function which we parametrize using
input-convex neural networks. By design, the quantile function is monotone with
respect to the input quantile levels and hence avoids quantile crossing. We
provide two options to train MQF$^2$: with energy score or with maximum
likelihood. Experimental results on real-world and synthetic datasets show that
our model has comparable performance with state-of-the-art methods in terms of
single time step metrics while capturing the time dependency structure."
"Although the existing max-value entropy search (MES) is based on the widely
celebrated notion of mutual information, its empirical performance can suffer
due to two misconceptions whose implications on the exploration-exploitation
trade-off are investigated in this paper. These issues are essential in the
development of future acquisition functions and the improvement of the existing
ones as they encourage an accurate measure of the mutual information such as
the rectified MES (RMES) acquisition function we develop in this work. Unlike
the evaluation of MES, we derive a closed-form probability density for the
observation conditioned on the max-value and employ stochastic gradient ascent
with reparameterization to efficiently optimize RMES. As a result of a more
principled acquisition function, RMES shows a consistent improvement over MES
in several synthetic function benchmarks and real-world optimization problems."
"We study sampling problems associated with potentials that lack smoothness.
The potentials can be either convex or non-convex. Departing from the standard
smooth setting, the potentials are only assumed to be weakly smooth or
non-smooth, or the summation of multiple such functions. We develop a sampling
algorithm that resembles proximal algorithms in optimization for this
challenging sampling task. Our algorithm is based on a special case of Gibbs
sampling known as the alternating sampling framework (ASF). The key
contribution of this work is a practical realization of the ASF based on
rejection sampling for both non-convex and convex potentials that are not
necessarily smooth. In almost all the cases of sampling considered in this
work, our proximal sampling algorithm achieves better complexity than all
existing methods."
"This is a brief technical note to clarify the state of lower bounds on regret
for reinforcement learning. In particular, this paper:
  - Reproduces a lower bound on regret for reinforcement learning, similar to
the result of Theorem 5 in the journal UCRL2 paper (Jaksch et al 2010).
  - Clarifies that the proposed proof of Theorem 6 in the REGAL paper (Bartlett
and Tewari 2009) does not hold using the standard techniques without further
work. We suggest that this result should instead be considered a conjecture as
it has no rigorous proof.
  - Suggests that the conjectured lower bound given by (Bartlett and Tewari
2009) is incorrect and, in fact, it is possible to improve the scaling of the
upper bound to match the weaker lower bounds presented in this paper.
  We hope that this note serves to clarify existing results in the field of
reinforcement learning and provides interesting motivation for future work."
"When approximating a black-box function, sampling with active learning
focussing on regions with non-linear responses tends to improve accuracy. We
present the FLOLA-Voronoi method introduced previously for deterministic
responses, and theoretically derive the impact of output uncertainty. The
algorithm automatically puts more emphasis on exploration to provide more
information to the models."
"We propose a new PAC-Bayesian bound and a way of constructing a hypothesis
space, so that the bound is convex in the posterior distribution and also
convex in a trade-off parameter between empirical performance of the posterior
distribution and its complexity. The complexity is measured by the
Kullback-Leibler divergence to a prior. We derive an alternating procedure for
minimizing the bound. We show that the bound can be rewritten as a
one-dimensional function of the trade-off parameter and provide sufficient
conditions under which the function has a single global minimum. When the
conditions are satisfied the alternating minimization is guaranteed to converge
to the global minimum of the bound. We provide experimental results
demonstrating that rigorous minimization of the bound is competitive with
cross-validation in tuning the trade-off between complexity and empirical
performance. In all our experiments the trade-off turned to be quasiconvex even
when the sufficient conditions were violated."
"Inverse problems in imaging such as denoising, deblurring, superresolution
(SR) have been addressed for many decades. In recent years, convolutional
neural networks (CNNs) have been widely used for many inverse problem areas.
Although their indisputable success, CNNs are not mathematically validated as
to how and what they learn. In this paper, we prove that during training, CNN
elements solve for inverse problems which are optimum solutions stored as CNN
neuron filters. We discuss the necessity of mutual coherence between CNN layer
elements in order for a network to converge to the optimum solution. We prove
that required mutual coherence can be provided by the usage of residual
learning and skip connections. We have set rules over training sets and depth
of networks for better convergence, i.e. performance."
"Current imitation learning techniques are too restrictive because they
require the agent and expert to share the same action space. However,
oftentimes agents that act differently from the expert can solve the task just
as good. For example, a person lifting a box can be imitated by a ceiling
mounted robot or a desktop-based robotic-arm. In both cases, the end goal of
lifting the box is achieved, perhaps using different strategies. We denote this
setup as \textit{Inspiration Learning} - knowledge transfer between agents that
operate in different action spaces. Since state-action expert demonstrations
can no longer be used, Inspiration learning requires novel methods to guide the
agent towards the end goal. In this work, we rely on ideas of Preferential
based Reinforcement Learning (PbRL) to design Advantage Actor-Critic algorithms
for solving inspiration learning tasks. Unlike classic actor-critic
architectures, the critic we use consists of two parts: a) a state-value
estimation as in common actor-critic algorithms and b) a single step reward
function derived from an expert/agent classifier. We show that our method is
capable of extending the current imitation framework to new horizons. This
includes continuous-to-discrete action imitation, as well as primitive-to-macro
action imitation."
"To cluster data that are not linearly separable in the original feature
space, $k$-means clustering was extended to the kernel version. However, the
performance of kernel $k$-means clustering largely depends on the choice of
kernel function. To mitigate this problem, multiple kernel learning has been
introduced into the $k$-means clustering to obtain an optimal kernel
combination for clustering. Despite the success of multiple kernel $k$-means
clustering in various scenarios, few of the existing work update the
combination coefficients based on the diversity of kernels, which leads to the
result that the selected kernels contain high redundancy and would degrade the
clustering performance and efficiency. In this paper, we propose a simple but
efficient strategy that selects a diverse subset from the pre-specified kernels
as the representative kernels, and then incorporate the subset selection
process into the framework of multiple $k$-means clustering. The representative
kernels can be indicated as the significant combination weights. Due to the
non-convexity of the obtained objective function, we develop an alternating
minimization method to optimize the combination coefficients of the selected
kernels and the cluster membership alternatively. We evaluate the proposed
approach on several benchmark and real-world datasets. The experimental results
demonstrate the competitiveness of our approach in comparison with the
state-of-the-art methods."
"Electronic health records (EHR) are rich heterogeneous collection of patient
health information, whose broad adoption provides great opportunities for
systematic health data mining. However, heterogeneous EHR data types and biased
ascertainment impose computational challenges. Here, we present mixEHR, an
unsupervised generative model integrating collaborative filtering and latent
topic models, which jointly models the discrete distributions of data
observation bias and actual data using latent disease-topic distributions. We
apply mixEHR on 12.8 million phenotypic observations from the MIMIC dataset,
and use it to reveal latent disease topics, interpret EHR results, impute
missing data, and predict mortality in intensive care units. Using both
simulation and real data, we show that mixEHR outperforms previous methods and
reveals meaningful multi-disease insights."
"We provide single-model estimates of aleatoric and epistemic uncertainty for
deep neural networks. To estimate aleatoric uncertainty, we propose
Simultaneous Quantile Regression (SQR), a loss function to learn all the
conditional quantiles of a given target variable. These quantiles can be used
to compute well-calibrated prediction intervals. To estimate epistemic
uncertainty, we propose Orthonormal Certificates (OCs), a collection of diverse
non-constant functions that map all training samples to zero. These
certificates map out-of-distribution examples to non-zero values, signaling
epistemic uncertainty. Our uncertainty estimators are computationally
attractive, as they do not require ensembling or retraining deep models, and
achieve competitive performance."
"Many important data analysis applications present with severely imbalanced
datasets with respect to the target variable. A typical example is medical
image analysis, where positive samples are scarce, while performance is
commonly estimated against the correct detection of these positive examples. We
approach this challenge by formulating the problem as anomaly detection with
generative models. We train a generative model without supervision on the
`negative' (common) datapoints and use this model to estimate the likelihood of
unseen data. A successful model allows us to detect the `positive' case as low
likelihood datapoints.
  In this position paper, we present the use of state-of-the-art deep
generative models (GAN and VAE) for the estimation of a likelihood of the data.
Our results show that on the one hand both GANs and VAEs are able to separate
the `positive' and `negative' samples in the MNIST case. On the other hand, for
the NLST case, neither GANs nor VAEs were able to capture the complexity of the
data and discriminate anomalies at the level that this task requires. These
results show that even though there are a number of successes presented in the
literature for using generative models in similar applications, there remain
further challenges for broad successful implementation."
"In this paper, we propose a novel linear discriminant analysis criterion via
the Bhattacharyya error bound estimation based on a novel L1-norm (L1BLDA) and
L2-norm (L2BLDA). Both L1BLDA and L2BLDA maximize the between-class scatters
which are measured by the weighted pairwise distances of class means and
meanwhile minimize the within-class scatters under the L1-norm and L2-norm,
respectively. The proposed models can avoid the small sample size (SSS) problem
and have no rank limit that may encounter in LDA. It is worth mentioning that,
the employment of L1-norm gives a robust performance of L1BLDA, and L1BLDA is
solved through an effective non-greedy alternating direction method of
multipliers (ADMM), where all the projection vectors can be obtained once for
all. In addition, the weighting constants of L1BLDA and L2BLDA between the
between-class and within-class terms are determined by the involved data set,
which makes our L1BLDA and L2BLDA adaptive. The experimental results on both
benchmark data sets as well as the handwritten digit databases demonstrate the
effectiveness of the proposed methods."
"Determinantal point processes (DPPs) have attracted significant attention as
an elegant model that is able to capture the balance between quality and
diversity within sets. DPPs are parameterized by a positive semi-definite
kernel matrix. While DPPs have substantial expressive power, they are
fundamentally limited by the parameterization of the kernel matrix and their
inability to capture nonlinear interactions between items within sets. We
present the deep DPP model as way to address these limitations, by using a deep
feed-forward neural network to learn the kernel matrix. In addition to allowing
us to capture nonlinear item interactions, the deep DPP also allows easy
incorporation of item metadata into DPP learning. Since the learning target is
the DPP kernel matrix, the deep DPP allows us to use existing DPP algorithms
for efficient learning, sampling, and prediction. Through an evaluation on
several real-world datasets, we show experimentally that the deep DPP can
provide a considerable improvement in the predictive performance of DPPs, while
also outperforming strong baseline models in many cases."
"We consider a classifier whose test set is exposed to various perturbations
that are not present in the training set. These test samples still contain
enough features to map them to the same class as their unperturbed counterpart.
Current architectures exhibit rapid degradation of accuracy when trained on
standard datasets but then used to classify perturbed samples of that data. To
address this, we present a novel architecture named DeepConsensus that
significantly improves generalization to these test-time perturbations. Our key
insight is that deep neural networks should directly consider summaries of low
and high level features when making classifications. Existing convolutional
neural networks can be augmented with DeepConsensus, leading to improved
resistance against large and small perturbations on MNIST, EMNIST,
FashionMNIST, CIFAR10 and SVHN datasets."
"Machine learning applications in medical imaging are frequently limited by
the lack of quality labeled data. In this paper, we explore the self training
method, a form of semi-supervised learning, to address the labeling burden. By
integrating reinforcement learning, we were able to expand the application of
self training to complex segmentation networks without any further human
annotation. The proposed approach, reinforced self training (ReST), fine tunes
a semantic segmentation networks by introducing a policy network that learns to
generate pseudolabels. We incorporate an expert demonstration network, based on
inverse reinforcement learning, to enhance clinical validity and convergence of
the policy network. The model was tested on a pulmonary nodule segmentation
task in chest X-rays and achieved the performance of a standard U-Net while
using only 50% of the labeled data, by exploiting unlabeled data. When the same
number of labeled data was used, a moderate to significant cross validation
accuracy improvement was achieved depending on the absolute number of labels
used."
"Learning the activities of animals is important for the purpose of monitoring
their welfare vis a vis their behaviour with respect to their environment and
conspecifics. While previous works have largely focused on activity recognition
in a single animal, little or no work has been done in learning the collective
behaviour of animals. In this work, we address the problem of recognising the
collective movement activities of a group of sheep in a flock. We present a
discriminative framework that learns to track the positions and velocities of
all the animals in the flock in an online manner whilst estimating their
collective activity. We investigate the performance of two simple deep network
architectures and show that we can learn the collective activities with good
accuracy even when the distribution of the activities is skewed."
"Node embedding is the task of extracting informative and descriptive features
over the nodes of a graph. The importance of node embeddings for graph
analytics, as well as learning tasks such as node classification, link
prediction and community detection, has led to increased interest on the
problem leading to a number of recent advances. Much like PCA in the feature
domain, node embedding is an inherently \emph{unsupervised} task; in lack of
metadata used for validation, practical methods may require standardization and
limiting the use of tunable hyperparameters. Finally, node embedding methods
are faced with maintaining scalability in the face of large-scale real-world
graphs of ever-increasing sizes. In the present work, we propose an adaptive
node embedding framework that adjusts the embedding process to a given
underlying graph, in a fully unsupervised manner. To achieve this, we adopt the
notion of a tunable node similarity matrix that assigns weights on paths of
different length. The design of the multilength similarities ensures that the
resulting embeddings also inherit interpretable spectral properties. The
proposed model is carefully studied, interpreted, and numerically evaluated
using stochastic block models. Moreover, an algorithmic scheme is proposed for
training the model parameters effieciently and in an unsupervised manner. We
perform extensive node classification, link prediction, and clustering
experiments on many real world graphs from various domains, and compare with
state-of-the-art scalable and unsupervised node embedding alternatives. The
proposed method enjoys superior performance in many cases, while also yielding
interpretable information on the underlying structure of the graph."
"We consider the task of Inverse Reinforcement Learning in Contextual Markov
Decision Processes (MDPs). In this setting, contexts, which define the reward
and transition kernel, are sampled from a distribution. In addition, although
the reward is a function of the context, it is not provided to the agent.
Instead, the agent observes demonstrations from an optimal policy. The goal is
to learn the reward mapping, such that the agent will act optimally even when
encountering previously unseen contexts, also known as zero-shot transfer. We
formulate this problem as a non-differential convex optimization problem and
propose a novel algorithm to compute its subgradients. Based on this scheme, we
analyze several methods both theoretically, where we compare the sample
complexity and scalability, and empirically. Most importantly, we show both
theoretically and empirically that our algorithms perform zero-shot transfer
(generalize to new and unseen contexts). Specifically, we present empirical
experiments in a dynamic treatment regime, where the goal is to learn a reward
function which explains the behavior of expert physicians based on recorded
data of them treating patients diagnosed with sepsis."
"Hyperbolic embeddings have recently gained attention in machine learning due
to their ability to represent hierarchical data more accurately and succinctly
than their Euclidean analogues. However, multi-relational knowledge graphs
often exhibit multiple simultaneous hierarchies, which current hyperbolic
models do not capture. To address this, we propose a model that embeds
multi-relational graph data in the Poincar\'e ball model of hyperbolic space.
Our Multi-Relational Poincar\'e model (MuRP) learns relation-specific
parameters to transform entity embeddings by M\""obius matrix-vector
multiplication and M\""obius addition. Experiments on the hierarchical WN18RR
knowledge graph show that our Poincar\'e embeddings outperform their Euclidean
counterpart and existing embedding methods on the link prediction task,
particularly at lower dimensionality."
"Humans are able to perform a myriad of sophisticated tasks by drawing upon
skills acquired through prior experience. For autonomous agents to have this
capability, they must be able to extract reusable skills from past experience
that can be recombined in new ways for subsequent tasks. Furthermore, when
controlling complex high-dimensional morphologies, such as humanoid bodies,
tasks often require coordination of multiple skills simultaneously. Learning
discrete primitives for every combination of skills quickly becomes
prohibitive. Composable primitives that can be recombined to create a large
variety of behaviors can be more suitable for modeling this combinatorial
explosion. In this work, we propose multiplicative compositional policies
(MCP), a method for learning reusable motor skills that can be composed to
produce a range of complex behaviors. Our method factorizes an agent's skills
into a collection of primitives, where multiple primitives can be activated
simultaneously via multiplicative composition. This flexibility allows the
primitives to be transferred and recombined to elicit new behaviors as
necessary for novel tasks. We demonstrate that MCP is able to extract
composable skills for highly complex simulated characters from pre-training
tasks, such as motion imitation, and then reuse these skills to solve
challenging continuous control tasks, such as dribbling a soccer ball to a
goal, and picking up an object and transporting it to a target location."
"This work investigates fundamental questions related to learning features in
convolutional neural networks (CNN). Empirical findings across multiple
architectures such as VGG, ResNet, Inception, DenseNet and MobileNet indicate
that weights near the center of a filter are larger than weights on the
outside. Current regularization schemes violate this principle. Thus, we
introduce Locality-promoting Regularization (LOCO-Reg), which yields accuracy
gains across multiple architectures and datasets. We also show theoretically
that the empirical finding is a consequence of maximizing feature cohesion
under the assumption of spatial locality."
"A common strategy to train deep neural networks (DNNs) is to use very large
architectures and to train them until they (almost) achieve zero training
error. Empirically observed good generalization performance on test data, even
in the presence of lots of label noise, corroborate such a procedure. On the
other hand, in statistical learning theory it is known that over-fitting models
may lead to poor generalization properties, occurring in e.g. empirical risk
minimization (ERM) over too large hypotheses classes. Inspired by this
contradictory behavior, so-called interpolation methods have recently received
much attention, leading to consistent and optimally learning methods for some
local averaging schemes with zero training error. However, there is no
theoretical analysis of interpolating ERM-like methods so far. We take a step
in this direction by showing that for certain, large hypotheses classes, some
interpolating ERMs enjoy very good statistical guarantees while others fail in
the worst sense. Moreover, we show that the same phenomenon occurs for DNNs
with zero training error and sufficiently large architectures."
"Structural learning of directed acyclic graphs (DAGs) or Bayesian networks
has been studied extensively under the assumption that data are independent. We
propose a new Gaussian DAG model for dependent data which assumes the
observations are correlated according to an undirected network. Under this
model, we develop a method to estimate the DAG structure given a topological
ordering of the nodes. The proposed method jointly estimates the Bayesian
network and the correlations among observations by optimizing a scoring
function based on penalized likelihood. We show that under some mild
conditions, the proposed method produces consistent estimators after one
iteration. Extensive numerical experiments also demonstrate that by jointly
estimating the DAG structure and the sample correlation, our method achieves
much higher accuracy in structure learning. When the node ordering is unknown,
through experiments on synthetic and real data, we show that our algorithm can
be used to estimate the correlations between samples, with which we can
de-correlate the dependent data to significantly improve the performance of
classical DAG learning methods."
"Graph Neural Networks (graph NNs) are a promising deep learning approach for
analyzing graph-structured data. However, it is known that they do not improve
(or sometimes worsen) their predictive performance as we pile up many layers
and add non-lineality. To tackle this problem, we investigate the expressive
power of graph NNs via their asymptotic behaviors as the layer size tends to
infinity. Our strategy is to generalize the forward propagation of a Graph
Convolutional Network (GCN), which is a popular graph NN variant, as a specific
dynamical system. In the case of a GCN, we show that when its weights satisfy
the conditions determined by the spectra of the (augmented) normalized
Laplacian, its output exponentially approaches the set of signals that carry
information of the connected components and node degrees only for
distinguishing nodes. Our theory enables us to relate the expressive power of
GCNs with the topological information of the underlying graphs inherent in the
graph spectra. To demonstrate this, we characterize the asymptotic behavior of
GCNs on the Erd\H{o}s -- R\'{e}nyi graph. We show that when the Erd\H{o}s --
R\'{e}nyi graph is sufficiently dense and large, a broad range of GCNs on it
suffers from the ""information loss"" in the limit of infinite layers with high
probability. Based on the theory, we provide a principled guideline for weight
normalization of graph NNs. We experimentally confirm that the proposed weight
scaling enhances the predictive performance of GCNs in real data. Code is
available at https://github.com/delta2323/gnn-asymptotics."
"Graphs arise naturally in many real-world applications including social
networks, recommender systems, ontologies, biology, and computational finance.
Traditionally, machine learning models for graphs have been mostly designed for
static graphs. However, many applications involve evolving graphs. This
introduces important challenges for learning and inference since nodes,
attributes, and edges change over time. In this survey, we review the recent
advances in representation learning for dynamic graphs, including dynamic
knowledge graphs. We describe existing models from an encoder-decoder
perspective, categorize these encoders and decoders based on the techniques
they employ, and analyze the approaches in each category. We also review
several prominent applications and widely used datasets and highlight
directions for future research."
"In correlation clustering, we are given $n$ objects together with a binary
similarity score between each pair of them. The goal is to partition the
objects into clusters so to minimise the disagreements with the scores. In this
work we investigate correlation clustering as an active learning problem: each
similarity score can be learned by making a query, and the goal is to minimise
both the disagreements and the total number of queries. On the one hand, we
describe simple active learning algorithms, which provably achieve an almost
optimal trade-off while giving cluster recovery guarantees, and we test them on
different datasets. On the other hand, we prove information-theoretical bounds
on the number of queries necessary to guarantee a prescribed disagreement
bound. These results give a rich characterization of the trade-off between
queries and clustering error."
"Among American women, the rate of breast cancer is only second to lung
cancer. An estimated 12.4% women will develop breast cancer over the course of
their lifetime. The widespread use of social media across the socio-economic
spectrum offers unparalleled ways to facilitate information sharing, in
particular as it pertains to health. Social media is also used by many
healthcare stakeholders, ranging from government agencies to healthcare
industry, to disseminate health information and to engage patients. The purpose
of this study is to investigate people's perceptions and attitudes relate to
breast cancer, especially those that are related to physical activities, on
Twitter. To achieve this, we first identified and collected tweets related to
breast cancer; and then used topic modeling and sentiment analysis techniques
to understanding discussion themes and quantify Twitter users' perceptions and
emotions w.r.t breast cancer to answer 5 research questions."
"Effective medical test suggestions benefit both patients and physicians to
conserve time and improve diagnosis accuracy. In this work, we show that an
agent can learn to suggest effective medical tests. We formulate the problem as
a stage-wise Markov decision process and propose a reinforcement learning
method to train the agent. We introduce a new representation of multiple action
policy along with the training method of the proposed representation.
Furthermore, a new exploration scheme is proposed to accelerate the learning of
disease distributions. Our experimental results demonstrate that the accuracy
of disease diagnosis can be significantly improved with good medical test
suggestions."
"Influence functions estimate the effect of removing a training point on a
model without the need to retrain. They are based on a first-order Taylor
approximation that is guaranteed to be accurate for sufficiently small changes
to the model, and so are commonly used to study the effect of individual points
in large datasets. However, we often want to study the effects of large groups
of training points, e.g., to diagnose batch effects or apportion credit between
different data sources. Removing such large groups can result in significant
changes to the model. Are influence functions still accurate in this setting?
In this paper, we find that across many different types of groups and for a
range of real-world datasets, the predicted effect (using influence functions)
of a group correlates surprisingly well with its actual effect, even if the
absolute and relative errors are large. Our theoretical analysis shows that
such strong correlation arises only under certain settings and need not hold in
general, indicating that real-world datasets have particular properties that
allow the influence approximation to be accurate."
"Ensemble approaches for uncertainty estimation have recently been applied to
the tasks of misclassification detection, out-of-distribution input detection
and adversarial attack detection. Prior Networks have been proposed as an
approach to efficiently \emph{emulate} an ensemble of models for classification
by parameterising a Dirichlet prior distribution over output distributions.
These models have been shown to outperform alternative ensemble approaches,
such as Monte-Carlo Dropout, on the task of out-of-distribution input
detection. However, scaling Prior Networks to complex datasets with many
classes is difficult using the training criteria originally proposed. This
paper makes two contributions. First, we show that the appropriate training
criterion for Prior Networks is the \emph{reverse} KL-divergence between
Dirichlet distributions. This addresses issues in the nature of the training
data target distributions, enabling prior networks to be successfully trained
on classification tasks with arbitrarily many classes, as well as improving
out-of-distribution detection performance. Second, taking advantage of this new
training criterion, this paper investigates using Prior Networks to detect
adversarial attacks and proposes a generalized form of adversarial training. It
is shown that the construction of successful \emph{adaptive} whitebox attacks,
which affect the prediction and evade detection, against Prior Networks trained
on CIFAR-10 and CIFAR-100 using the proposed approach requires a greater amount
of computational effort than against networks defended using standard
adversarial training or MC-dropout."
"Event ticket price prediction is important to marketing strategy for any
sports team or musical ensemble. An accurate prediction model can help the
marketing team to make promotion plan more effectively and efficiently.
However, given all the historical transaction records, it is challenging to
predict the sale price of the remaining seats at any future timestamp, not only
because that the sale price is relevant to a lot of features (seat locations,
date-to-event of the transaction, event date, team performance, etc.), but also
because of the temporal and spatial sparsity in the dataset. For a
game/concert, the ticket selling price of one seat is only observable once at
the time of sale. Furthermore, some seats may not even be purchased (therefore
no record available). In fact, data sparsity is commonly encountered in many
prediction problems. Here, we propose a bi-level optimizing deep neural network
to address the curse of spatio-temporal sparsity. Specifically, we introduce
coarsening and refining layers, and design a bi-level loss function to
integrate different level of loss for better prediction accuracy. Our model can
discover the interrelations among ticket sale price, seat locations, selling
time, event information, etc. Experiments show that our proposed model
outperforms other benchmark methods in real-world ticket selling price
prediction."
"Formal verification of neural networks is essential for their deployment in
safety-critical areas. Many available formal verification methods have been
shown to be instances of a unified Branch and Bound (BaB) formulation. We
propose a novel framework for designing an effective branching strategy for
BaB. Specifically, we learn a graph neural network (GNN) to imitate the strong
branching heuristic behaviour. Our framework differs from previous methods for
learning to branch in two main aspects. Firstly, our framework directly treats
the neural network we want to verify as a graph input for the GNN. Secondly, we
develop an intuitive forward and backward embedding update schedule.
Empirically, our framework achieves roughly $50\%$ reduction in both the number
of branches and the time required for verification on various convolutional
networks when compared to the best available hand-designed branching strategy.
In addition, we show that our GNN model enjoys both horizontal and vertical
transferability. Horizontally, the model trained on easy properties performs
well on properties of increased difficulty levels. Vertically, the model
trained on small neural networks achieves similar performance on large neural
networks."
"In this paper, we investigate the problem of learning disentangled
representations. Given a pair of images sharing some attributes, we aim to
create a low-dimensional representation which is split into two parts: a shared
representation that captures the common information between the images and an
exclusive representation that contains the specific information of each image.
To address this issue, we propose a model based on mutual information
estimation without relying on image reconstruction or image generation. Mutual
information maximization is performed to capture the attributes of data in the
shared and exclusive representations while we minimize the mutual information
between the shared and exclusive representation to enforce representation
disentanglement. We show that these representations are useful to perform
downstream tasks such as image classification and image retrieval based on the
shared or exclusive component. Moreover, classification results show that our
model outperforms the state-of-the-art model based on VAE/GAN approaches in
representation disentanglement."
"Telecommunication (Telco) outdoor position recovery aims to localize outdoor
mobile devices by leveraging measurement report (MR) data. Unfortunately, Telco
position recovery requires sufficient amount of MR samples across different
areas and suffers from high data collection cost. For an area with scarce MR
samples, it is hard to achieve good accuracy. In this paper, by leveraging the
recently developed transfer learning techniques, we design a novel Telco
position recovery framework, called TLoc, to transfer good models in the
carefully selected source domains (those fine-grained small subareas) to a
target one which originally suffers from poor localization accuracy.
Specifically, TLoc introduces three dedicated components: 1) a new coordinate
space to divide an area of interest into smaller domains, 2) a similarity
measurement to select best source domains, and 3) an adaptation of an existing
transfer learning approach. To the best of our knowledge, TLoc is the first
framework that demonstrates the efficacy of applying transfer learning in the
Telco outdoor position recovery. To exemplify, on the 2G GSM and 4G LTE MR
datasets in Shanghai, TLoc outperforms a nontransfer approach by 27.58% and
26.12% less median errors, and further leads to 47.77% and 49.22% less median
errors than a recent fingerprinting approach NBL."
"Gradient boosting algorithms construct a regression predictor using a linear
combination of ``base learners''. Boosting also offers an approach to obtaining
robust non-parametric regression estimators that are scalable to applications
with many explanatory variables. The robust boosting algorithm is based on a
two-stage approach, similar to what is done for robust linear regression: it
first minimizes a robust residual scale estimator, and then improves it by
optimizing a bounded loss function. Unlike previous robust boosting proposals
this approach does not require computing an ad-hoc residual scale estimator in
each boosting iteration. Since the loss functions involved in this robust
boosting algorithm are typically non-convex, a reliable initialization step is
required, such as an L1 regression tree, which is also fast to compute. A
robust variable importance measure can also be calculated via a permutation
procedure. Thorough simulation studies and several data analyses show that,
when no atypical observations are present, the robust boosting approach works
as well as the standard gradient boosting with a squared loss. Furthermore,
when the data contain outliers, the robust boosting estimator outperforms the
alternatives in terms of prediction error and variable selection accuracy."
"Despite remarkable success in practice, modern machine learning models have
been found to be susceptible to adversarial attacks that make
human-imperceptible perturbations to the data, but result in serious and
potentially dangerous prediction errors. To address this issue, practitioners
often use adversarial training to learn models that are robust against such
attacks at the cost of higher generalization error on unperturbed test sets.
The conventional wisdom is that more training data should shrink the gap
between the generalization error of adversarially-trained models and standard
models. However, we study the training of robust classifiers for both Gaussian
and Bernoulli models under $\ell_\infty$ attacks, and we prove that more data
may actually increase this gap. Furthermore, our theoretical results identify
if and when additional data will finally begin to shrink the gap. Lastly, we
experimentally demonstrate that our results also hold for linear regression
models, which may indicate that this phenomenon occurs more broadly."
"Reinforcement learning (RL) has traditionally been understood from an
episodic perspective; the concept of non-episodic RL, where there is no restart
and therefore no reliable recovery, remains elusive. A fundamental question in
non-episodic RL is how to measure the performance of a learner and derive
algorithms to maximize such performance. Conventional wisdom is to maximize the
difference between the average reward received by the learner and the maximal
long-term average reward. In this paper, we argue that if the total time budget
is relatively limited compared to the complexity of the environment, such
comparison may fail to reflect the finite-time optimality of the learner. We
propose a family of measures, called $\gamma$-regret, which we believe to
better capture the finite-time optimality. We give motivations and derive lower
and upper bounds for such measures. Note: A follow-up work (arXiv:2010.00587)
has improved both our lower and upper bound, the gap is now closed at
$\tilde{\Theta}\left(\frac{\sqrt{SAT}}{(1 - \gamma)^{\frac{1}{2}}}\right)$."
"We present a new method for evaluating and training unnormalized density
models. Our approach only requires access to the gradient of the unnormalized
model's log-density. We estimate the Stein discrepancy between the data density
$p(x)$ and the model density $q(x)$ defined by a vector function of the data.
We parameterize this function with a neural network and fit its parameters to
maximize the discrepancy. This yields a novel goodness-of-fit test which
outperforms existing methods on high dimensional data. Furthermore, optimizing
$q(x)$ to minimize this discrepancy produces a novel method for training
unnormalized models which scales more gracefully than existing methods. The
ability to both learn and compare models is a unique feature of the proposed
method."
"We develop two new stochastic Gauss-Newton algorithms for solving a class of
non-convex stochastic compositional optimization problems frequently arising in
practice. We consider both the expectation and finite-sum settings under
standard assumptions, and use both classical stochastic and SARAH estimators
for approximating function values and Jacobians. In the expectation case, we
establish $\mathcal{O}(\varepsilon^{-2})$ iteration-complexity to achieve a
stationary point in expectation and estimate the total number of stochastic
oracle calls for both function value and its Jacobian, where $\varepsilon$ is a
desired accuracy. In the finite sum case, we also estimate
$\mathcal{O}(\varepsilon^{-2})$ iteration-complexity and the total oracle calls
with high probability. To our best knowledge, this is the first time such
global stochastic oracle complexity is established for stochastic Gauss-Newton
methods. Finally, we illustrate our theoretical results via two numerical
examples on both synthetic and real datasets."
"Exploration-exploitation dilemma has long been a crucial issue in
reinforcement learning. In this paper, we propose a new approach to
automatically balance between these two. Our method is built upon the Soft
Actor-Critic (SAC) algorithm, which uses an ""entropy temperature"" that balances
the original task reward and the policy entropy, and hence controls the
trade-off between exploitation and exploration. It is empirically shown that
SAC is very sensitive to this hyperparameter, and the follow-up work (SAC-v2),
which uses constrained optimization for automatic adjustment, has some
limitations. The core of our method, namely Meta-SAC, is to use metagradient
along with a novel meta objective to automatically tune the entropy temperature
in SAC. We show that Meta-SAC achieves promising performances on several of the
Mujoco benchmarking tasks, and outperforms SAC-v2 over 10% in one of the most
challenging tasks, humanoid-v2."
"Messenger advertisements (ads) give direct and personal user experience
yielding high conversion rates and sales. However, people are skeptical about
ads and sometimes perceive them as spam, which eventually leads to a decrease
in user satisfaction. Targeted advertising, which serves ads to individuals who
may exhibit interest in a particular advertising message, is strongly required.
The key to the success of precise user targeting lies in learning the accurate
user and ad representation in the embedding space. Most of the previous studies
have limited the representation learning in the Euclidean space, but recent
studies have suggested hyperbolic manifold learning for the distinct projection
of complex network properties emerging from real-world datasets such as social
networks, recommender systems, and advertising. We propose a framework that can
effectively learn the hierarchical structure in users and ads on the hyperbolic
space, and extend to the Multi-Manifold Learning. Our method constructs
multiple hyperbolic manifolds with learnable curvatures and maps the
representation of user and ad to each manifold. The origin of each manifold is
set as the centroid of each user cluster. The user preference for each ad is
estimated using the distance between two entities in the hyperbolic space, and
the final prediction is determined by aggregating the values calculated from
the learned multiple manifolds. We evaluate our method on public benchmark
datasets and a large-scale commercial messenger system LINE, and demonstrate
its effectiveness through improved performance."
"Label distribution learning (LDL) is an interpretable and general learning
paradigm that has been applied in many real-world applications. In contrast to
the simple logical vector in single-label learning (SLL) and multi-label
learning (MLL), LDL assigns labels with a description degree to each instance.
In practice, two challenges exist in LDL, namely, how to address the
dimensional gap problem during the learning process of LDL and how to exactly
recover label distributions from existing logical labels, i.e., Label
Enhancement (LE). For most existing LDL and LE algorithms, the fact that the
dimension of the input matrix is much higher than that of the output one is
alway ignored and it typically leads to the dimensional reduction owing to the
unidirectional projection. The valuable information hidden in the feature space
is lost during the mapping process. To this end, this study considers
bidirectional projections function which can be applied in LE and LDL problems
simultaneously. More specifically, this novel loss function not only considers
the mapping errors generated from the projection of the input space into the
output one but also accounts for the reconstruction errors generated from the
projection of the output space back to the input one. This loss function aims
to potentially reconstruct the input data from the output data. Therefore, it
is expected to obtain more accurate results. Finally, experiments on several
real-world datasets are carried out to demonstrate the superiority of the
proposed method for both LE and LDL."
"Meta-learning algorithms aim to learn two components: a model that predicts
targets for a task, and a base learner that quickly updates that model when
given examples from a new task. This additional level of learning can be
powerful, but it also creates another potential source for overfitting, since
we can now overfit in either the model or the base learner. We describe both of
these forms of metalearning overfitting, and demonstrate that they appear
experimentally in common meta-learning benchmarks. We then use an
information-theoretic framework to discuss meta-augmentation, a way to add
randomness that discourages the base learner and model from learning trivial
solutions that do not generalize to new tasks. We demonstrate that
meta-augmentation produces large complementary benefits to recently proposed
meta-regularization techniques."
"The reliability of machine learning systems critically assumes that the
associations between features and labels remain similar between training and
test distributions. However, unmeasured variables, such as confounders, break
this assumption---useful correlations between features and labels at training
time can become useless or even harmful at test time. For example, high obesity
is generally predictive for heart disease, but this relation may not hold for
smokers who generally have lower rates of obesity and higher rates of heart
disease. We present a framework for making models robust to spurious
correlations by leveraging humans' common sense knowledge of causality.
Specifically, we use human annotation to augment each training example with a
potential unmeasured variable (i.e. an underweight patient with heart disease
may be a smoker), reducing the problem to a covariate shift problem. We then
introduce a new distributionally robust optimization objective over unmeasured
variables (UV-DRO) to control the worst-case loss over possible test-time
shifts. Empirically, we show improvements of 5-10% on a digit recognition task
confounded by rotation, and 1.5-5% on the task of analyzing NYPD Police Stops
confounded by location."
"This paper introduces single-partition adaptive Q-learning (SPAQL), an
algorithm for model-free episodic reinforcement learning (RL), which adaptively
partitions the state-action space of a Markov decision process (MDP), while
simultaneously learning a time-invariant policy (i. e., the mapping from states
to actions does not depend explicitly on the episode time step) for maximizing
the cumulative reward. The trade-off between exploration and exploitation is
handled by using a mixture of upper confidence bounds (UCB) and Boltzmann
exploration during training, with a temperature parameter that is automatically
tuned as training progresses. The algorithm is an improvement over adaptive
Q-learning (AQL). It converges faster to the optimal solution, while also using
fewer arms. Tests on episodes with a large number of time steps show that SPAQL
has no problems scaling, unlike AQL. Based on this empirical evidence, we claim
that SPAQL may have a higher sample efficiency than AQL, thus being a relevant
contribution to the field of efficient model-free RL methods."
"The financial sector presents many opportunities to apply various machine
learning techniques. Centralized machine learning creates a constraint which
limits further applications in finance sectors. Data privacy is a fundamental
challenge for a variety of finance and insurance applications that account on
learning a model across different sections. In this paper, we define a new
practical scheme of collaborative machine learning that one party owns data,
but another party owns labels only, and term this \textbf{Asymmetrically
Collaborative Machine Learning}. For this scheme, we propose a novel
privacy-preserving architecture where two parties can collaboratively train a
deep learning model efficiently while preserving the privacy of each party's
data. More specifically, we decompose the forward propagation and
backpropagation of the neural network into four different steps and propose a
novel protocol to handle information leakage in these steps. Our extensive
experiments on different datasets demonstrate not only stable training without
accuracy loss, but also more than 100 times speedup compared with the
state-of-the-art system."
"Fraudulent activities are an expensive problem for many financial
institutions, costing billions of dollars to corporations annually. More
commonly occurring activities in this regard are credit card frauds. In this
context, the credit card fraud detection concept has been developed over the
lines of incorporating the uncertainty in our prediction system to ensure
better judgment in such a crucial task. We propose to use a sparse Gaussian
classification method to work with the large data-set and use the concept of
pseudo or inducing inputs. We perform the same with different sets of kernels
and the different number of inducing data points to show the best accuracy was
obtained with the selection of RBF kernel with a higher number of inducing
points. Our approach was able to work over large financial data given the
stochastic nature of our method employed and also good test accuracy with low
variance over the prediction suggesting confidence and robustness in our model.
Using the methodologies of Bayesian learning techniques with the incorporated
inducing points phenomenon, are successfully able to obtain a healthy accuracy
and a high confidence score."
"We focus on the problem of black-box adversarial attacks, where the aim is to
generate adversarial examples for deep learning models solely based on
information limited to output label~(hard label) to a queried data input. We
propose a simple and efficient Bayesian Optimization~(BO) based approach for
developing black-box adversarial attacks. Issues with BO's performance in high
dimensions are avoided by searching for adversarial examples in a structured
low-dimensional subspace. We demonstrate the efficacy of our proposed attack
method by evaluating both $\ell_\infty$ and $\ell_2$ norm constrained
untargeted and targeted hard label black-box attacks on three standard datasets
- MNIST, CIFAR-10 and ImageNet. Our proposed approach consistently achieves 2x
to 10x higher attack success rate while requiring 10x to 20x fewer queries
compared to the current state-of-the-art black-box adversarial attacks."
"Determining the traffic scenario space is a major challenge for the
homologation and coverage assessment of automated driving functions. In
contrast to current approaches that are mainly scenario-based and rely on
expert knowledge, we introduce two data driven autoencoding models that learn a
latent representation of traffic scenes. First is a CNN based spatio-temporal
model that autoencodes a grid of traffic participants' positions. Secondly, we
develop a pure temporal RNN based model that auto-encodes a sequence of sets.
To handle the unordered set data, we had to incorporate the permutation
invariance property. Finally, we show how the latent scenario embeddings can be
used for clustering traffic scenarios and similarity retrieval."
"We study bandit convex optimization methods that adapt to the norm of the
comparator, a topic that has only been studied before for its full-information
counterpart. Specifically, we develop convex bandit algorithms with regret
bounds that are small whenever the norm of the comparator is small. We first
use techniques from the full-information setting to develop comparator-adaptive
algorithms for linear bandits. Then, we extend the ideas to convex bandits with
Lipschitz or smooth loss functions, using a new single-point gradient estimator
and carefully designed surrogate losses."
"It is customary for researchers and practitioners to fit linear models in
order to predict NBA player's salary based on the players' performance on
court. On the contrary, we focus on the players salary share (with regards to
the team payroll) by first selecting the most important determinants or
statistics (years of experience in the league, games played, etc.) and then
utilise them to predict the player salaries by employing a non linear Random
Forest machine learning algorithm. We externally evaluate our salary
predictions, thus we avoid the phenomenon of over-fitting observed in most
papers. Overall, using data from three distinct periods, 2017-2019 we identify
the important factors that achieve very satisfactory salary predictions and we
draw useful conclusions."
"We consider statistical inference for impulse responses in sparse, structural
high-dimensional vector autoregressive (SVAR) systems. We introduce consistent
estimators of impulse responses in the high-dimensional setting and suggest
valid inference procedures for the same parameters. Statistical inference in
our setting is much more involved since standard procedures, like the
delta-method, do not apply. By using local projection equations, we first
construct a de-sparsified version of regularized estimators of the moving
average parameters associated with the VAR system. We then obtain estimators of
the structural impulse responses by combining the aforementioned de-sparsified
estimators with a non-regularized estimator of the contemporaneous impact
matrix, also taking into account the high-dimensionality of the system. We show
that the distribution of the derived estimators of structural impulse responses
has a Gaussian limit. We also present a valid bootstrap procedure to estimate
this distribution. Applications of the inference procedure in the construction
of confidence intervals for impulse responses as well as in tests for forecast
error variance decomposition are presented. Our procedure is illustrated by
means of simulations."
"We perform a careful, thorough, and large scale empirical study of the
correspondence between wide neural networks and kernel methods. By doing so, we
resolve a variety of open questions related to the study of infinitely wide
neural networks. Our experimental results include: kernel methods outperform
fully-connected finite-width networks, but underperform convolutional finite
width networks; neural network Gaussian process (NNGP) kernels frequently
outperform neural tangent (NT) kernels; centered and ensembled finite networks
have reduced posterior variance and behave more similarly to infinite networks;
weight decay and the use of a large learning rate break the correspondence
between finite and infinite networks; the NTK parameterization outperforms the
standard parameterization for finite width networks; diagonal regularization of
kernels acts similarly to early stopping; floating point precision limits
kernel performance beyond a critical dataset size; regularized ZCA whitening
improves accuracy; finite network performance depends non-monotonically on
width in ways not captured by double descent phenomena; equivariance of CNNs is
only beneficial for narrow networks far from the kernel regime. Our experiments
additionally motivate an improved layer-wise scaling for weight decay which
improves generalization in finite-width networks. Finally, we develop improved
best practices for using NNGP and NT kernels for prediction, including a novel
ensembling technique. Using these best practices we achieve state-of-the-art
results on CIFAR-10 classification for kernels corresponding to each
architecture class we consider."
"We are interested in clustering parts of a given single multi-variate series
in an unsupervised manner. We would like to segment and cluster the series such
that the resulting blocks present in each cluster are coherent with respect to
a known model (e.g. physics model). Data points are said to be coherent if they
can be described using this model with the same parameters. We have designed an
algorithm based on dynamic programming with constraints on the number of
clusters, the number of transitions as well as the minimal size of a block such
that the clusters are coherent with this process. We present an use-case:
clustering of petrophysical series using the Waxman-Smits equation."
"Forecasting of time series in continuous systems becomes an increasingly
relevant task due to recent developments in IoT and 5G. The popular forecasting
model ARIMA is applied to a large variety of applications for decades. An
online variant of ARIMA applies the Online Newton Step in order to learn the
underlying process of the time series. This optimization method has pitfalls
concerning the computational complexity and convergence. Thus, this work
focuses on the computational less expensive Online Gradient Descent
optimization method, which became popular for learning of neural networks in
recent years. For the iterative training of such models, we propose a new
approach combining different Online Gradient Descent learners (such as Adam,
AMSGrad, Adagrad, Nesterov) to achieve fast convergence. The evaluation on
synthetic data and experimental datasets show that the proposed approach
outperforms the existing methods resulting in an overall lower prediction
error."
"We investigate the problem of exact cluster recovery using oracle queries.
Previous results show that clusters in Euclidean spaces that are convex and
separated with a margin can be reconstructed exactly using only $O(\log n)$
same-cluster queries, where $n$ is the number of input points. In this work, we
study this problem in the more challenging non-convex setting. We introduce a
structural characterization of clusters, called $(\beta,\gamma)$-convexity,
that can be applied to any finite set of points equipped with a metric (or even
a semimetric, as the triangle inequality is not needed). Using
$(\beta,\gamma)$-convexity, we can translate natural density properties of
clusters (which include, for instance, clusters that are strongly non-convex in
$\mathbb{R}^d$) into a graph-theoretic notion of convexity. By exploiting this
convexity notion, we design a deterministic algorithm that recovers
$(\beta,\gamma)$-convex clusters using $O(k^2 \log n + k^2
(6/\beta\gamma)^{dens(X)})$ same-cluster queries, where $k$ is the number of
clusters and $dens(X)$ is the density dimension of the semimetric. We show that
an exponential dependence on the density dimension is necessary, and we also
show that, if we are allowed to make $O(k^2 + k\log n)$ additional queries to a
""cluster separation"" oracle, then we can recover clusters that have different
and arbitrary scales, even when the scale of each cluster is unknown."
"In this paper, we develop a new classification method for manifold-valued
data in the framework of probabilistic learning vector quantization. In many
classification scenarios, the data can be naturally represented by symmetric
positive definite matrices, which are inherently points that live on a curved
Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds,
traditional Euclidean machine learning algorithms yield poor results on such
data. In this paper, we generalize the probabilistic learning vector
quantization algorithm for data points living on the manifold of symmetric
positive definite matrices equipped with Riemannian natural metric
(affine-invariant metric). By exploiting the induced Riemannian distance, we
derive the probabilistic learning Riemannian space quantization algorithm,
obtaining the learning rule through Riemannian gradient descent. Empirical
investigations on synthetic data, image data , and motor imagery EEG data
demonstrate the superior performance of the proposed method."
"We study the problem of Online Convex Optimization (OCO) with memory, which
allows loss functions to depend on past decisions and thus captures temporal
effects of learning problems. In this paper, we introduce dynamic policy regret
as the performance measure to design algorithms robust to non-stationary
environments, which competes algorithms' decisions with a sequence of changing
comparators. We propose a novel algorithm for OCO with memory that provably
enjoys an optimal dynamic policy regret in terms of time horizon,
non-stationarity measure, and memory length. The key technical challenge is how
to control the switching cost, the cumulative movements of player's decisions,
which is neatly addressed by a novel switching-cost-aware online ensemble
approach equipped with a new meta-base decomposition of dynamic policy regret
and a careful design of meta-learner and base-learner that explicitly
regularizes the switching cost. The results are further applied to tackle
non-stationarity in online non-stochastic control (Agarwal et al., 2019), i.e.,
controlling a linear dynamical system with adversarial disturbance and convex
cost functions. We derive a novel gradient-based controller with dynamic policy
regret guarantees, which is the first controller provably competitive to a
sequence of changing policies for online non-stochastic control."
"Robustness of deep neural networks against adversarial perturbations is a
pressing concern motivated by recent findings showing the pervasive nature of
such vulnerabilities. One method of characterizing the robustness of a neural
network model is through its Lipschitz constant, which forms a robustness
certificate. A natural question to ask is, for a fixed model class (such as
neural networks) and a dataset of size $n$, what is the smallest achievable
Lipschitz constant among all models that fit the dataset? Recently, (Bubeck et
al., 2020) conjectured that when using two-layer networks with $k$ neurons to
fit a generic dataset, the smallest Lipschitz constant is
$\Omega(\sqrt{\frac{n}{k}})$. This implies that one would require one neuron
per data point to robustly fit the data. In this work we derive a lower bound
on the Lipschitz constant for any arbitrary model class with bounded Rademacher
complexity. Our result coincides with that conjectured in (Bubeck et al., 2020)
for two-layer networks under the assumption of bounded weights. However, due to
our result's generality, we also derive bounds for multi-layer neural networks,
discovering that one requires $\log n$ constant-sized layers to robustly fit
the data. Thus, our work establishes a law of robustness for weight bounded
neural networks and provides formal evidence on the necessity of
over-parametrization in deep learning."
"Subsequence-based time series classification algorithms provide accurate and
interpretable models, but training these models is extremely computation
intensive. The asymptotic time complexity of subsequence-based algorithms
remains a higher-order polynomial, because these algorithms are based on
exhaustive search for highly discriminative subsequences. Pattern sampling has
been proposed as an effective alternative to mitigate the pattern explosion
phenomenon. Therefore, we employ pattern sampling to extract discriminative
features from discretized time series data. A weighted trie is created based on
the discretized time series data to sample highly discriminative patterns.
These sampled patterns are used to identify the shapelets which are used to
transform the time series classification problem into a feature-based
classification problem. Finally, a classification model can be trained using
any off-the-shelf algorithm. Creating a pattern sampler requires a small number
of patterns to be evaluated compared to an exhaustive search as employed by
previous approaches. Compared to previously proposed algorithms, our approach
requires considerably less computational and memory resources. Experiments
demonstrate how the proposed approach fares in terms of classification accuracy
and runtime performance."
"Learning from heterogeneous data poses challenges such as combining data from
various sources and of different types. Meanwhile, heterogeneous data are often
associated with missingness in real-world applications due to heterogeneity and
noise of input sources. In this work, we propose the variational selective
autoencoder (VSAE), a general framework to learn representations from
partially-observed heterogeneous data. VSAE learns the latent dependencies in
heterogeneous data by modeling the joint distribution of observed data,
unobserved data, and the imputation mask which represents how the data are
missing. It results in a unified model for various downstream tasks including
data generation and imputation. Evaluation on both low-dimensional and
high-dimensional heterogeneous datasets for these two tasks shows improvement
over state-of-the-art models."
"Even when unable to run experiments, practitioners can evaluate prospective
policies, using previously logged data. However, while the bandits literature
has adopted a diverse set of objectives, most research on off-policy evaluation
to date focuses on the expected reward. In this paper, we introduce Lipschitz
risk functionals, a broad class of objectives that subsumes conditional
value-at-risk (CVaR), variance, mean-variance, many distorted risks, and CPT
risks, among others. We propose Off-Policy Risk Assessment (OPRA), a framework
that first estimates a target policy's CDF and then generates plugin estimates
for any collection of Lipschitz risks, providing finite sample guarantees that
hold simultaneously over the entire class. We instantiate OPRA with both
importance sampling and doubly robust estimators. Our primary theoretical
contributions are (i) the first uniform concentration inequalities for both CDF
estimators in contextual bandits and (ii) error bounds on our Lipschitz risk
estimates, which all converge at a rate of $O(1/\sqrt{n})$."
"One of the key drivers of complexity in the classical (stochastic)
multi-armed bandit (MAB) problem is the difference between mean rewards in the
top two arms, also known as the instance gap. The celebrated Upper Confidence
Bound (UCB) policy is among the simplest optimism-based MAB algorithms that
naturally adapts to this gap: for a horizon of play n, it achieves optimal
O(log n) regret in instances with ""large"" gaps, and a near-optimal O(\sqrt{n
log n}) minimax regret when the gap can be arbitrarily ""small."" This paper
provides new results on the arm-sampling behavior of UCB, leading to several
important insights. Among these, it is shown that arm-sampling rates under UCB
are asymptotically deterministic, regardless of the problem complexity. This
discovery facilitates new sharp asymptotics and a novel alternative proof for
the O(\sqrt{n log n}) minimax regret of UCB. Furthermore, the paper also
provides the first complete process-level characterization of the MAB problem
under UCB in the conventional diffusion scaling. Among other things, the
""small"" gap worst-case lens adopted in this paper also reveals profound
distinctions between the behavior of UCB and Thompson Sampling, such as an
""incomplete learning"" phenomenon characteristic of the latter."
"Tensor completion is the problem of estimating the missing values of
high-order data from partially observed entries. Data corruption due to
prevailing outliers poses major challenges to traditional tensor completion
algorithms, which catalyzed the development of robust algorithms that alleviate
the effect of outliers. However, existing robust methods largely presume that
the corruption is sparse, which may not hold in practice. In this paper, we
develop a two-stage robust tensor completion approach to deal with tensor
completion of visual data with a large amount of gross corruption. A novel
coarse-to-fine framework is proposed which uses a global coarse completion
result to guide a local patch refinement process. To efficiently mitigate the
effect of a large number of outliers on tensor recovery, we develop a new
M-estimator-based robust tensor ring recovery method which can adaptively
identify the outliers and alleviate their negative effect in the optimization.
The experimental results demonstrate the superior performance of the proposed
approach over state-of-the-art robust algorithms for tensor completion."
"With the rise of machine learning and deep learning based applications in
practice, monitoring, i.e. verifying that these operate within specification,
has become an important practical problem. An important aspect of this
monitoring is to check whether the inputs (or intermediates) have strayed from
the distribution they were validated for, which can void the performance
assurances obtained during testing.
  There are two common approaches for this. The, perhaps, more classical one is
outlier detection or novelty detection, where, for a single input we ask
whether it is an outlier, i.e. exceedingly unlikely to have originated from a
reference distribution. The second, perhaps more recent approach, is to
consider a larger number of inputs and compare its distribution to a reference
distribution (e.g. sampled during testing). This is done under the label drift
detection.
  In this work, we bridge the gap between outlier detection and drift detection
through comparing a given number of inputs to an automatically chosen part of
the reference distribution."
"In this paper, a robust classification-autoencoder (CAE) is proposed, which
has strong ability to recognize outliers and defend adversaries. The main idea
is to change the autoencoder from an unsupervised learning model into a
classifier, where the encoder is used to compress samples with different labels
into disjoint compression spaces and the decoder is used to recover samples
from their compression spaces. The encoder is used both as a compressed feature
learner and as a classifier, and the decoder is used to decide whether the
classification given by the encoder is correct by comparing the input sample
with the output. Since adversary samples are seemingly inevitable for the
current DNN framework, the list classifier to defend adversaries is introduced
based on CAE, which outputs several labels and the corresponding samples
recovered by the CAE. Extensive experimental results are used to show that the
CAE achieves state of the art to recognize outliers by finding almost all
outliers; the list classifier gives near lossless classification in the sense
that the output list contains the correct label for almost all adversaries and
the size of the output list is reasonably small."
"We propose Pathfinder, a variational method for approximately sampling from
differentiable log densities. Starting from a random initialization, Pathfinder
locates normal approximations to the target density along a quasi-Newton
optimization path, with local covariance estimated using the inverse Hessian
estimates produced by the optimizer. Pathfinder returns draws from the
approximation with the lowest estimated Kullback-Leibler (KL) divergence to the
true posterior. We evaluate Pathfinder on a wide range of posterior
distributions, demonstrating that its approximate draws are better than those
from automatic differentiation variational inference (ADVI) and comparable to
those produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as
measured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC
runs, Pathfinder requires one to two orders of magnitude fewer log density and
gradient evaluations, with greater reductions for more challenging posteriors.
Importance resampling over multiple runs of Pathfinder improves the diversity
of approximate draws, reducing 1-Wasserstein distance further and providing a
measure of robustness to optimization failures on plateaus, saddle points, or
in minor modes. The Monte Carlo KL divergence estimates are embarrassingly
parallelizable in the core Pathfinder algorithm, as are multiple runs in the
resampling version, further increasing Pathfinder's speed advantage with
multiple cores."
"In several real world applications, machine learning models are deployed to
make predictions on data whose distribution changes gradually along time,
leading to a drift between the train and test distributions. Such models are
often re-trained on new data periodically, and they hence need to generalize to
data not too far into the future. In this context, there is much prior work on
enhancing temporal generalization, e.g. continuous transportation of past data,
kernel smoothed time-sensitive parameters and more recently, adversarial
learning of time-invariant features. However, these methods share several
limitations, e.g, poor scalability, training instability, and dependence on
unlabeled data from the future. Responding to the above limitations, we propose
a simple method that starts with a model with time-sensitive parameters but
regularizes its temporal complexity using a Gradient Interpolation (GI) loss.
GI allows the decision boundary to change along time and can still prevent
overfitting to the limited training time snapshots by allowing task-specific
control over changes along time. We compare our method to existing baselines on
multiple real-world datasets, which show that GI outperforms more complicated
generative and adversarial approaches on the one hand, and simpler gradient
regularization methods on the other."
"Traffic flow forecasting is a crucial task in urban computing. The challenge
arises as traffic flows often exhibit intrinsic and latent spatio-temporal
correlations that cannot be identified by extracting the spatial and temporal
patterns of traffic data separately. We argue that such correlations are
universal and play a pivotal role in traffic flow. We put forward {spacetime
interval learning} as a paradigm to explicitly capture these correlations
through a unified analysis of both spatial and temporal features. Unlike the
state-of-the-art methods, which are restricted to a particular road network, we
model the universal spatio-temporal correlations that are transferable from
cities to cities. To this end, we propose a new spacetime interval learning
framework that constructs a local-spacetime context of a traffic sensor
comprising the data from its neighbors within close time points. Based on this
idea, we introduce local spacetime neural network (STNN), which employs novel
spacetime convolution and attention mechanism to learn the universal
spatio-temporal correlations. The proposed STNN captures local traffic
patterns, which does not depend on a specific network structure. As a result, a
trained STNN model can be applied on any unseen traffic networks. We evaluate
the proposed STNN on two public real-world traffic datasets and a simulated
dataset on dynamic networks. The experiment results show that STNN not only
improves prediction accuracy by 4% over state-of-the-art methods, but is also
effective in handling the case when the traffic network undergoes dynamic
changes as well as the superior generalization capability."
"Heterogeneous multi-typed, multimodal relational data is increasingly
available in many domains and their exploratory analysis poses several
challenges. We advance the state-of-the-art in neural unsupervised learning to
analyze such data. We design the first neural method for collective matrix
tri-factorization of arbitrary collections of matrices to perform spectral
clustering of all constituent entities and learn cluster associations.
Experiments on benchmark datasets demonstrate its efficacy over previous
non-neural approaches. Leveraging signals from multi-way clustering and
collective matrix completion we design a unique technique, called Discordance
Analysis, to reveal information discrepancies across subsets of matrices in a
collection with respect to two entities. We illustrate its utility in quality
assessment of knowledge bases and in improving representation learning."
"The rampant adoption of ML methodologies has revealed that models are usually
adopted to make decisions without taking into account the uncertainties in
their predictions. More critically, they can be vulnerable to adversarial
examples. Thus, we believe that developing ML systems that take into account
predictive uncertainties and are robust against adversarial examples is a must
for critical, real-world tasks. We start with a case study in retailing. We
propose a robust implementation of the Nerlove-Arrow model using a Bayesian
structural time series model. Its Bayesian nature facilitates incorporating
prior information reflecting the manager's views, which can be updated with
relevant data. However, this case adopted classical Bayesian techniques, such
as the Gibbs sampler. Nowadays, the ML landscape is pervaded with neural
networks and this chapter also surveys current developments in this sub-field.
Then, we tackle the problem of scaling Bayesian inference to complex models and
large data regimes. In the first part, we propose a unifying view of two
different Bayesian inference algorithms, Stochastic Gradient Markov Chain Monte
Carlo (SG-MCMC) and Stein Variational Gradient Descent (SVGD), leading to
improved and efficient novel sampling schemes. In the second part, we develop a
framework to boost the efficiency of Bayesian inference in probabilistic models
by embedding a Markov chain sampler within a variational posterior
approximation. After that, we present an alternative perspective on adversarial
classification based on adversarial risk analysis, and leveraging the scalable
Bayesian approaches from chapter 2. In chapter 4 we turn to reinforcement
learning, introducing Threatened Markov Decision Processes, showing the
benefits of accounting for adversaries in RL while the agent learns."
"In the regression problem, we consider the problem of estimating the variance
function by the means of aggregation methods. We focus on two particular
aggregation setting: Model Selection aggregation (MS) and Convex aggregation
(C) where the goal is to select the best candidate and to build the best convex
combination of candidates respectively among a collection of candidates. In
both cases, the construction of the estimator relies on a two-step procedure
and requires two independent samples. The first step exploits the first sample
to build the candidate estimators for the variance function by the
residual-based method and then the second dataset is used to perform the
aggregation step. We show the consistency of the proposed method with respect
to the L 2error both for MS and C aggregations. We evaluate the performance of
these two methods in the heteroscedastic model and illustrate their interest in
the regression problem with reject option."
"Random forests have been widely used for their ability to provide so-called
importance measures, which give insight at a global (per dataset) level on the
relevance of input variables to predict a certain output. On the other hand,
methods based on Shapley values have been introduced to refine the analysis of
feature relevance in tree-based models to a local (per instance) level. In this
context, we first show that the global Mean Decrease of Impurity (MDI) variable
importance scores correspond to Shapley values under some conditions. Then, we
derive a local MDI importance measure of variable relevance, which has a very
natural connection with the global MDI measure and can be related to a new
notion of local feature relevance. We further link local MDI importances with
Shapley values and discuss them in the light of related measures from the
literature. The measures are illustrated through experiments on several
classification and regression problems."
"Because of its sample efficiency, Bayesian optimization (BO) has become a
popular approach dealing with expensive black-box optimization problems, such
as hyperparameter optimization (HPO). Recent empirical experiments showed that
the loss landscapes of HPO problems tend to be more benign than previously
assumed, i.e. in the best case uni-modal and convex, such that a BO framework
could be more efficient if it can focus on those promising local regions. In
this paper, we propose BOinG, a two-stage approach that is tailored toward
mid-sized configuration spaces, as one encounters in many HPO problems. In the
first stage, we build a scalable global surrogate model with a random forest to
describe the overall landscape structure. Further, we choose a promising
subregion via a bottom-up approach on the upper-level tree structure. In the
second stage, a local model in this subregion is utilized to suggest the point
to be evaluated next. Empirical experiments show that BOinG is able to exploit
the structure of typical HPO problems and performs particularly well on
mid-sized problems from synthetic functions and HPO."
"Deep Neural Networks (DNNs) have performed admirably in classification tasks.
However, the characterization of their classification uncertainties, required
for certain applications, has been lacking. In this work, we investigate the
issue by assessing DNNs' ability to estimate conditional probabilities and
propose a framework for systematic uncertainty characterization. Denoting the
input sample as x and the category as y, the classification task of assigning a
category y to a given input x can be reduced to the task of estimating the
conditional probabilities p(y|x), as approximated by the DNN at its last layer
using the softmax function. Since softmax yields a vector whose elements all
fall in the interval (0, 1) and sum to 1, it suggests a probabilistic
interpretation to the DNN's outcome. Using synthetic and real-world datasets,
we look into the impact of various factors, e.g., probability density f(x) and
inter-categorical sparsity, on the precision of DNNs' estimations of p(y|x),
and find that the likelihood probability density and the inter-categorical
sparsity have greater impacts than the prior probability to DNNs'
classification uncertainty."
"Federated learning of deep learning models for supervised tasks, e.g. image
classification and segmentation, has found many applications: for example in
human-in-the-loop tasks such as film post-production where it enables sharing
of domain expertise of human artists in an efficient and effective fashion. In
many such applications, we need to protect the training data from being leaked
when gradients are shared in the training process due to IP or privacy
concerns. Recent works have demonstrated that it is possible to reconstruct the
training data from gradients for an image-classification model when its
architecture is known. However, there is still an incomplete theoretical
understanding of the efficacy and failure of such attacks. In this paper, we
analyse the source of training-data leakage from gradients. We formulate the
problem of training data reconstruction as solving an optimisation problem
iteratively for each layer. The layer-wise objective function is primarily
defined by weights and gradients from the current layer as well as the output
from the reconstruction of the subsequent layer, but it might also involve a
'pull-back' constraint from the preceding layer. Training data can be
reconstructed when we solve the problem backward from the output of the network
through each layer. Based on this formulation, we are able to attribute the
potential leakage of the training data in a deep network to its architecture.
We also propose a metric to measure the level of security of a deep learning
model against gradient-based attacks on the training data."
"We investigate the generalization properties of a self-training algorithm
with halfspaces. The approach learns a list of halfspaces iteratively from
labeled and unlabeled training data, in which each iteration consists of two
steps: exploration and pruning. In the exploration phase, the halfspace is
found sequentially by maximizing the unsigned-margin among unlabeled examples
and then assigning pseudo-labels to those that have a distance higher than the
current threshold. The pseudo-labeled examples are then added to the training
set, and a new classifier is learned. This process is repeated until no more
unlabeled examples remain for pseudo-labeling. In the pruning phase,
pseudo-labeled samples that have a distance to the last halfspace greater than
the associated unsigned-margin are then discarded. We prove that the
misclassification error of the resulting sequence of classifiers is bounded and
show that the resulting semi-supervised approach never degrades performance
compared to the classifier learned using only the initial labeled training set.
Experiments carried out on a variety of benchmarks demonstrate the efficiency
of the proposed approach compared to state-of-the-art methods."
"We comment on a recent TKDE paper ""Linear Approximation of F-measure for the
Performance Evaluation of Classification Algorithms on Imbalanced Data Sets"",
and make two improvements related to comparison of F-measures for two
prediction rules."
"We present a numerical method to model dynamical systems from data. We use
the recently introduced method Scalable Probabilistic Approximation (SPA) to
project points from a Euclidean space to convex polytopes and represent these
projected states of a system in new, lower-dimensional coordinates denoting
their position in the polytope. We then introduce a specific nonlinear
transformation to construct a model of the dynamics in the polytope and to
transform back into the original state space. To overcome the potential loss of
information from the projection to a lower-dimensional polytope, we use memory
in the sense of the delay-embedding theorem of Takens. By construction, our
method produces stable models. We illustrate the capacity of the method to
reproduce even chaotic dynamics and attractors with multiple connected
components on various examples."
"Hybrid methods have been shown to outperform pure statistical and pure deep
learning methods at forecasting tasks and quantifying the associated
uncertainty with those forecasts (prediction intervals). One example is
Exponential Smoothing Recurrent Neural Network (ES-RNN), a hybrid between a
statistical forecasting model and a recurrent neural network variant. ES-RNN
achieves a 9.4\% improvement in absolute error in the Makridakis-4 Forecasting
Competition. This improvement and similar outperformance from other hybrid
models have primarily been demonstrated only on univariate datasets.
Difficulties with applying hybrid forecast methods to multivariate data include
($i$) the high computational cost involved in hyperparameter tuning for models
that are not parsimonious, ($ii$) challenges associated with auto-correlation
inherent in the data, as well as ($iii$) complex dependency (cross-correlation)
between the covariates that may be hard to capture. This paper presents
Multivariate Exponential Smoothing Long Short Term Memory (MES-LSTM), a
generalized multivariate extension to ES-RNN, that overcomes these challenges.
MES-LSTM utilizes a vectorized implementation. We test MES-LSTM on several
aggregated coronavirus disease of 2019 (COVID-19) morbidity datasets and find
our hybrid approach shows consistent, significant improvement over pure
statistical and deep learning methods at forecast accuracy and prediction
interval construction."
"Optimal transport (OT) formalizes the problem of finding an optimal coupling
between probability measures given a cost matrix. The inverse problem of
inferring the cost given a coupling is Inverse Optimal Transport (IOT). IOT is
less well understood than OT. We formalize and systematically analyze the
properties of IOT using tools from the study of entropy-regularized OT.
Theoretical contributions include characterization of the manifold of
cross-ratio equivalent costs, the implications of model priors, and derivation
of an MCMC sampler. Empirical contributions include visualizations of
cross-ratio equivalent effect on basic examples and simulations validating
theoretical results."
"Conjoint analysis is a popular experimental design used to measure
multidimensional preferences. Researchers examine how varying a factor of
interest, while controlling for other relevant factors, influences
decision-making. Currently, there exist two methodological approaches to
analyzing data from a conjoint experiment. The first focuses on estimating the
average marginal effects of each factor while averaging over the other factors.
Although this allows for straightforward design-based estimation, the results
critically depend on the distribution of other factors and how interaction
effects are aggregated. An alternative model-based approach can compute various
quantities of interest, but requires researchers to correctly specify the
model, a challenging task for conjoint analysis with many factors and possible
interactions. In addition, a commonly used logistic regression has poor
statistical properties even with a moderate number of factors when
incorporating interactions. We propose a new hypothesis testing approach based
on the conditional randomization test to answer the most fundamental question
of conjoint analysis: Does a factor of interest matter in any way given the
other factors? Our methodology is solely based on the randomization of factors,
and hence is free from assumptions. Yet, it allows researchers to use any test
statistic, including those based on complex machine learning algorithms. As a
result, we are able to combine the strengths of the existing design-based and
model-based approaches. We illustrate the proposed methodology through conjoint
analysis of immigration preferences and political candidate evaluation. We also
extend the proposed approach to test for regularity assumptions commonly used
in conjoint analysis. An open-source software package is available for
implementing the proposed methodology."
"All numerical weather prediction models used for the wind industry need to
produce their forecasts starting from the main synoptic hours 00, 06, 12, and
18 UTC, once the analysis becomes available. The six-hour latency time between
two consecutive model runs calls for strategies to fill the gap by providing
new accurate predictions having, at least, hourly frequency. This is done to
accommodate the request of frequent, accurate and fresh information from
traders and system regulators to continuously adapt their work strategies.
Here, we propose a strategy where quasi-real time observed wind speed and
weather model predictions are combined by means of a novel Ensemble Model
Output Statistics (EMOS) strategy. The success of our strategy is measured by
comparisons against observed wind speed from SYNOP stations over Italy in the
years 2018 and 2019."
"Kernel mean embeddings are a powerful tool to represent probability
distributions over arbitrary spaces as single points in a Hilbert space. Yet,
the cost of computing and storing such embeddings prohibits their direct use in
large-scale settings. We propose an efficient approximation procedure based on
the Nystr\""om method, which exploits a small random subset of the dataset. Our
main result is an upper bound on the approximation error of this procedure. It
yields sufficient conditions on the subsample size to obtain the standard
$n^{-1/2}$ rate while reducing computational costs. We discuss applications of
this result for the approximation of the maximum mean discrepancy and
quadrature rules, and illustrate our theoretical findings with numerical
experiments."
"We present a unified probabilistic gradient boosting framework for regression
tasks that models and predicts the entire conditional distribution of a
univariate response variable as a function of covariates. Our likelihood-based
approach allows us to either model all conditional moments of a parametric
distribution, or to approximate the conditional cumulative distribution
function via Normalizing Flows. As underlying computational backbones, our
framework is based on XGBoost and LightGBM. Modelling and predicting the entire
conditional distribution greatly enhances existing tree-based gradient boosting
implementations, as it allows to create probabilistic forecasts from which
prediction intervals and quantiles of interest can be derived. Empirical
results show that our framework achieves state-of-the-art forecast accuracy."
"The best neural architecture for a given machine learning problem depends on
many factors: not only the complexity and structure of the dataset, but also on
resource constraints including latency, compute, energy consumption, etc.
Neural architecture search (NAS) for tabular datasets is an important but
under-explored problem. Previous NAS algorithms designed for image search
spaces incorporate resource constraints directly into the reinforcement
learning (RL) rewards. However, for NAS on tabular datasets, this protocol
often discovers suboptimal architectures. This paper develops TabNAS, a new and
more effective approach to handle resource constraints in tabular NAS using an
RL controller motivated by the idea of rejection sampling. TabNAS immediately
discards any architecture that violates the resource constraints without
training or learning from that architecture. TabNAS uses a Monte-Carlo-based
correction to the RL policy gradient update to account for this extra filtering
step. Results on several tabular datasets demonstrate the superiority of TabNAS
over previous reward-shaping methods: it finds better models that obey the
constraints."
"This paper tackles the problem of decomposing binary data using matrix
factorization. We consider the family of mean-parametrized Bernoulli models, a
class of generative models that are well suited for modeling binary data and
enables interpretability of the factors. We factorize the Bernoulli parameter
and consider an additional Beta prior on one of the factors to further improve
the model's expressive power. While similar models have been proposed in the
literature, they only exploit the Beta prior as a proxy to ensure a valid
Bernoulli parameter in a Bayesian setting; in practice it reduces to a uniform
or uninformative prior. Besides, estimation in these models has focused on
costly Bayesian inference. In this paper, we propose a simple yet very
efficient majorization-minimization algorithm for maximum a posteriori
estimation. Our approach leverages the Beta prior whose parameters can be tuned
to improve performance in matrix completion tasks. Experiments conducted on
three public binary datasets show that our approach offers an excellent
trade-off between prediction performance, computational complexity, and
interpretability."
"Estimating the effects of continuous-valued interventions from observational
data is a critically important task for climate science, healthcare, and
economics. Recent work focuses on designing neural network architectures and
regularization functions to allow for scalable estimation of average and
individual-level dose-response curves from high-dimensional, large-sample data.
Such methodologies assume ignorability (observation of all confounding
variables) and positivity (observation of all treatment levels for every
covariate value describing a set of units), assumptions problematic in the
continuous treatment regime. Scalable sensitivity and uncertainty analyses to
understand the ignorance induced in causal estimates when these assumptions are
relaxed are less studied. Here, we develop a continuous treatment-effect
marginal sensitivity model (CMSM) and derive bounds that agree with the
observed data and a researcher-defined level of hidden confounding. We
introduce a scalable algorithm and uncertainty-aware deep models to derive and
estimate these bounds for high-dimensional, large-sample observational data. We
work in concert with climate scientists interested in the climatological
impacts of human emissions on cloud properties using satellite observations
from the past 15 years. This problem is known to be complicated by many
unobserved confounders."
"Tree-based methods are popular machine learning techniques used in various
fields. In this work, we review their foundations and a general framework the
importance sampled learning ensemble (ISLE) that accelerates their fitting
process. Furthermore, we describe a model combination strategy called the
adaptive regression by mixing (ARM), which is feasible for tree-based methods
via ISLE. Moreover, three modified ISLEs are proposed, and their performance
are evaluated on the real data sets."
"Within the field of causal inference, we consider the problem of estimating
heterogeneous treatment effects from data. We propose and validate a novel
approach for learning feature representations to aid the estimation of the
conditional average treatment effect or CATE. Our method focuses on an
intermediate layer in a neural network trained to predict the outcome from the
features. In contrast to previous approaches that encourage the distribution of
representations to be treatment-invariant, we leverage a genetic algorithm that
optimizes over representations useful for predicting the outcome to select
those less useful for predicting the treatment. This allows us to retain
information within the features useful for predicting outcome even if that
information may be related to treatment assignment. We validate our method on
synthetic examples and illustrate its use on a real life dataset."
"The work in ICML'09 showed that the derivatives of the classical multi-class
logistic regression loss function could be re-written in terms of a pre-chosen
""base class"" and applied the new derivatives in the popular boosting framework.
In order to make use of the new derivatives, one must have a strategy to
identify/choose the base class at each boosting iteration. The idea of
""adaptive base class boost"" (ABC-Boost) in ICML'09, adopted a computationally
expensive ""exhaustive search"" strategy for the base class at each iteration. It
has been well demonstrated that ABC-Boost, when integrated with trees, can
achieve substantial improvements in many multi-class classification tasks.
Furthermore, the work in UAI'10 derived the explicit second-order tree split
gain formula which typically improved the classification accuracy considerably,
compared with using only the fist-order information for tree-splitting, for
both multi-class and binary-class classification tasks. In this paper, we
develop a unified framework for effectively selecting the base class by
introducing a series of ideas to improve the computational efficiency of
ABC-Boost. Our framework has parameters $(s,g,w)$. At each boosting iteration,
we only search for the ""$s$-worst classes"" (instead of all classes) to
determine the base class. We also allow a ""gap"" $g$ when conducting the search.
That is, we only search for the base class at every $g+1$ iterations. We
furthermore allow a ""warm up"" stage by only starting the search after $w$
boosting iterations. The parameters $s$, $g$, $w$, can be viewed as tunable
parameters and certain combinations of $(s,g,w)$ may even lead to better test
accuracy than the ""exhaustive search"" strategy. Overall, our proposed framework
provides a robust and reliable scheme for implementing ABC-Boost in practice."
"Elimination algorithms for bandit identification, which prune the plausible
correct answers sequentially until only one remains, are computationally
convenient since they reduce the problem size over time. However, existing
elimination strategies are often not fully adaptive (they update their sampling
rule infrequently) and are not easy to extend to combinatorial settings, where
the set of answers is exponentially large in the problem dimension. On the
other hand, most existing fully-adaptive strategies to tackle general
identification problems are computationally demanding since they repeatedly
test the correctness of every answer, without ever reducing the problem size.
We show that adaptive methods can be modified to use elimination in both their
stopping and sampling rules, hence obtaining the best of these two worlds: the
algorithms (1) remain fully adaptive, (2) suffer a sample complexity that is
never worse of their non-elimination counterpart, and (3) provably eliminate
certain wrong answers early. We confirm these benefits experimentally, where
elimination improves significantly the computational complexity of adaptive
methods on common tasks like best-arm identification in linear bandits."
"Nonlocal operators with integral kernels have become a popular tool for
designing solution maps between function spaces, due to their efficiency in
representing long-range dependence and the attractive feature of being
resolution-invariant. In this work, we provide a rigorous identifiability
analysis and convergence study for the learning of kernels in nonlocal
operators. It is found that the kernel learning is an ill-posed or even
ill-defined inverse problem, leading to divergent estimators in the presence of
modeling errors or measurement noises. To resolve this issue, we propose a
nonparametric regression algorithm with a novel data adaptive RKHS Tikhonov
regularization method based on the function space of identifiability. The
method yields a noisy-robust convergent estimator of the kernel as the data
resolution refines, on both synthetic and real-world datasets. In particular,
the method successfully learns a homogenized model for the stress wave
propagation in a heterogeneous solid, revealing the unknown governing laws from
real-world data at microscale. Our regularization method outperforms baseline
methods in robustness, generalizability and accuracy."
"This paper considers a joint multi-graph inference and clustering problem for
simultaneous inference of node centrality and association of graph signals with
their graphs. We study a mixture model of filtered low pass graph signals with
possibly non-white and low-rank excitation. While the mixture model is
motivated from practical scenarios, it presents significant challenges to prior
graph learning methods. As a remedy, we consider an inference problem focusing
on the node centrality of graphs. We design an expectation-maximization (EM)
algorithm with a unique low-rank plus sparse prior derived from low pass signal
property. We propose a novel online EM algorithm for inference from streaming
data. As an example, we extend the online algorithm to detect if the signals
are generated from an abnormal graph. We show that the proposed algorithms
converge to a stationary point of the maximum-a-posterior (MAP) problem.
Numerical experiments support our analysis."
"State-of-the-art machine-learning-based models are a popular choice for
modeling and forecasting energy behavior in buildings because given enough
data, they are good at finding spatiotemporal patterns and structures even in
scenarios where the complexity prohibits analytical descriptions. However,
their architecture typically does not hold physical correspondence to
mechanistic structures linked with governing physical phenomena. As a result,
their ability to successfully generalize for unobserved timesteps depends on
the representativeness of the dynamics underlying the observed system in the
data, which is difficult to guarantee in real-world engineering problems such
as control and energy management in digital twins. In response, we present a
framework that combines lumped-parameter models in the form of linear
time-invariant (LTI) state-space models (SSMs) with unsupervised reduced-order
modeling in a subspace-based domain adaptation (SDA) framework. SDA is a type
of transfer-learning (TL) technique, typically adopted for exploiting labeled
data from one domain to predict in a different but related target domain for
which labeled data is limited. We introduce a novel SDA approach where instead
of labeled data, we leverage the geometric structure of the LTI SSM governed by
well-known heat transfer ordinary differential equations to forecast for
unobserved timesteps beyond observed measurement data. Fundamentally, our
approach geometrically aligns the physics-derived and data-derived embedded
subspaces closer together. In this initial exploration, we evaluate the
physics-based SDA framework on a demonstrative heat conduction scenario by
varying the thermophysical properties of the source and target systems to
demonstrate the transferability of mechanistic models from a physics-based
domain to a data domain."
"Inferring chemical reaction networks (CRN) from concentration time series is
a challenge encouragedby the growing availability of quantitative temporal data
at the cellular level. This motivates thedesign of algorithms to infer the
preponderant reactions between the molecular species observed ina given
biochemical process, and build CRN structure and kinetics models. Existing
ODE-basedinference methods such as SINDy resort to least square regression
combined with sparsity-enforcingpenalization, such as Lasso. However, we
observe that these methods fail to learn sparse modelswhen the input time
series are only available in wild type conditions, i.e. without the possibility
toplay with combinations of zeroes in the initial conditions. We present a CRN
inference algorithmwhich enforces sparsity by inferring reactions in a
sequential fashion within a search tree of boundeddepth, ranking the inferred
reaction candidates according to the variance of their kinetics on
theirsupporting transitions, and re-optimizing the kinetic parameters of the
CRN candidates on the wholetrace in a final pass. We show that Reactmine
succeeds both on simulation data by retrievinghidden CRNs where SINDy fails,
and on two real datasets, one of fluorescence videomicroscopyof cell cycle and
circadian clock markers, the other one of biomedical measurements of
systemiccircadian biomarkers possibly acting on clock gene expression in
peripheral organs, by inferringpreponderant regulations in agreement with
previous model-based analyses. The code is available
athttps://gitlab.inria.fr/julmarti/crninf/ together with introductory
notebooks."
"We propose a non-asymptotic convergence analysis of a two-step approach to
learn a conditional value-at-risk (VaR) and a conditional expected shortfall
(ES) using Rademacher bounds, in a non-parametric setup allowing for
heavy-tails on the financial loss. Our approach for the VaR is extended to the
problem of learning at once multiple VaRs corresponding to different quantile
levels. This results in efficient learning schemes based on neural network
quantile and least-squares regressions. An a posteriori Monte Carlo procedure
is introduced to estimate distances to the ground-truth VaR and ES. This is
illustrated by numerical experiments in a Student-$t$ toy model and a financial
case study where the objective is to learn a dynamic initial margin."
"Causal representation learning seeks to extract high-level latent factors
from low-level sensory data. Most existing methods rely on observational data
and structural assumptions (e.g., conditional independence) to identify the
latent factors. However, interventional data is prevalent across applications.
Can interventional data facilitate causal representation learning? We explore
this question in this paper. The key observation is that interventional data
often carries geometric signatures of the latent factors' support (i.e. what
values each latent can possibly take). For example, when the latent factors are
causally connected, interventions can break the dependency between the
intervened latents' support and their ancestors'. Leveraging this fact, we
prove that the latent causal factors can be identified up to permutation and
scaling given data from perfect $do$ interventions. Moreover, we can achieve
block affine identification, namely the estimated latent factors are only
entangled with a few other latents if we have access to data from imperfect
interventions. These results highlight the unique power of interventional data
in causal representation learning; they can enable provable identification of
latent factors without any assumptions about their distributions or dependency
structure."
"Computing the Jacobian of the solution of an optimization problem is a
central problem in machine learning, with applications in hyperparameter
optimization, meta-learning, optimization as a layer, and dataset distillation,
to name a few. Unrolled differentiation is a popular heuristic that
approximates the solution using an iterative solver and differentiates it
through the computational path. This work provides a non-asymptotic
convergence-rate analysis of this approach on quadratic objectives for gradient
descent and the Chebyshev method. We show that to ensure convergence of the
Jacobian, we can either 1) choose a large learning rate leading to a fast
asymptotic convergence but accept that the algorithm may have an arbitrarily
long burn-in phase or 2) choose a smaller learning rate leading to an immediate
but slower convergence. We refer to this phenomenon as the curse of unrolling.
Finally, we discuss open problems relative to this approach, such as deriving a
practical update rule for the optimal unrolling strategy and making novel
connections with the field of Sobolev orthogonal polynomials."
"Deep Ensembles (DE) are a prominent approach for achieving excellent
performance on key metrics such as accuracy, calibration, uncertainty
estimation, and out-of-distribution detection. However, hardware limitations of
real-world systems constrain to smaller ensembles and lower-capacity networks,
significantly deteriorating their performance and properties. We introduce
Packed-Ensembles (PE), a strategy to design and train lightweight structured
ensembles by carefully modulating the dimension of their encoding space. We
leverage grouped convolutions to parallelize the ensemble into a single shared
backbone and forward pass to improve training and inference speeds. PE is
designed to operate within the memory limits of a standard neural network. Our
extensive research indicates that PE accurately preserves the properties of DE,
such as diversity, and performs equally well in terms of accuracy, calibration,
out-of-distribution detection, and robustness to distribution shift. We make
our code available at https://github.com/ENSTA-U2IS/torch-uncertainty."
"Generalized sliced Wasserstein distance is a variant of sliced Wasserstein
distance that exploits the power of non-linear projection through a given
defining function to better capture the complex structures of the probability
distributions. Similar to sliced Wasserstein distance, generalized sliced
Wasserstein is defined as an expectation over random projections which can be
approximated by the Monte Carlo method. However, the complexity of that
approximation can be expensive in high-dimensional settings. To that end, we
propose to form deterministic and fast approximations of the generalized sliced
Wasserstein distance by using the concentration of random projections when the
defining functions are polynomial function, circular function, and neural
network type function. Our approximations hinge upon an important result that
one-dimensional projections of a high-dimensional random vector are
approximately Gaussian."
"We present new algorithms for online convex optimization over unbounded
domains that obtain parameter-free regret in high-probability given access only
to potentially heavy-tailed subgradient estimates. Previous work in unbounded
domains considers only in-expectation results for sub-exponential subgradients.
Unlike in the bounded domain case, we cannot rely on straight-forward
martingale concentration due to exponentially large iterates produced by the
algorithm. We develop new regularization techniques to overcome these problems.
Overall, with probability at most $\delta$, for all comparators $\mathbf{u}$
our algorithm achieves regret $\tilde{O}(\| \mathbf{u} \| T^{1/\mathfrak{p}}
\log (1/\delta))$ for subgradients with bounded $\mathfrak{p}^{th}$ moments for
some $\mathfrak{p} \in (1, 2]$."
"We consider estimation models of the form $Y=X^*+N$, where $X^*$ is some
$m$-dimensional signal we wish to recover, and $N$ is symmetrically distributed
noise that may be unbounded in all but a small $\alpha$ fraction of the
entries. We introduce a family of algorithms that under mild assumptions
recover the signal $X^*$ in all estimation problems for which there exists a
sum-of-squares algorithm that succeeds in recovering the signal $X^*$ when the
noise $N$ is Gaussian. This essentially shows that it is enough to design a
sum-of-squares algorithm for an estimation problem with Gaussian noise in order
to get the algorithm that works with the symmetric noise model. Our framework
extends far beyond previous results on symmetric noise models and is even
robust to adversarial perturbations.
  As concrete examples, we investigate two problems for which no efficient
algorithms were known to work for heavy-tailed noise: tensor PCA and sparse
PCA. For the former, our algorithm recovers the principal component in
polynomial time when the signal-to-noise ratio is at least
$\tilde{O}(n^{p/4}/\alpha)$, that matches (up to logarithmic factors) current
best known algorithmic guarantees for Gaussian noise. For the latter, our
algorithm runs in quasipolynomial time and matches the state-of-the-art
guarantees for quasipolynomial time algorithms in the case of Gaussian noise.
Using a reduction from the planted clique problem, we provide evidence that the
quasipolynomial time is likely to be necessary for sparse PCA with symmetric
noise.
  In our proofs we use bounds on the covering numbers of sets of
pseudo-expectations, which we obtain by certifying in sum-of-squares upper
bounds on the Gaussian complexities of sets of solutions. This approach for
bounding the covering numbers of sets of pseudo-expectations may be interesting
in its own right and may find other application in future works."
"We propose a new method for optimistic planning in infinite-horizon
discounted Markov decision processes based on the idea of adding regularization
to the updates of an otherwise standard approximate value iteration procedure.
This technique allows us to avoid contraction and monotonicity arguments
typically required by existing analyses of approximate dynamic programming
methods, and in particular to use approximate transition functions estimated
via least-squares procedures in MDPs with linear function approximation. We use
our method to recover known guarantees in tabular MDPs and to provide a
computationally efficient algorithm for learning near-optimal policies in
discounted linear mixture MDPs from a single stream of experience, and show it
achieves near-optimal statistical guarantees."
"We consider the problem of optimizing expensive black-box functions over
high-dimensional combinatorial spaces which arises in many science,
engineering, and ML applications. We use Bayesian Optimization (BO) and propose
a novel surrogate modeling approach for efficiently handling a large number of
binary and categorical parameters. The key idea is to select a number of
discrete structures from the input space (the dictionary) and use them to
define an ordinal embedding for high-dimensional combinatorial structures. This
allows us to use existing Gaussian process models for continuous spaces. We
develop a principled approach based on binary wavelets to construct
dictionaries for binary spaces, and propose a randomized construction method
that generalizes to categorical spaces. We provide theoretical justification to
support the effectiveness of the dictionary-based embeddings. Our experiments
on diverse real-world benchmarks demonstrate the effectiveness of our proposed
surrogate modeling approach over state-of-the-art BO methods."
"This study explores the number of neurons required for a Rectified Linear
Unit (ReLU) neural network to approximate multivariate monomials. We establish
an exponential lower bound on the complexity of any shallow network
approximating the product function over a general compact domain. We also
demonstrate this lower bound doesn't apply to normalized Lipschitz monomials
over the unit cube. These findings suggest that shallow ReLU networks
experience the curse of dimensionality when expressing functions with a
Lipschitz parameter scaling with the dimension of the input, and that the
expressive power of neural networks is more dependent on their depth rather
than overall complexity."
"We study new types of dynamic allocation problems the {\sl Halting Bandit}
models. As an application, we obtain new proofs for the classic Gittins index
decomposition result and recent results of the authors in `Multi-armed bandits
under general depreciation and commitment.'"
"The support vector machine (SVM) is a supervised learning algorithm that
finds a maximum-margin linear classifier, often after mapping the data to a
high-dimensional feature space via the kernel trick. Recent work has
demonstrated that in certain sufficiently overparameterized settings, the SVM
decision function coincides exactly with the minimum-norm label interpolant.
This phenomenon of support vector proliferation (SVP) is especially interesting
because it allows us to understand SVM performance by leveraging recent
analyses of harmless interpolation in linear and kernel models. However,
previous work on SVP has made restrictive assumptions on the data/feature
distribution and spectrum. In this paper, we present a new and flexible
analysis framework for proving SVP in an arbitrary reproducing kernel Hilbert
space with a flexible class of generative models for the labels. We present
conditions for SVP for features in the families of general bounded orthonormal
systems (e.g. Fourier features) and independent sub-Gaussian features. In both
cases, we show that SVP occurs in many interesting settings not covered by
prior work, and we leverage these results to prove novel generalization results
for kernel SVM classification."
"If $X,Y,Z$ denote sets of random variables, two different data sources may
contain samples from $P_{X,Y}$ and $P_{Y,Z}$, respectively. We argue that
causal discovery can help inferring properties of the `unobserved joint
distributions' $P_{X,Y,Z}$ or $P_{X,Z}$. The properties may be conditional
independences (as in `integrative causal inference') or also quantitative
statements about dependences.
  More generally, we define a learning scenario where the input is a subset of
variables and the label is some statistical property of that subset. Sets of
jointly observed variables define the training points, while unobserved sets
are possible test points. To solve this learning task, we infer, as an
intermediate step, a causal model from the observations that then entails
properties of unobserved sets. Accordingly, we can define the VC dimension of a
class of causal models and derive generalization bounds for the predictions.
  Here, causal discovery becomes more modest and better accessible to empirical
tests than usual: rather than trying to find a causal hypothesis that is `true'
a causal hypothesis is {\it useful} whenever it correctly predicts statistical
properties of unobserved joint distributions. This way, a sparse causal graph
that omits weak influences may be more useful than a dense one (despite being
less accurate) because it is able to reconstruct the full joint distribution
from marginal distributions of smaller subsets.
  Within such a `pragmatic' application of causal discovery, some popular
heuristic approaches become justified in retrospect. It is, for instance,
allowed to infer DAGs from partial correlations instead of conditional
independences if the DAGs are only used to predict partial correlations."
"In this work, we propose a novel framework for large-scale Gaussian process
(GP) modeling. Contrary to the global, and local approximations proposed in the
literature to address the computational bottleneck with exact GP modeling, we
employ a combined global-local approach in building the approximation. Our
framework uses a subset-of-data approach where the subset is a union of a set
of global points designed to capture the global trend in the data, and a set of
local points specific to a given testing location to capture the local trend
around the testing location. The correlation function is also modeled as a
combination of a global, and a local kernel. The performance of our framework,
which we refer to as TwinGP, is on par or better than the state-of-the-art GP
modeling methods at a fraction of their computational cost."
"Many machine learning problems can be seen as approximating a \textit{target}
distribution using a \textit{particle} distribution by minimizing their
statistical discrepancy. Wasserstein Gradient Flow can move particles along a
path that minimizes the $f$-divergence between the target and particle
distributions. To move particles, we need to calculate the corresponding
velocity fields derived from a density ratio function between these two
distributions. Previous works estimated such density ratio functions and then
differentiated the estimated ratios. These approaches may suffer from
overfitting, leading to a less accurate estimate of the velocity fields.
Inspired by non-parametric curve fitting, we directly estimate these velocity
fields using interpolation techniques. We prove that our estimators are
consistent under mild conditions. We validate their effectiveness using novel
applications on domain adaptation and missing data imputation."
"We present a PTAS for learning random constant-depth networks. We show that
for any fixed $\epsilon>0$ and depth $i$, there is a poly-time algorithm that
for any distribution on $\sqrt{d} \cdot \mathbb{S}^{d-1}$ learns random Xavier
networks of depth $i$, up to an additive error of $\epsilon$. The algorithm
runs in time and sample complexity of
$(\bar{d})^{\mathrm{poly}(\epsilon^{-1})}$, where $\bar d$ is the size of the
network. For some cases of sigmoid and ReLU-like activations the bound can be
improved to $(\bar{d})^{\mathrm{polylog}(\epsilon^{-1})}$, resulting in a
quasi-poly-time algorithm for learning constant depth random networks."
"Neural processes are a family of probabilistic models that inherit the
flexibility of neural networks to parameterize stochastic processes. Despite
providing well-calibrated predictions, especially in regression problems, and
quick adaptation to new tasks, the Gaussian assumption that is commonly used to
represent the predictive likelihood fails to capture more complicated
distributions such as multimodal ones. To overcome this limitation, we propose
Conditional Quantile Neural Processes (CQNPs), a new member of the neural
processes family, which exploits the attractive properties of quantile
regression in modeling the distributions irrespective of their form. By
introducing an extension of quantile regression where the model learns to focus
on estimating informative quantiles, we show that the sampling efficiency and
prediction accuracy can be further enhanced. Our experiments with real and
synthetic datasets demonstrate substantial improvements in predictive
performance compared to the baselines, and better modeling of heterogeneous
distributions' characteristics such as multimodality."
"The projection operation is a critical component in a wide range of
optimization algorithms, such as online gradient descent (OGD), for enforcing
constraints and achieving optimal regret bounds. However, it suffers from
computational complexity limitations in high-dimensional settings or when
dealing with ill-conditioned constraint sets. Projection-free algorithms
address this issue by replacing the projection oracle with more efficient
optimization subroutines. But to date, these methods have been developed
primarily in the Euclidean setting, and while there has been growing interest
in optimization on Riemannian manifolds, there has been essentially no work in
trying to utilize projection-free tools here. An apparent issue is that
non-trivial affine functions are generally non-convex in such domains. In this
paper, we present methods for obtaining sub-linear regret guarantees in online
geodesically convex optimization on curved spaces for two scenarios: when we
have access to (a) a separation oracle or (b) a linear optimization oracle. For
geodesically convex losses, and when a separation oracle is available, our
algorithms achieve $O(T^{1/2}\:)$ and $O(T^{3/4}\;)$ adaptive regret guarantees
in the full information setting and the bandit setting, respectively. When a
linear optimization oracle is available, we obtain regret rates of
$O(T^{3/4}\;)$ for geodesically convex losses and $O(T^{2/3}\; log T )$ for
strongly geodesically convex losses."
"We study the problem of best-arm identification (BAI) in the fixed-budget
setting with heterogeneous reward variances. We propose two variance-adaptive
BAI algorithms for this setting: SHVar for known reward variances and SHAdaVar
for unknown reward variances. Our algorithms rely on non-uniform budget
allocations among the arms where the arms with higher reward variances are
pulled more often than those with lower variances. The main algorithmic novelty
is in the design of SHAdaVar, which allocates budget greedily based on
overestimating the unknown reward variances. We bound probabilities of
misidentifying the best arms in both SHVar and SHAdaVar. Our analyses rely on
novel lower bounds on the number of pulls of an arm that do not require
closed-form solutions to the budget allocation problem. Since one of our budget
allocation problems is analogous to the optimal experiment design with unknown
variances, we believe that our results are of a broad interest. Our experiments
validate our theory, and show that SHVar and SHAdaVar outperform algorithms
from prior works with analytical guarantees."
"The success of SGD in deep learning has been ascribed by prior works to the
implicit bias induced by finite batch sizes (""SGD noise""). While prior works
focused on offline learning (i.e., multiple-epoch training), we study the
impact of SGD noise on online (i.e., single epoch) learning. Through an
extensive empirical analysis of image and language data, we demonstrate that
small batch sizes do not confer any implicit bias advantages in online
learning. In contrast to offline learning, the benefits of SGD noise in online
learning are strictly computational, facilitating more cost-effective gradient
steps. This suggests that SGD in the online regime can be construed as taking
noisy steps along the ""golden path"" of the noiseless gradient descent
algorithm. We study this hypothesis and provide supporting evidence in loss and
function space. Our findings challenge the prevailing understanding of SGD and
offer novel insights into its role in online learning."
"Stress testing refers to the application of adverse financial or
macroeconomic scenarios to a portfolio. For this purpose, financial or
macroeconomic risk factors are linked with asset returns, typically via a
factor model. We expand the range of risk factors by adapting
dimension-reduction techniques from unsupervised learning, namely PCA and
autoencoders. This results in aggregated risk factors, encompassing a global
factor, factors representing broad geographical regions, and factors specific
to cyclical and defensive industries. As the adapted PCA and autoencoders
provide an interpretation of the latent factors, this methodology is also
valuable in other areas where dimension-reduction and explainability are
crucial."
"This paper provides norm-based generalization bounds for the Transformer
architecture that do not depend on the input sequence length. We employ a
covering number based approach to prove our bounds. We use three novel covering
number bounds for the function class of bounded linear transformations to upper
bound the Rademacher complexity of the Transformer. Furthermore, we show this
generalization bound applies to the common Transformer training technique of
masking and then predicting the masked word. We also run a simulated study on a
sparse majority data set that empirically validates our theoretical findings."
"Gaussian process regression is widely used because of its ability to provide
well-calibrated uncertainty estimates and handle small or sparse datasets.
However, it struggles with high-dimensional data. One possible way to scale
this technique to higher dimensions is to leverage the implicit low-dimensional
manifold upon which the data actually lies, as postulated by the manifold
hypothesis. Prior work ordinarily requires the manifold structure to be
explicitly provided though, i.e. given by a mesh or be known to be one of the
well-known manifolds like the sphere. In contrast, in this paper we propose a
Gaussian process regression technique capable of inferring implicit structure
directly from data (labeled and unlabeled) in a fully differentiable way. For
the resulting model, we discuss its convergence to the Mat\'ern Gaussian
process on the assumed manifold. Our technique scales up to hundreds of
thousands of data points, and may improve the predictive performance and
calibration of the standard Gaussian process regression in high-dimensional
settings."
"Missing data is a common problem in practical data science settings. Various
imputation methods have been developed to deal with missing data. However, even
though the labels are available in the training data in many situations, the
common practice of imputation usually only relies on the input and ignores the
label. We propose Classification Based on MissForest Imputation (CBMI), a
classification strategy that initializes the predicted test label with missing
values and stacks the label with the input for imputation, allowing the label
and the input to be imputed simultaneously. In addition, we propose the
imputation using labels (IUL) algorithm, an imputation strategy that stacks the
label into the input and illustrates how it can significantly improve the
imputation quality. Experiments show that CBMI has classification accuracy when
the test set contains missing data, especially for imbalanced data and
categorical data. Moreover, for both the regression and classification, IUL
consistently shows significantly better results than imputation based on only
the input data."
"We present an adaptive approach for robust learning from corrupted training
sets. We identify corrupted and non-corrupted samples with latent Bernoulli
variables and thus formulate the learning problem as maximization of the
likelihood where latent variables are marginalized. The resulting problem is
solved via variational inference, using an efficient Expectation-Maximization
based method. The proposed approach improves over the state-of-the-art by
automatically inferring the corruption level, while adding minimal
computational overhead. We demonstrate our robust learning method and its
parameter-free nature on a wide variety of machine learning tasks including
online learning and deep learning where it adapts to different levels of noise
and maintains high prediction accuracy."
"The incorporation of advanced sensors and machine learning techniques has
enabled modern manufacturing enterprises to perform data-driven
classification-based anomaly detection based on the sensor data collected in
manufacturing processes. However, one critical challenge is that newly
presented defect category may manifest as the manufacturing process continues,
resulting in monitoring performance deterioration of previously trained machine
learning models. Hence, there is an increasing need for empowering machine
learning models to learn continually. Among all continual learning methods,
memory-based continual learning has the best performance but faces the
constraints of data storage capacity. To address this issue, this paper
develops a novel pseudo replay-based continual learning framework by
integrating class incremental learning and oversampling-based data generation.
Without storing all the data, the developed framework could generate
high-quality data representing previous classes to train machine learning model
incrementally when new category anomaly occurs. In addition, it could even
enhance the monitoring performance since it also effectively improves the data
quality. The effectiveness of the proposed framework is validated in three
cases studies, which leverages supervised classification problem for anomaly
detection. The experimental results show that the developed method is very
promising in detecting novel anomaly while maintaining a good performance on
the previous task and brings up more flexibility in model architecture."
"Bayesian optimization (BO) offers an elegant approach for efficiently
optimizing black-box functions. However, acquisition criteria demand their own
challenging inner-optimization, which can induce significant overhead. Many
practical BO methods, particularly in high dimension, eschew a formal,
continuous optimization of the acquisition function and instead search
discretely over a finite set of space-filling candidates. Here, we propose to
use candidates which lie on the boundary of the Voronoi tessellation of the
current design points, so they are equidistant to two or more of them. We
discuss strategies for efficient implementation by directly sampling the
Voronoi boundary without explicitly generating the tessellation, thus
accommodating large designs in high dimension. On a battery of test problems
optimized via Gaussian processes with expected improvement, our proposed
approach significantly improves the execution time of a multi-start continuous
search without a loss in accuracy."
"In many classification applications, the prediction of a deep neural network
(DNN) based classifier needs to be accompanied by some confidence indication.
Two popular approaches for that aim are: 1) Calibration: modifies the
classifier's softmax values such that the maximal value better estimates the
correctness probability; and 2) Conformal Prediction (CP): produces a
prediction set of candidate labels that contains the true label with a
user-specified probability, guaranteeing marginal coverage, rather than, e.g.,
per class coverage. In practice, both types of indications are desirable, yet,
so far the interplay between them has not been investigated. We start this
paper with an extensive empirical study of the effect of the popular
Temperature Scaling (TS) calibration on prominent CP methods and reveal that
while it improves the class-conditional coverage of adaptive CP methods,
surprisingly, it negatively affects their prediction set sizes. Subsequently,
we explore the effect of TS beyond its calibration application and offer simple
guidelines for practitioners to trade prediction set size and conditional
coverage of adaptive CP methods while effectively combining them with
calibration. Finally, we present a theoretical analysis of the effect of TS on
the prediction set sizes, revealing several mathematical properties of the
procedure, according to which we provide reasoning for this unintuitive
phenomenon."
"In statistics and machine learning, detecting dependencies in datasets is a
central challenge. We propose a novel neural network model for supervised graph
structure learning, i.e., the process of learning a mapping between
observational data and their underlying dependence structure. The model is
trained with variably shaped and coupled simulated input data and requires only
a single forward pass through the trained network for inference. By leveraging
structural equation models and employing randomly generated multivariate
Chebyshev polynomials for the simulation of training data, our method
demonstrates robust generalizability across both linear and various types of
non-linear dependencies. We introduce a novel bilinear attention mechanism
(BAM) for explicit processing of dependency information, which operates on the
level of covariance matrices of transformed data and respects the geometry of
the manifold of symmetric positive definite matrices. Empirical evaluation
demonstrates the robustness of our method in detecting a wide range of
dependencies, excelling in undirected graph estimation and proving competitive
in completed partially directed acyclic graph estimation through a novel
two-step approach."
"Deep Neural Networks (DNNs) do not inherently compute or exhibit
empirically-justified task confidence. In mission critical applications, it is
important to both understand associated DNN reasoning and its supporting
evidence. In this paper, we propose a novel Bayesian approach to extract
explanations, justifications, and uncertainty estimates from DNNs. Our approach
is efficient both in terms of memory and computation, and can be applied to any
black box DNN without any retraining, including applications to anomaly
detection and out-of-distribution detection tasks. We validate our approach on
the CIFAR-10 dataset, and show that it can significantly improve the
interpretability and reliability of DNNs."
"The asymptotically precise estimation of the generalization of kernel methods
has recently received attention due to the parallels between neural networks
and their associated kernels. However, prior works derive such estimates for
training by kernel ridge regression (KRR), whereas neural networks are
typically trained with gradient descent (GD). In the present work, we consider
the training of kernels with a family of $\textit{spectral algorithms}$
specified by profile $h(\lambda)$, and including KRR and GD as special cases.
Then, we derive the generalization error as a functional of learning profile
$h(\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional
translation-invariant model. Under power-law assumptions on the spectrum of the
kernel and target, we use our framework to (i) give full loss asymptotics for
both noisy and noiseless observations (ii) show that the loss localizes on
certain spectral scales, giving a new perspective on the KRR saturation
phenomenon (iii) conjecture, and demonstrate for the considered data models,
the universality of the loss w.r.t. non-spectral details of the problem, but
only in case of noisy observation."
"The last decade has seen blossoming research in deep learning theory
attempting to answer, ""Why does deep learning generalize?"" A powerful shift in
perspective precipitated this progress: the study of overparametrized models in
the interpolation regime. In this paper, we argue that another perspective
shift is due, since some of the desirable qualities of LLMs are not a
consequence of good statistical generalization and require a separate
theoretical explanation. Our core argument relies on the observation that AR
probabilistic models are inherently non-identifiable: models zero or near-zero
KL divergence apart -- thus, equivalent test loss -- can exhibit markedly
different behaviors. We support our position with mathematical examples and
empirical observations, illustrating why non-identifiability has practical
relevance through three case studies: (1) the non-identifiability of zero-shot
rule extrapolation; (2) the approximate non-identifiability of in-context
learning; and (3) the non-identifiability of fine-tunability. We review
promising research directions focusing on LLM-relevant generalization measures,
transferability, and inductive biases."
"We leverage offline data to facilitate online learning in stochastic
multi-armed bandits. The probability distributions that govern the offline data
and the online rewards can be different. Without any non-trivial upper bound on
their difference, we show that no non-anticipatory policy can outperform the
UCB policy by (Auer et al. 2002), even in the presence of offline data. In
complement, we propose an online policy MIN-UCB, which outperforms UCB when a
non-trivial upper bound is given. MIN-UCB adaptively chooses to utilize the
offline data when they are deemed informative, and to ignore them otherwise.
MIN-UCB is shown to be tight in terms of both instance independent and
dependent regret bounds. Finally, we corroborate the theoretical results with
numerical experiments."
"High-dimensional linear bandits with low-dimensional structure have received
considerable attention in recent studies due to their practical significance.
The most common structure in the literature is sparsity. However, it may not be
available in practice. Symmetry, where the reward is invariant under certain
groups of transformations on the set of arms, is another important inductive
bias in the high-dimensional case that covers many standard structures,
including sparsity. In this work, we study high-dimensional symmetric linear
bandits where the symmetry is hidden from the learner, and the correct symmetry
needs to be learned in an online setting. We examine the structure of a
collection of hidden symmetry and provide a method based on model selection
within the collection of low-dimensional subspaces. Our algorithm achieves a
regret bound of $ O(d_0^{2/3} T^{2/3} \log(d))$, where $d$ is the ambient
dimension which is potentially very large, and $d_0$ is the dimension of the
true low-dimensional subspace such that $d_0 \ll d$. With an extra assumption
on well-separated models, we can further improve the regret to $
O(d_0\sqrt{T\log(d)} )$."
"Since neural classifiers are known to be sensitive to adversarial
perturbations that alter their accuracy, \textit{certification methods} have
been developed to provide provable guarantees on the insensitivity of their
predictions to such perturbations. Furthermore, in safety-critical
applications, the frequentist interpretation of the confidence of a classifier
(also known as model calibration) can be of utmost importance. This property
can be measured via the Brier score or the expected calibration error. We show
that attacks can significantly harm calibration, and thus propose certified
calibration as worst-case bounds on calibration under adversarial
perturbations. Specifically, we produce analytic bounds for the Brier score and
approximate bounds via the solution of a mixed-integer program on the expected
calibration error. Finally, we propose novel calibration attacks and
demonstrate how they can improve model calibration through \textit{adversarial
calibration training}."
"Directed acyclic graph (DAG) learning is a rapidly expanding field of
research. Though the field has witnessed remarkable advances over the past few
years, it remains statistically and computationally challenging to learn a
single (point estimate) DAG from data, let alone provide uncertainty
quantification. Our article addresses the difficult task of quantifying graph
uncertainty by developing a Bayesian variational inference framework based on
novel distributions that have support directly on the space of DAGs. The
distributions, which we use to form our prior and variational posterior, are
induced by a projection operation, whereby an arbitrary continuous distribution
is projected onto the space of sparse weighted acyclic adjacency matrices
(matrix representations of DAGs) with probability mass on exact zeros. Though
the projection constitutes a combinatorial optimization problem, it is solvable
at scale via recently developed techniques that reformulate acyclicity as a
continuous constraint. We empirically demonstrate that our method, ProDAG, can
deliver accurate inference and often outperforms existing state-of-the-art
alternatives."
"With the increasing collection of time series data from various domains,
there arises a strong demand for general time series forecasting models
pre-trained on a large number of time-series datasets to support a variety of
downstream prediction tasks. Enabling general time series forecasting faces two
challenges: how to obtain unified representations from multi-domian time series
data, and how to capture domain-specific features from time series data across
various domains for adaptive transfer in downstream tasks. To address these
challenges, we propose a Register Assisted General Time Series Forecasting
Model with Decomposed Frequency Learning (ROSE), a novel pre-trained model for
time series forecasting. ROSE employs Decomposed Frequency Learning for the
pre-training task, which decomposes coupled semantic and periodic information
in time series with frequency-based masking and reconstruction to obtain
unified representations across domains. We also equip ROSE with a Time Series
Register, which learns to generate a register codebook to capture
domain-specific representations during pre-training and enhances
domain-adaptive transfer by selecting related register tokens on downstream
tasks. After pre-training on large-scale time series data, ROSE achieves
state-of-the-art forecasting performance on 8 real-world benchmarks.
Remarkably, even in few-shot scenarios, it demonstrates competitive or superior
performance compared to existing methods trained with full data."
"Recent research on the grokking phenomenon has illuminated the intricacies of
neural networks' training dynamics and their generalization behaviors. Grokking
refers to a sharp rise of the network's generalization accuracy on the test
set, which occurs long after an extended overfitting phase, during which the
network perfectly fits the training set. While the existing research primarily
focus on shallow networks such as 2-layer MLP and 1-layer Transformer, we
explore grokking on deep networks (e.g. 12-layer MLP). We empirically replicate
the phenomenon and find that deep neural networks can be more susceptible to
grokking than its shallower counterparts. Meanwhile, we observe an intriguing
multi-stage generalization phenomenon when increase the depth of the MLP model
where the test accuracy exhibits a secondary surge, which is scarcely seen on
shallow models. We further uncover compelling correspondences between the
decreasing of feature ranks and the phase transition from overfitting to the
generalization stage during grokking. Additionally, we find that the
multi-stage generalization phenomenon often aligns with a double-descent
pattern in feature ranks. These observations suggest that internal feature rank
could serve as a more promising indicator of the model's generalization
behavior compared to the weight-norm. We believe our work is the first one to
dive into grokking in deep neural networks, and investigate the relationship of
feature rank and generalization performance."
"Interactive-Grounded Learning (IGL) [Xie et al., 2021] is a powerful
framework in which a learner aims at maximizing unobservable rewards through
interacting with an environment and observing reward-dependent feedback on the
taken actions. To deal with personalized rewards that are ubiquitous in
applications such as recommendation systems, Maghakian et al. [2022] study a
version of IGL with context-dependent feedback, but their algorithm does not
come with theoretical guarantees. In this work, we consider the same problem
and provide the first provably efficient algorithms with sublinear regret under
realizability. Our analysis reveals that the step-function estimator of prior
work can deviate uncontrollably due to finite-sample effects. Our solution is a
novel Lipschitz reward estimator which underestimates the true reward and
enjoys favorable generalization performances. Building on this estimator, we
propose two algorithms, one based on explore-then-exploit and the other based
on inverse-gap weighting. We apply IGL to learning from image feedback and
learning from text feedback, which are reward-free settings that arise in
practice. Experimental results showcase the importance of using our Lipschitz
reward estimator and the overall effectiveness of our algorithms."
"This paper presents a method for estimating the hallucination rate for
in-context learning (ICL) with generative AI. In ICL, a conditional generative
model (CGM) is prompted with a dataset and a prediction question and asked to
generate a response. One interpretation of ICL assumes that the CGM computes
the posterior predictive of an unknown Bayesian model, which implicitly defines
a joint distribution over observable datasets and latent mechanisms. This joint
distribution factorizes into two components: the model prior over mechanisms
and the model likelihood of datasets given a mechanism. With this perspective,
we define a hallucination as a generated response to the prediction question
with low model likelihood given the mechanism. We develop a new method that
takes an ICL problem and estimates the probability that a CGM will generate a
hallucination. Our method only requires generating prediction questions and
responses from the CGM and evaluating its response log probability. We
empirically evaluate our method using large language models for synthetic
regression and natural language ICL tasks."
"Trajectory inference seeks to recover the temporal dynamics of a population
from snapshots of its (uncoupled) temporal marginals, i.e. where observed
particles are not tracked over time. Lavenant et al. arXiv:2102.09204 addressed
this challenging problem under a stochastic differential equation (SDE) model
with a gradient-driven drift in the observed space, introducing a minimum
entropy estimator relative to the Wiener measure. Chizat et al.
arXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL)
algorithm using Schr\""odinger bridges. Motivated by the overwhelming success of
observable state space models in the traditional paired trajectory inference
problem (e.g. target tracking), we extend the above framework to a class of
latent SDEs in the form of observable state space models. In this setting, we
use partial observations to infer trajectories in the latent space under a
specified dynamics model (e.g. the constant velocity/acceleration models from
target tracking). We introduce PO-MFL to solve this latent trajectory inference
problem and provide theoretical guarantees by extending the results of
arXiv:2102.09204 to the partially observed setting. We leverage the MFL
framework of arXiv:2205.07146, yielding an algorithm based on entropic OT
between dynamics-adjusted adjacent time marginals. Experiments validate the
robustness of our method and the exponential convergence of the MFL dynamics,
and demonstrate significant outperformance over the latent-free method of
arXiv:2205.07146 in key scenarios."
"This paper proposes techniques to enhance the performance of non-causal
models for causal inference using data from randomized experiments. In domains
like advertising, customer retention, and precision medicine, non-causal models
that predict outcomes under no intervention are often used to score individuals
and rank them according to the expected effectiveness of an intervention (e.g,
an ad, a retention incentive, a nudge). However, these scores may not perfectly
correspond to intervention effects due to the inherent non-causal nature of the
models. To address this limitation, we propose causal fine-tuning and effect
calibration, two techniques that leverage experimental data to refine the
output of non-causal models for different causal tasks, including effect
estimation, effect ordering, and effect classification. They are underpinned by
two key advantages. First, they can effectively integrate the predictive
capabilities of general non-causal models with the requirements of a causal
task in a specific context, allowing decision makers to support diverse causal
applications with a ""foundational"" scoring model. Second, through simulations
and an empirical example, we demonstrate that they can outperform the
alternative of building a causal-effect model from scratch, particularly when
the available experimental data is limited and the non-causal scores already
capture substantial information about the relative sizes of causal effects.
Overall, this research underscores the practical advantages of combining
experimental data with non-causal models to support causal applications."
"Neural processes (NPs) are a powerful family of meta-learning models that
seek to approximate the posterior predictive map of the ground-truth stochastic
process from which each dataset in a meta-dataset is sampled. There are many
cases in which practitioners, besides having access to the dataset of interest,
may also have access to other datasets that share similarities with it. In this
case, integrating these datasets into the NP can improve predictions. We equip
NPs with this functionality and describe this paradigm as in-context in-context
learning. Standard NP architectures, such as the convolutional conditional NP
(ConvCNP) or the family of transformer neural processes (TNPs), are not capable
of in-context in-context learning, as they are only able to condition on a
single dataset. We address this shortcoming by developing the in-context
in-context learning pseudo-token TNP (ICICL-TNP). The ICICL-TNP builds on the
family of PT-TNPs, which utilise pseudo-token-based transformer architectures
to sidestep the quadratic computational complexity associated with regular
transformer architectures. Importantly, the ICICL-TNP is capable of
conditioning on both sets of datapoints and sets of datasets, enabling it to
perform in-context in-context learning. We demonstrate the importance of
in-context in-context learning and the effectiveness of the ICICL-TNP in a
number of experiments."
"In our contemporary era, meteorological weather forecasts increasingly
incorporate ensemble predictions of visibility - a parameter of great
importance in aviation, maritime navigation, and air quality assessment, with
direct implications for public health. However, this weather variable falls
short of the predictive accuracy achieved for other quantities issued by
meteorological centers. Therefore, statistical post-processing is recommended
to enhance the reliability and accuracy of predictions. By estimating the
predictive distributions of the variables with the aid of historical
observations and forecasts, one can achieve statistical consistency between
true observations and ensemble predictions. Visibility observations, following
the recommendation of the World Meteorological Organization, are typically
reported in discrete values; hence, the predictive distribution of the weather
quantity takes the form of a discrete parametric law. Recent studies
demonstrated that the application of classification algorithms can successfully
improve the skill of such discrete forecasts; however, a frequently emerging
issue is that certain spatial and/or temporal dependencies could be lost
between marginals. Based on visibility ensemble forecasts of the European
Centre for Medium-Range Weather Forecasts for 30 locations in Central Europe,
we investigate whether the inclusion of Copernicus Atmosphere Monitoring
Service (CAMS) predictions of the same weather quantity as an additional
covariate could enhance the skill of the post-processing methods and whether it
contributes to the successful integration of spatial dependence between
marginals. Our study confirms that post-processed forecasts are substantially
superior to raw and climatological predictions, and the utilization of CAMS
forecasts provides a further significant enhancement both in the univariate and
multivariate setup."
"Bayesian neural network (BNN) approximates the posterior distribution of
model parameters and utilizes the posterior for prediction via Bayesian Model
Averaging (BMA). The quality of the posterior approximation is critical for
achieving accurate and robust predictions. It is known that flatness in the
loss landscape is strongly associated with generalization performance, and it
necessitates consideration to improve the quality of the posterior
approximation. In this work, we empirically demonstrate that BNNs often
struggle to capture the flatness. Moreover, we provide both experimental and
theoretical evidence showing that BMA can be ineffective without ensuring
flatness. To address this, we propose Sharpness-Aware Bayesian Model Averaging
(SA-BMA), a novel optimizer that seeks flat posteriors by calculating
divergence in the parameter space. SA-BMA aligns with the intrinsic nature of
BNN and the generalized version of existing sharpness-aware optimizers for DNN.
In addition, we suggest a Bayesian Transfer Learning scheme to efficiently
leverage pre-trained DNN. We validate the efficacy of SA-BMA in enhancing
generalization performance in few-shot classification and distribution shift by
ensuring flat posterior."
"Causal discovery in time series is a rapidly evolving field with a wide
variety of applications in other areas such as climate science and
neuroscience. Traditional approaches assume a stationary causal graph, which
can be adapted to nonstationary time series with time-dependent effects or
heterogeneous noise. In this work we address nonstationarity via
regime-dependent causal structures. We first establish identifiability for
high-order Markov Switching Models, which provide the foundations for
identifiable regime-dependent causal discovery. Our empirical studies
demonstrate the scalability of our proposed approach for high-order
regime-dependent structure estimation, and we illustrate its applicability on
brain activity data."
"Self-Distillation is a special type of knowledge distillation where the
student model has the same architecture as the teacher model. Despite using the
same architecture and the same training data, self-distillation has been
empirically observed to improve performance, especially when applied
repeatedly. For such a process, there is a fundamental question of interest:
How much gain is possible by applying multiple steps of self-distillation? To
investigate this relative gain, we propose studying the simple but canonical
task of linear regression. Our analysis shows that the excess risk achieved by
multi-step self-distillation can significantly improve upon a single step of
self-distillation, reducing the excess risk by a factor as large as $d$, where
$d$ is the input dimension. Empirical results on regression tasks from the UCI
repository show a reduction in the learnt model's risk (MSE) by up to 47%."
"Elliptical slice sampling, when adapted to linearly truncated multivariate
normal distributions, is a rejection-free Markov chain Monte Carlo method. At
its core, it requires analytically constructing an ellipse-polytope
intersection. The main novelty of this paper is an algorithm that computes this
intersection in $\mathcal{O}(m \log m)$ time, where $m$ is the number of linear
inequality constraints representing the polytope. We show that an
implementation based on this algorithm enhances numerical stability, speeds up
running time, and is easy to parallelize for launching multiple Markov chains."
"We study the problem of learning a partially observed matrix under the low
rank assumption in the presence of fully observed side information that depends
linearly on the true underlying matrix. This problem consists of an important
generalization of the Matrix Completion problem, a central problem in
Statistics, Operations Research and Machine Learning, that arises in
applications such as recommendation systems, signal processing, system
identification and image denoising. We formalize this problem as an
optimization problem with an objective that balances the strength of the fit of
the reconstruction to the observed entries with the ability of the
reconstruction to be predictive of the side information. We derive a
mixed-projection reformulation of the resulting optimization problem and
present a strong semidefinite cone relaxation. We design an efficient, scalable
alternating direction method of multipliers algorithm that produces high
quality feasible solutions to the problem of interest. Our numerical results
demonstrate that in the small rank regime ($k \leq 15$), our algorithm outputs
solutions that achieve on average $79\%$ lower objective value and $90.1\%$
lower $\ell_2$ reconstruction error than the solutions returned by the best
performing benchmark method on synthetic data. The runtime of our algorithm is
competitive with and often superior to that of the benchmark methods. Our
algorithm is able to solve problems with $n = 10000$ rows and $m = 10000$
columns in less than a minute. On large scale real world data, our algorithm
produces solutions that achieve $67\%$ lower out of sample error than benchmark
methods in $97\%$ less execution time."
"In the drug discovery process, where experiments can be costly and
time-consuming, computational models that predict drug-target interactions are
valuable tools to accelerate the development of new therapeutic agents.
Estimating the uncertainty inherent in these neural network predictions
provides valuable information that facilitates optimal decision-making when
risk assessment is crucial. However, such models can be poorly calibrated,
which results in unreliable uncertainty estimates that do not reflect the true
predictive uncertainty. In this study, we compare different metrics, including
accuracy and calibration scores, used for model hyperparameter tuning to
investigate which model selection strategy achieves well-calibrated models.
Furthermore, we propose to use a computationally efficient Bayesian uncertainty
estimation method named Bayesian Linear Probing (BLP), which generates
Hamiltonian Monte Carlo (HMC) trajectories to obtain samples for the parameters
of a Bayesian Logistic Regression fitted to the hidden layer of the baseline
neural network. We report that BLP improves model calibration and achieves the
performance of common uncertainty quantification methods by combining the
benefits of uncertainty estimation and probability calibration methods.
Finally, we show that combining post hoc calibration method with
well-performing uncertainty quantification approaches can boost model accuracy
and calibration."
"Estimation of distribution algorithms (EDAs) constitute a new branch of
evolutionary optimization algorithms, providing effective and efficient
optimization performance in a variety of research areas. Recent studies have
proposed new EDAs that employ mutation operators in standard EDAs to increase
the population diversity. We present a new mutation operator, a matrix
transpose, specifically designed for Bayesian structure learning, and we
evaluate its performance in Bayesian structure learning. The results indicate
that EDAs with transpose mutation give markedly better performance than
conventional EDAs."
"In this paper we introduce a method for significantly improving the signal to
noise ratio in financial data. The approach relies on combining a target
variable with different context variables and use auto-encoders (AEs) to learn
reconstructions of the combined inputs. The objective is to obtain agreement
among pairs of AEs which are trained on related but different inputs and for
which they are forced to find common ground. The training process is set up as
a ""conversation"" where the models take turns at producing a prediction
(speaking) and reconciling own predictions with the output of the other AE
(listening), until an agreement is reached. This leads to a new way of
constraining the complexity of the data representation generated by the AE.
Unlike standard regularization whose strength needs to be decided by the
designer, the proposed mutual regularization uses the partner network to detect
and amend the lack of generality of the learned representation of the data. The
integration of alternative perspectives enhances the de-noising capacity of a
single AE and allows us to discover new regularities in financial time-series
which can be converted into profitable trading strategies."
"Recent advances in machine learning have significantly improved prediction
accuracy in various applications. However, ensuring the calibration of
probabilistic predictions remains a significant challenge. Despite efforts to
enhance model calibration, the rigorous statistical evaluation of model
calibration remains less explored. In this work, we develop confidence
intervals the $\ell_2$ Expected Calibration Error (ECE). We consider
top-1-to-$k$ calibration, which includes both the popular notion of confidence
calibration as well as full calibration. For a debiased estimator of the ECE,
we show asymptotic normality, but with different convergence rates and
asymptotic variances for calibrated and miscalibrated models. We develop
methods to construct asymptotically valid confidence intervals for the ECE,
accounting for this behavior as well as non-negativity. Our theoretical
findings are supported through extensive experiments, showing that our methods
produce valid confidence intervals with shorter lengths compared to those
obtained by resampling-based methods."
"Causal Temporal Representation Learning (Ctrl) methods aim to identify the
temporal causal dynamics of complex nonstationary temporal sequences. Despite
the success of existing Ctrl methods, they require either directly observing
the domain variables or assuming a Markov prior on them. Such requirements
limit the application of these methods in real-world scenarios when we do not
have such prior knowledge of the domain variables. To address this problem,
this work adopts a sparse transition assumption, aligned with intuitive human
understanding, and presents identifiability results from a theoretical
perspective. In particular, we explore under what conditions on the
significance of the variability of the transitions we can build a model to
identify the distribution shifts. Based on the theoretical result, we introduce
a novel framework, Causal Temporal Representation Learning with Nonstationary
Sparse Transition (CtrlNS), designed to leverage the constraints on transition
sparsity and conditional independence to reliably identify both distribution
shifts and latent factors. Our experimental evaluations on synthetic and
real-world datasets demonstrate significant improvements over existing
baselines, highlighting the effectiveness of our approach."
"We derive approximation bounds for learning single neuron models using
thresholded gradient descent when both the labels and the covariates are
possibly corrupted adversarially. We assume the data follows the model $y =
\sigma(\mathbf{w}^{*} \cdot \mathbf{x}) + \xi,$ where $\sigma$ is a nonlinear
activation function, the noise $\xi$ is Gaussian, and the covariate vector
$\mathbf{x}$ is sampled from a sub-Gaussian distribution. We study sigmoidal,
leaky-ReLU, and ReLU activation functions and derive a
$O(\nu\sqrt{\epsilon\log(1/\epsilon)})$ approximation bound in $\ell_{2}$-norm,
with sample complexity $O(d/\epsilon)$ and failure probability
$e^{-\Omega(d)}$.
  We also study the linear regression problem, where $\sigma(\mathbf{x}) =
\mathbf{x}$. We derive a $O(\nu\epsilon\log(1/\epsilon))$ approximation bound,
improving upon the previous $O(\nu)$ approximation bounds for the
gradient-descent based iterative thresholding algorithms of Bhatia et al.
(NeurIPS 2015) and Shen and Sanghavi (ICML 2019). Our algorithm has a
$O(\textrm{polylog}(N,d)\log(R/\epsilon))$ runtime complexity when
$\|\mathbf{w}^{*}\|_2 \leq R$, improving upon the
$O(\text{polylog}(N,d)/\epsilon^2)$ runtime complexity of Awasthi et al.
(NeurIPS 2022)."
"The paper presents a novel approach for unsupervised techniques in the field
of clustering. A new method is proposed to enhance existing literature models
using the proper Bayesian bootstrap to improve results in terms of robustness
and interpretability. Our approach is organized in two steps: k-means
clustering is used for prior elicitation, then proper Bayesian bootstrap is
applied as resampling method in an ensemble clustering approach. Results are
analyzed introducing measures of uncertainty based on Shannon entropy. The
proposal provides clear indication on the optimal number of clusters, as well
as a better representation of the clustered data. Empirical results are
provided on simulated data showing the methodological and empirical advances
obtained."
"Scientific modeling and engineering applications rely heavily on parameter
estimation methods to fit physical models and calibrate numerical simulations
using real-world measurements. In the absence of analytic statistical models
with tractable likelihoods, modern simulation-based inference (SBI) methods
first use a numerical simulator to generate a dataset of parameters and
simulated outputs. This dataset is then used to approximate the likelihood and
estimate the system parameters given observation data. Several SBI methods
employ machine learning emulators to accelerate data generation and parameter
estimation. However, applying these approaches to high-dimensional physical
systems remains challenging due to the cost and complexity of training
high-dimensional emulators. This paper introduces Embed and Emulate (E&E): a
new SBI method based on contrastive learning that efficiently handles
high-dimensional data and complex, multimodal parameter posteriors. E&E learns
a low-dimensional latent embedding of the data (i.e., a summary statistic) and
a corresponding fast emulator in the latent space, eliminating the need to run
expensive simulations or a high dimensional emulator during inference. We
illustrate the theoretical properties of the learned latent space through a
synthetic experiment and demonstrate superior performance over existing methods
in a realistic, non-identifiable parameter estimation task using the
high-dimensional, chaotic Lorenz 96 system."
"This article makes discrete masked models for the generative modeling of
discrete data controllable. The goal is to generate samples of a discrete
random variable that adheres to a posterior distribution, satisfies specific
constraints, or optimizes a reward function. This methodological development
enables broad applications across downstream tasks such as class-specific image
generation and protein design. Existing approaches for controllable generation
of masked models typically rely on task-specific fine-tuning or additional
modifications, which can be inefficient and resource-intensive. To overcome
these limitations, we propose a novel plug-and-play framework based on
importance sampling that bypasses the need for training a conditional score.
Our framework is agnostic to the choice of control criteria, requires no
gradient information, and is well-suited for tasks such as posterior sampling,
Bayesian inverse problems, and constrained generation. We demonstrate the
effectiveness of our approach through extensive experiments, showcasing its
versatility across multiple domains, including protein design."
"Although existing variational graph autoencoders (VGAEs) have been widely
used for modeling and generating graph-structured data, most of them are still
not flexible enough to approximate the sparse and skewed latent node
representations, especially those of document relational networks (DRNs) with
discrete observations. To analyze a collection of interconnected documents, a
typical branch of Bayesian models, specifically relational topic models (RTMs),
has proven their efficacy in describing both link structures and document
contents of DRNs, which motives us to incorporate RTMs with existing VGAEs to
alleviate their potential issues when modeling the generation of DRNs. In this
paper, moving beyond the sophisticated approximate assumptions of traditional
RTMs, we develop a graph Poisson factor analysis (GPFA), which provides
analytic conditional posteriors to improve the inference accuracy, and extend
GPFA to a multi-stochastic-layer version named graph Poisson gamma belief
network (GPGBN) to capture the hierarchical document relationships at multiple
semantic levels. Then, taking GPGBN as the decoder, we combine it with various
Weibull-based graph inference networks, resulting in two variants of Weibull
graph auto-encoder (WGAE), equipped with model inference algorithms.
Experimental results demonstrate that our models can extract high-quality
hierarchical latent document representations and achieve promising performance
on various graph analytic tasks."
"We establish a new theoretical framework for learning under multi-class,
instance-dependent label noise. This framework casts learning with label noise
as a form of domain adaptation, in particular, domain adaptation under
posterior drift. We introduce the concept of \emph{relative signal strength}
(RSS), a pointwise measure that quantifies the transferability from noisy to
clean posterior. Using RSS, we establish nearly matching upper and lower bounds
on the excess risk. Our theoretical findings support the simple \emph{Noise
Ignorant Empirical Risk Minimization (NI-ERM)} principle, which minimizes
empirical risk while ignoring label noise. Finally, we translate this
theoretical insight into practice: by using NI-ERM to fit a linear classifier
on top of a self-supervised feature extractor, we achieve state-of-the-art
performance on the CIFAR-N data challenge."
"Generalized eigenvalue problems (GEPs) find applications in various fields of
science and engineering. For example, principal component analysis, Fisher's
discriminant analysis, and canonical correlation analysis are specific
instances of GEPs and are widely used in statistical data processing. In this
work, we study GEPs under generative priors, assuming that the underlying
leading generalized eigenvector lies within the range of a Lipschitz continuous
generative model. Under appropriate conditions, we show that any optimal
solution to the corresponding optimization problems attains the optimal
statistical rate. Moreover, from a computational perspective, we propose an
iterative algorithm called the Projected Rayleigh Flow Method (PRFM) to
approximate the optimal solution. We theoretically demonstrate that under
suitable assumptions, PRFM converges linearly to an estimated vector that
achieves the optimal statistical rate. Numerical results are provided to
demonstrate the effectiveness of the proposed method."
"Robust estimation provides essential tools for analyzing data that contain
outliers, ensuring that statistical models remain reliable even in the presence
of some anomalous data. While robust methods have long been available in R,
users of Python have lacked a comprehensive package that offers these methods
in a cohesive framework. RobPy addresses this gap by offering a wide range of
robust methods in Python, built upon established libraries including NumPy,
SciPy, and scikit-learn. This package includes tools for robust preprocessing,
univariate estimation, covariance matrices, regression, and principal component
analysis, which are able to detect outliers and to mitigate their effect. In
addition, RobPy provides specialized diagnostic plots for visualizing casewise
and cellwise outliers. This paper presents the structure of the RobPy package,
demonstrates its functionality through examples, and compares its features to
existing implementations in other statistical software. By bringing robust
methods to Python, RobPy enables more users to perform robust data analysis in
a modern and versatile programming language."
"Transformers can efficiently learn in-context from example demonstrations.
Most existing theoretical analyses studied the in-context learning (ICL)
ability of transformers for linear function classes, where it is typically
shown that the minimizer of the pretraining loss implements one gradient
descent step on the least squares objective. However, this simplified linear
setting arguably does not demonstrate the statistical efficiency of ICL, since
the pretrained transformer does not outperform directly solving linear
regression on the test prompt. In this paper, we study ICL of a nonlinear
function class via transformer with nonlinear MLP layer: given a class of
\textit{single-index} target functions $f_*(\boldsymbol{x}) =
\sigma_*(\langle\boldsymbol{x},\boldsymbol{\beta}\rangle)$, where the index
features $\boldsymbol{\beta}\in\mathbb{R}^d$ are drawn from a $r$-dimensional
subspace, we show that a nonlinear transformer optimized by gradient descent
(with a pretraining sample complexity that depends on the \textit{information
exponent} of the link functions $\sigma_*$) learns $f_*$ in-context with a
prompt length that only depends on the dimension of the distribution of target
functions $r$; in contrast, any algorithm that directly learns $f_*$ on test
prompt yields a statistical complexity that scales with the ambient dimension
$d$. Our result highlights the adaptivity of the pretrained transformer to
low-dimensional structures of the function class, which enables
sample-efficient ICL that outperforms estimators that only have access to the
in-context data."
"Topic modeling, or more broadly, dimensionality reduction, techniques provide
powerful tools for uncovering patterns in large datasets and are widely applied
across various domains. We investigate how Non-negative Matrix Factorization
(NMF) can introduce bias in the representation of data groups, such as those
defined by demographics or protected attributes. We present an approach, called
Fairer-NMF, that seeks to minimize the maximum reconstruction loss for
different groups relative to their size and intrinsic complexity. Further, we
present two algorithms for solving this problem. The first is an alternating
minimization (AM) scheme and the second is a multiplicative updates (MU) scheme
which demonstrates a reduced computational time compared to AM while still
achieving similar performance. Lastly, we present numerical experiments on
synthetic and real datasets to evaluate the overall performance and trade-offs
of Fairer-NMF"
"Federated Learning (FL) is a distributed learning approach that trains neural
networks across multiple devices while keeping their local data private.
However, FL often faces challenges due to data heterogeneity, leading to
inconsistent local optima among clients. These inconsistencies can cause
unfavorable convergence behavior and generalization performance degradation.
Existing studies mainly describe this issue through \textit{convergence
analysis}, focusing on how well a model fits training data, or through
\textit{algorithmic stability}, which examines the generalization gap. However,
neither approach precisely captures the generalization performance of FL
algorithms, especially for neural networks. In this paper, we introduce the
first generalization dynamics analysis framework in federated optimization,
highlighting the trade-offs between model stability and optimization. Through
this framework, we show how the generalization of FL algorithms is affected by
the interplay of algorithmic stability and optimization. This framework applies
to standard federated optimization and its advanced versions, like server
momentum. We find that fast convergence from large local steps or accelerated
momentum enlarges stability but obtains better generalization performance. Our
insights into these trade-offs can guide the practice of future algorithms for
better generalization."
"Thompson sampling (TS) has optimal regret and excellent empirical performance
in multi-armed bandit problems. Yet, in Bayesian optimization, TS underperforms
popular acquisition functions (e.g., EI, UCB). TS samples arms according to the
probability that they are optimal. A recent algorithm, P-Star Sampler (PSS),
performs such a sampling via Hit-and-Run. We present an improved version,
Stagger Thompson Sampler (STS). STS more precisely locates the maximizer than
does TS using less computation time. We demonstrate that STS outperforms TS,
PSS, and other acquisition methods in numerical experiments of optimizations of
several test functions across a broad range of dimension. Additionally, since
PSS was originally presented not as a standalone acquisition method but as an
input to a batching algorithm called Minimal Terminal Variance (MTV), we also
demon-strate that STS matches PSS performance when used as the input to MTV."
"We introduce a new maximum entropy reinforcement learning framework based on
the distribution of states and actions visited by a policy. More precisely, an
intrinsic reward function is added to the reward function of the Markov
decision process that shall be controlled. For each state and action, this
intrinsic reward is the relative entropy of the discounted distribution of
states and actions (or features from these states and actions) visited during
the next time steps. We first prove that an optimal exploration policy, which
maximizes the expected discounted sum of intrinsic rewards, is also a policy
that maximizes a lower bound on the state-action value function of the decision
process under some assumptions. We also prove that the visitation distribution
used in the intrinsic reward definition is the fixed point of a contraction
operator. Following, we describe how to adapt existing algorithms to learn this
fixed point and compute the intrinsic rewards to enhance exploration. A new
practical off-policy maximum entropy reinforcement learning algorithm is
finally introduced. Empirically, exploration policies have good state-action
space coverage, and high-performing control policies are computed efficiently."
"Cost-sensitive loss functions are crucial in many real-world prediction
problems, where different types of errors are penalized differently; for
example, in medical diagnosis, a false negative prediction can lead to worse
consequences than a false positive prediction. However, traditional PAC
learning theory has mostly focused on the symmetric 0-1 loss, leaving
cost-sensitive losses largely unaddressed. In this work, we extend the
celebrated theory of boosting to incorporate both cost-sensitive and
multi-objective losses. Cost-sensitive losses assign costs to the entries of a
confusion matrix, and are used to control the sum of prediction errors
accounting for the cost of each error type. Multi-objective losses, on the
other hand, simultaneously track multiple cost-sensitive losses, and are useful
when the goal is to satisfy several criteria at once (e.g., minimizing false
positives while keeping false negatives below a critical threshold). We develop
a comprehensive theory of cost-sensitive and multi-objective boosting,
providing a taxonomy of weak learning guarantees that distinguishes which
guarantees are trivial (i.e., can always be achieved), which ones are boostable
(i.e., imply strong learning), and which ones are intermediate, implying
non-trivial yet not arbitrarily accurate learning. For binary classification,
we establish a dichotomy: a weak learning guarantee is either trivial or
boostable. In the multiclass setting, we describe a more intricate landscape of
intermediate weak learning guarantees. Our characterization relies on a
geometric interpretation of boosting, revealing a surprising equivalence
between cost-sensitive and multi-objective losses."
"Multivariate time series forecasting is crucial for various applications,
such as financial investment, energy management, weather forecasting, and
traffic optimization. However, accurate forecasting is challenging due to two
main factors. First, real-world time series often show heterogeneous temporal
patterns caused by distribution shifts over time. Second, correlations among
channels are complex and intertwined, making it hard to model the interactions
among channels precisely and flexibly.
  In this study, we address these challenges by proposing a general framework
called DUET, which introduces dual clustering on the temporal and channel
dimensions to enhance multivariate time series forecasting. First, we design a
Temporal Clustering Module (TCM) that clusters time series into fine-grained
distributions to handle heterogeneous temporal patterns. For different
distribution clusters, we design various pattern extractors to capture their
intrinsic temporal patterns, thus modeling the heterogeneity. Second, we
introduce a novel Channel-Soft-Clustering strategy and design a Channel
Clustering Module (CCM), which captures the relationships among channels in the
frequency domain through metric learning and applies sparsification to mitigate
the adverse effects of noisy channels. Finally, DUET combines TCM and CCM to
incorporate both the temporal and channel dimensions. Extensive experiments on
25 real-world datasets from 10 application domains, demonstrate the
state-of-the-art performance of DUET."
"Surrogate models are often used as computationally efficient approximations
to complex simulation models, enabling tasks such as solving inverse problems,
sensitivity analysis, and probabilistic forward predictions, which would
otherwise be computationally infeasible. During training, surrogate parameters
are fitted such that the surrogate reproduces the simulation model's outputs as
closely as possible. However, the simulation model itself is merely a
simplification of the real-world system, often missing relevant processes or
suffering from misspecifications e.g., in inputs or boundary conditions. Hints
about these might be captured in real-world measurement data, and yet, we
typically ignore those hints during surrogate building. In this paper, we
propose two novel probabilistic approaches to integrate simulation data and
real-world measurement data during surrogate training. The first method trains
separate surrogate models for each data source and combines their predictive
distributions, while the second incorporates both data sources by training a
single surrogate. We show the conceptual differences and benefits of the two
approaches through both synthetic and real-world case studies. The results
demonstrate the potential of these methods to improve predictive accuracy,
predictive coverage, and to diagnose problems in the underlying simulation
model. These insights can improve system understanding and future model
development."
"The accuracy of deep neural networks is significantly influenced by the
effectiveness of mini-batch construction during training. In single-label
scenarios, such as binary and multi-class classification tasks, it has been
demonstrated that batch selection algorithms preferring samples with higher
uncertainty achieve better performance than difficulty-based methods. Although
there are two batch selection methods tailored for multi-label data, none of
them leverage important uncertainty information. Adapting the concept of
uncertainty to multi-label data is not a trivial task, since there are two
issues that should be tackled. First, traditional variance or entropy-based
uncertainty measures ignore fluctuations of predictions within sliding windows
and the importance of the current model state. Second, existing multi-label
methods do not explicitly exploit the label correlations, particularly the
uncertainty-based label correlations that evolve during the training process.
In this paper, we propose an uncertainty-based multi-label batch selection
algorithm. It assesses uncertainty for each label by considering differences
between successive predictions and the confidence of current outputs, and
further leverages dynamic uncertainty-based label correlations to emphasize
instances whose uncertainty is synergistically expressed across multiple
labels. Empirical studies demonstrate the effectiveness of our method in
improving the performance and accelerating the convergence of various
multi-label deep learning models."
"Traditional analyses of gradient descent optimization show that, when the
largest eigenvalue of the loss Hessian - often referred to as the sharpness -
is below a critical learning-rate threshold, then training is 'stable' and
training loss decreases monotonically. Recent studies, however, have suggested
that the majority of modern deep neural networks achieve good performance
despite operating outside this stable regime. In this work, we demonstrate that
such instabilities, induced by large learning rates, move model parameters
toward flatter regions of the loss landscape. Our crucial insight lies in
noting that, during these instabilities, the orientation of the Hessian
eigenvectors rotate. This, we conjecture, allows the model to explore regions
of the loss landscape that display more desirable geometrical properties for
generalization, such as flatness. These rotations are a consequence of network
depth, and we prove that for any network with depth > 1, unstable growth in
parameters cause rotations in the principal components of the Hessian, which
promote exploration of the parameter space away from unstable directions. Our
empirical studies reveal an implicit regularization effect in gradient descent
with large learning rates operating beyond the stability threshold. We find
these lead to excellent generalization performance on modern benchmark
datasets."
"Algorithmic bias is a pressing concern in educational data mining (EDM), as
it risks amplifying inequities in learning outcomes. The Area Between ROC
Curves (ABROCA) metric is frequently used to measure discrepancies in model
performance across demographic groups to quantify overall model fairness.
However, its skewed distribution--especially when class or group imbalances
exist--makes significance testing challenging. This study investigates ABROCA's
distributional properties and contributes robust methods for its significance
testing. Specifically, we address (1) whether ABROCA follows any known
distribution, (2) how to reliably test for algorithmic bias using ABROCA, and
(3) the statistical power achievable with ABROCA-based bias assessments under
typical EDM sample specifications. Simulation results confirm that ABROCA does
not match standard distributions, including those suited to accommodate
skewness. We propose nonparametric randomization tests for ABROCA and
demonstrate that reliably detecting bias with ABROCA requires large sample
sizes or substantial effect sizes, particularly in imbalanced settings.
Findings suggest that ABROCA-based bias evaluation based on sample sizes common
in EDM tends to be underpowered, undermining the reliability of conclusions
about model fairness. By offering open-source code to simulate power and
statistically test ABROCA, this paper aims to foster more reliable statistical
testing in EDM research. It supports broader efforts toward replicability and
equity in educational modeling."
"Black-box optimization is often encountered for decision-making in complex
systems management, where the knowledge of system is limited. Under these
circumstances, it is essential to balance the utilization of new information
with computational efficiency. In practice, decision-makers often face the dual
tasks of optimization and statistical inference for the optimal performance, in
order to achieve it with a high reliability. Our goal is to address the dual
tasks in an online fashion. Wu et al (2022) [arXiv preprint: 2210.06737] point
out that the sample average of performance estimates generated by the
optimization algorithm needs not to admit a central limit theorem. We propose
an algorithm that not only tackles this issue, but also provides an online
consistent estimator for the variance of the performance. Furthermore, we
characterize the convergence rate of the coverage probabilities of the
asymptotic confidence intervals."
"Existing score-based methods for directed acyclic graph (DAG) learning from
observational data struggle to recover the causal graph accurately and
sample-efficiently. To overcome this, in this study, we propose DrBO (DAG
recovery via Bayesian Optimization)-a novel DAG learning framework leveraging
Bayesian optimization (BO) to find high-scoring DAGs. We show that, by
sophisticatedly choosing the promising DAGs to explore, we can find
higher-scoring ones much more efficiently. To address the scalability issues of
conventional BO in DAG learning, we replace Gaussian Processes commonly
employed in BO with dropout neural networks, trained in a continual manner,
which allows for (i) flexibly modeling the DAG scores without overfitting, (ii)
incorporation of uncertainty into the estimated scores, and (iii) scaling with
the number of evaluations. As a result, DrBO is computationally efficient and
can find the accurate DAG in fewer trials and less time than existing
state-of-the-art methods. This is demonstrated through an extensive set of
empirical evaluations on many challenging settings with both synthetic and real
data. Our implementation is available at https://github.com/baosws/DrBO."
"In recent work on time-series prediction, Transformers and even large
language models have garnered significant attention due to their strong
capabilities in sequence modeling. However, in practical deployments,
time-series prediction often requires operation in resource-constrained
environments, such as edge devices, which are unable to handle the
computational overhead of large models. To address such scenarios, some
lightweight models have been proposed, but they exhibit poor performance on
non-stationary sequences. In this paper, we propose $\textit{SWIFT}$, a
lightweight model that is not only powerful, but also efficient in deployment
and inference for Long-term Time Series Forecasting (LTSF). Our model is based
on three key points: (i) Utilizing wavelet transform to perform lossless
downsampling of time series. (ii) Achieving cross-band information fusion with
a learnable filter. (iii) Using only one shared linear layer or one shallow MLP
for sub-series' mapping. We conduct comprehensive experiments, and the results
show that $\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on
multiple datasets, offering a promising method for edge computing and
deployment in this task. Moreover, it is noteworthy that the number of
parameters in $\textit{SWIFT-Linear}$ is only 25\% of what it would be with a
single-layer linear model for time-domain prediction. Our code is available at
https://github.com/LancelotXWX/SWIFT."
"We introduce Random Feature Representation Boosting (RFRBoost), a novel
method for constructing deep residual random feature neural networks (RFNNs)
using boosting theory. RFRBoost uses random features at each layer to learn the
functional gradient of the network representation, enhancing performance while
preserving the convex optimization benefits of RFNNs. In the case of MSE loss,
we obtain closed-form solutions to greedy layer-wise boosting with random
features. For general loss functions, we show that fitting random feature
residual blocks reduces to solving a quadratically constrained least squares
problem. We demonstrate, through numerical experiments on 91 tabular datasets
for regression and classification, that RFRBoost significantly outperforms
traditional RFNNs and end-to-end trained MLP ResNets, while offering
substantial computational advantages and theoretical guarantees stemming from
boosting theory."
"We develop and demonstrate a probabilistic method for classifying rare
objects in surveys with the particular goal of building very pure samples. It
works by modifying the output probabilities from a classifier so as to
accommodate our expectation (priors) concerning the relative frequencies of
different classes of objects. We demonstrate our method using the Discrete
Source Classifier, a supervised classifier currently based on Support Vector
Machines, which we are developing in preparation for the Gaia data analysis.
DSC classifies objects using their very low resolution optical spectra. We look
in detail at the problem of quasar classification, because identification of a
pure quasar sample is necessary to define the Gaia astrometric reference frame.
By varying a posterior probability threshold in DSC we can trade off sample
completeness and contamination. We show, using our simulated data, that it is
possible to achieve a pure sample of quasars (upper limit on contamination of 1
in 40,000) with a completeness of 65% at magnitudes of G=18.5, and 50% at
G=20.0, even when quasars have a frequency of only 1 in every 2000 objects. The
star sample completeness is simultaneously 99% with a contamination of 0.7%.
Including parallax and proper motion in the classifier barely changes the
results. We further show that not accounting for class priors in the target
population leads to serious misclassifications and poor predictions for sample
completeness and contamination. (Truncated)"
"Dimensional reduction of high dimensional data can be achieved by keeping
only the relevant eigenmodes after principal component analysis. However,
differentiating relevant eigenmodes from the random noise eigenmodes is
problematic. A new method based on the random matrix theory and a statistical
goodness-of-fit test is proposed in this paper. It is validated by numerical
simulations and applied to real-time magnetic resonance cardiac cine images."
"Music genre classification is an essential tool for music information
retrieval systems and it has been finding critical applications in various
media platforms. Two important problems of the automatic music genre
classification are feature extraction and classifier design. This paper
investigates inter-genre similarity modelling (IGS) to improve the performance
of automatic music genre classification. Inter-genre similarity information is
extracted over the mis-classified feature population. Once the inter-genre
similarity is modelled, elimination of the inter-genre similarity reduces the
inter-genre confusion and improves the identification rates. Inter-genre
similarity modelling is further improved with iterative IGS modelling(IIGS) and
score modelling for IGS elimination(SMIGS). Experimental results with promising
classification improvements are provided."
"User authentication and intrusion detection differ from standard
classification problems in that while we have data generated from legitimate
users, impostor or intrusion data is scarce or non-existent. We review existing
techniques for dealing with this problem and propose a novel alternative based
on a principled statistical decision-making view point. We examine the
technique on a toy problem and validate it on complex real-world data from an
RFID based access control system. The results indicate that it can
significantly outperform the classical world model approach. The method could
be more generally useful in other decision-making scenarios where there is a
lack of adversary data."
"We consider a joint processing of $n$ independent sparse regression problems.
Each is based on a sample $(y_{i1},x_{i1})...,(y_{im},x_{im})$ of $m$ \iid
observations from $y_{i1}=x_{i1}\t\beta_i+\eps_{i1}$, $y_{i1}\in \R$, $x_{i
1}\in\R^p$, $i=1,...,n$, and $\eps_{i1}\dist N(0,\sig^2)$, say. $p$ is large
enough so that the empirical risk minimizer is not consistent. We consider
three possible extensions of the lasso estimator to deal with this problem, the
lassoes, the group lasso and the RING lasso, each utilizing a different
assumption how these problems are related. For each estimator we give a
Bayesian interpretation, and we present both persistency analysis and
non-asymptotic error bounds based on restricted eigenvalue - type assumptions."
"The problem of clustering is considered, for the case when each data point is
a sample generated by a stationary ergodic process. We propose a very natural
asymptotic notion of consistency, and show that simple consistent algorithms
exist, under most general non-parametric assumptions. The notion of consistency
is as follows: two samples should be put into the same cluster if and only if
they were generated by the same distribution. With this notion of consistency,
clustering generalizes such classical statistical problems as homogeneity
testing and process classification. We show that, for the case of a known
number of clusters, consistency can be achieved under the only assumption that
the joint distribution of the data is stationary ergodic (no parametric or
Markovian assumptions, no assumptions of independence, neither between nor
within the samples). If the number of clusters is unknown, consistency can be
achieved under appropriate assumptions on the mixing rates of the processes.
(again, no parametric or independence assumptions). In both cases we give
examples of simple (at most quadratic in each argument) algorithms which are
consistent."
"As increasing amounts of sensitive personal information is aggregated into
data repositories, it has become important to develop mechanisms for processing
the data without revealing information about individual data instances. The
differential privacy model provides a framework for the development and
theoretical analysis of such mechanisms. In this paper, we propose an algorithm
for learning a discriminatively trained multi-class Gaussian classifier that
satisfies differential privacy using a large margin loss function with a
perturbed regularization term. We present a theoretical upper bound on the
excess risk of the classifier introduced by the perturbation."
"Model selection in clustering requires (i) to specify a suitable clustering
principle and (ii) to control the model order complexity by choosing an
appropriate number of clusters depending on the noise level in the data. We
advocate an information theoretic perspective where the uncertainty in the
measurements quantizes the set of data partitionings and, thereby, induces
uncertainty in the solution space of clusterings. A clustering model, which can
tolerate a higher level of fluctuations in the measurements than alternative
models, is considered to be superior provided that the clustering solution is
equally informative. This tradeoff between \emph{informativeness} and
\emph{robustness} is used as a model selection criterion. The requirement that
data partitionings should generalize from one data set to an equally probable
second data set gives rise to a new notion of structure induced information."
"We show that the sets in a family with finite VC dimension can be uniformly
approximated within a given error by a finite partition. Immediate corollaries
include the fact that VC classes have finite bracketing numbers, satisfy
uniform laws of averages under strong dependence, and exhibit uniform mixing.
Our results are based on recent work concerning uniform laws of averages for VC
classes under ergodic sampling."
"We study the problem of finding the smallest $m$ such that every element of
an exponential family can be written as a mixture of $m$ elements of another
exponential family. We propose an approach based on coverings and packings of
the face lattice of the corresponding convex support polytopes and results from
coding theory. We show that $m=q^{N-1}$ is the smallest number for which any
distribution of $N$ $q$-ary variables can be written as mixture of $m$
independent $q$-ary variables. Furthermore, we show that any distribution of
$N$ binary variables is a mixture of $m = 2^{N-(k+1)}(1+ 1/(2^k-1))$ elements
of the $k$-interaction exponential family."
"We propose a graphical model for representing networks of stochastic
processes, the minimal generative model graph. It is based on reduced
factorizations of the joint distribution over time. We show that under
appropriate conditions, it is unique and consistent with another type of
graphical model, the directed information graph, which is based on a
generalization of Granger causality. We demonstrate how directed information
quantifies Granger causality in a particular sequential prediction setting. We
also develop efficient methods to estimate the topological structure from data
that obviate estimating the joint statistics. One algorithm assumes
upper-bounds on the degrees and uses the minimal dimension statistics
necessary. In the event that the upper-bounds are not valid, the resulting
graph is nonetheless an optimal approximation. Another algorithm uses
near-minimal dimension statistics when no bounds are known but the distribution
satisfies a certain criterion. Analogous to how structure learning algorithms
for undirected graphical models use mutual information estimates, these
algorithms use directed information estimates. We characterize the
sample-complexity of two plug-in directed information estimators and obtain
confidence intervals. For the setting when point estimates are unreliable, we
propose an algorithm that uses confidence intervals to identify the best
approximation that is robust to estimation error. Lastly, we demonstrate the
effectiveness of the proposed algorithms through analysis of both synthetic
data and real data from the Twitter network. In the latter case, we identify
which news sources influence users in the network by merely analyzing tweet
times."
"In this paper we consider sparse approximation problems, that is, general
$l_0$ minimization problems with the $l_0$-""norm"" of a vector being a part of
constraints or objective function. In particular, we first study the
first-order optimality conditions for these problems. We then propose penalty
decomposition (PD) methods for solving them in which a sequence of penalty
subproblems are solved by a block coordinate descent (BCD) method. Under some
suitable assumptions, we establish that any accumulation point of the sequence
generated by the PD methods satisfies the first-order optimality conditions of
the problems. Furthermore, for the problems in which the $l_0$ part is the only
nonconvex part, we show that such an accumulation point is a local minimizer of
the problems. In addition, we show that any accumulation point of the sequence
generated by the BCD method is a saddle point of the penalty subproblem.
Moreover, for the problems in which the $l_0$ part is the only nonconvex part,
we establish that such an accumulation point is a local minimizer of the
penalty subproblem. Finally, we test the performance of our PD methods by
applying them to sparse logistic regression, sparse inverse covariance
selection, and compressed sensing problems. The computational results
demonstrate that our methods generally outperform the existing methods in terms
of solution quality and/or speed."
"In this work we consider the stochastic minimization of nonsmooth convex loss
functions, a central problem in machine learning. We propose a novel algorithm
called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which
exploits the structure of common nonsmooth loss functions to achieve optimal
convergence rates for a class of problems including SVMs. It is the first
stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing
nonsmooth loss functions (with strong convexity). The fast rates are confirmed
by empirical comparisons, in which ANSGD significantly outperforms previous
subgradient descent algorithms including SGD."
"Oriental ink painting, called Sumi-e, is one of the most appealing painting
styles that has attracted artists around the world. Major challenges in
computer-based Sumi-e simulation are to abstract complex scene information and
draw smooth and natural brush strokes. To automatically find such strokes, we
propose to model the brush as a reinforcement learning agent, and learn desired
brush-trajectories by maximizing the sum of rewards in the policy search
framework. We also provide elaborate design of actions, states, and rewards
tailored for a Sumi-e agent. The effectiveness of our proposed approach is
demonstrated through simulated Sumi-e experiments."
"We present the mixture-of-parents maximum entropy Markov model (MoP-MEMM), a
class of directed graphical models extending MEMMs. The MoP-MEMM allows
tractable incorporation of long-range dependencies between nodes by restricting
the conditional distribution of each node to be a mixture of distributions
given the parents. We show how to efficiently compute the exact marginal
posterior node distributions, regardless of the range of the dependencies. This
enables us to model non-sequential correlations present within text documents,
as well as between interconnected documents, such as hyperlinked web pages. We
apply the MoP-MEMM to a named entity recognition task and a web page
classification task. In each, our model shows significant improvement over the
basic MEMM, and is competitive with other long-range sequence models that use
approximate inference."
"We introduce a copula mixture model to perform dependency-seeking clustering
when co-occurring samples from different data sources are available. The model
takes advantage of the great flexibility offered by the copulas framework to
extend mixtures of Canonical Correlation Analysis to multivariate data with
arbitrary continuous marginal densities. We formulate our model as a
non-parametric Bayesian mixture, while providing efficient MCMC inference.
Experiments on synthetic and real data demonstrate that the increased
flexibility of the copula mixture significantly improves the clustering and the
interpretability of the results."
"We consider a Bayesian method for learning the Bayesian network structure
from complete data. Recently, Koivisto and Sood (2004) presented an algorithm
that for any single edge computes its marginal posterior probability in O(n
2^n) time, where n is the number of attributes; the number of parents per
attribute is bounded by a constant. In this paper we show that the posterior
probabilities for all the n (n - 1) potential edges can be computed in O(n 2^n)
total time. This result is achieved by a forward-backward technique and fast
Moebius transform algorithms, which are of independent interest. The resulting
speedup by a factor of about n^2 allows us to experimentally study the
statistical power of learning moderate-size networks. We report results from a
simulation study that covers data sets with 20 to 10,000 records over 5 to 25
discrete attributes"
"We provide faster algorithms for the problem of Gaussian summation, which
occurs in many machine learning methods. We develop two new extensions - an
O(Dp) Taylor expansion for the Gaussian kernel with rigorous error bounds and a
new error control scheme integrating any arbitrary approximation method -
within the best discretealgorithmic framework using adaptive hierarchical data
structures. We rigorously evaluate these techniques empirically in the context
of optimal bandwidth selection in kernel density estimation, revealing the
strengths and weaknesses of current state-of-the-art approaches for the first
time. Our results demonstrate that the new error control scheme yields improved
performance, whereas the series expansion approach is only effective in low
dimensions (five or less)."
"Estimating the level set of a signal from measurements is a task that arises
in a variety of fields, including medical imaging, astronomy, and digital
elevation mapping. Motivated by scenarios where accurate and complete
measurements of the signal may not available, we examine here a simple
procedure for estimating the level set of a signal from highly incomplete
measurements, which may additionally be corrupted by additive noise. The
proposed procedure is based on box-constrained Total Variation (TV)
regularization. We demonstrate the performance of our approach, relative to
existing state-of-the-art techniques for level set estimation from compressive
measurements, via several simulation examples."
"We introduce a method to learn a mixture of submodular ""shells"" in a
large-margin setting. A submodular shell is an abstract submodular function
that can be instantiated with a ground set and a set of parameters to produce a
submodular function. A mixture of such shells can then also be so instantiated
to produce a more complex submodular function. What our algorithm learns are
the mixture weights over such shells. We provide a risk bound guarantee when
learning in a large-margin structured-prediction setting using a projected
subgradient method when only approximate submodular optimization is possible
(such as with submodular function maximization). We apply this method to the
problem of multi-document summarization and produce the best results reported
so far on the widely used NIST DUC-05 through DUC-07 document summarization
corpora."
"Collaborative filtering (CF) and content-based filtering (CBF) have widely
been used in information filtering applications. Both approaches have their
strengths and weaknesses which is why researchers have developed hybrid
systems. This paper proposes a novel approach to unify CF and CBF in a
probabilistic framework, named collaborative ensemble learning. It uses
probabilistic SVMs to model each user's profile (as CBF does).At the prediction
phase, it combines a society OF users profiles, represented by their respective
SVM models, to predict an active users preferences(the CF idea).The combination
scheme is embedded in a probabilistic framework and retains an intuitive
explanation.Moreover, collaborative ensemble learning does not require a global
training stage and thus can incrementally incorporate new data.We report
results based on two data sets. For the Reuters-21578 text data set, we
simulate user ratings under the assumption that each user is interested in only
one category. In the second experiment, we use users' opinions on a set of 642
art images that were collected through a web-based survey. For both data sets,
collaborative ensemble achieved excellent performance in terms of
recommendation accuracy."
"Minibatching is a very well studied and highly popular technique in
supervised learning, used by practitioners due to its ability to accelerate
training through better utilization of parallel processing power and reduction
of stochastic variance. Another popular technique is importance sampling -- a
strategy for preferential sampling of more important examples also capable of
accelerating the training process. However, despite considerable effort by the
community in these areas, and due to the inherent technical difficulty of the
problem, there is no existing work combining the power of importance sampling
with the strength of minibatching. In this paper we propose the first {\em
importance sampling for minibatches} and give simple and rigorous complexity
analysis of its performance. We illustrate on synthetic problems that for
training data of certain properties, our sampling can lead to several orders of
magnitude improvement in training time. We then test the new sampling on
several popular datasets, and show that the improvement can reach an order of
magnitude."
"Protein structure prediction has been a grand challenge problem in the
structure biology over the last few decades. Protein quality assessment plays a
very important role in protein structure prediction. In the paper, we propose a
new protein quality assessment method which can predict both local and global
quality of the protein 3D structural models. Our method uses both multi and
single model quality assessment method for global quality assessment, and uses
chemical, physical, geo-metrical features, and global quality score for local
quality assessment. CASP9 targets are used to generate the features for local
quality assessment. We evaluate the performance of our local quality assessment
method on CASP10, which is comparable with two stage-of-art QA methods based on
the average absolute distance between the real and predicted distance. In
addition, we blindly tested our method on CASP11, and the good performance
shows that combining single and multiple model quality assessment method could
be a good way to improve the accuracy of model quality assessment, and the
random forest technique could be used to train a good local quality assessment
model."
"Multivariate pattern analyses approaches in neuroimaging are fundamentally
concerned with investigating the quantity and type of information processed by
various regions of the human brain; typically, estimates of classification
accuracy are used to quantify information. While a extensive and powerful
library of methods can be applied to train and assess classifiers, it is not
always clear how to use the resulting measures of classification performance to
draw scientific conclusions: e.g. for the purpose of evaluating redundancy
between brain regions. An additional confound for interpreting classification
performance is the dependence of the error rate on the number and choice of
distinct classes obtained for the classification task. In contrast, mutual
information is a quantity defined independently of the experimental design, and
has ideal properties for comparative analyses. Unfortunately, estimating the
mutual information based on observations becomes statistically infeasible in
high dimensions without some kind of assumption or prior.
  In this paper, we construct a novel classification-based estimator of mutual
information based on high-dimensional asymptotics. We show that in a particular
limiting regime, the mutual information is an invertible function of the
expected $k$-class Bayes error. While the theory is based on a large-sample,
high-dimensional limit, we demonstrate through simulations that our proposed
estimator has superior performance to the alternatives in problems of moderate
dimensionality."
"Motivated by big data applications, first-order methods have been extremely
popular in recent years. However, naive gradient methods generally converge
slowly. Hence, much efforts have been made to accelerate various first-order
methods. This paper proposes two accelerated methods towards solving structured
linearly constrained convex programming, for which we assume composite convex
objective.
  The first method is the accelerated linearized augmented Lagrangian method
(LALM). At each update to the primal variable, it allows linearization to the
differentiable function and also the augmented term, and thus it enables easy
subproblems. Assuming merely weak convexity, we show that LALM owns $O(1/t)$
convergence if parameters are kept fixed during all the iterations and can be
accelerated to $O(1/t^2)$ if the parameters are adapted, where $t$ is the
number of total iterations.
  The second method is the accelerated linearized alternating direction method
of multipliers (LADMM). In addition to the composite convexity, it further
assumes two-block structure on the objective. Different from classic ADMM, our
method allows linearization to the objective and also augmented term to make
the update simple. Assuming strong convexity on one block variable, we show
that LADMM also enjoys $O(1/t^2)$ convergence with adaptive parameters. This
result is a significant improvement over that in [Goldstein et. al, SIIMS'14],
which requires strong convexity on both block variables and no linearization to
the objective or augmented term.
  Numerical experiments are performed on quadratic programming, image
denoising, and support vector machine. The proposed accelerated methods are
compared to nonaccelerated ones and also existing accelerated methods. The
results demonstrate the validness of acceleration and superior performance of
the proposed methods over existing ones."
"Spectral methods are popular in detecting global structures in the given data
that can be represented as a matrix. However when the data matrix is sparse or
noisy, classic spectral methods usually fail to work, due to localization of
eigenvectors (or singular vectors) induced by the sparsity or noise. In this
work, we propose a general method to solve the localization problem by learning
a regularization matrix from the localized eigenvectors. Using matrix
perturbation analysis, we demonstrate that the learned regularizations suppress
down the eigenvalues associated with localized eigenvectors and enable us to
recover the informative eigenvectors representing the global structure. We show
applications of our method in several inference problems: community detection
in networks, clustering from pairwise similarities, rank estimation and matrix
completion problems. Using extensive experiments, we illustrate that our method
solves the localization problem and works down to the theoretical detectability
limits in different kinds of synthetic data. This is in contrast with existing
spectral algorithms based on data matrix, non-backtracking matrix, Laplacians
and those with rank-one regularizations, which perform poorly in the sparse
case with noise."
"Motivated by applications in neuroimaging analysis, we propose a new
regression model, Sparse TensOr REsponse regression (STORE), with a tensor
response and a vector predictor. STORE embeds two key sparse structures:
element-wise sparsity and low-rankness. It can handle both a non-symmetric and
a symmetric tensor response, and thus is applicable to both structural and
functional neuroimaging data. We formulate the parameter estimation as a
non-convex optimization problem, and develop an efficient alternating updating
algorithm. We establish a non-asymptotic estimation error bound for the actual
estimator obtained from the proposed algorithm. This error bound reveals an
interesting interaction between the computational efficiency and the
statistical rate of convergence. When the distribution of the error tensor is
Gaussian, we further obtain a fast estimation error rate which allows the
tensor dimension to grow exponentially with the sample size. We illustrate the
efficacy of our model through intensive simulations and an analysis of the
Autism spectrum disorder neuroimaging data."
"We present a new autoencoder-type architecture that is trainable in an
unsupervised mode, sustains both generation and inference, and has the quality
of conditional and unconditional samples boosted by adversarial learning.
Unlike previous hybrids of autoencoders and adversarial networks, the
adversarial game in our approach is set up directly between the encoder and the
generator, and no external mappings are trained in the process of learning. The
game objective compares the divergences of each of the real and the generated
data distributions with the prior distribution in the latent space. We show
that direct generator-vs-encoder game leads to a tight coupling of the two
components, resulting in samples and reconstructions of a comparable quality to
some recently-proposed more complex architectures."
"This paper presents an automated supervised method for Persian wordnet
construction. Using a Persian corpus and a bi-lingual dictionary, the initial
links between Persian words and Princeton WordNet synsets have been generated.
These links will be discriminated later as correct or incorrect by employing
seven features in a trained classification system. The whole method is just a
classification system, which has been trained on a train set containing FarsNet
as a set of correct instances. State of the art results on the automatically
derived Persian wordnet is achieved. The resulted wordnet with a precision of
91.18% includes more than 16,000 words and 22,000 synsets."
"Persistence diagrams (PDs) play a key role in topological data analysis
(TDA), in which they are routinely used to describe topological properties of
complicated shapes. PDs enjoy strong stability properties and have proven their
utility in various learning contexts. They do not, however, live in a space
naturally endowed with a Hilbert structure and are usually compared with
specific distances, such as the bottleneck distance. To incorporate PDs in a
learning pipeline, several kernels have been proposed for PDs with a strong
emphasis on the stability of the RKHS distance w.r.t. perturbations of the PDs.
In this article, we use the Sliced Wasserstein approximation SW of the
Wasserstein distance to define a new kernel for PDs, which is not only provably
stable but also provably discriminative (depending on the number of points in
the PDs) w.r.t. the Wasserstein distance $d_1$ between PDs. We also demonstrate
its practicality, by developing an approximation technique to reduce kernel
computation time, and show that our proposal compares favorably to existing
kernels for PDs on several benchmarks."
"Datasets are often reused to perform multiple statistical analyses in an
adaptive way, in which each analysis may depend on the outcomes of previous
analyses on the same dataset. Standard statistical guarantees do not account
for these dependencies and little is known about how to provably avoid
overfitting and false discovery in the adaptive setting. We consider a natural
formalization of this problem in which the goal is to design an algorithm that,
given a limited number of i.i.d.~samples from an unknown distribution, can
answer adaptively-chosen queries about that distribution.
  We present an algorithm that estimates the expectations of $k$ arbitrary
adaptively-chosen real-valued estimators using a number of samples that scales
as $\sqrt{k}$. The answers given by our algorithm are essentially as accurate
as if fresh samples were used to evaluate each estimator. In contrast, prior
work yields error guarantees that scale with the worst-case sensitivity of each
estimator. We also give a version of our algorithm that can be used to verify
answers to such queries where the sample complexity depends logarithmically on
the number of queries $k$ (as in the reusable holdout technique).
  Our algorithm is based on a simple approximate median algorithm that
satisfies the strong stability guarantees of differential privacy. Our
techniques provide a new approach for analyzing the generalization guarantees
of differentially private algorithms."
"We present novel minibatch stochastic optimization methods for empirical risk
minimization problems, the methods efficiently leverage variance reduced
first-order and sub-sampled higher-order information to accelerate the
convergence speed. For quadratic objectives, we prove improved iteration
complexity over state-of-the-art under reasonable assumptions. We also provide
empirical evidence of the advantages of our method compared to existing
approaches in the literature."
"Quantum annealers aim at solving non-convex optimization problems by
exploiting cooperative tunneling effects to escape local minima. The underlying
idea consists in designing a classical energy function whose ground states are
the sought optimal solutions of the original optimization problem and add a
controllable quantum transverse field to generate tunneling processes. A key
challenge is to identify classes of non-convex optimization problems for which
quantum annealing remains efficient while thermal annealing fails. We show that
this happens for a wide class of problems which are central to machine
learning. Their energy landscapes is dominated by local minima that cause
exponential slow down of classical thermal annealers while simulated quantum
annealing converges efficiently to rare dense regions of optimal solutions."
"Recommender systems aim to find an accurate and efficient mapping from
historic data of user-preferred items to a new item that is to be liked by a
user. Towards this goal, energy-based sequence generative adversarial nets
(EB-SeqGANs) are adopted for recommendation by learning a generative model for
the time series of user-preferred items. By recasting the energy function as
the feature function, the proposed EB-SeqGANs is interpreted as an instance of
maximum-entropy imitation learning."
"We consider the problem of learning the functions computing children from
parents in a Structural Causal Model once the underlying causal graph has been
identified. This is in some sense the second step after causal discovery.
Taking a probabilistic approach to estimating these functions, we derive a
natural myopic active learning scheme that identifies the intervention which is
optimally informative about all of the unknown functions jointly, given
previously observed data. We test the derived algorithms on simple examples, to
demonstrate that they produce a structured exploration policy that
significantly improves on unstructured base-lines."
"Model-based clustering is a popular approach for clustering multivariate data
which has seen applications in numerous fields. Nowadays, high-dimensional data
are more and more common and the model-based clustering approach has adapted to
deal with the increasing dimensionality. In particular, the development of
variable selection techniques has received a lot of attention and research
effort in recent years. Even for small size problems, variable selection has
been advocated to facilitate the interpretation of the clustering results. This
review provides a summary of the methods developed for variable selection in
model-based clustering. Existing R packages implementing the different methods
are indicated and illustrated in application to two data analysis examples."
"Much work has been done in understanding human creativity and defining
measures to evaluate creativity. This is necessary mainly for the reason of
having an objective and automatic way of quantifying creative artifacts. In
this work, we propose a regression-based learning framework which takes into
account quantitatively the essential criteria for creativity like novelty,
influence, value and unexpectedness. As it is often the case with most creative
domains, there is no clear ground truth available for creativity. Our proposed
learning framework is applicable to all creative domains; yet we evaluate it on
a dataset of movies created from IMDb and Rotten Tomatoes due to availability
of audience and critic scores, which can be used as proxy ground truth labels
for creativity. We report promising results and observations from our
experiments in the following ways : 1) Correlation of creative criteria with
critic scores, 2) Improvement in movie rating prediction with inclusion of
various creative criteria, and 3) Identification of creative movies."
"This paper proposes Deep Hyperalignment (DHA) as a regularized, deep
extension, scalable Hyperalignment (HA) method, which is well-suited for
applying functional alignment to fMRI datasets with nonlinearity,
high-dimensionality (broad ROI), and a large number of subjects. Unlink
previous methods, DHA is not limited by a restricted fixed kernel function.
Further, it uses a parametric approach, rank-$m$ Singular Value Decomposition
(SVD), and stochastic gradient descent for optimization. Therefore, DHA has a
suitable time complexity for large datasets, and DHA does not require the
training data when it computes the functional alignment for a new subject.
Experimental studies on multi-subject fMRI analysis confirm that the DHA method
achieves superior performance to other state-of-the-art HA algorithms."
"Several statistical approaches based on reproducing kernels have been
proposed to detect abrupt changes arising in the full distribution of the
observations and not only in the mean or variance. Some of these approaches
enjoy good statistical properties (oracle inequality, \ldots). Nonetheless,
they have a high computational cost both in terms of time and memory. This
makes their application difficult even for small and medium sample sizes ($n<
10^4$). This computational issue is addressed by first describing a new
efficient and exact algorithm for kernel multiple change-point detection with
an improved worst-case complexity that is quadratic in time and linear in
space. It allows dealing with medium size signals (up to $n \approx 10^5$).
Second, a faster but approximation algorithm is described. It is based on a
low-rank approximation to the Gram matrix. It is linear in time and space. This
approximation algorithm can be applied to large-scale signals ($n \geq 10^6$).
These exact and approximation algorithms have been implemented in \texttt{R}
and \texttt{C} for various kernels. The computational and statistical
performances of these new algorithms have been assessed through empirical
experiments. The runtime of the new algorithms is observed to be faster than
that of other considered procedures. Finally, simulations confirmed the higher
statistical accuracy of kernel-based approaches to detect changes that are not
only in the mean. These simulations also illustrate the flexibility of
kernel-based approaches to analyze complex biological profiles made of DNA copy
number and allele B frequencies. An R package implementing the approach will be
made available on github."
"The Epicurean Philosophy is commonly thought as simplistic and hedonistic.
Here I discuss how this is a misconception and explore its link to
Reinforcement Learning. Based on the letters of Epicurus, I construct an
objective function for hedonism which turns out to be equivalent of the
Reinforcement Learning objective function when omitting the discount factor. I
then discuss how Plato and Aristotle 's views that can be also loosely linked
to Reinforcement Learning, as well as their weaknesses in relationship to it.
Finally, I emphasise the close affinity of the Epicurean views and the Bellman
equation."
"The development of new technology such as wearables that record high-quality
single channel ECG, provides an opportunity for ECG screening in a larger
population, especially for atrial fibrillation screening. The main goal of this
study is to develop an automatic classification algorithm for normal sinus
rhythm (NSR), atrial fibrillation (AF), other rhythms (O), and noise from a
single channel short ECG segment (9-60 seconds). For this purpose, signal
quality index (SQI) along with dense convolutional neural networks was used.
Two convolutional neural network (CNN) models (main model that accepts 15
seconds ECG and secondary model that processes 9 seconds shorter ECG) were
trained using the training data set. If the recording is determined to be of
low quality by SQI, it is immediately classified as noisy. Otherwise, it is
transformed to a time-frequency representation and classified with the CNN as
NSR, AF, O, or noise. At the final step, a feature-based post-processing
algorithm classifies the rhythm as either NSR or O in case the CNN model's
discrimination between the two is indeterminate. The best result achieved at
the official phase of the PhysioNet/CinC challenge on the blind test set was
0.80 (F1 for NSR, AF, and O were 0.90, 0.80, and 0.70, respectively)."
"The kernel embedding algorithm is an important component for adapting kernel
methods to large datasets. Since the algorithm consumes a major computation
cost in the testing phase, we propose a novel teacher-learner framework of
learning computation-efficient kernel embeddings from specific data. In the
framework, the high-precision embeddings (teacher) transfer the data
information to the computation-efficient kernel embeddings (learner). We
jointly select informative embedding functions and pursue an orthogonal
transformation between two embeddings. We propose a novel approach of
constrained variational expectation maximization (CVEM), where the alternate
direction method of multiplier (ADMM) is applied over a nonconvex domain in the
maximization step. We also propose two specific formulations based on the
prevalent Random Fourier Feature (RFF), the masked and blocked version of
Computation-Efficient RFF (CERF), by imposing a random binary mask or a block
structure on the transformation matrix. By empirical studies of several
applications on different real-world datasets, we demonstrate that the CERF
significantly improves the performance of kernel methods upon the RFF, under
certain arithmetic operation requirements, and suitable for structured matrix
multiplication in Fastfood type algorithms."
"Generative adversarial networks (GANs) are innovative techniques for learning
generative models of complex data distributions from samples. Despite
remarkable recent improvements in generating realistic images, one of their
major shortcomings is the fact that in practice, they tend to produce samples
with little diversity, even when trained on diverse datasets. This phenomenon,
known as mode collapse, has been the main focus of several recent advances in
GANs. Yet there is little understanding of why mode collapse happens and why
existing approaches are able to mitigate mode collapse. We propose a principled
approach to handling mode collapse, which we call packing. The main idea is to
modify the discriminator to make decisions based on multiple samples from the
same class, either real or artificially generated. We borrow analysis tools
from binary hypothesis testing---in particular the seminal result of Blackwell
[Bla53]---to prove a fundamental connection between packing and mode collapse.
We show that packing naturally penalizes generators with mode collapse, thereby
favoring generator distributions with less mode collapse during the training
process. Numerical experiments on benchmark datasets suggests that packing
provides significant improvements in practice as well."
"Intelligent transportation systems (ITSs) will be a major component of
tomorrow's smart cities. However, realizing the true potential of ITSs requires
ultra-low latency and reliable data analytics solutions that can combine, in
real-time, a heterogeneous mix of data stemming from the ITS network and its
environment. Such data analytics capabilities cannot be provided by
conventional cloud-centric data processing techniques whose communication and
computing latency can be high. Instead, edge-centric solutions that are
tailored to the unique ITS environment must be developed. In this paper, an
edge analytics architecture for ITSs is introduced in which data is processed
at the vehicle or roadside smart sensor level in order to overcome the ITS
latency and reliability challenges. With a higher capability of passengers'
mobile devices and intra-vehicle processors, such a distributed edge computing
architecture can leverage deep learning techniques for reliable mobile sensing
in ITSs. In this context, the ITS mobile edge analytics challenges pertaining
to heterogeneous data, autonomous control, vehicular platoon control, and
cyber-physical security are investigated. Then, different deep learning
solutions for such challenges are proposed. The proposed deep learning
solutions will enable ITS edge analytics by endowing the ITS devices with
powerful computer vision and signal processing functions. Preliminary results
show that the proposed edge analytics architecture, coupled with the power of
deep learning algorithms, can provide a reliable, secure, and truly smart
transportation environment."
"This paper studies the complexity of the stochastic gradient algorithm for
PCA when the data are observed in a streaming setting. We also propose an
online approach for selecting the learning rate. Simulation experiments confirm
the practical relevance of the plain stochastic gradient approach and that
drastic improvements can be achieved by learning the learning rate."
"We present a new method of blackbox optimization via gradient approximation
with the use of structured random orthogonal matrices, providing more accurate
estimators than baselines and with provable theoretical guarantees. We show
that this algorithm can be successfully applied to learn better quality compact
policies than those using standard gradient estimation techniques. The compact
policies we learn have several advantages over unstructured ones, including
faster training algorithms and faster inference. These benefits are important
when the policy is deployed on real hardware with limited resources. Further,
compact policies provide more scalable architectures for derivative-free
optimization (DFO) in high-dimensional spaces. We show that most robotics tasks
from the OpenAI Gym can be solved using neural networks with less than 300
parameters, with almost linear time complexity of the inference phase, with up
to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm
introduced by Salimans et al. (2017). We do not need heuristics such as fitness
shaping to learn good quality policies, resulting in a simple and theoretically
motivated training mechanism."
"Skip-gram with negative sampling, a popular variant of Word2vec originally
designed and tuned to create word embeddings for Natural Language Processing,
has been used to create item embeddings with successful applications in
recommendation. While these fields do not share the same type of data, neither
evaluate on the same tasks, recommendation applications tend to use the same
already tuned hyperparameters values, even if optimal hyperparameters values
are often known to be data and task dependent. We thus investigate the marginal
importance of each hyperparameter in a recommendation setting through large
hyperparameter grid searches on various datasets. Results reveal that
optimizing neglected hyperparameters, namely negative sampling distribution,
number of epochs, subsampling parameter and window-size, significantly improves
performance on a recommendation task, and can increase it by an order of
magnitude. Importantly, we find that optimal hyperparameters configurations for
Natural Language Processing tasks and Recommendation tasks are noticeably
different."
"Meta-learning algorithms use past experience to learn to quickly solve new
tasks. In the context of reinforcement learning, meta-learning algorithms
acquire reinforcement learning procedures to solve new problems more
efficiently by utilizing experience from prior tasks. The performance of
meta-learning algorithms depends on the tasks available for meta-training: in
the same way that supervised learning generalizes best to test points drawn
from the same distribution as the training points, meta-learning methods
generalize best to tasks from the same distribution as the meta-training tasks.
In effect, meta-reinforcement learning offloads the design burden from
algorithm design to task design. If we can automate the process of task design
as well, we can devise a meta-learning algorithm that is truly automated. In
this work, we take a step in this direction, proposing a family of unsupervised
meta-learning algorithms for reinforcement learning. We motivate and describe a
general recipe for unsupervised meta-reinforcement learning, and present an
instantiation of this approach. Our conceptual and theoretical contributions
consist of formulating the unsupervised meta-reinforcement learning problem and
describing how task proposals based on mutual information can be used to train
optimal meta-learners. Our experimental results indicate that unsupervised
meta-reinforcement learning effectively acquires accelerated reinforcement
learning procedures without the need for manual task design and these
procedures exceed the performance of learning from scratch."
"In the past, Acoustic Scene Classification systems have been based on hand
crafting audio features that are input to a classifier. Nowadays, the common
trend is to adopt data driven techniques, e.g., deep learning, where audio
representations are learned from data. In this paper, we propose a system that
consists of a simple fusion of two methods of the aforementioned types: a deep
learning approach where log-scaled mel-spectrograms are input to a
convolutional neural network, and a feature engineering approach, where a
collection of hand-crafted features is input to a gradient boosting machine. We
first show that both methods provide complementary information to some extent.
Then, we use a simple late fusion strategy to combine both methods. We report
classification accuracy of each method individually and the combined system on
the TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms
each of the individual methods and attains a classification accuracy of 72.8%
on the evaluation set, improving the baseline system by 11.8%."
"State space models (SSM) have been widely applied for the analysis and
visualization of large sequential datasets. Sequential Monte Carlo (SMC) is a
very popular particle-based method to sample latent states from intractable
posteriors. However, SSM is significantly influenced by the choice of the
proposal. Recently Hamiltonian Monte Carlo (HMC) sampling has shown success in
many practical problems. In this paper, we propose an SMC augmented by HMC
(HSMC) for inference and model learning of nonlinear SSM, which can exempt us
from learning proposals and reduce the model complexity significantly. Based on
the measure preserving property of HMC, the particles directly generated by
transition function can approximate the posterior of latent states arbitrarily
well. In order to better adapt to the local geometry of latent space, the HMC
is conducted on Riemannian manifold defined by a positive definite metric. In
addition, we show that the proposed HSMC method can improve SSMs realized by
both Gaussian Processes (GP) and Neural Network (NN)."
"Effective representation learning of electronic health records is a
challenging task and is becoming more important as the availability of such
data is becoming pervasive. The data contained in these records are irregular
and contain multiple modalities such as notes, and medical codes. They are
preempted by medical conditions the patient may have, and are typically jotted
down by medical staff. Accompanying codes are notes containing valuable
information about patients beyond the structured information contained in
electronic health records. We use transformer networks and the recently
proposed BERT language model to embed these data streams into a unified vector
representation. The presented approach effectively encodes a patient's visit
data into a single distributed representation, which can be used for downstream
tasks. Our model demonstrates superior performance and generalization on
mortality, readmission and length of stay tasks using the publicly available
MIMIC-III ICU dataset. Code avaialble at
https://github.com/sajaddarabi/TAPER-EHR"
"Crowdsourcing is a strategy to categorize data through the contribution of
many individuals. A wide range of theoretical and algorithmic contributions are
based on the model of Dawid and Skene [1]. Recently it was shown in [2,3] that,
in certain regimes, belief propagation is asymptotically optimal for data
generated from the Dawid-Skene model. This paper is motivated by this recent
progress. We analyze the dense limit of the Dawid-Skene model. It is shown that
it belongs to a larger class of low-rank matrix estimation problems for which
it is possible to express the asymptotic, Bayes-optimal, performance in a
simple closed form. In the dense limit the mapping to a low-rank matrix
estimation problem provides an approximate message passing algorithm that
solves the problem algorithmically. We identify the regions where the algorithm
efficiently computes the Bayes-optimal estimates. Our analysis refines the
results of [2,3] about optimality of message passing algorithms by
characterizing regions of parameters where these algorithms do not match the
Bayes-optimal performance. We further study numerically the performance of
approximate message passing, derived in the dense limit, on sparse instances
and carry out experiments on a real world dataset."
"We introduce a new model of stochastic bandits with adversarial corruptions
which aims to capture settings where most of the input follows a stochastic
pattern but some fraction of it can be adversarially changed to trick the
algorithm, e.g., click fraud, fake reviews and email spam. The goal of this
model is to encourage the design of bandit algorithms that (i) work well in
mixed adversarial and stochastic models, and (ii) whose performance
deteriorates gracefully as we move from fully stochastic to fully adversarial
models.
  In our model, the rewards for all arms are initially drawn from a
distribution and are then altered by an adaptive adversary. We provide a simple
algorithm whose performance gracefully degrades with the total corruption the
adversary injected in the data, measured by the sum across rounds of the
biggest alteration the adversary made in the data in that round; this total
corruption is denoted by $C$. Our algorithm provides a guarantee that retains
the optimal guarantee (up to a logarithmic term) if the input is stochastic and
whose performance degrades linearly to the amount of corruption $C$, while
crucially being agnostic to it. We also provide a lower bound showing that this
linear degradation is necessary if the algorithm achieves optimal performance
in the stochastic setting (the lower bound works even for a known amount of
corruption, a special case in which our algorithm achieves optimal performance
without the extra logarithm)."
"Electrocardiogram (ECG) can be reliably used as a measure to monitor the
functionality of the cardiovascular system. Recently, there has been a great
attention towards accurate categorization of heartbeats. While there are many
commonalities between different ECG conditions, the focus of most studies has
been classifying a set of conditions on a dataset annotated for that task
rather than learning and employing a transferable knowledge between different
tasks. In this paper, we propose a method based on deep convolutional neural
networks for the classification of heartbeats which is able to accurately
classify five different arrhythmias in accordance with the AAMI EC57 standard.
Furthermore, we suggest a method for transferring the knowledge acquired on
this task to the myocardial infarction (MI) classification task. We evaluated
the proposed method on PhysionNet's MIT-BIH and PTB Diagnostics datasets.
According to the results, the suggested method is able to make predictions with
the average accuracies of 93.4% and 95.9% on arrhythmia classification and MI
classification, respectively."
"Recently, deep learning has been applied to many security-sensitive
applications, such as facial authentication. The existence of adversarial
examples hinders such applications. The state-of-the-art result on defense
shows that adversarial training can be applied to train a robust model on MNIST
against adversarial examples; but it fails to achieve a high empirical
worst-case accuracy on a more complex task, such as CIFAR-10 and SVHN. In our
work, we propose curriculum adversarial training (CAT) to resolve this issue.
The basic idea is to develop a curriculum of adversarial examples generated by
attacks with a wide range of strengths. With two techniques to mitigate the
forgetting and the generalization issues, we demonstrate that CAT can improve
the prior art's empirical worst-case accuracy by a large margin of 25% on
CIFAR-10 and 35% on SVHN. At the same, the model's performance on
non-adversarial inputs is comparable to the state-of-the-art models."
"Machine learning methods have gained a great deal of popularity in recent
years among public administration scholars and practitioners. These techniques
open the door to the analysis of text, image and other types of data that allow
us to test foundational theories of public administration and to develop new
theories. Despite the excitement surrounding machine learning methods, clarity
regarding their proper use and potential pitfalls is lacking. This paper
attempts to fill this gap in the literature through providing a machine
learning ""guide to practice"" for public administration scholars and
practitioners. Here, we take a foundational view of machine learning and
describe how these methods can enrich public administration research and
practice through their ability develop new measures, tap into new sources of
data and conduct statistical inference and causal inference in a principled
manner. We then turn our attention to the pitfalls of using these methods such
as unvalidated measures and lack of interpretability. Finally, we demonstrate
how machine learning techniques can help us learn about organizational
reputation in federal agencies through an illustrated example using tweets from
13 executive federal agencies."
"Structural Causal Models (SCMs) provide a popular causal modeling framework.
In this work, we show that SCMs are not flexible enough to give a complete
causal representation of dynamical systems at equilibrium. Instead, we propose
a generalization of the notion of an SCM, that we call Causal Constraints Model
(CCM), and prove that CCMs do capture the causal semantics of such systems. We
show how CCMs can be constructed from differential equations and initial
conditions and we illustrate our ideas further on a simple but ubiquitous
(bio)chemical reaction. Our framework also allows to model functional laws,
such as the ideal gas law, in a sensible and intuitive way."
"We introduce a distributionally robust maximum likelihood estimation model
with a Wasserstein ambiguity set to infer the inverse covariance matrix of a
$p$-dimensional Gaussian random vector from $n$ independent samples. The
proposed model minimizes the worst case (maximum) of Stein's loss across all
normal reference distributions within a prescribed Wasserstein distance from
the normal distribution characterized by the sample mean and the sample
covariance matrix. We prove that this estimation problem is equivalent to a
semidefinite program that is tractable in theory but beyond the reach of
general purpose solvers for practically relevant problem dimensions $p$. In the
absence of any prior structural information, the estimation problem has an
analytical solution that is naturally interpreted as a nonlinear shrinkage
estimator. Besides being invertible and well-conditioned even for $p>n$, the
new shrinkage estimator is rotation-equivariant and preserves the order of the
eigenvalues of the sample covariance matrix. These desirable properties are not
imposed ad hoc but emerge naturally from the underlying distributionally robust
optimization model. Finally, we develop a sequential quadratic approximation
algorithm for efficiently solving the general estimation problem subject to
conditional independence constraints typically encountered in Gaussian
graphical models."
"A universal rule-based self-learning approach using deep reinforcement
learning (DRL) is proposed for the first time to solve nonlinear ordinary
differential equations and partial differential equations. The solver consists
of a deep neural network-structured actor that outputs candidate solutions, and
a critic derived only from physical rules (governing equations and boundary and
initial conditions). Solutions in discretized time are treated as multiple
tasks sharing the same governing equation, and the current step parameters
provide an ideal initialization for the next owing to the temporal continuity
of the solutions, which shows a transfer learning characteristic and indicates
that the DRL solver has captured the intrinsic nature of the equation. The
approach is verified through solving the Schr\""odinger, Navier-Stokes,
Burgers', Van der Pol, and Lorenz equations and an equation of motion. The
results indicate that the approach gives solutions with high accuracy, and the
solution process promises to get faster."
"Feed-forward networks are widely used in cross-modal applications to bridge
modalities by mapping distributed vectors of one modality to the other, or to a
shared space. The predicted vectors are then used to perform e.g., retrieval or
labeling. Thus, the success of the whole system relies on the ability of the
mapping to make the neighborhood structure (i.e., the pairwise similarities) of
the predicted vectors akin to that of the target vectors. However, whether this
is achieved has not been investigated yet. Here, we propose a new similarity
measure and two ad hoc experiments to shed light on this issue. In three
cross-modal benchmarks we learn a large number of language-to-vision and
vision-to-language neural network mappings (up to five layers) using a rich
diversity of image and text features and loss functions. Our results reveal
that, surprisingly, the neighborhood structure of the predicted vectors
consistently resembles more that of the input vectors than that of the target
vectors. In a second experiment, we further show that untrained nets do not
significantly disrupt the neighborhood (i.e., semantic) structure of the input
vectors."
"The principle of equivariance to symmetry transformations enables a
theoretically grounded approach to neural network architecture design.
Equivariant networks have shown excellent performance and data efficiency on
vision and medical imaging problems that exhibit symmetries. Here we show how
this principle can be extended beyond global symmetries to local gauge
transformations. This enables the development of a very general class of
convolutional neural networks on manifolds that depend only on the intrinsic
geometry, and which includes many popular methods from equivariant and
geometric deep learning. We implement gauge equivariant CNNs for signals
defined on the surface of the icosahedron, which provides a reasonable
approximation of the sphere. By choosing to work with this very regular
manifold, we are able to implement the gauge equivariant convolution using a
single conv2d call, making it a highly scalable and practical alternative to
Spherical CNNs. Using this method, we demonstrate substantial improvements over
previous methods on the task of segmenting omnidirectional images and global
climate patterns."
"Deep reinforcement learning approaches have shown impressive results in a
variety of different domains, however, more complex heterogeneous architectures
such as world models require the different neural components to be trained
separately instead of end-to-end. While a simple genetic algorithm recently
showed end-to-end training is possible, it failed to solve a more complex 3D
task. This paper presents a method called Deep Innovation Protection (DIP) that
addresses the credit assignment problem in training complex heterogenous neural
network models end-to-end for such environments. The main idea behind the
approach is to employ multiobjective optimization to temporally reduce the
selection pressure on specific components in multi-component network, allowing
other components to adapt. We investigate the emergent representations of these
evolved networks, which learn to predict properties important for the survival
of the agent, without the need for a specific forward-prediction loss."
"Deep unfolding is a promising deep-learning technique in which an iterative
algorithm is unrolled to a deep network architecture with trainable parameters.
In the case of gradient descent algorithms, as a result of the training
process, one often observes the acceleration of the convergence speed with
learned non-constant step size parameters whose behavior is not intuitive nor
interpretable from conventional theory. In this paper, we provide a theoretical
interpretation of the learned step size of deep-unfolded gradient descent
(DUGD). We first prove that the training process of DUGD reduces not only the
mean squared error loss but also the spectral radius related to the convergence
rate. Next, we show that minimizing the upper bound of the spectral radius
naturally leads to the Chebyshev step which is a sequence of the step size
based on Chebyshev polynomials. The numerical experiments confirm that the
Chebyshev steps qualitatively reproduce the learned step size parameters in
DUGD, which provides a plausible interpretation of the learned parameters.
Additionally, we show that the Chebyshev steps achieve the lower bound of the
convergence rate for the first-order method in a specific limit without
learning parameters or momentum terms."
"Differences in data size per class, also known as imbalanced data
distribution, have become a common problem affecting data quality. Big Data
scenarios pose a new challenge to traditional imbalanced classification
algorithms, since they are not prepared to work with such amount of data. Split
data strategies and lack of data in the minority class due to the use of
MapReduce paradigm have posed new challenges for tackling the imbalance between
classes in Big Data scenarios. Ensembles have shown to be able to successfully
address imbalanced data problems. Smart Data refers to data of enough quality
to achieve high performance models. The combination of ensembles and Smart
Data, achieved through Big Data preprocessing, should be a great synergy. In
this paper, we propose a novel Smart Data driven Decision Trees Ensemble
methodology for addressing the imbalanced classification problem in Big Data
domains, namely SD_DeTE methodology. This methodology is based on the learning
of different decision trees using distributed quality data for the ensemble
process. This quality data is achieved by fusing Random Discretization,
Principal Components Analysis and clustering-based Random Oversampling for
obtaining different Smart Data versions of the original data. Experiments
carried out in 21 binary adapted datasets have shown that our methodology
outperforms Random Forest."
"Training machine learning models on mobile devices has the potential of
improving both privacy and accuracy of the models. However, one of the major
obstacles to achieving this goal is the memory limitation of mobile devices.
Reducing training memory enables models with high-dimensional weight matrices,
like automatic speech recognition (ASR) models, to be trained on-device. In
this paper, we propose approximating the gradient matrices of deep neural
networks using a low-rank parameterization as an avenue to save training
memory. The low-rank gradient approximation enables more advanced,
memory-intensive optimization techniques to be run on device. Our experimental
results show that we can reduce the training memory by about 33.0% for Adam
optimization. It uses comparable memory to momentum optimization and achieves a
4.5% relative lower word error rate on an ASR personalization task."
"Unpaired image-to-image translation has attracted significant interest due to
the invention of CycleGAN, a method which utilizes a combination of adversarial
and cycle consistency losses to avoid the need for paired data. It is known
that the CycleGAN problem might admit multiple solutions, and our goal in this
paper is to analyze the space of exact solutions and to give perturbation
bounds for approximate solutions. We show theoretically that the exact solution
space is invariant with respect to automorphisms of the underlying probability
spaces, and, furthermore, that the group of automorphisms acts freely and
transitively on the space of exact solutions. We examine the case of zero
`pure' CycleGAN loss first in its generality, and, subsequently, expand our
analysis to approximate solutions for `extended' CycleGAN loss where identity
loss term is included. In order to demonstrate that these results are
applicable, we show that under mild conditions nontrivial smooth automorphisms
exist. Furthermore, we provide empirical evidence that neural networks can
learn these automorphisms with unexpected and unwanted results. We conclude
that finding optimal solutions to the CycleGAN loss does not necessarily lead
to the envisioned result in image-to-image translation tasks and that
underlying hidden symmetries can render the result utterly useless."
"Forest-based methods have recently gained in popularity for non-parametric
treatment effect estimation. Building on this line of work, we introduce causal
survival forests, which can be used to estimate heterogeneous treatment effects
in a survival and observational setting where outcomes may be right-censored.
Our approach relies on orthogonal estimating equations to robustly adjust for
both censoring and selection effects under unconfoundedness. In our
experiments, we find our approach to perform well relative to a number of
baselines."
"In this paper, we study the problem of recommendation system where the users
and items to be recommended are rich data structures with multiple entity types
and with multiple sources of side-information in the form of graphs. We provide
a general formulation for the problem that captures the complexities of modern
real-world recommendations and generalizes many existing formulations. In our
formulation, each user/document that requires a recommendation and each item or
tag that is to be recommended, both are modeled by a set of static entities and
a dynamic component. The relationships between entities are captured by several
weighted bipartite graphs. To effectively exploit these complex interactions
and learn the recommendation model, we propose MEDRES- a multiple graph-CNN
based novel deep-learning architecture. MEDRES uses AL-GCN, a novel graph
convolution network block, that harnesses strong representative features from
the underlying graphs. Moreover, in order to capture highly heterogeneous
engagement of different users with the system and constraints on the number of
items to be recommended, we propose a novel ranking metric pAp@k along with a
method to optimize the metric directly. We demonstrate effectiveness of our
method on two benchmarks: a) citation data, b) Flickr data. In addition, we
present two real-world case studies of our formulation and the MEDRES
architecture. We show how our technique can be used to naturally model the
message recommendation problem and the teams recommendation problem in the
Microsoft Teams (MSTeams) product and demonstrate that it is 5-6% points more
accurate than the production-grade models."
"This paper addresses classification tasks on a particular target domain in
which labeled training data are only available from source domains different
from (but related to) the target. Two closely related frameworks, domain
adaptation and domain generalization, are concerned with such tasks, where the
only difference between those frameworks is the availability of the unlabeled
target data: domain adaptation can leverage unlabeled target information, while
domain generalization cannot. We propose Scatter Component Analyis (SCA), a
fast representation learning algorithm that can be applied to both domain
adaptation and domain generalization. SCA is based on a simple geometrical
measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA
finds a representation that trades between maximizing the separability of
classes, minimizing the mismatch between domains, and maximizing the
separability of data; each of which is quantified through scatter. The
optimization problem of SCA can be reduced to a generalized eigenvalue problem,
which results in a fast and exact solution. Comprehensive experiments on
benchmark cross-domain object recognition datasets verify that SCA performs
much faster than several state-of-the-art algorithms and also provides
state-of-the-art classification accuracy in both domain adaptation and domain
generalization. We also show that scatter can be used to establish a
theoretical generalization bound in the case of domain adaptation."
"We propose a max-pooling based loss function for training Long Short-Term
Memory (LSTM) networks for small-footprint keyword spotting (KWS), with low
CPU, memory, and latency requirements. The max-pooling loss training can be
further guided by initializing with a cross-entropy loss trained network. A
posterior smoothing based evaluation approach is employed to measure keyword
spotting performance. Our experimental results show that LSTM models trained
using cross-entropy loss or max-pooling loss outperform a cross-entropy loss
trained baseline feed-forward Deep Neural Network (DNN). In addition,
max-pooling loss trained LSTM with randomly initialized network performs better
compared to cross-entropy loss trained LSTM. Finally, the max-pooling loss
trained LSTM initialized with a cross-entropy pre-trained network shows the
best performance, which yields $67.6\%$ relative reduction compared to baseline
feed-forward DNN in Area Under the Curve (AUC) measure."
"Multiple cause-of-death data provides a valuable source of information that
can be used to enhance health standards by predicting health related
trajectories in societies with large populations. These data are often
available in large quantities across U.S. states and require Big Data
techniques to uncover complex hidden patterns. We design two different classes
of models suitable for large-scale analysis of mortality data, a Hadoop-based
ensemble of random forests trained over N-grams, and the DeepDeath, a deep
classifier based on the recurrent neural network (RNN). We apply both classes
to the mortality data provided by the National Center for Health Statistics and
show that while both perform significantly better than the random classifier,
the deep model that utilizes long short-term memory networks (LSTMs), surpasses
the N-gram based models and is capable of learning the temporal aspect of the
data without a need for building ad-hoc, expert-driven features."
"Few-shot learning aims to train efficient predictive models with a few
examples. The lack of training data leads to poor models that perform
high-variance or low-confidence predictions. In this paper, we propose to
meta-learn the ensemble of epoch-wise empirical Bayes models (E3BM) to achieve
robust predictions. ""Epoch-wise"" means that each training epoch has a Bayes
model whose parameters are specifically learned and deployed. ""Empirical"" means
that the hyperparameters, e.g., used for learning and ensembling the epoch-wise
models, are generated by hyperprior learners conditional on task-specific data.
We introduce four kinds of hyperprior learners by considering inductive vs.
transductive, and epoch-dependent vs. epoch-independent, in the paradigm of
meta-learning. We conduct extensive experiments for five-class few-shot tasks
on three challenging benchmarks: miniImageNet, tieredImageNet, and FC100, and
achieve top performance using the epoch-dependent transductive hyperprior
learner, which captures the richest information. Our ablation study shows that
both ""epoch-wise ensemble"" and ""empirical"" encourage high efficiency and
robustness in the model performance."
"Personalized cancer vaccines are envisioned as the next generation rational
cancer immunotherapy. The key step in developing personalized therapeutic
cancer vaccines is to identify tumor-specific neoantigens that are on the
surface of tumor cells. A promising method for this is through de novo peptide
sequencing from mass spectrometry data. In this paper we introduce DeepNovoV2,
the state-of-the-art model for peptide sequencing. In DeepNovoV2, a spectrum is
directly represented as a set of (m/z, intensity) pairs, therefore it does not
suffer from the accuracy-speed/memory trade-off problem. The model combines an
order invariant network structure (T-Net) and recurrent neural networks and
provides a complete end-to-end training and prediction framework to sequence
patterns of peptides. Our experiments on a wide variety of data from different
species show that DeepNovoV2 outperforms previous state-of-the-art methods,
achieving 13.01-23.95\% higher accuracy at the peptide level."
"Intelligent motion planning is one of the core components in automated
vehicles, which has received extensive interests. Traditional motion planning
methods suffer from several drawbacks in terms of optimality, efficiency and
generalization capability. Sampling based methods cannot guarantee the
optimality of the generated trajectories. Whereas the optimization-based
methods are not able to perform motion planning in real-time, and limited by
the simplified formalization. In this work, we propose a learning-based
approach to handle those shortcomings. Mixed Integer Quadratic Problem based
optimization (MIQP) is used to generate the optimal lane-change trajectories
which served as the training dataset for learning-based action generation
algorithms. A hierarchical supervised learning model is devised to make the
fast lane-change decision. Numerous experiments have been conducted to evaluate
the optimality, efficiency, and generalization capability of the proposed
approach. The experimental results indicate that the proposed model outperforms
several commonly used motion planning baselines."
"We consider the estimation of two-sample integral functionals, of the type
that occur naturally, for example, when the object of interest is a divergence
between unknown probability densities. Our first main result is that, in wide
generality, a weighted nearest neighbour estimator is efficient, in the sense
of achieving the local asymptotic minimax lower bound. Moreover, we also prove
a corresponding central limit theorem, which facilitates the construction of
asymptotically valid confidence intervals for the functional, having
asymptotically minimal width. One interesting consequence of our results is the
discovery that, for certain functionals, the worst-case performance of our
estimator may improve on that of the natural `oracle' estimator, which is given
access to the values of the unknown densities at the observations."
"In this work, we move beyond the traditional complex-valued representations,
introducing more expressive hypercomplex representations to model entities and
relations for knowledge graph embeddings. More specifically, quaternion
embeddings, hypercomplex-valued embeddings with three imaginary components, are
utilized to represent entities. Relations are modelled as rotations in the
quaternion space. The advantages of the proposed approach are: (1) Latent
inter-dependencies (between all components) are aptly captured with Hamilton
product, encouraging a more compact interaction between entities and relations;
(2) Quaternions enable expressive rotation in four-dimensional space and have
more degree of freedom than rotation in complex plane; (3) The proposed
framework is a generalization of ComplEx on hypercomplex space while offering
better geometrical interpretations, concurrently satisfying the key desiderata
of relational representation learning (i.e., modeling symmetry, anti-symmetry
and inversion). Experimental results demonstrate that our method achieves
state-of-the-art performance on four well-established knowledge graph
completion benchmarks."
"Arguably, unsupervised learning plays a crucial role in the majority of
algorithms for processing brain imaging. A recently introduced unsupervised
approach Deep InfoMax (DIM) is a promising tool for exploring brain structure
in a flexible non-linear way. In this paper, we investigate the use of variants
of DIM in a setting of progression to Alzheimer's disease in comparison with
supervised AlexNet and ResNet inspired convolutional neural networks. As a
benchmark, we use a classification task between four groups: patients with
stable, and progressive mild cognitive impairment (MCI), with Alzheimer's
disease, and healthy controls. Our dataset is comprised of 828 subjects from
the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Our
experiments highlight encouraging evidence of the high potential utility of DIM
in future neuroimaging studies."
"The use of target networks has been a popular and key component of recent
deep Q-learning algorithms for reinforcement learning, yet little is known from
the theory side. In this work, we introduce a new family of target-based
temporal difference (TD) learning algorithms and provide theoretical analysis
on their convergences. In contrast to the standard TD-learning, target-based TD
algorithms maintain two separate learning parameters-the target variable and
online variable. Particularly, we introduce three members in the family, called
the averaging TD, double TD, and periodic TD, where the target variable is
updated through an averaging, symmetric, or periodic fashion, mirroring those
techniques used in deep Q-learning practice.
  We establish asymptotic convergence analyses for both averaging TD and double
TD and a finite sample analysis for periodic TD. In addition, we also provide
some simulation results showing potentially superior convergence of these
target-based TD algorithms compared to the standard TD-learning. While this
work focuses on linear function approximation and policy evaluation setting, we
consider this as a meaningful step towards the theoretical understanding of
deep Q-learning variants with target networks."
"In order to help undergraduate students towards successfully completing their
degrees, developing tools that can assist students during the course selection
process is a significant task in the education domain. The optimal set of
courses for each student should include courses that help him/her graduate in a
timely fashion and for which he/she is well-prepared for so as to get a good
grade in. To this end, we propose two different grade-aware course
recommendation approaches to recommend to each student his/her optimal set of
courses. The first approach ranks the courses by using an objective function
that differentiates between courses that are expected to increase or decrease a
student's GPA. The second approach combines the grades predicted by grade
prediction methods with the rankings produced by course recommendation methods
to improve the final course rankings. To obtain the course rankings in the
first approach, we adapt two widely-used representation learning techniques to
learn the optimal temporal ordering between courses. Our experiments on a large
dataset obtained from the University of Minnesota that includes students from
23 different majors show that the grade-aware course recommendation methods can
do better on recommending more courses in which the students are expected to
perform well and recommending fewer courses in which they are expected not to
perform well in than grade-unaware course recommendation methods."
"Gaussian processes are ubiquitous in nature and engineering. A case in point
is a class of neural networks in the infinite-width limit, whose priors
correspond to Gaussian processes. Here we perturbatively extend this
correspondence to finite-width neural networks, yielding non-Gaussian processes
as priors. The methodology developed herein allows us to track the flow of
preactivation distributions by progressively integrating out random variables
from lower to higher layers, reminiscent of renormalization-group flow. We
further develop a perturbative procedure to perform Bayesian inference with
weakly non-Gaussian priors."
"We explore the hyperparameter space of reservoir computers used for
forecasting of the chaotic Lorenz '63 attractor with Bayesian optimization. We
use a new measure of reservoir performance, designed to emphasize learning the
global climate of the forecasted system rather than short-term prediction. We
find that optimizing over this measure more quickly excludes reservoirs that
fail to reproduce the climate. The results of optimization are surprising: the
optimized parameters often specify a reservoir network with very low
connectivity. Inspired by this observation, we explore reservoir designs with
even simpler structure, and find well-performing reservoirs that have zero
spectral radius and no recurrence. These simple reservoirs provide
counterexamples to widely used heuristics in the field, and may be useful for
hardware implementations of reservoir computers."
"It is notoriously difficult to control the behavior of reinforcement learning
agents. Agents often learn to exploit the environment or reward signal and need
to be retrained multiple times. The multi-objective reinforcement learning
(MORL) framework separates a reward function into several objectives. An ideal
MORL agent learns to generalize to novel combinations of objectives allowing
for better control of an agent's behavior without requiring retraining. Many
MORL approaches use a weight vector to parameterize the importance of each
objective. However, this approach suffers from lack of expressiveness and
interpretability. We propose using propositional logic to specify the
importance of multiple objectives. By using a logic where predicates correspond
directly to objectives, specifications are inherently more interpretable.
Additionally the set of specifications that can be expressed with formal
languages is a superset of what can be expressed by weight vectors. In this
paper, we define a formal language based on propositional logic with
quantitative semantics. We encode logical specifications using a recurrent
neural network and show that MORL agents parameterized by these encodings are
able to generalize to novel specifications over objectives and achieve
performance comparable to single objective baselines."
"Magnetoencephalography and electroencephalography (M/EEG) are non-invasive
modalities that measure the weak electromagnetic fields generated by neural
activity. Estimating the location and magnitude of the current sources that
generated these electromagnetic fields is a challenging ill-posed regression
problem known as \emph{source imaging}. When considering a group study, a
common approach consists in carrying out the regression tasks independently for
each subject. An alternative is to jointly localize sources for all subjects
taken together, while enforcing some similarity between them. By pooling all
measurements in a single multi-task regression, one makes the problem better
posed, offering the ability to identify more sources and with greater
precision. The Minimum Wasserstein Estimates (MWE) promotes focal activations
that do not perfectly overlap for all subjects, thanks to a regularizer based
on Optimal Transport (OT) metrics. MWE promotes spatial proximity on the
cortical mantel while coping with the varying noise levels across subjects. On
realistic simulations, MWE decreases the localization error by up to 4 mm per
source compared to individual solutions. Experiments on the Cam-CAN dataset
show a considerable improvement in spatial specificity in population imaging.
Our analysis of a multimodal dataset shows how multi-subject source
localization closes the gap between MEG and fMRI for brain mapping."
"Microstructures of a material form the bridge linking processing conditions -
which can be controlled, to the material property - which is the primary
interest in engineering applications. Thus a critical task in material design
is establishing the processing-structure relationship, which requires domain
expertise and techniques that can model the high-dimensional material
microstructure. This work proposes a deep learning based approach that models
the processing-structure relationship as a conditional image synthesis problem.
In particular, we develop an auxiliary classifier Wasserstein GAN with gradient
penalty (ACWGAN-GP) to synthesize microstructures under a given processing
condition. This approach is free of feature engineering, requires modest domain
knowledge and is applicable to a wide range of material systems. We demonstrate
this approach using the ultra high carbon steel (UHCS) database, where each
microstructure is annotated with a label describing the cooling method it was
subjected to. Our results show that ACWGAN-GP can synthesize high-quality
multiphase microstructures for a given cooling method."
"What makes a task relatively more or less difficult for a machine compared to
a human? Much AI/ML research has focused on expanding the range of tasks that
machines can do, with a focus on whether machines can beat humans. Allowing for
differences in scale, we can seek interesting (anomalous) pairs of tasks T, T'.
We define interesting in this way: The ""harder to learn"" relation is reversed
when comparing human intelligence (HI) to AI. While humans seems to be able to
understand problems by formulating rules, ML using neural networks does not
rely on constructing rules. We discuss a novel approach where the challenge is
to ""perform well under rules that have been created by human beings."" We
suggest that this provides a rigorous and precise pathway for understanding the
difference between the two kinds of learning. Specifically, we suggest a large
and extensible class of learning tasks, formulated as learning under rules.
With these tasks, both the AI and HI will be studied with rigor and precision.
The immediate goal is to find interesting groundtruth rule pairs. In the long
term, the goal will be to understand, in a generalizable way, what
distinguishes interesting pairs from ordinary pairs, and to define saliency
behind interesting pairs. This may open new ways of thinking about AI, and
provide unexpected insights into human learning."
"Manually labelling large collections of text data is a time-consuming,
expensive, and laborious task, but one that is necessary to support machine
learning based on text datasets. Active learning has been shown to be an
effective way to alleviate some of the effort required in utilising large
collections of unlabelled data for machine learning tasks without needing to
fully label them. The representation mechanism used to represent text documents
when performing active learning, however, has a significant influence on how
effective the process will be. While simple vector representations such as bag
of words have been shown to be an effective way to represent documents during
active learning, the emergence of representation mechanisms based on the word
embeddings prevalent in neural network research (e.g. word2vec and
transformer-based models like BERT) offer a promising, and as yet not fully
explored, alternative. This paper describes a large-scale evaluation of the
effectiveness of different text representation mechanisms for active learning
across 8 datasets from varied domains. This evaluation shows that using
representations based on modern word embeddings---especially BERT---, which
have not yet been widely used in active learning, achieves a significant
improvement over more commonly used vector-based methods like bag of words."
"Proximal operators are of particular interest in optimization problems
dealing with non-smooth objectives because in many practical cases they lead to
optimization algorithms whose updates can be computed in closed form or very
efficiently. A well-known example is the proximal operator of the vector
$\ell_1$ norm, which is given by the soft-thresholding operator. In this paper
we study the proximal operator of the mixed $\ell_{1,\infty}$ matrix norm and
show that it can be computed in closed form by applying the well-known
soft-thresholding operator to each column of the matrix. However, unlike the
vector $\ell_1$ norm case where the threshold is constant, in the mixed
$\ell_{1,\infty}$ norm case each column of the matrix might require a different
threshold and all thresholds depend on the given matrix. We propose a general
iterative algorithm for computing these thresholds, as well as two efficient
implementations that further exploit easy to compute lower bounds for the mixed
norm of the optimal solution. Experiments on large-scale synthetic and real
data indicate that the proposed methods can be orders of magnitude faster than
state-of-the-art methods."
"Non-parallel voice conversion (VC) is a technique for learning mappings
between source and target speeches without using a parallel corpus. Recently,
cycle-consistent adversarial network (CycleGAN)-VC and CycleGAN-VC2 have shown
promising results regarding this problem and have been widely used as benchmark
methods. However, owing to the ambiguity of the effectiveness of
CycleGAN-VC/VC2 for mel-spectrogram conversion, they are typically used for
mel-cepstrum conversion even when comparative methods employ mel-spectrogram as
a conversion target. To address this, we examined the applicability of
CycleGAN-VC/VC2 to mel-spectrogram conversion. Through initial experiments, we
discovered that their direct applications compromised the time-frequency
structure that should be preserved during conversion. To remedy this, we
propose CycleGAN-VC3, an improvement of CycleGAN-VC2 that incorporates
time-frequency adaptive normalization (TFAN). Using TFAN, we can adjust the
scale and bias of the converted features while reflecting the time-frequency
structure of the source mel-spectrogram. We evaluated CycleGAN-VC3 on
inter-gender and intra-gender non-parallel VC. A subjective evaluation of
naturalness and similarity showed that for every VC pair, CycleGAN-VC3
outperforms or is competitive with the two types of CycleGAN-VC2, one of which
was applied to mel-cepstrum and the other to mel-spectrogram. Audio samples are
available at
http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/cyclegan-vc3/index.html."
"We propose a variational Bayesian (VB) procedure for high-dimensional linear
model inferences with heavy tail shrinkage priors, such as student-t prior.
Theoretically, we establish the consistency of the proposed VB method and prove
that under the proper choice of prior specifications, the contraction rate of
the VB posterior is nearly optimal. It justifies the validity of VB inference
as an alternative of Markov Chain Monte Carlo (MCMC) sampling. Meanwhile,
comparing to conventional MCMC methods, the VB procedure achieves much higher
computational efficiency, which greatly alleviates the computing burden for
modern machine learning applications such as massive data analysis. Through
numerical studies, we demonstrate that the proposed VB method leads to shorter
computing time, higher estimation accuracy, and lower variable selection error
than competitive sparse Bayesian methods."
"(Gradient) Expectation Maximization (EM) is a widely used algorithm for
estimating the maximum likelihood of mixture models or incomplete data
problems. A major challenge facing this popular technique is how to effectively
preserve the privacy of sensitive data. Previous research on this problem has
already lead to the discovery of some Differentially Private (DP) algorithms
for (Gradient) EM. However, unlike in the non-private case, existing techniques
are not yet able to provide finite sample statistical guarantees. To address
this issue, we propose in this paper the first DP version of (Gradient) EM
algorithm with statistical guarantees. Moreover, we apply our general framework
to three canonical models: Gaussian Mixture Model (GMM), Mixture of Regressions
Model (MRM) and Linear Regression with Missing Covariates (RMC). Specifically,
for GMM in the DP model, our estimation error is near optimal in some cases.
For the other two models, we provide the first finite sample statistical
guarantees. Our theory is supported by thorough numerical experiments."
"We address the problem of credit assignment in reinforcement learning and
explore fundamental questions regarding the way in which an agent can best use
additional computation to propagate new information, by planning with internal
models of the world to improve its predictions. Particularly, we work to
understand the gains and peculiarities of planning employed as forethought via
forward models or as hindsight operating with backward models. We establish the
relative merits, limitations and complementary properties of both planning
mechanisms in carefully constructed scenarios. Further, we investigate the best
use of models in planning, primarily focusing on the selection of states in
which predictions should be (re)-evaluated. Lastly, we discuss the issue of
model estimation and highlight a spectrum of methods that stretch from explicit
environment-dynamics predictors to more abstract planner-aware models."
"Safe exploration presents a major challenge in reinforcement learning (RL):
when active data collection requires deploying partially trained policies, we
must ensure that these policies avoid catastrophically unsafe regions, while
still enabling trial and error learning. In this paper, we target the problem
of safe exploration in RL by learning a conservative safety estimate of
environment states through a critic, and provably upper bound the likelihood of
catastrophic failures at every training iteration. We theoretically
characterize the tradeoff between safety and policy improvement, show that the
safety constraints are likely to be satisfied with high probability during
training, derive provable convergence guarantees for our approach, which is no
worse asymptotically than standard RL, and demonstrate the efficacy of the
proposed approach on a suite of challenging navigation, manipulation, and
locomotion tasks. Empirically, we show that the proposed approach can achieve
competitive task performance while incurring significantly lower catastrophic
failure rates during training than prior methods. Videos are at this url
https://sites.google.com/view/conservative-safety-critics/home"
"Multiclass classifiers are often designed and evaluated only on a sample from
the classes on which they will eventually be applied. Hence, their final
accuracy remains unknown. In this work we study how a classifier's performance
over the initial class sample can be used to extrapolate its expected accuracy
on a larger, unobserved set of classes. For this, we define a measure of
separation between correct and incorrect classes that is independent of the
number of classes: the ""reversed ROC"" (rROC), which is obtained by replacing
the roles of classes and data-points in the common ROC. We show that the
classification accuracy is a function of the rROC in multiclass classifiers,
for which the learned representation of data from the initial class sample
remains unchanged when new classes are added. Using these results we formulate
a robust neural-network-based algorithm, ""CleaneX"", which learns to estimate
the accuracy of such classifiers on arbitrarily large sets of classes. Unlike
previous methods, our method uses both the observed accuracies of the
classifier and densities of classification scores, and therefore achieves
remarkably better predictions than current state-of-the-art methods on both
simulations and real datasets of object detection, face recognition, and brain
decoding."
"Commonly used evaluation measures including Recall, Precision, F-Measure and
Rand Accuracy are biased and should not be used without clear understanding of
the biases, and corresponding identification of chance or base case levels of
the statistic. Using these measures a system that performs worse in the
objective sense of Informedness, can appear to perform better under any of
these commonly used measures. We discuss several concepts and measures that
reflect the probability that prediction is informed versus chance. Informedness
and introduce Markedness as a dual measure for the probability that prediction
is marked versus chance. Finally we demonstrate elegant connections between the
concepts of Informedness, Markedness, Correlation and Significance as well as
their intuitive relationships with Recall and Precision, and outline the
extension from the dichotomous case to the general multi-class case."
"This paper formalises the problem of online algorithm selection in the
context of Reinforcement Learning. The setup is as follows: given an episodic
task and a finite number of off-policy RL algorithms, a meta-algorithm has to
decide which RL algorithm is in control during the next episode so as to
maximize the expected return. The article presents a novel meta-algorithm,
called Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is
to freeze the policy updates at each epoch, and to leave a rebooted stochastic
bandit in charge of the algorithm selection. Under some assumptions, a thorough
theoretical analysis demonstrates its near-optimality considering the
structural sampling budget limitations. ESBAS is first empirically evaluated on
a dialogue task where it is shown to outperform each individual algorithm in
most configurations. ESBAS is then adapted to a true online setting where
algorithms update their policies after each transition, which we call SSBAS.
SSBAS is evaluated on a fruit collection task where it is shown to adapt the
stepsize parameter more efficiently than the classical hyperbolic decay, and on
an Atari game, where it improves the performance by a wide margin."
"Neural network based architectures used for sound recognition are usually
adapted from other application domains such as image recognition, which may not
harness the time-frequency representation of a signal. The ConditionaL Neural
Networks (CLNN) and its extension the Masked ConditionaL Neural Networks
(MCLNN) are designed for multidimensional temporal signal recognition. The CLNN
is trained over a window of frames to preserve the inter-frame relation, and
the MCLNN enforces a systematic sparseness over the network's links that mimics
a filterbank-like behavior. The masking operation induces the network to learn
in frequency bands, which decreases the network susceptibility to
frequency-shifts in time-frequency representations. Additionally, the mask
allows an exploration of a range of feature combinations concurrently analogous
to the manual handcrafting of the optimum collection of features for a
recognition task. MCLNN have achieved competitive performance on the Ballroom
music dataset compared to several hand-crafted attempts and outperformed models
based on state-of-the-art Convolutional Neural Networks."
"Clustering is inherently ill-posed: there often exist multiple valid
clusterings of a single dataset, and without any additional information a
clustering system has no way of knowing which clustering it should produce.
This motivates the use of constraints in clustering, as they allow users to
communicate their interests to the clustering system. Active constraint-based
clustering algorithms select the most useful constraints to query, aiming to
produce a good clustering using as few constraints as possible. We propose
COBRA, an active method that first over-clusters the data by running K-means
with a $K$ that is intended to be too large, and subsequently merges the
resulting small clusters into larger ones based on pairwise constraints. In its
merging step, COBRA is able to keep the number of pairwise queries low by
maximally exploiting constraint transitivity and entailment. We experimentally
show that COBRA outperforms the state of the art in terms of clustering quality
and runtime, without requiring the number of clusters in advance."
"Genealogy research is the study of family history using available resources
such as historical records. Ancestry provides its customers with one of the
world's largest online genealogical index with billions of records from a wide
range of sources, including vital records such as birth and death certificates,
census records, court and probate records among many others. Search at Ancestry
aims to return relevant records from various record types, allowing our
subscribers to build their family trees, research their family history, and
make meaningful discoveries about their ancestors from diverse perspectives. In
a modern search engine designed for genealogical study, the appropriate ranking
of search results to provide highly relevant information represents a daunting
challenge. In particular, the disparity in historical records makes it
inherently difficult to score records in an equitable fashion. Herein, we
provide an overview of our solutions to overcome such record disparity problems
in the Ancestry search engine. Specifically, we introduce customized coordinate
ascent (customized CA) to speed up ranking within a specific record type. We
then propose stochastic search (SS) that linearly combines ranked results
federated across contents from various record types. Furthermore, we propose a
novel information retrieval metric, normalized cumulative entropy (NCE), to
measure the diversity of results. We demonstrate the effectiveness of these two
algorithms in terms of relevance (by NDCG) and diversity (by NCE) if applicable
in the offline experiments using real customer data at Ancestry."
"We study data poisoning attacks in the online setting where training items
arrive sequentially, and the attacker may perturb the current item to
manipulate online learning. Importantly, the attacker has no knowledge of
future training items nor the data generating distribution. We formulate online
data poisoning attack as a stochastic optimal control problem, and solve it
with model predictive control and deep reinforcement learning. We also upper
bound the suboptimality suffered by the attacker for not knowing the data
generating distribution. Experiments validate our control approach in
generating near-optimal attacks on both supervised and unsupervised learning
tasks."
"Learning with sparse rewards remains a significant challenge in reinforcement
learning (RL), especially when the aim is to train a policy capable of
achieving multiple different goals. To date, the most successful approaches for
dealing with multi-goal, sparse reward environments have been model-free RL
algorithms. In this work we propose PlanGAN, a model-based algorithm
specifically designed for solving multi-goal tasks in environments with sparse
rewards. Our method builds on the fact that any trajectory of experience
collected by an agent contains useful information about how to achieve the
goals observed during that trajectory. We use this to train an ensemble of
conditional generative models (GANs) to generate plausible trajectories that
lead the agent from its current state towards a specified goal. We then combine
these imagined trajectories into a novel planning algorithm in order to achieve
the desired goal as efficiently as possible. The performance of PlanGAN has
been tested on a number of robotic navigation/manipulation tasks in comparison
with a range of model-free reinforcement learning baselines, including
Hindsight Experience Replay. Our studies indicate that PlanGAN can achieve
comparable performance whilst being around 4-8 times more sample efficient."
"In recent times, neural networks have become a powerful tool for the analysis
of complex and abstract data models. However, their introduction intrinsically
increases our uncertainty about which features of the analysis are
model-related and which are due to the neural network. This means that
predictions by neural networks have biases which cannot be trivially
distinguished from being due to the true nature of the creation and observation
of data or not. In order to attempt to address such issues we discuss Bayesian
neural networks: neural networks where the uncertainty due to the network can
be characterised. In particular, we present the Bayesian statistical framework
which allows us to categorise uncertainty in terms of the ingrained randomness
of observing certain data and the uncertainty from our lack of knowledge about
how data can be created and observed. In presenting such techniques we show how
errors in prediction by neural networks can be obtained in principle, and
provide the two favoured methods for characterising these errors. We will also
describe how both of these methods have substantial pitfalls when put into
practice, highlighting the need for other statistical techniques to truly be
able to do inference when using neural networks."
"While many approaches exist in the literature to learn low-dimensional
representations for data collections in multiple modalities, the
generalizability of multi-modal nonlinear embeddings to previously unseen data
is a rather overlooked subject. In this work, we first present a theoretical
analysis of learning multi-modal nonlinear embeddings in a supervised setting.
Our performance bounds indicate that for successful generalization in
multi-modal classification and retrieval problems, the regularity of the
interpolation functions extending the embedding to the whole data space is as
important as the between-class separation and cross-modal alignment criteria.
We then propose a multi-modal nonlinear representation learning algorithm that
is motivated by these theoretical findings, where the embeddings of the
training samples are optimized jointly with the Lipschitz regularity of the
interpolators. Experimental comparison to recent multi-modal and single-modal
learning algorithms suggests that the proposed method yields promising
performance in multi-modal image classification and cross-modal image-text
retrieval applications."
"Medical image segmentation is inherently an ambiguous task due to factors
such as partial volumes and variations in anatomical definitions. While in most
cases the segmentation uncertainty is around the border of structures of
interest, there can also be considerable inter-rater differences. The class of
conditional variational autoencoders (cVAE) offers a principled approach to
inferring distributions over plausible segmentations that are conditioned on
input images. Segmentation uncertainty estimated from samples of such
distributions can be more informative than using pixel level probability
scores. In this work, we propose a novel conditional generative model that is
based on conditional Normalizing Flow (cFlow). The basic idea is to increase
the expressivity of the cVAE by introducing a cFlow transformation step after
the encoder. This yields improved approximations of the latent posterior
distribution, allowing the model to capture richer segmentation variations.
With this we show that the quality and diversity of samples obtained from our
conditional generative model is enhanced. Performance of our model, which we
call cFlow Net, is evaluated on two medical imaging datasets demonstrating
substantial improvements in both qualitative and quantitative measures when
compared to a recent cVAE based model."
"This article provides an overview of the Collective Knowledge technology (CK
or cKnowledge). CK attempts to make it easier to reproduce ML&systems research,
deploy ML models in production, and adapt them to continuously changing data
sets, models, research techniques, software, and hardware. The CK concept is to
decompose complex systems and ad-hoc research projects into reusable
sub-components with unified APIs, CLI, and JSON meta description. Such
components can be connected into portable workflows using DevOps principles
combined with reusable automation actions, software detection plugins, meta
packages, and exposed optimization parameters. CK workflows can automatically
plug in different models, data and tools from different vendors while building,
running and benchmarking research code in a unified way across diverse
platforms and environments. Such workflows also help to perform whole system
optimization, reproduce results, and compare them using public or private
scoreboards on the CK platform (https://cKnowledge.io). For example, the
modular CK approach was successfully validated with industrial partners to
automatically co-design and optimize software, hardware, and machine learning
models for reproducible and efficient object detection in terms of speed,
accuracy, energy, size, and other characteristics. The long-term goal is to
simplify and accelerate the development and deployment of ML models and systems
by helping researchers and practitioners to share and reuse their knowledge,
experience, best practices, artifacts, and techniques using open CK APIs."
"Finding the factors contributing to criminal activities and their
consequences is essential to improve quantitative crime research. To respond to
this concern, we examine an extensive set of features from different
perspectives and explanations. Our study aims to build data-driven models for
predicting future crime occurrences. In this paper, we propose the use of
streetlight infrastructure and Foursquare data along with demographic
characteristics for improving future crime incident prediction. We evaluate the
classification performance based on various feature combinations as well as
with the baseline model. Our proposed model was tested on each smallest
geographic region in Halifax, Canada. Our findings demonstrate the
effectiveness of integrating diverse sources of data to gain satisfactory
classification performance."
"Behavioral cues play a significant part in human communication and cognitive
perception. In most professional domains, employee recruitment policies are
framed such that both professional skills and personality traits are adequately
assessed. Hiring interviews are structured to evaluate expansively a potential
employee's suitability for the position - their professional qualifications,
interpersonal skills, ability to perform in critical and stressful situations,
in the presence of time and resource constraints, etc. Therefore, candidates
need to be aware of their positive and negative attributes and be mindful of
behavioral cues that might have adverse effects on their success. We propose a
multimodal analytical framework that analyzes the candidate in an interview
scenario and provides feedback for predefined labels such as engagement,
speaking rate, eye contact, etc. We perform a comprehensive analysis that
includes the interviewee's facial expressions, speech, and prosodic
information, using the video, audio, and text transcripts obtained from the
recorded interview. We use these multimodal data sources to construct a
composite representation, which is used for training machine learning
classifiers to predict the class labels. Such analysis is then used to provide
constructive feedback to the interviewee for their behavioral cues and body
language. Experimental validation showed that the proposed methodology achieved
promising results."
"We investigate a strategy for improving the efficiency of contrastive
learning of visual representations by leveraging a small amount of supervised
information during pre-training. We propose a semi-supervised loss, SuNCEt,
based on noise-contrastive estimation and neighbourhood component analysis,
that aims to distinguish examples of different classes in addition to the
self-supervised instance-wise pretext tasks. On ImageNet, we find that SuNCEt
can be used to match the semi-supervised learning accuracy of previous
contrastive approaches while using less than half the amount of pre-training
and compute. Our main insight is that leveraging even a small amount of labeled
data during pre-training, and not only during fine-tuning, provides an
important signal that can significantly accelerate contrastive learning of
visual representations. Our code is available online at
github.com/facebookresearch/suncet."
"A popular theory of perceptual processing holds that the brain learns both a
generative model of the world and a paired recognition model using variational
Bayesian inference. Most hypotheses of how the brain might learn these models
assume that neurons in a population are conditionally independent given their
common inputs. This simplification is likely not compatible with the type of
local recurrence observed in the brain. Seeking an alternative that is
compatible with complex inter-dependencies yet consistent with known biology,
we argue here that the cortex may learn with an adversarial algorithm. Many
observable symptoms of this approach would resemble known neural phenomena,
including wake/sleep cycles and oscillations that vary in magnitude with
surprise, and we describe how further predictions could be tested. We
illustrate the idea on recurrent neural networks trained to model image and
video datasets. This framework for learning brings variational inference closer
to neuroscience and yields multiple testable hypotheses."
"Continuously learning to solve unseen tasks with limited experience has been
extensively pursued in meta-learning and continual learning, but with
restricted assumptions such as accessible task distributions, independently and
identically distributed tasks, and clear task delineations. However, real-world
physical tasks frequently violate these assumptions, resulting in performance
degradation. This paper proposes a continual online model-based reinforcement
learning approach that does not require pre-training to solve task-agnostic
problems with unknown task boundaries. We maintain a mixture of experts to
handle nonstationarity, and represent each different type of dynamics with a
Gaussian Process to efficiently leverage collected data and expressively model
uncertainty. We propose a transition prior to account for the temporal
dependencies in streaming data and update the mixture online via sequential
variational inference. Our approach reliably handles the task distribution
shift by generating new models for never-before-seen dynamics and reusing old
models for previously seen dynamics. In experiments, our approach outperforms
alternative methods in non-stationary tasks, including classic control with
changing dynamics and decision making in different driving scenarios."
"This paper addresses the problem of model-free reinforcement learning for
Robust Markov Decision Process (RMDP) with large state spaces. The goal of the
RMDP framework is to find a policy that is robust against the parameter
uncertainties due to the mismatch between the simulator model and real-world
settings. We first propose the Robust Least Squares Policy Evaluation
algorithm, which is a multi-step online model-free learning algorithm for
policy evaluation. We prove the convergence of this algorithm using stochastic
approximation techniques. We then propose Robust Least Squares Policy Iteration
(RLSPI) algorithm for learning the optimal robust policy. We also give a
general weighted Euclidean norm bound on the error (closeness to optimality) of
the resulting policy. Finally, we demonstrate the performance of our RLSPI
algorithm on some standard benchmark problems."
"Automated per-instance algorithm selection often outperforms single learners.
Key to algorithm selection via meta-learning is often the (meta) features,
which sometimes though do not provide enough information to train a
meta-learner effectively. We propose a Siamese Neural Network architecture for
automated algorithm selection that focuses more on 'alike performing' instances
than meta-features. Our work includes a novel performance metric and method for
selecting training samples. We introduce further the concept of 'Algorithm
Performance Personas' that describe instances for which the single algorithms
perform alike. The concept of 'alike performing algorithms' as ground truth for
selecting training samples is novel and provides a huge potential as we
believe. In this proposal, we outline our ideas in detail and provide the first
evidence that our proposed metric is better suitable for training sample
selection that standard performance metrics such as absolute errors."
"We propose a versatile framework based on random search, Sparse-RS, for
score-based sparse targeted and untargeted attacks in the black-box setting.
Sparse-RS does not rely on substitute models and achieves state-of-the-art
success rate and query efficiency for multiple sparse attack models:
$l_0$-bounded perturbations, adversarial patches, and adversarial frames. The
$l_0$-version of untargeted Sparse-RS outperforms all black-box and even all
white-box attacks for different models on MNIST, CIFAR-10, and ImageNet.
Moreover, our untargeted Sparse-RS achieves very high success rates even for
the challenging settings of $20\times20$ adversarial patches and $2$-pixel wide
adversarial frames for $224\times224$ images. Finally, we show that Sparse-RS
can be applied to generate targeted universal adversarial patches where it
significantly outperforms the existing approaches. The code of our framework is
available at https://github.com/fra31/sparse-rs."
"For a certain scaling of the initialization of stochastic gradient descent
(SGD), wide neural networks (NN) have been shown to be well approximated by
reproducing kernel Hilbert space (RKHS) methods. Recent empirical work showed
that, for some classification tasks, RKHS methods can replace NNs without a
large loss in performance. On the other hand, two-layers NNs are known to
encode richer smoothness classes than RKHS and we know of special examples for
which SGD-trained NN provably outperform RKHS. This is true even in the wide
network limit, for a different scaling of the initialization.
  How can we reconcile the above claims? For which tasks do NNs outperform
RKHS? If covariates are nearly isotropic, RKHS methods suffer from the curse of
dimensionality, while NNs can overcome it by learning the best low-dimensional
representation. Here we show that this curse of dimensionality becomes milder
if the covariates display the same low-dimensional structure as the target
function, and we precisely characterize this tradeoff. Building on these
results, we present the spiked covariates model that can capture in a unified
framework both behaviors observed in earlier work.
  We hypothesize that such a latent low-dimensional structure is present in
image classification. We test numerically this hypothesis by showing that
specific perturbations of the training distribution degrade the performances of
RKHS methods much more significantly than NNs."
"Robust loss functions are essential for training accurate deep neural
networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown
that the commonly used Cross Entropy (CE) loss is not robust to noisy labels.
Whilst new loss functions have been designed, they are only partially robust.
In this paper, we theoretically show by applying a simple normalization that:
any loss can be made robust to noisy labels. However, in practice, simply being
robust is not sufficient for a loss function to train accurate DNNs. By
investigating several robust loss functions, we find that they suffer from a
problem of underfitting. To address this, we propose a framework to build
robust loss functions called Active Passive Loss (APL). APL combines two robust
loss functions that mutually boost each other. Experiments on benchmark
datasets demonstrate that the family of new loss functions created by our APL
framework can consistently outperform state-of-the-art methods by large
margins, especially under large noise rates such as 60% or 80% incorrect
labels."
"We examine the task of locating a target region among those induced by
intersections of $n$ halfspaces in $\mathbb{R}^d$. This generic task connects
to fundamental machine learning problems, such as training a perceptron and
learning a $\phi$-separable dichotomy. We investigate the average teaching
complexity of the task, i.e., the minimal number of samples (halfspace queries)
required by a teacher to help a version-space learner in locating a randomly
selected target. As our main result, we show that the average-case teaching
complexity is $\Theta(d)$, which is in sharp contrast to the worst-case
teaching complexity of $\Theta(n)$. If instead, we consider the average-case
learning complexity, the bounds have a dependency on $n$ as $\Theta(n)$ for
\tt{i.i.d.} queries and $\Theta(d \log(n))$ for actively chosen queries by the
learner. Our proof techniques are based on novel insights from computational
geometry, which allow us to count the number of convex polytopes and faces in a
Euclidean space depending on the arrangement of halfspaces. Our insights allow
us to establish a tight bound on the average-case complexity for
$\phi$-separable dichotomies, which generalizes the known $\mathcal{O}(d)$
bound on the average number of ""extreme patterns"" in the classical
computational geometry literature (Cover, 1965)."
"Deep Neural Networks are often brittle on image classification tasks and
known to misclassify inputs. While these misclassifications may be inevitable,
all failure modes cannot be considered equal. Certain misclassifications (eg.
classifying the image of a dog to an airplane) can perplex humans and result in
the loss of human trust in the system. Even worse, these errors (eg. a person
misclassified as a primate) can have odious societal impacts. Thus, in this
work, we aim to reduce inexplicable errors. To address this challenge, we first
discuss methods to obtain the class-level semantics that capture the human's
expectation ($M^h$) regarding which classes are semantically close {\em vs.}
ones that are far away. We show that for popular image benchmarks (like
CIFAR-10, CIFAR-100, ImageNet), class-level semantics can be readily obtained
by leveraging either human subject studies or publicly available human-curated
knowledge bases. Second, we propose the use of Weighted Loss Functions (WLFs)
to penalize misclassifications by the weight of their inexplicability. Finally,
we show that training (or fine-tuning) existing classifiers with the proposed
methods lead to Deep Neural Networks that have (1) comparable top-1 accuracy,
(2) more explicable failure modes on both in-distribution and
out-of-distribution (OOD) test data, and (3) incur significantly less cost in
the gathering of additional human labels compared to existing works."
"The central challenge in automated synthesis planning is to be able to
generate and predict outcomes of a diverse set of chemical reactions. In
particular, in many cases, the most likely synthesis pathway cannot be applied
due to additional constraints, which requires proposing alternative chemical
reactions. With this in mind, we present Molecule Edit Graph Attention Network
(MEGAN), an end-to-end encoder-decoder neural model. MEGAN is inspired by
models that express a chemical reaction as a sequence of graph edits, akin to
the arrow pushing formalism. We extend this model to retrosynthesis prediction
(predicting substrates given the product of a chemical reaction) and scale it
up to large datasets. We argue that representing the reaction as a sequence of
edits enables MEGAN to efficiently explore the space of plausible chemical
reactions, maintaining the flexibility of modeling the reaction in an
end-to-end fashion, and achieving state-of-the-art accuracy in standard
benchmarks. Code and trained models are made available online at
https://github.com/molecule-one/megan."
"This paper formulates hypothesis verification as an RL problem. Specifically,
we aim to build an agent that, given a hypothesis about the dynamics of the
world, can take actions to generate observations which can help predict whether
the hypothesis is true or false. Existing RL algorithms fail to solve this
task, even for simple environments. In order to train the agents, we exploit
the underlying structure of many hypotheses, factorizing them as
{pre-condition, action sequence, post-condition} triplets. By leveraging this
structure we show that RL agents are able to succeed at the task. Furthermore,
subsequent fine-tuning of the policies allows the agent to correctly verify
hypotheses not amenable to the above factorization."
"The COVID-19 pandemic has created unprecedented challenges worldwide.
Strained healthcare providers make difficult decisions on patient triage,
treatment and care management on a daily basis. Policy makers have imposed
social distancing measures to slow the disease, at a steep economic price. We
design analytical tools to support these decisions and combat the pandemic.
Specifically, we propose a comprehensive data-driven approach to understand the
clinical characteristics of COVID-19, predict its mortality, forecast its
evolution, and ultimately alleviate its impact. By leveraging cohort-level
clinical data, patient-level hospital data, and census-level epidemiological
data, we develop an integrated four-step approach, combining descriptive,
predictive and prescriptive analytics. First, we aggregate hundreds of clinical
studies into the most comprehensive database on COVID-19 to paint a new
macroscopic picture of the disease. Second, we build personalized calculators
to predict the risk of infection and mortality as a function of demographics,
symptoms, comorbidities, and lab values. Third, we develop a novel
epidemiological model to project the pandemic's spread and inform social
distancing policies. Fourth, we propose an optimization model to re-allocate
ventilators and alleviate shortages. Our results have been used at the clinical
level by several hospitals to triage patients, guide care management, plan ICU
capacity, and re-distribute ventilators. At the policy level, they are
currently supporting safe back-to-work policies at a major institution and
equitable vaccine distribution planning at a major pharmaceutical company, and
have been integrated into the US Center for Disease Control's pandemic
forecast."
"Generative adversarial networks (GANs) have shown remarkable success in
generating realistic data from some predefined prior distribution (e.g.,
Gaussian noises). However, such prior distribution is often independent of real
data and thus may lose semantic information (e.g., geometric structure or
content in images) of data. In practice, the semantic information might be
represented by some latent distribution learned from data. However, such latent
distribution may incur difficulties in data sampling for GANs. In this paper,
rather than sampling from the predefined prior distribution, we propose an
LCCGAN model with local coordinate coding (LCC) to improve the performance of
generating data. First, we propose an LCC sampling method in LCCGAN to sample
meaningful points from the latent manifold. With the LCC sampling method, we
can exploit the local information on the latent manifold and thus produce new
data with promising quality. Second, we propose an improved version, namely
LCCGAN++, by introducing a higher-order term in the generator approximation.
This term is able to achieve better approximation and thus further improve the
performance. More critically, we derive the generalization bound for both
LCCGAN and LCCGAN++ and prove that a low-dimensional input is sufficient to
achieve good generalization performance. Extensive experiments on four
benchmark datasets demonstrate the superiority of the proposed method over
existing GANs."
"We consider the problem of classification of points sampled from an unknown
probability measure on a Euclidean space. We study the question of querying the
class label at a very small number of judiciously chosen points so as to be
able to attach the appropriate class label to every point in the set. Our
approach is to consider the unknown probability measure as a convex combination
of the conditional probabilities for each class. Our technique involves the use
of a highly localized kernel constructed from Hermite polynomials, in order to
create a hierarchical estimate of the supports of the constituent probability
measures. We do not need to make any assumptions on the nature of any of the
probability measures nor know in advance the number of classes involved. We
give theoretical guarantees measured by the $F$-score for our classification
scheme. Examples include classification in hyper-spectral images and MNIST
classification."
"Quantum machine learning -- and specifically Variational Quantum Algorithms
(VQAs) -- offers a powerful, flexible paradigm for programming near-term
quantum computers, with applications in chemistry, metrology, materials
science, data science, and mathematics. Here, one trains an ansatz, in the form
of a parameterized quantum circuit, to accomplish a task of interest. However,
challenges have recently emerged suggesting that deep ansatzes are difficult to
train, due to flat training landscapes caused by randomness or by hardware
noise. This motivates our work, where we present a variable structure approach
to build ansatzes for VQAs. Our approach, called VAns (Variable Ansatz),
applies a set of rules to both grow and (crucially) remove quantum gates in an
informed manner during the optimization. Consequently, VAns is ideally suited
to mitigate trainability and noise-related issues by keeping the ansatz
shallow. We employ VAns in the variational quantum eigensolver for condensed
matter and quantum chemistry applications, in the quantum autoencoder for data
compression and in unitary compilation problems showing successful results in
all cases."
"We propose a distributionally robust classification model with a fairness
constraint that encourages the classifier to be fair in view of the equality of
opportunity criterion. We use a type-$\infty$ Wasserstein ambiguity set
centered at the empirical distribution to model distributional uncertainty and
derive a conservative reformulation for the worst-case equal opportunity
unfairness measure. We establish that the model is equivalent to a mixed binary
optimization problem, which can be solved by standard off-the-shelf solvers. To
improve scalability, we further propose a convex, hinge-loss-based model for
large problem instances whose reformulation does not incur any binary
variables. Moreover, we also consider the distributionally robust learning
problem with a generic ground transportation cost to hedge against the
uncertainties in the label and sensitive attribute. Finally, we numerically
demonstrate that our proposed approaches improve fairness with negligible loss
of predictive accuracy."
"In implementations of the functional data methods, the effect of the initial
choice of an orthonormal basis has not gained much attention in the past.
Typically, several standard bases such as Fourier, wavelets, splines, etc. are
considered to transform observed functional data and a choice is made without
any formal criteria indicating which of the bases is preferable for the initial
transformation of the data into functions. In an attempt to address this issue,
we propose a strictly data-driven method of orthogonal basis selection. The
method uses recently introduced orthogonal spline bases called the splinets
obtained by efficient orthogonalization of the B-splines. The algorithm learns
from the data in the machine learning style to efficiently place knots. The
optimality criterion is based on the average (per functional data point) mean
square error and is utilized both in the learning algorithms and in comparison
studies. The latter indicates efficiency that is particularly evident for the
sparse functional data and to a lesser degree in analyses of responses to
complex physical systems."
"To develop a machine sound monitoring system, a method for detecting
anomalous sound is proposed. Exact likelihood estimation using Normalizing
Flows is a promising technique for unsupervised anomaly detection, but it can
fail at out-of-distribution detection since the likelihood is affected by the
smoothness of the data. To improve the detection performance, we train the
model to assign higher likelihood to target machine sounds and lower likelihood
to sounds from other machines of the same machine type. We demonstrate that
this enables the model to incorporate a self-supervised classification-based
approach. Experiments conducted using the DCASE 2020 Challenge Task2 dataset
showed that the proposed method improves the AUC by 4.6% on average when using
Masked Autoregressive Flow (MAF) and by 5.8% when using Glow, which is a
significant improvement over the previous method."
"Wire-feed laser additive manufacturing (WLAM) is gaining wide interest due to
its high level of automation, high deposition rates, and good quality of
printed parts. In-process monitoring and feedback controls that would reduce
the uncertainty in the quality of the material are in the early stages of
development. Machine learning promises the ability to accelerate the adoption
of new processes and property design in additive manufacturing by making
process-structure-property connections between process setting inputs and
material quality outcomes. The molten pool dimensional information and
temperature are the indicators for achieving the high quality of the build,
which can be directly controlled by processing parameters. For the purpose of
in situ quality control, the process parameters should be controlled in
real-time based on sensed information from the process, in particular the
molten pool. Thus, the molten pool-process relations are of preliminary
importance. This paper analyzes experimentally collected in situ sensing data
from the molten pool under a set of controlled process parameters in a WLAM
system. The variations in the steady-state and transient state of the molten
pool are presented with respect to the change of independent process
parameters. A multi-modality convolutional neural network (CNN) architecture is
proposed for predicting the control parameter directly from the measurable
molten pool sensor data for achieving desired geometric and microstructural
properties. Dropout and regularization are applied to the CNN architecture to
avoid the problem of overfitting. The results highlighted that the multi-modal
CNN, which receives temperature profile as an external feature to the features
extracted from the image data, has improved prediction performance compared to
the image-based uni-modality CNN approach."
"Causal learning is the key to obtaining stable predictions and answering
\textit{what if} problems in decision-makings. In causal learning, it is
central to seek methods to estimate the average treatment effect (ATE) from
observational data. The Double/Debiased Machine Learning (DML) is one of the
prevalent methods to estimate ATE. However, the DML estimators can suffer from
an \textit{error-compounding issue} and even give extreme estimates when the
propensity scores are close to 0 or 1. Previous studies have overcome this
issue through some empirical tricks such as propensity score trimming, yet none
of the existing works solves it from a theoretical standpoint. In this paper,
we propose a \textit{Robust Causal Learning (RCL)} method to offset the
deficiencies of DML estimators. Theoretically, the RCL estimators i) satisfy
the (higher-order) orthogonal condition and are as \textit{consistent and
doubly robust} as the DML estimators, and ii) get rid of the error-compounding
issue. Empirically, the comprehensive experiments show that: i) the RCL
estimators give more stable estimations of the causal parameters than DML; ii)
the RCL estimators outperform traditional estimators and their variants when
applying different machine learning models on both simulation and benchmark
datasets, and a mimic consumer credit dataset generated by WGAN."
"Vector autoregression (VAR) is a fundamental tool for modeling multivariate
time series. However, as the number of component series is increased, the VAR
model becomes overparameterized. Several authors have addressed this issue by
incorporating regularized approaches, such as the lasso in VAR estimation.
Traditional approaches address overparameterization by selecting a low lag
order, based on the assumption of short range dependence, assuming that a
universal lag order applies to all components. Such an approach constrains the
relationship between the components and impedes forecast performance. The
lasso-based approaches work much better in high-dimensional situations but do
not incorporate the notion of lag order selection.
  We propose a new class of hierarchical lag structures (HLag) that embed the
notion of lag selection into a convex regularizer. The key modeling tool is a
group lasso with nested groups which guarantees that the sparsity pattern of
lag coefficients honors the VAR's ordered structure. The HLag framework offers
three structures, which allow for varying levels of flexibility. A simulation
study demonstrates improved performance in forecasting and lag order selection
over previous approaches, and a macroeconomic application further highlights
forecasting improvements as well as HLag's convenient, interpretable output."
"Several new estimation methods have been recently proposed for the linear
regression model with observation error in the design. Different assumptions on
the data generating process have motivated different estimators and analysis.
In particular, the literature considered (1) observation errors in the design
uniformly bounded by some $\bar \delta$, and (2) zero mean independent
observation errors. Under the first assumption, the rates of convergence of the
proposed estimators depend explicitly on $\bar \delta$, while the second
assumption has been applied when an estimator for the second moment of the
observational error is available. This work proposes and studies two new
estimators which, compared to other procedures for regression models with
errors in the design, exploit an additional $l_{\infty}$-norm regularization.
The first estimator is applicable when both (1) and (2) hold but does not
require an estimator for the second moment of the observational error. The
second estimator is applicable under (2) and requires an estimator for the
second moment of the observation error. Importantly, we impose no assumption on
the accuracy of this pilot estimator, in contrast to the previously known
procedures. As the recent proposals, we allow the number of covariates to be
much larger than the sample size. We establish the rates of convergence of the
estimators and compare them with the bounds obtained for related estimators in
the literature. These comparisons show interesting insights on the interplay of
the assumptions and the achievable rates of convergence."
"Model selection is indispensable to high-dimensional sparse modeling in
selecting the best set of covariates among a sequence of candidate models. Most
existing work assumes implicitly that the model is correctly specified or of
fixed dimensions. Yet model misspecification and high dimensionality are common
in real applications. In this paper, we investigate two classical
Kullback-Leibler divergence and Bayesian principles of model selection in the
setting of high-dimensional misspecified models. Asymptotic expansions of these
principles reveal that the effect of model misspecification is crucial and
should be taken into account, leading to the generalized AIC and generalized
BIC in high dimensions. With a natural choice of prior probabilities, we
suggest the generalized BIC with prior probability which involves a logarithmic
factor of the dimensionality in penalizing model complexity. We further
establish the consistency of the covariance contrast matrix estimator in a
general setting. Our results and new method are supported by numerical studies."
"Given an ensemble of randomized regression trees, it is possible to
restructure them as a collection of multilayered neural networks with
particular connection weights. Following this principle, we reformulate the
random forest method of Breiman (2001) into a neural network setting, and in
turn propose two new hybrid procedures that we call neural random forests. Both
predictors exploit prior knowledge of regression trees for their architecture,
have less parameters to tune than standard networks, and less restrictions on
the geometry of the decision boundaries than trees. Consistency results are
proved, and substantial numerical evidence is provided on both synthetic and
real data sets to assess the excellent performance of our methods in a large
variety of prediction problems."
"We consider regret minimization in repeated games with non-convex loss
functions. Minimizing the standard notion of regret is computationally
intractable. Thus, we define a natural notion of regret which permits efficient
optimization and generalizes offline guarantees for convergence to an
approximate local optimum. We give gradient-based methods that achieve optimal
regret, which in turn guarantee convergence to equilibrium in this framework."
"Describing the dimension reduction (DR) techniques by means of probabilistic
models has recently been given special attention. Probabilistic models, in
addition to a better interpretability of the DR methods, provide a framework
for further extensions of such algorithms. One of the new approaches to the
probabilistic DR methods is to preserving the internal structure of data. It is
meant that it is not necessary that the data first be converted from the matrix
or tensor format to the vector format in the process of dimensionality
reduction. In this paper, a latent variable model for matrix-variate data for
canonical correlation analysis (CCA) is proposed. Since in general there is not
any analytical maximum likelihood solution for this model, we present two
approaches for learning the parameters. The proposed methods are evaluated
using the synthetic data in terms of convergence and quality of mappings. Also,
real data set is employed for assessing the proposed methods with several
probabilistic and none-probabilistic CCA based approaches. The results confirm
the superiority of the proposed methods with respect to the competing
algorithms. Moreover, this model can be considered as a framework for further
extensions."
"We call a learner super-teachable if a teacher can trim down an iid training
set while making the learner learn even better. We provide sharp super-teaching
guarantees on two learners: the maximum likelihood estimator for the mean of a
Gaussian, and the large margin classifier in 1D. For general learners, we
provide a mixed-integer nonlinear programming-based algorithm to find a super
teaching set. Empirical experiments show that our algorithm is able to find
good super-teaching sets for both regression and classification problems."
"Understanding statistical inference under possibly non-sparse
high-dimensional models has gained much interest recently. For a given
component of the regression coefficient, we show that the difficulty of the
problem depends on the sparsity of the corresponding row of the precision
matrix of the covariates, not the sparsity of the regression coefficients. We
develop new concepts of uniform and essentially uniform non-testability that
allow the study of limitations of tests across a broad set of alternatives.
Uniform non-testability identifies a collection of alternatives such that the
power of any test, against any alternative in the group, is asymptotically at
most equal to the nominal size. Implications of the new constructions include
new minimax testability results that, in sharp contrast to the current results,
do not depend on the sparsity of the regression parameters. We identify new
tradeoffs between testability and feature correlation. In particular, we show
that, in models with weak feature correlations, minimax lower bound can be
attained by a test whose power has the $\sqrt{n}$ rate, regardless of the size
of the model sparsity."
"Fully Homomorphic Encryption (FHE) refers to a set of encryption schemes that
allow computations to be applied directly on encrypted data without requiring a
secret key. This enables novel application scenarios where a client can safely
offload storage and computation to a third-party cloud provider without having
to trust the software and the hardware vendors with the decryption keys. Recent
advances in both FHE schemes and implementations have moved such applications
from theoretical possibilities into the realm of practicalities.
  This paper proposes a compact and well-reasoned interface called the
Homomorphic Instruction Set Architecture (HISA) for developing FHE
applications. Just as the hardware ISA interface enabled hardware advances to
proceed independent of software advances in the compiler and language runtimes,
HISA decouples compiler optimizations and runtimes for supporting FHE
applications from advancements in the underlying FHE schemes.
  This paper demonstrates the capabilities of HISA by building an end-to-end
software stack for evaluating neural network models on encrypted data. Our
stack includes an end-to-end compiler, runtime, and a set of optimizations. Our
approach shows generated code, on a set of popular neural network
architectures, is faster than hand-optimized implementations."
"Learning from data has led to a paradigm shift in computational materials
science. In particular, it has been shown that neural networks can learn the
potential energy surface and interatomic forces through examples, thus
bypassing the computationally expensive density functional theory calculations.
Combining many-body techniques with a deep learning approach, we demonstrate
that a fully-connected neural network is able to learn the complex collective
behavior of electrons in strongly correlated systems. Specifically, we consider
the Anderson-Hubbard (AH) model, which is a canonical system for studying the
interplay between electron correlation and strong localization. The ground
states of the AH model on a square lattice are obtained using the real-space
Gutzwiller method. The obtained solutions are used to train a multi-task
multi-layer neural network, which subsequently can accurately predict
quantities such as the local probability of double occupation and the
quasiparticle weight, given the disorder potential in the neighborhood as the
input."
"Fairness, through its many forms and definitions, has become an important
issue facing the machine learning community. In this work, we consider how to
incorporate group fairness constraints in kernel regression methods, applicable
to Gaussian processes, support vector machines, neural network regression and
decision tree regression. Further, we focus on examining the effect of
incorporating these constraints in decision tree regression, with direct
applications to random forests and boosted trees amongst other widespread
popular inference techniques. We show that the order of complexity of memory
and computation is preserved for such models and tightly bound the expected
perturbations to the model in terms of the number of leaves of the trees.
Importantly, the approach works on trained models and hence can be easily
applied to models in current use and group labels are only required on training
data."
"We study a general problem of allocating limited resources to heterogeneous
customers over time under model uncertainty. Each type of customer can be
serviced using different actions, each of which stochastically consumes some
combination of resources, and returns different rewards for the resources
consumed. We consider a general model where the resource consumption
distribution associated with each (customer type, action)-combination is not
known, but is consistent and can be learned over time. In addition, the
sequence of customer types to arrive over time is arbitrary and completely
unknown.
  We overcome both the challenges of model uncertainty and customer
heterogeneity by judiciously synthesizing two algorithmic frameworks from the
literature: inventory balancing, which ""reserves"" a portion of each resource
for high-reward customer types which could later arrive, and online learning,
which shows how to ""explore"" the resource consumption distributions of each
customer type under different actions. We define an auxiliary problem, which
allows for existing competitive ratio and regret bounds to be seamlessly
integrated. Furthermore, we show that the performance guarantee generated by
our framework is tight, that is, we provide an information-theoretic lower
bound which shows that both the loss from competitive ratio and the loss for
regret are relevant in the combined problem.
  Finally, we demonstrate the efficacy of our algorithms on a publicly
available hotel data set. Our framework is highly practical in that it requires
no historical data (no fitted customer choice models, nor forecasting of
customer arrival patterns) and can be used to initialize allocation strategies
in fast-changing environments."
"New estimates for the population risk are established for two-layer neural
networks. These estimates are nearly optimal in the sense that the error rates
scale in the same way as the Monte Carlo error rates. They are equally
effective in the over-parametrized regime when the network size is much larger
than the size of the dataset. These new estimates are a priori in nature in the
sense that the bounds depend only on some norms of the underlying functions to
be fitted, not the parameters in the model, in contrast with most existing
results which are a posteriori in nature. Using these a priori estimates, we
provide a perspective for understanding why two-layer neural networks perform
better than the related kernel methods."
"Recently, due to the increasing popularity of social media, the necessity for
extracting information from informal text types, such as microblog texts, has
gained significant attention. In this study, we focused on the Named Entity
Recognition (NER) problem on informal text types for Turkish. We utilized a
semi-supervised learning approach based on neural networks. We applied a fast
unsupervised method for learning continuous representations of words in vector
space. We made use of these obtained word embeddings, together with language
independent features that are engineered to work better on informal text types,
for generating a Turkish NER system on microblog texts. We evaluated our
Turkish NER system on Twitter messages and achieved better F-score performances
than the published results of previously proposed NER systems on Turkish
tweets. Since we did not employ any language dependent features, we believe
that our method can be easily adapted to microblog texts in other
morphologically rich languages."
"When machine learning systems fail because of adversarial manipulation, how
should society expect the law to respond? Through scenarios grounded in
adversarial ML literature, we explore how some aspects of computer crime,
copyright, and tort law interface with perturbation, poisoning, model stealing
and model inversion attacks to show how some attacks are more likely to result
in liability than others. We end with a call for action to ML researchers to
invest in transparent benchmarks of attacks and defenses; architect ML systems
with forensics in mind and finally, think more about adversarial machine
learning in the context of civil liberties. The paper is targeted towards ML
researchers who have no legal background."
"A fundamental problem in geophysical modeling is related to the
identification and approximation of causal structures among physical processes.
However, resolving the bidirectional mappings between physical parameters and
model state variables (i.e., solving the forward and inverse problems) is
challenging, especially when parameter dimensionality is high. Deep learning
has opened a new door toward knowledge representation and complex pattern
identification. In particular, the recently introduced generative adversarial
networks (GANs) hold strong promises in learning cross-domain mappings for
image translation. This study presents a state-parameter identification GAN
(SPID-GAN) for simultaneously learning bidirectional mappings between a
high-dimensional parameter space and the corresponding model state space.
SPID-GAN is demonstrated using a series of representative problems from
subsurface flow modeling. Results show that SPID-GAN achieves satisfactory
performance in identifying the bidirectional state-parameter mappings,
providing a new deep-learning-based, knowledge representation paradigm for a
wide array of complex geophysical problems."
"Beta Basis Function Neural Network (BBFNN) is a special kind of kernel basis
neural networks. It is a feedforward network typified by the use of beta
function as a hidden activation function. Beta is a flexible transfer function
representing richer forms than the common existing functions. As in every
network, the architecture setting as well as the learning method are two main
gauntlets faced by BBFNN. In this paper, new architecture and training
algorithm are proposed for the BBFNN. An Extreme Learning Machine (ELM) is used
as a training approach of BBFNN with the aim of quickening the training
process. The peculiarity of ELM is permitting a certain decrement of the
computing time and complexity regarding the already used BBFNN learning
algorithms such as backpropagation, OLS, etc. For the architectural design, a
recurrent structure is added to the common BBFNN architecture in order to make
it more able to deal with complex, non linear and time varying problems.
Throughout this paper, the conceived recurrent ELM-trained BBFNN is tested on a
number of tasks related to time series prediction, classification and
regression. Experimental results show noticeable achievements of the proposed
network compared to common feedforward and recurrent networks trained by ELM
and using hyperbolic tangent as activation function. These achievements are in
terms of accuracy and robustness against data breakdowns such as noise signals."
"$\newcommand{\ball}{\mathbb{B}}\newcommand{\dsQ}{{\mathcal{Q}}}\newcommand{\dsS}{{\mathcal{S}}}$In
this work we study a fair variant of the near neighbor problem. Namely, given a
set of $n$ points $P$ and a parameter $r$, the goal is to preprocess the
points, such that given a query point $q$, any point in the $r$-neighborhood of
the query, i.e., $\ball(q,r)$, have the same probability of being reported as
the near neighbor.
  We show that LSH based algorithms can be made fair, without a significant
loss in efficiency. Specifically, we show an algorithm that reports a point in
the $r$-neighborhood of a query $q$ with almost uniform probability. The query
time is proportional to $O\bigl( \mathrm{dns}(q.r) \dsQ(n,c) \bigr)$, and its
space is $O(\dsS(n,c))$, where $\dsQ(n,c)$ and $\dsS(n,c)$ are the query time
and space of an LSH algorithm for $c$-approximate near neighbor, and
$\mathrm{dns}(q,r)$ is a function of the local density around $q$.
  Our approach works more generally for sampling uniformly from a
sub-collection of sets of a given collection and can be used in a few other
applications. Finally, we run experiments to show performance of our approach
on real data."
"In this paper we first analyzed the inductive bias underlying the data
scattered across complex free energy landscapes (FEL), and exploited it to
train deep neural networks which yield reduced and clustered representation for
the FEL. Our parametric method, called Information Distilling of Metastability
(IDM), is end-to-end differentiable thus scalable to ultra-large dataset. IDM
is also a clustering algorithm and is able to cluster the samples in the
meantime of reducing the dimensions. Besides, as an unsupervised learning
method, IDM differs from many existing dimensionality reduction and clustering
methods in that it neither requires a cherry-picked distance metric nor the
ground-true number of clusters, and that it can be used to unroll and zoom-in
the hierarchical FEL with respect to different timescales. Through multiple
experiments, we show that IDM can achieve physically meaningful representations
which partition the FEL into well-defined metastable states hence are amenable
for downstream tasks such as mechanism analysis and kinetic modeling."
"In this paper we use game theory to model poisoning attack scenarios. We
prove the non-existence of pure strategy Nash Equilibrium in the attacker and
defender game. We then propose a mixed extension of our game model and an
algorithm to approximate the Nash Equilibrium strategy for the defender. We
then demonstrate the effectiveness of the mixed defence strategy generated by
the algorithm, in an experiment."
"This paper introduces Task 2 of the DCASE2019 Challenge, titled ""Audio
tagging with noisy labels and minimal supervision"". This task was hosted on the
Kaggle platform as ""Freesound Audio Tagging 2019"". The task evaluates systems
for multi-label audio tagging using a large set of noisy-labeled data, and a
much smaller set of manually-labeled data, under a large vocabulary setting of
80 everyday sound classes. In addition, the proposed dataset poses an acoustic
mismatch problem between the noisy train set and the test set due to the fact
that they come from different web audio sources. This can correspond to a
realistic scenario given by the difficulty in gathering large amounts of
manually labeled data. We present the task setup, the FSDKaggle2019 dataset
prepared for this scientific evaluation, and a baseline system consisting of a
convolutional neural network. All these resources are freely available."
"We propose a scalable framework for inference in an inhomogeneous Poisson
process modeled by a continuous sigmoidal Cox process that assumes the
corresponding intensity function is given by a Gaussian process (GP) prior
transformed with a scaled logistic sigmoid function. We present a tractable
representation of the likelihood through augmentation with a superposition of
Poisson processes. This view enables a structured variational approximation
capturing dependencies across variables in the model. Our framework avoids
discretization of the domain, does not require accurate numerical integration
over the input space and is not limited to GPs with squared exponential
kernels. We evaluate our approach on synthetic and real-world data showing that
its benefits are particularly pronounced on multivariate input settings where
it overcomes the limitations of mean-field methods and sampling schemes. We
provide the state of-the-art in terms of speed, accuracy and uncertainty
quantification trade-offs."
"We study the problem of online influence maximization in social networks. In
this problem, a learner aims to identify the set of ""best influencers"" in a
network by interacting with it, i.e., repeatedly selecting seed nodes and
observing activation feedback in the network. We capitalize on an important
property of the influence maximization problem named network assortativity,
which is ignored by most existing works in online influence maximization. To
realize network assortativity, we factorize the activation probability on the
edges into latent factors on the corresponding nodes, including influence
factor on the giving nodes and susceptibility factor on the receiving nodes. We
propose an upper confidence bound based online learning solution to estimate
the latent factors, and therefore the activation probabilities. Considerable
regret reduction is achieved by our factorization based online influence
maximization algorithm. And extensive empirical evaluations on two real-world
networks showed the effectiveness of our proposed solution."
"Residual Networks with convolutional layers are widely used in the field of
machine learning. Since they effectively extract features from input data by
stacking multiple layers, they can achieve high accuracy in many applications.
However, the stacking of many layers raises their computation costs. To address
this problem, we propose Network Implosion, it erases multiple layers from
Residual Networks without degrading accuracy. Our key idea is to introduce a
priority term that identifies the importance of a layer; we can select
unimportant layers according to the priority and erase them after the training.
In addition, we retrain the networks to avoid critical drops in accuracy after
layer erasure. A theoretical assessment reveals that our erasure and retraining
scheme can erase layers without accuracy drop, and achieve higher accuracy than
is possible with training from scratch. Our experiments show that Network
Implosion can, for classification on Cifar-10/100 and ImageNet, reduce the
number of layers by 24.00 to 42.86 percent without any drop in accuracy."
"Network pruning is a promising avenue for compressing deep neural networks. A
typical approach to pruning starts by training a model and then removing
redundant parameters while minimizing the impact on what is learned.
Alternatively, a recent approach shows that pruning can be done at
initialization prior to training, based on a saliency criterion called
connection sensitivity. However, it remains unclear exactly why pruning an
untrained, randomly initialized neural network is effective. In this work, by
noting connection sensitivity as a form of gradient, we formally characterize
initialization conditions to ensure reliable connection sensitivity
measurements, which in turn yields effective pruning results. Moreover, we
analyze the signal propagation properties of the resulting pruned networks and
introduce a simple, data-free method to improve their trainability. Our
modifications to the existing pruning at initialization method lead to improved
results on all tested network models for image classification tasks.
Furthermore, we empirically study the effect of supervision for pruning and
demonstrate that our signal propagation perspective, combined with unsupervised
pruning, can be useful in various scenarios where pruning is applied to
non-standard arbitrarily-designed architectures."
"Handwriting disorder (termed dysgraphia) is a far from a singular problem as
nearly 8.6% of the population in France is considered dysgraphic. Moreover,
research highlights the fundamental importance to detect and remediate these
handwriting difficulties as soon as possible as they may affect a child's
entire life, undermining performance and self-confidence in a wide variety of
school activities. At the moment, the detection of handwriting difficulties is
performed through a standard test called BHK. This detection, performed by
therapists, is laborious because of its high cost and subjectivity. We present
a digital approach to identify and characterize handwriting difficulties via a
Recurrent Neural Network model (RNN). The child under investigation is asked to
write on a graphics tablet all the letters of the alphabet as well as the ten
digits. Once complete, the RNN delivers a diagnosis in a few milliseconds and
demonstrates remarkable efficiency as it correctly identifies more than 90% of
children diagnosed as dysgraphic using the BHK test. The main advantage of our
tablet-based system is that it captures the dynamic features of writing --
something a human expert, such as a teacher, is unable to do. We show that
incorporating the dynamic information available by the use of tablet is highly
beneficial to our digital test to discriminate between typically-developing and
dysgraphic children."
"We consider in this paper the problem of optimal experiment design where a
decision maker can choose which points to sample to obtain an estimate
$\hat{\beta}$ of the hidden parameter $\beta^{\star}$ of an underlying linear
model. The key challenge of this work lies in the heteroscedasticity assumption
that we make, meaning that each covariate has a different and unknown variance.
The goal of the decision maker is then to figure out on the fly the optimal way
to allocate the total budget of $T$ samples between covariates, as sampling
several times a specific one will reduce the variance of the estimated model
around it (but at the cost of a possible higher variance elsewhere). By trying
to minimize the $\ell^2$-loss $\mathbb{E}
[\lVert\hat{\beta}-\beta^{\star}\rVert^2]$ the decision maker is actually
minimizing the trace of the covariance matrix of the problem, which corresponds
then to online A-optimal design. Combining techniques from bandit and convex
optimization we propose a new active sampling algorithm and we compare it with
existing ones. We provide theoretical guarantees of this algorithm in different
settings, including a $\mathcal{O}(T^{-2})$ regret bound in the case where the
covariates form a basis of the feature space, generalizing and improving
existing results. Numerical experiments validate our theoretical findings."
"Deep neural networks achieve stellar generalisation even when they have
enough parameters to easily fit all their training data. We study this
phenomenon by analysing the dynamics and the performance of over-parameterised
two-layer neural networks in the teacher-student setup, where one network, the
student, is trained on data generated by another network, called the teacher.
We show how the dynamics of stochastic gradient descent (SGD) is captured by a
set of differential equations and prove that this description is asymptotically
exact in the limit of large inputs. Using this framework, we calculate the
final generalisation error of student networks that have more parameters than
their teachers. We find that the final generalisation error of the student
increases with network size when training only the first layer, but stays
constant or even decreases with size when training both layers. We show that
these different behaviours have their root in the different solutions SGD finds
for different activation functions. Our results indicate that achieving good
generalisation in neural networks goes beyond the properties of SGD alone and
depends on the interplay of at least the algorithm, the model architecture, and
the data set."
"Building an open-domain conversational agent is a challenging problem.
Current evaluation methods, mostly post-hoc judgments of static conversation,
do not capture conversation quality in a realistic interactive context. In this
paper, we investigate interactive human evaluation and provide evidence for its
necessity; we then introduce a novel, model-agnostic, and dataset-agnostic
method to approximate it. In particular, we propose a self-play scenario where
the dialog system talks to itself and we calculate a combination of proxies
such as sentiment and semantic coherence on the conversation trajectory. We
show that this metric is capable of capturing the human-rated quality of a
dialog model better than any automated metric known to-date, achieving a
significant Pearson correlation (r>.7, p<.05). To investigate the strengths of
this novel metric and interactive evaluation in comparison to state-of-the-art
metrics and human evaluation of static conversations, we perform extended
experiments with a set of models, including several that make novel
improvements to recent hierarchical dialog generation architectures through
sentiment and semantic knowledge distillation on the utterance level. Finally,
we open-source the interactive evaluation platform we built and the dataset we
collected to allow researchers to efficiently deploy and evaluate dialog
models."
"The scope of research in the domain of activation functions remains limited
and centered around improving the ease of optimization or generalization
quality of neural networks (NNs). However, to develop a deeper understanding of
deep learning, it becomes important to look at the non linear component of NNs
more carefully. In this paper, we aim to provide a generic form of activation
function along with appropriate mathematical grounding so as to allow for
insights into the working of NNs in future. We propose ""Self-Learnable
Activation Functions"" (SLAF), which are learned during training and are capable
of approximating most of the existing activation functions. SLAF is given as a
weighted sum of pre-defined basis elements which can serve for a good
approximation of the optimal activation function. The coefficients for these
basis elements allow a search in the entire space of continuous functions
(consisting of all the conventional activations). We propose various training
routines which can be used to achieve performance with SLAF equipped neural
networks (SLNNs). We prove that SLNNs can approximate any neural network with
lipschitz continuous activations, to any arbitrary error highlighting their
capacity and possible equivalence with standard NNs. Also, SLNNs can be
completely represented as a collections of finite degree polynomial upto the
very last layer obviating several hyper parameters like width and depth. Since
the optimization of SLNNs is still a challenge, we show that using SLAF along
with standard activations (like ReLU) can provide performance improvements with
only a small increase in number of parameters."
"Polysomnography (PSG) is the gold standard for diagnosing sleep obstructive
apnea (OSA). It allows monitoring of breathing events throughout the night. The
detection of these events is usually done by trained sleep experts. However,
this task is tedious, highly time-consuming and subject to important
inter-scorer variability. In this study, we adapted our state-of-the-art deep
learning method for sleep event detection, DOSED, to the detection of sleep
breathing events in PSG for the diagnosis of OSA. We used a dataset of 52 PSG
recordings with apnea-hypopnea event scoring from 5 trained sleep experts. We
assessed the performance of the automatic approach and compared it to the
inter-scorer performance for both the diagnosis of OSA severity and, at the
microscale, for the detection of single breathing events. We observed that
human sleep experts reached an average accuracy of 75\% while the automatic
approach reached 81\% for sleep apnea severity diagnosis. The F1 score for
individual event detection was 0.55 for experts and 0.57 for the automatic
approach, on average. These results demonstrate that the automatic approach can
perform at a sleep expert level for the diagnosis of OSA."
"Expanding the receptive field to capture large-scale context is key to
obtaining good performance in dense prediction tasks, such as human pose
estimation. While many state-of-the-art fully-convolutional architectures
enlarge the receptive field by reducing resolution using strided convolution or
pooling layers, the most straightforward strategy is adopting large filters.
This, however, is costly because of the quadratic increase in the number of
parameters and multiply-add operations. In this work, we explore using
learnable box filters to allow for convolution with arbitrarily large kernel
size, while keeping the number of parameters per filter constant. In addition,
we use precomputed summed-area tables to make the computational cost of
convolution independent of the filter size. We adapt and incorporate the box
filter as a differentiable module in a fully-convolutional neural network, and
demonstrate its competitive performance on popular benchmarks for the task of
human pose estimation."
"In recent years, memory-augmented neural networks(MANNs) have shown promising
power to enhance the memory ability of neural networks for sequential
processing tasks. However, previous MANNs suffer from complex memory addressing
mechanism, making them relatively hard to train and causing computational
overheads. Moreover, many of them reuse the classical RNN structure such as
LSTM for memory processing, causing inefficient exploitations of memory
information. In this paper, we introduce a novel MANN, the Auto-addressing and
Recurrent Memory Integrating Network (ARMIN) to address these issues. The ARMIN
only utilizes hidden state ht for automatic memory addressing, and uses a novel
RNN cell for refined integration of memory information. Empirical results on a
variety of experiments demonstrate that the ARMIN is more light-weight and
efficient compared to existing memory networks. Moreover, we demonstrate that
the ARMIN can achieve much lower computational overhead than vanilla LSTM while
keeping similar performances. Codes are available on github.com/zoharli/armin."
"Due to a resource-constrained environment, network compression has become an
important part of deep neural networks research. In this paper, we propose a
new compression method, \textit{Inter-Layer Weight Prediction} (ILWP) and
quantization method which quantize the predicted residuals between the weights
in all convolution layers based on an inter-frame prediction method in
conventional video coding schemes. Furthermore, we found a phenomenon
\textit{Smoothly Varying Weight Hypothesis} (SVWH) which is that the weights in
adjacent convolution layers share strong similarity in shapes and values, i.e.,
the weights tend to vary smoothly along with the layers. Based on SVWH, we
propose a second ILWP and quantization method which quantize the predicted
residuals between the weights in adjacent convolution layers. Since the
predicted weight residuals tend to follow Laplace distributions with very low
variance, the weight quantization can more effectively be applied, thus
producing more zero weights and enhancing the weight compression ratio. In
addition, we propose a new \textit{inter-layer loss} for eliminating
non-texture bits, which enabled us to more effectively store only texture bits.
That is, the proposed loss regularizes the weights such that the collocated
weights between the adjacent two layers have the same values. Finally, we
propose an ILWP with an inter-layer loss and quantization method. Our
comprehensive experiments show that the proposed method achieves a much higher
weight compression rate at the same accuracy level compared with the previous
quantization-based compression methods in deep neural networks."
"Different neural network (NN) architectures have different advantages.
Convolutional neural networks (CNNs) achieved enormous success in computer
vision, while recurrent neural networks (RNNs) gained popularity in speech
recognition. It is not known which type of NN architecture is the best fit for
classification of communication signals. In this work, we compare the behavior
of fully-connected NN (FC), CNN, RNN, and bi-directional RNN (BiRNN) in a
spectrum sensing task. The four NN architectures are compared on their
detection performance, requirement of training data, computational complexity,
and memory requirement. Given abundant training data and computational and
memory resources, CNN, RNN, and BiRNN are shown to achieve similar performance.
The performance of FC is worse than that of the other three types, except in
the case where computational complexity is stringently limited."
"Epidemiologists use a variety of statistical algorithms for the early
detection of outbreaks. The practical usefulness of such methods highly depends
on the trade-off between the detection rate of outbreaks and the chances of
raising a false alarm. Recent research has shown that the use of machine
learning for the fusion of multiple statistical algorithms improves outbreak
detection. Instead of relying only on the binary output (alarm or no alarm) of
the statistical algorithms, we propose to make use of their p-values for
training a fusion classifier. In addition, we also show that adding additional
features and adapting the labeling of an epidemic period may further improve
performance. For comparison and evaluation, a new measure is introduced which
captures the performance of an outbreak detection method with respect to a low
rate of false alarms more precisely than previous works. Our results on
synthetic data show that it is challenging to improve the performance with a
trainable fusion method based on machine learning. In particular, the use of a
fusion classifier that is only based on binary outputs of the statistical
surveillance methods can make the overall performance worse than directly using
the underlying algorithms. However, the use of p-values and additional
information for the learning is promising, enabling to identify more valuable
patterns to detect outbreaks."
"Visual summarization of clinical data collected on patients contained within
the electronic health record (EHR) may enable precise and rapid triage at the
time of patient presentation to an emergency department (ED). The triage
process is critical in the appropriate allocation of resources and in
anticipating eventual patient disposition, typically admission to the hospital
or discharge home. EHR data are high-dimensional and complex, but offer the
opportunity to discover and characterize underlying data-driven patient
phenotypes. These phenotypes will enable improved, personalized therapeutic
decision making and prognostication. In this work, we focus on the challenge of
two-dimensional patient projections. A low dimensional embedding offers visual
interpretability lost in higher dimensions. While linear dimensionality
reduction techniques such as principal component analysis are often used
towards this aim, they are insufficient to describe the variance of patient
data. In this work, we employ the newly-described non-linear embedding
technique called uniform manifold approximation and projection (UMAP). UMAP
seeks to capture both local and global structures in high-dimensional data. We
then use Gaussian mixture models to identify clusters in the embedded data and
use the adjusted Rand index (ARI) to establish stability in the discovery of
these clusters. This technique is applied to five common clinical chief
complaints from a real-world ED EHR dataset, describing the emergent properties
of discovered clusters. We observe clinically-relevant cluster attributes,
suggesting that visual embeddings of EHR data using non-linear dimensionality
reduction is a promising approach to reveal data-driven patient phenotypes. In
the five chief complaints, we find between 2 and 6 clusters, with the peak mean
pairwise ARI between subsequent training iterations to range from 0.35 to 0.74."
"The problem of accelerating drug discovery relies heavily on automatic tools
to optimize precursor molecules to afford them with better biochemical
properties. Our work in this paper substantially extends prior state-of-the-art
on graph-to-graph translation methods for molecular optimization. In
particular, we realize coherent multi-resolution representations by
interweaving the encoding of substructure components with the atom-level
encoding of the original molecular graph. Moreover, our graph decoder is fully
autoregressive, and interleaves each step of adding a new substructure with the
process of resolving its attachment to the emerging molecule. We evaluate our
model on multiple molecular optimization tasks and show that our model
significantly outperforms previous state-of-the-art baselines."
"Although distributed computing can significantly reduce the training time of
deep neural networks, scaling the training process while maintaining high
efficiency and final accuracy is challenging. Distributed asynchronous training
enjoys near-linear speedup, but asynchrony causes gradient staleness - the main
difficulty in scaling stochastic gradient descent to large clusters. Momentum,
which is often used to accelerate convergence and escape local minima,
exacerbates the gradient staleness, thereby hindering convergence. We propose
DANA: a novel technique for asynchronous distributed SGD with momentum that
mitigates gradient staleness by computing the gradient on an estimated future
position of the model's parameters. Thereby, we show for the first time that
momentum can be fully incorporated in asynchronous training with almost no
ramifications to final accuracy. Our evaluation on the CIFAR and ImageNet
datasets shows that DANA outperforms existing methods, in both final accuracy
and convergence speed while scaling up to a total batch size of 16K on 64
asynchronous workers."
"This study develops an unsupervised learning algorithm for products of expert
capsules with dynamic routing. Analogous to binary-valued neurons in Restricted
Boltzmann Machines, the magnitude of a squashed capsule firing takes values
between zero and one, representing the probability of the capsule being on.
This analogy motivates the design of an energy function for capsule networks.
In order to have an efficient sampling procedure where hidden layer nodes are
not connected, the energy function is made consistent with dynamic routing in
the sense of the probability of a capsule firing, and inference on the capsule
network is computed with the dynamic routing between capsules procedure. In
order to optimize the log-likelihood of the visible layer capsules, the
gradient is found in terms of this energy function. The developed unsupervised
learning algorithm is used to train a capsule network on standard vision
datasets, and is able to generate realistic looking images from its learned
distribution."
"Most of the existing approaches focus on specific visual tasks while ignoring
the relations between them. Estimating task relation sheds light on the
learning of high-order semantic concepts, e.g., transfer learning. How to
reveal the underlying relations between different visual tasks remains largely
unexplored. In this paper, we propose a novel \textbf{L}earnable
\textbf{P}arameter \textbf{S}imilarity (\textbf{LPS}) method that learns an
effective metric to measure the similarity of second-order semantics hidden in
trained models. LPS is achieved by using a second-order neural network to align
high-dimensional model parameters and learning second-order similarity in an
end-to-end way. In addition, we create a model set called ModelSet500 as a
parameter similarity learning benchmark that contains 500 trained models.
Extensive experiments on ModelSet500 validate the effectiveness of the proposed
method. Code will be released at
\url{https://github.com/Wanggcong/learnable-parameter-similarity}."
"The increasing inclusion of Deep Learning (DL) models in safety-critical
systems such as autonomous vehicles have led to the development of multiple
model-based DL testing techniques. One common denominator of these testing
techniques is the automated generation of test cases, e.g., new inputs
transformed from the original training data with the aim to optimize some test
adequacy criteria. So far, the effectiveness of these approaches has been
hindered by their reliance on random fuzzing or transformations that do not
always produce test cases with a good diversity. To overcome these limitations,
we propose, DeepEvolution, a novel search-based approach for testing DL models
that relies on metaheuristics to ensure a maximum diversity in generated test
cases. We assess the effectiveness of DeepEvolution in testing computer-vision
DL models and found that it significantly increases the neuronal coverage of
generated test cases. Moreover, using DeepEvolution, we could successfully find
several corner-case behaviors. Finally, DeepEvolution outperformed Tensorfuzz
(a coverage-guided fuzzing tool developed at Google Brain) in detecting latent
defects introduced during the quantization of the models. These results suggest
that search-based approaches can help build effective testing tools for DL
systems."
"We present a pairwise learning to rank approach based on a neural net, called
DirectRanker, that generalizes the RankNet architecture. We show mathematically
that our model is reflexive, antisymmetric, and transitive allowing for
simplified training and improved performance. Experimental results on the LETOR
MSLR-WEB10K, MQ2007 and MQ2008 datasets show that our model outperforms
numerous state-of-the-art methods, while being inherently simpler in structure
and using a pairwise approach only."
"As artificial intelligence and machine learning algorithms make further
inroads into society, calls are increasing from multiple stakeholders for these
algorithms to explain their outputs. At the same time, these stakeholders,
whether they be affected citizens, government regulators, domain experts, or
system developers, present different requirements for explanations. Toward
addressing these needs, we introduce AI Explainability 360
(http://aix360.mybluemix.net/), an open-source software toolkit featuring eight
diverse and state-of-the-art explainability methods and two evaluation metrics.
Equally important, we provide a taxonomy to help entities requiring
explanations to navigate the space of explanation methods, not only those in
the toolkit but also in the broader literature on explainability. For data
scientists and other users of the toolkit, we have implemented an extensible
software architecture that organizes methods according to their place in the AI
modeling pipeline. We also discuss enhancements to bring research innovations
closer to consumers of explanations, ranging from simplified, more accessible
versions of algorithms, to tutorials and an interactive web demo to introduce
AI explainability to different audiences and application domains. Together, our
toolkit and taxonomy can help identify gaps where more explainability methods
are needed and provide a platform to incorporate them as they are developed."
"Capturing sentence semantics plays a vital role in a range of text mining
applications. Despite continuous efforts on the development of related datasets
and models in the general domain, both datasets and models are limited in
biomedical and clinical domains. The BioCreative/OHNLP organizers have made the
first attempt to annotate 1,068 sentence pairs from clinical notes and have
called for a community effort to tackle the Semantic Textual Similarity
(BioCreative/OHNLP STS) challenge. We developed models using traditional
machine learning and deep learning approaches. For the post challenge, we focus
on two models: the Random Forest and the Encoder Network. We applied sentence
embeddings pre-trained on PubMed abstracts and MIMIC-III clinical notes and
updated the Random Forest and the Encoder Network accordingly. The official
results demonstrated our best submission was the ensemble of eight models. It
achieved a Person correlation coefficient of 0.8328, the highest performance
among 13 submissions from 4 teams. For the post challenge, the performance of
both Random Forest and the Encoder Network was improved; in particular, the
correlation of the Encoder Network was improved by ~13%. During the challenge
task, no end-to-end deep learning models had better performance than machine
learning models that take manually-crafted features. In contrast, with the
sentence embeddings pre-trained on biomedical corpora, the Encoder Network now
achieves a correlation of ~0.84, which is higher than the original best model.
The ensembled model taking the improved versions of the Random Forest and
Encoder Network as inputs further increased performance to 0.8528. Deep
learning models with sentence embeddings pre-trained on biomedical corpora
achieve the highest performance on the test set."
"Credit scoring models support loan approval decisions in the financial
services industry. Lenders train these models on data from previously granted
credit applications, where the borrowers' repayment behavior has been observed.
This approach creates sample bias. The scoring model (i.e., classifier) is
trained on accepted cases only. Applying the resulting model to screen credit
applications from the population of all borrowers degrades model performance.
Reject inference comprises techniques to overcome sampling bias through
assigning labels to rejected cases. The paper makes two contributions. First,
we propose a self-learning framework for reject inference. The framework is
geared toward real-world credit scoring requirements through considering
distinct training regimes for iterative labeling and model training. Second, we
introduce a new measure to assess the effectiveness of reject inference
strategies. Our measure leverages domain knowledge to avoid artificial labeling
of rejected cases during strategy evaluation. We demonstrate this approach to
offer a robust and operational assessment of reject inference strategies.
Experiments on a real-world credit scoring data set confirm the superiority of
the adjusted self-learning framework over regular self-learning and previous
reject inference strategies. We also find strong evidence in favor of the
proposed evaluation measure assessing reject inference strategies more
reliably, raising the performance of the eventual credit scoring model."
"In this paper we present an early Apprenticeship Learning approach to mimic
the behaviour of different players in a short adaption of the interactive
fiction Anchorhead. Our motivation is the need to understand and simulate
player behaviour to create systems to aid the design and personalisation of
Interactive Narratives (INs). INs are partially observable for the players and
their goals are dynamic as a result. We used Receding Horizon IRL (RHIRL) to
learn players' goals in the form of reward functions, and derive policies to
imitate their behaviour. Our preliminary results suggest that RHIRL is able to
learn action sequences to complete a game, and provided insights towards
generating behaviour more similar to specific players."
"We present MDP Playground, a testbed for Reinforcement Learning (RL) agents
with dimensions of hardness that can be controlled independently to challenge
agents in different ways and obtain varying degrees of hardness in toy and
complex RL environments. We consider and allow control over a wide variety of
dimensions, including delayed rewards, sequence lengths, reward density,
stochasticity, image representations, irrelevant features, time unit, action
range and more. We define a parameterised collection of fast-to-run toy
environments in OpenAI Gym by varying these dimensions and propose to use these
to understand agents better. We then show how to design experiments using MDP
Playground to gain insights on the toy environments. We also provide wrappers
that can inject many of these dimensions into any Gym environment. We
experiment with these wrappers on Atari and Mujoco to allow for understanding
the effects of these dimensions on environments that are more complex than the
toy environments. We also compare the effect of the dimensions on the toy and
complex environments. Finally, we show how to use MDP Playground to debug
agents, to study the interaction of multiple dimensions and describe further
use-cases."
"We consider the problem of learning to play a repeated multi-agent game with
an unknown reward function. Single player online learning algorithms attain
strong regret bounds when provided with full information feedback, which
unfortunately is unavailable in many real-world scenarios. Bandit feedback
alone, i.e., observing outcomes only for the selected action, yields
substantially worse performance. In this paper, we consider a natural model
where, besides a noisy measurement of the obtained reward, the player can also
observe the opponents' actions. This feedback model, together with a regularity
assumption on the reward function, allows us to exploit the correlations among
different game outcomes by means of Gaussian processes (GPs). We propose a
novel confidence-bound based bandit algorithm GP-MW, which utilizes the GP
model for the reward function and runs a multiplicative weight (MW) method. We
obtain novel kernel-dependent regret bounds that are comparable to the known
bounds in the full information setting, while substantially improving upon the
existing bandit results. We experimentally demonstrate the effectiveness of
GP-MW in random matrix games, as well as real-world problems of traffic routing
and movie recommendation. In our experiments, GP-MW consistently outperforms
several baselines, while its performance is often comparable to methods that
have access to full information feedback."
"In this paper, we have proposed a deep quantum SVM formulation, and further
demonstrated a quantum-clustering framework based on the quantum deep SVM
formulation, deep convolutional neural networks, and quantum K-Means
clustering. We have investigated the run time computational complexity of the
proposed quantum deep clustering framework and compared with the possible
classical implementation. Our investigation shows that the proposed quantum
version of deep clustering formulation demonstrates a significant performance
gain (exponential speed up gains in many sections) against the possible
classical implementation. The proposed theoretical quantum deep clustering
framework is also interesting & novel research towards the quantum-classical
machine learning formulation to articulate the maximum performance."
"Objective: A median of 14.4% of patient undergone at least one adverse event
during surgery and a third of them are preventable. The occurrence of adverse
events forces surgeons to implement corrective strategies and, thus, deviate
from the standard surgical process. Therefore, it is clear that the automatic
identification of adverse events is a major challenge for patient safety. In
this paper, we have proposed a method enabling us to identify such deviations.
We have focused on identifying surgeons' deviations from standard surgical
processes due to surgical events rather than anatomic specificities. This is
particularly challenging, given the high variability in typical surgical
procedure workflows. Methods: We have introduced a new approach designed to
automatically detect and distinguish surgical process deviations based on
multi-dimensional non-linear temporal scaling with a hidden semi-Markov model
using manual annotation of surgical processes. The approach was then evaluated
using cross-validation. Results: The best results have over 90% accuracy.
Recall and precision were superior at 70%. We have provided a detailed analysis
of the incorrectly-detected observations. Conclusion: Multi-dimensional
non-linear temporal scaling with a hidden semi-Markov model provides promising
results for detecting deviations. Our error analysis of the
incorrectly-detected observations offers different leads in order to further
improve our method. Significance: Our method demonstrated the feasibility of
automatically detecting surgical deviations that could be implemented for both
skill analysis and developing situation awareness-based computer-assisted
surgical systems."
"Bipolar disorder (BPD) is a chronic mental illness characterized by extreme
mood and energy changes from mania to depression. These changes drive behaviors
that often lead to devastating personal or social consequences. BPD is managed
clinically with regular interactions with care providers, who assess mood,
energy levels, and the form and content of speech. Recent work has proposed
smartphones for monitoring mood using speech. However, these works do not
predict when to intervene. Predicting when to intervene is challenging because
there is not a single measure that is relevant for every person: different
individuals may have different levels of symptom severity considered typical.
Additionally, this typical mood, or baseline, may change over time, making a
single symptom threshold insufficient. This work presents an innovative
approach that expands clinical mood monitoring to predict when interventions
are necessary using an anomaly detection framework, which we call Temporal
Normalization. We first validate the model using a dataset annotated for
clinical interventions and then incorporate this method in a deep learning
framework to predict mood anomalies from natural, unstructured, telephone
speech data. The combination of these approaches provides a framework to enable
real-world speech-focused mood monitoring."
"The recent success of generative adversarial networks and variational
learning suggests training a classifier network may work well in addressing the
classical two-sample problem. Network-based tests have the computational
advantage that the algorithm scales to large samples. This paper proposes a
two-sample statistic which is the difference of the logit function, provided by
a trained classification neural network, evaluated on the testing set split of
the two datasets. Theoretically, we prove the testing power to differentiate
two sub-exponential densities given that the network is sufficiently
parametrized. When the two densities lie on or near to low-dimensional
manifolds embedded in possibly high-dimensional space, the needed network
complexity is reduced to only scale with the intrinsic dimensionality. Both the
approximation and estimation error analysis are based on a new result of
near-manifold integral approximation. In experiments, the proposed method
demonstrates better performance than previous network-based tests using
classification accuracy as the two-sample statistic, and compares favorably to
certain kernel maximum mean discrepancy tests on synthetic datasets and
hand-written digit datasets."
"While progress has been made in understanding the robustness of machine
learning classifiers to test-time adversaries (evasion attacks), fundamental
questions remain unresolved. In this paper, we use optimal transport to
characterize the minimum possible loss in an adversarial classification
scenario. In this setting, an adversary receives a random labeled example from
one of two classes, perturbs the example subject to a neighborhood constraint,
and presents the modified example to the classifier. We define an appropriate
cost function such that the minimum transportation cost between the
distributions of the two classes determines the minimum $0-1$ loss for any
classifier. When the classifier comes from a restricted hypothesis class, the
optimal transportation cost provides a lower bound. We apply our framework to
the case of Gaussian data with norm-bounded adversaries and explicitly show
matching bounds for the classification and transport problems as well as the
optimality of linear classifiers. We also characterize the sample complexity of
learning in this setting, deriving and extending previously known results as a
special case. Finally, we use our framework to study the gap between the
optimal classification performance possible and that currently achieved by
state-of-the-art robustly trained neural networks for datasets of interest,
namely, MNIST, Fashion MNIST and CIFAR-10."
"Recent theoretical work has guaranteed that overparameterized networks
trained by gradient descent achieve arbitrarily low training error, and
sometimes even low test error. The required width, however, is always
polynomial in at least one of the sample size $n$, the (inverse) target error
$1/\epsilon$, and the (inverse) failure probability $1/\delta$. This work shows
that $\widetilde{\Theta}(1/\epsilon)$ iterations of gradient descent with
$\widetilde{\Omega}(1/\epsilon^2)$ training examples on two-layer ReLU networks
of any width exceeding $\mathrm{polylog}(n,1/\epsilon,1/\delta)$ suffice to
achieve a test misclassification error of $\epsilon$. We also prove that
stochastic gradient descent can achieve $\epsilon$ test error with
polylogarithmic width and $\widetilde{\Theta}(1/\epsilon)$ samples. The
analysis relies upon the separation margin of the limiting kernel, which is
guaranteed positive, can distinguish between true labels and random labels, and
can give a tight sample-complexity analysis in the infinite-width setting"
"Despite alarm over the reliance of machine learning systems on so-called
spurious patterns, the term lacks coherent meaning in standard statistical
frameworks. However, the language of causality offers clarity: spurious
associations are due to confounding (e.g., a common cause), but not direct or
indirect causal effects. In this paper, we focus on natural language
processing, introducing methods and resources for training models less
sensitive to spurious patterns. Given documents and their initial labels, we
task humans with revising each document so that it (i) accords with a
counterfactual target label; (ii) retains internal coherence; and (iii) avoids
unnecessary changes. Interestingly, on sentiment analysis and natural language
inference tasks, classifiers trained on original data fail on their
counterfactually-revised counterparts and vice versa. Classifiers trained on
combined datasets perform remarkably well, just shy of those specialized to
either domain. While classifiers trained on either original or manipulated data
alone are sensitive to spurious features (e.g., mentions of genre), models
trained on the combined data are less sensitive to this signal. Both datasets
are publicly available."
"Our community has greatly improved the efficiency of deep learning
applications, including by exploiting sparsity in inputs. Most of that work,
though, is for inference, where weight sparsity is known statically, and/or for
specialized hardware. We propose a scheme to leverage dynamic sparsity during
training. In particular, we exploit zeros introduced by the ReLU activation
function to both feature maps and their gradients. This is challenging because
the sparsity degree is moderate and the locations of zeros change over time. We
also rely purely on software. We identify zeros in a dense data representation
without transforming the data and performs conventional vectorized computation.
Variations of the scheme are applicable to all major components of training:
forward propagation, backward propagation by inputs, and backward propagation
by weights. Our method significantly outperforms a highly-optimized dense
direct convolution on several popular deep neural networks. At realistic
sparsity, we speed up the training of the non-initial convolutional layers in
VGG16, ResNet-34, ResNet-50, and Fixup ResNet-50 by 2.19x, 1.37x, 1.31x, and
1.51x respectively on an Intel Skylake-X CPU."
"The K-means algorithm is a widely used clustering algorithm that offers
simplicity and efficiency. However, the traditional K-means algorithm uses the
random method to determine the initial cluster centers, which make clustering
results prone to local optima and then result in worse clustering performance.
Many initialization methods have been proposed, but none of them can
dynamically adapt to datasets with various characteristics. In our previous
research, an initialization method for K-means based on hybrid distance was
proposed, and this algorithm can adapt to datasets with different
characteristics. However, it has the following drawbacks: (a) When calculating
density, the threshold cannot be uniquely determined, resulting in unstable
results. (b) Heavily depending on adjusting the parameter, the parameter must
be adjusted five times to obtain better clustering results. (c) The time
complexity of the algorithm is quadratic, which is difficult to apply to large
datasets. In the current paper, we proposed an adaptive initialization method
for the K-means algorithm (AIMK) to improve our previous work. AIMK can not
only adapt to datasets with various characteristics but also obtain better
clustering results within two interactions. In addition, we then leverage
random sampling in AIMK, which is named as AIMK-RS, to reduce the time
complexity. AIMK-RS is easily applied to large and high-dimensional datasets.
We compared AIMK and AIMK-RS with 10 different algorithms on 16 normal and six
extra-large datasets. The experimental results show that AIMK and AIMK-RS
outperform the current initialization methods and several well-known clustering
algorithms. Furthermore, AIMK-RS can significantly reduce the complexity of
applying it to extra-large datasets with high dimensions. The time complexity
of AIMK-RS is O(n)."
"Starting with Gilmer et al. (2018), several works have demonstrated the
inevitability of adversarial examples based on different assumptions about the
underlying input probability space. It remains unclear, however, whether these
results apply to natural image distributions. In this work, we assume the
underlying data distribution is captured by some conditional generative model,
and prove intrinsic robustness bounds for a general class of classifiers, which
solves an open problem in Fawzi et al. (2018). Building upon the
state-of-the-art conditional generative models, we study the intrinsic
robustness of two common image benchmarks under $\ell_2$ perturbations, and
show the existence of a large gap between the robustness limits implied by our
theory and the adversarial robustness achieved by current state-of-the-art
robust models. Code for all our experiments is available at
https://github.com/xiaozhanguva/Intrinsic-Rob."
"The diagnosis, prognosis, and treatment of patients with musculoskeletal
(MSK) disorders require radiology imaging (using computed tomography, magnetic
resonance imaging(MRI), and ultrasound) and their precise analysis by expert
radiologists. Radiology scans can also help assessment of metabolic health,
aging, and diabetes. This study presents how machinelearning, specifically deep
learning methods, can be used for rapidand accurate image analysis of MRI
scans, an unmet clinicalneed in MSK radiology. As a challenging example, we
focus on automatic analysis of knee images from MRI scans and study machine
learning classification of various abnormalities including meniscus and
anterior cruciate ligament tears. Using widely used convolutional neural
network (CNN) based architectures, we comparatively evaluated the knee
abnormality classification performances of different neural network
architectures under limited imaging data regime and compared single and
multi-view imaging when classifying the abnormalities. Promising results
indicated the potential use of multi-view deep learning based classification of
MSK abnormalities in routine clinical assessment."
"Zero-shot learning (ZSL) algorithms typically work by exploiting attribute
correlations to be able to make predictions in unseen classes. However, these
correlations do not remain intact at test time in most practical settings and
the resulting change in these correlations lead to adverse effects on zero-shot
learning performance. In this paper, we present a new paradigm for ZSL that:
(i) utilizes the class-attribute mapping of unseen classes to estimate the
change in target distribution (target shift), and (ii) propose a novel
technique called grouped Adversarial Learning (gAL) to reduce negative effects
of this shift. Our approach is widely applicable for several existing ZSL
algorithms, including those with implicit attribute predictions. We apply the
proposed technique ($g$AL) on three popular ZSL algorithms: ALE, SJE, and
DEVISE, and show performance improvements on 4 popular ZSL datasets: AwA2, aPY,
CUB and SUN. We obtain SOTA results on SUN and aPY datasets and achieve
comparable results on AwA2."
"The understanding of the surrounding environment plays a critical role in
autonomous robotic systems, such as self-driving cars. Extensive research has
been carried out concerning visual perception. Yet, to obtain a more complete
perception of the environment, autonomous systems of the future should also
take acoustic information into account. Recent sound event localization and
detection (SELD) frameworks utilize convolutional recurrent neural networks
(CRNNs). However, considering the recurrent nature of CRNNs, it becomes
challenging to implement them efficiently on embedded hardware. Not only are
their computations strenuous to parallelize, but they also require high memory
bandwidth and large memory buffers. In this work, we develop a more robust and
hardware-friendly novel architecture based on a temporal convolutional
network(TCN). The proposed framework (SELD-TCN) outperforms the
state-of-the-art SELDnet performance on four different datasets. Moreover,
SELD-TCN achieves 4x faster training time per epoch and 40x faster inference
time on an ordinary graphics processing unit (GPU)."
"In one-class novelty detection, a model learns solely on the in-class data to
single out out-class instances. Autoencoder (AE) variants aim to compactly
model the in-class data to reconstruct it exclusively, thus differentiating the
in-class from out-class by the reconstruction error. However, compact modeling
in an improper way might collapse the latent representations of the in-class
data and thus their reconstruction, which would lead to performance
deterioration. Moreover, to properly measure the reconstruction error of
high-dimensional data, a metric is required that captures high-level semantics
of the data. To this end, we propose Discriminative Compact AE (DCAE) that
learns both compact and collapse-free latent representations of the in-class
data, thereby reconstructing them both finely and exclusively. In DCAE, (a) we
force a compact latent space to bijectively represent the in-class data by
reconstructing them through internal discriminative layers of generative
adversarial nets. (b) Based on the deep encoder's vulnerability to open set
risk, out-class instances are encoded into the same compact latent space and
reconstructed poorly without sacrificing the quality of in-class data
reconstruction. (c) In inference, the reconstruction error is measured by a
novel metric that computes the dissimilarity between a query and its
reconstruction based on the class semantics captured by the internal
discriminator. Extensive experiments on public image datasets validate the
effectiveness of our proposed model on both novelty and adversarial example
detection, delivering state-of-the-art performance."
"Thompson sampling is one of the most widely used algorithms for many online
decision problems, due to its simplicity in implementation and superior
empirical performance over other state-of-the-art methods. Despite its
popularity and empirical success, it has remained an open problem whether
Thompson sampling can match the minimax lower bound $\Omega(\sqrt{KT})$ for
$K$-armed bandit problems, where $T$ is the total time horizon. In this paper,
we solve this long open problem by proposing a variant of Thompson sampling
called MOTS that adaptively clips the sampling instance of the chosen arm at
each time step. We prove that this simple variant of Thompson sampling achieves
the minimax optimal regret bound $O(\sqrt{KT})$ for finite time horizon $T$, as
well as the asymptotic optimal regret bound for Gaussian rewards when $T$
approaches infinity. To our knowledge, MOTS is the first Thompson sampling type
algorithm that achieves the minimax optimality for multi-armed bandit problems."
"A variational autoencoder (VAE) derived from Tsallis statistics called q-VAE
is proposed. In the proposed method, a standard VAE is employed to
statistically extract latent space hidden in sampled data, and this latent
space helps make robots controllable in feasible computational time and cost.
To improve the usefulness of the latent space, this paper focuses on
disentangled representation learning, e.g., $\beta$-VAE, which is the baseline
for it. Starting from a Tsallis statistics perspective, a new lower bound for
the proposed q-VAE is derived to maximize the likelihood of the sampled data,
which can be considered an adaptive $\beta$-VAE with deformed Kullback-Leibler
divergence. To verify the benefits of the proposed q-VAE, a benchmark task to
extract the latent space from the MNIST dataset was performed. The results
demonstrate that the proposed q-VAE improved disentangled representation while
maintaining the reconstruction accuracy of the data. In addition, it relaxes
the independency condition between data, which is demonstrated by learning the
latent dynamics of nonlinear dynamical systems. By combining disentangled
representation, the proposed q-VAE achieves stable and accurate long-term state
prediction from the initial state and the action sequence.
  The dataset for hexapod walking is available on IEEE Dataport, doi:
https://dx.doi.org/10.21227/99af-jw71."
"Even though deep learning has shown unmatched performance on various tasks,
neural networks have been shown to be vulnerable to small adversarial
perturbations of the input that lead to significant performance degradation. In
this work we extend the idea of adding white Gaussian noise to the network
weights and activations during adversarial training (PNI) to the injection of
colored noise for defense against common white-box and black-box attacks. We
show that our approach outperforms PNI and various previous approaches in terms
of adversarial accuracy on CIFAR-10 and CIFAR-100 datasets. In addition, we
provide an extensive ablation study of the proposed method justifying the
chosen configurations."
"Recent improvements in Generative Adversarial Neural Networks (GANs) have
shown their ability to generate higher quality samples as well as to learn good
representations for transfer learning. Most of the representation learning
methods based on GANs learn representations ignoring their post-use scenario,
which can lead to increased generalisation ability. However, the model can
become redundant if it is intended for a specific task. For example, assume we
have a vast unlabelled audio dataset, and we want to learn a representation
from this dataset so that it can be used to improve the emotion recognition
performance of a small labelled audio dataset. During the representation
learning training, if the model does not know the post emotion recognition
task, it can completely ignore emotion-related characteristics in the learnt
representation. This is a fundamental challenge for any unsupervised
representation learning model. In this paper, we aim to address this challenge
by proposing a novel GAN framework: Guided Generative Neural Network (GGAN),
which guides a GAN to focus on learning desired representations and generating
superior quality samples for audio data leveraging fewer labelled samples.
Experimental results show that using a very small amount of labelled data as
guidance, a GGAN learns significantly better representations."
"Alterations in historical manuscripts such as letters represent a promising
field of research. On the one hand, they help understand the construction of
text. On the other hand, topics that are being considered sensitive at the time
of the manuscript gain coherence and contextuality when taking alterations into
account, especially in the case of deletions. The analysis of alterations in
manuscripts, though, is a traditionally very tedious work. In this paper, we
present a machine learning-based approach to help categorize alterations in
documents. In particular, we present a new probabilistic model (Alteration
Latent Dirichlet Allocation, alterLDA in the following) that categorizes
content-related alterations. The method proposed here is developed based on
experiments carried out on the digital scholarly edition Berlin Intellectuals,
for which alterLDA achieves high performance in the recognition of alterations
on labelled data. On unlabelled data, applying alterLDA leads to interesting
new insights into the alteration behavior of authors, editors and other
manuscript contributors, as well as insights into sensitive topics in the
correspondence of Berlin intellectuals around 1800. In addition to the findings
based on the digital scholarly edition Berlin Intellectuals, we present a
general framework for the analysis of text genesis that can be used in the
context of other digital resources representing document variants. To that end,
we present in detail the methodological steps that are to be followed in order
to achieve such results, giving thereby a prime example of an Machine Learning
application the Digital Humanities."
"Object frequency in the real world often follows a power law, leading to a
mismatch between datasets with long-tailed class distributions seen by a
machine learning model and our expectation of the model to perform well on all
classes. We analyze this mismatch from a domain adaptation point of view. First
of all, we connect existing class-balanced methods for long-tailed
classification to target shift, a well-studied scenario in domain adaptation.
The connection reveals that these methods implicitly assume that the training
data and test data share the same class-conditioned distribution, which does
not hold in general and especially for the tail classes. While a head class
could contain abundant and diverse training examples that well represent the
expected data at inference time, the tail classes are often short of
representative training data. To this end, we propose to augment the classic
class-balanced learning by explicitly estimating the differences between the
class-conditioned distributions with a meta-learning approach. We validate our
approach with six benchmark datasets and three loss functions."
"Training generative models that can generate high-quality text with
sufficient diversity is an important open problem for Natural Language
Generation (NLG) community. Recently, generative adversarial models have been
applied extensively on text generation tasks, where the adversarially trained
generators alleviate the exposure bias experienced by conventional maximum
likelihood approaches and result in promising generation quality. However, due
to the notorious defect of mode collapse for adversarial training, the
adversarially trained generators face a quality-diversity trade-off, i.e., the
generator models tend to sacrifice generation diversity severely for increasing
generation quality. In this paper, we propose a novel approach which aims to
improve the performance of adversarial text generation via efficiently
decelerating mode collapse of the adversarial training. To this end, we
introduce a cooperative training paradigm, where a language model is
cooperatively trained with the generator and we utilize the language model to
efficiently shape the data distribution of the generator against mode collapse.
Moreover, instead of engaging the cooperative update for the generator in a
principled way, we formulate a meta learning mechanism, where the cooperative
update to the generator serves as a high level meta task, with an intuition of
ensuring the parameters of the generator after the adversarial update would
stay resistant against mode collapse. In the experiment, we demonstrate our
proposed approach can efficiently slow down the pace of mode collapse for the
adversarial text generators. Overall, our proposed method is able to outperform
the baseline approaches with significant margins in terms of both generation
quality and diversity in the testified domains."
"Learning-to-rank (LTR) has become a key technology in E-commerce
applications. Most existing LTR approaches follow a supervised learning
paradigm from offline labeled data collected from the online system. However,
it has been noticed that previous LTR models can have a good validation
performance over offline validation data but have a poor online performance,
and vice versa, which implies a possible large inconsistency between the
offline and online evaluation. We investigate and confirm in this paper that
such inconsistency exists and can have a significant impact on AliExpress
Search. Reasons for the inconsistency include the ignorance of item context
during the learning, and the offline data set is insufficient for learning the
context. Therefore, this paper proposes an evaluator-generator framework for
LTR with item context. The framework consists of an evaluator that generalizes
to evaluate recommendations involving the context, and a generator that
maximizes the evaluator score by reinforcement learning, and a discriminator
that ensures the generalization of the evaluator. Extensive experiments in
simulation environments and AliExpress Search online system show that, firstly,
the classic data-based metrics on the offline dataset can show significant
inconsistency with online performance, and can even be misleading. Secondly,
the proposed evaluator score is significantly more consistent with the online
performance than common ranking metrics. Finally, as the consequence, our
method achieves a significant improvement (\textgreater$2\%$) in terms of
Conversion Rate (CR) over the industrial-level fine-tuned model in online A/B
tests."
"Maximizing utility with a budget constraint is the primary goal for
advertisers in real-time bidding (RTB) systems. The policy maximizing the
utility is referred to as the optimal bidding strategy. Earlier works on
optimal bidding strategy apply model-based batch reinforcement learning methods
which can not generalize to unknown budget and time constraint. Further, the
advertiser observes a censored market price which makes direct evaluation
infeasible on batch test datasets. Previous works ignore the losing auctions to
alleviate the difficulty with censored states; thus significantly modifying the
test distribution. We address the challenge of lacking a clear evaluation
procedure as well as the error propagated through batch reinforcement learning
methods in RTB systems. We exploit two conditional independence structures in
the sequential bidding process that allow us to propose a novel practical
framework using the maximum entropy principle to imitate the behavior of the
true distribution observed in real-time traffic. Moreover, the framework allows
us to train a model that can generalize to the unseen budget conditions than
limit only to those observed in history. We compare our methods on two
real-world RTB datasets with several baselines and demonstrate significantly
improved performance under various budget settings."
"Blind modulation classification is an important step to implement cognitive
radio networks. The multiple-input multiple-output (MIMO) technique is widely
used in military and civil communication systems. Due to the lack of prior
information about channel parameters and the overlapping of signals in the MIMO
systems, the traditional likelihood-based and feature-based approaches cannot
be applied in these scenarios directly. Hence, in this paper, to resolve the
problem of blind modulation classification in MIMO systems, the time-frequency
analysis method based on the windowed short-time Fourier transform is used to
analyse the time-frequency characteristics of time-domain modulated signals.
Then the extracted time-frequency characteristics are converted into RGB
spectrogram images, and the convolutional neural network based on transfer
learning is applied to classify the modulation types according to the RGB
spectrogram images. Finally, a decision fusion module is used to fuse the
classification results of all the receive antennas. Through simulations, we
analyse the classification performance at different signal-to-noise ratios
(SNRs), the results indicate that, for the single-input single-output (SISO)
network, our proposed scheme can achieve 92.37% and 99.12% average
classification accuracy at SNRs of -4 dB and 10 dB, respectively. For the MIMO
network, our scheme achieves 80.42% and 87.92% average classification accuracy
at -4 dB and 10 dB, respectively. This outperforms the existing classification
methods based on baseband signals."
"Rice is the second most important cereal crop worldwide, and the first in
terms of number of people who depend on it as a major staple food. Rice blast
disease is the most important biotic constraint of rice cultivation causing
each year millions of dollars of losses. Despite the efforts for breeding new
resistant varieties, agricultural practices and chemical control are still the
most important methods for disease management. Thus, rice blast forecasting is
a primary tool to support rice growers in controlling the disease. In this
study, we compared four models for predicting rice blast disease, two
operational process-based models (Yoshino and WARM) and two approaches based on
machine learning algorithms (M5Rules and RNN), the former inducing a rule-based
model and the latter building a neural network. In situ telemetry is important
to obtain quality in-field data for predictive models and this was a key aspect
of the RICE-GUARD project on which this study is based. According to the
authors, this is the first time process-based and machine learning modelling
approaches for supporting plant disease management are compared."
"Recently, label distribution learning (LDL) has drawn much attention in
machine learning, where LDL model is learned from labelel instances. Different
from single-label and multi-label annotations, label distributions describe the
instance by multiple labels with different intensities and accommodate to more
general scenes. Since most existing machine learning datasets merely provide
logical labels, label distributions are unavailable in many real-world
applications. To handle this problem, we propose two novel label enhancement
methods, i.e., Label Enhancement with Sample Correlations (LESC) and
generalized Label Enhancement with Sample Correlations (gLESC). More
specifically, LESC employs a low-rank representation of samples in the feature
space, and gLESC leverages a tensor multi-rank minimization to further
investigate the sample correlations in both the feature space and label space.
Benefitting from the sample correlations, the proposed methods can boost the
performance of label enhancement. Extensive experiments on 14 benchmark
datasets demonstrate the effectiveness and superiority of our methods."
"Active illumination is a prominent complement to enhance 2D face recognition
and make it more robust, e.g., to spoofing attacks and low-light conditions. In
the present work we show that it is possible to adopt active illumination to
enhance state-of-the-art 2D face recognition approaches with 3D features, while
bypassing the complicated task of 3D reconstruction. The key idea is to project
over the test face a high spatial frequency pattern, which allows us to
simultaneously recover real 3D information plus a standard 2D facial image.
Therefore, state-of-the-art 2D face recognition solution can be transparently
applied, while from the high frequency component of the input image,
complementary 3D facial features are extracted. Experimental results on ND-2006
dataset show that the proposed ideas can significantly boost face recognition
performance and dramatically improve the robustness to spoofing attacks."
"Speaker embeddings (x-vectors) extracted from very short segments of speech
have recently been shown to give competitive performance in speaker
diarization. We generalize this recipe by extracting from each speech segment,
in parallel with the x-vector, also a diagonal precision matrix, thus providing
a path for the propagation of information about the quality of the speech
segment into a PLDA scoring backend. These precisions quantify the uncertainty
about what the values of the embeddings might have been if they had been
extracted from high quality speech segments. The proposed probabilistic
embeddings (x-vectors with precisions) are interfaced with the PLDA model by
treating the x-vectors as hidden variables and marginalizing them out. We apply
the proposed probabilistic embeddings as input to an agglomerative hierarchical
clustering (AHC) algorithm to do diarization in the DIHARD'19 evaluation set.
We compute the full PLDA likelihood 'by the book' for each clustering
hypothesis that is considered by AHC. We do joint discriminative training of
the PLDA parameters and of the probabilistic x-vector extractor. We demonstrate
accuracy gains relative to a baseline AHC algorithm, applied to traditional
xvectors (without uncertainty), and which uses averaging of binary
log-likelihood-ratios, rather than by-the-book scoring."
"In the contemporary big data realm, Deep Neural Networks (DNNs) are evolving
towards more complex architectures to achieve higher inference accuracy. Model
compression techniques can be leveraged to efficiently deploy such
compute-intensive architectures on resource-limited mobile devices. Such
methods comprise various hyper-parameters that require per-layer customization
to ensure high accuracy. Choosing such hyper-parameters is cumbersome as the
pertinent search space grows exponentially with model layers. This paper
introduces GeneCAI, a novel optimization method that automatically learns how
to tune per-layer compression hyper-parameters. We devise a bijective
translation scheme that encodes compressed DNNs to the genotype space. The
optimality of each genotype is measured using a multi-objective score based on
accuracy and number of floating point operations. We develop customized genetic
operations to iteratively evolve the non-dominated solutions towards the
optimal Pareto front, thus, capturing the optimal trade-off between model
accuracy and complexity. GeneCAI optimization method is highly scalable and can
achieve a near-linear performance boost on distributed multi-GPU platforms. Our
extensive evaluations demonstrate that GeneCAI outperforms existing rule-based
and reinforcement learning methods in DNN compression by finding models that
lie on a better accuracy-complexity Pareto curve."
"Location is key to spatialize internet-of-things (IoT) data. However, it is
challenging to use low-cost IoT devices for robust unsupervised localization
(i.e., localization without training data that have known location labels).
Thus, this paper proposes a deep reinforcement learning (DRL) based
unsupervised wireless-localization method. The main contributions are as
follows. (1) This paper proposes an approach to model a continuous
wireless-localization process as a Markov decision process (MDP) and process it
within a DRL framework. (2) To alleviate the challenge of obtaining rewards
when using unlabeled data (e.g., daily-life crowdsourced data), this paper
presents a reward-setting mechanism, which extracts robust landmark data from
unlabeled wireless received signal strengths (RSS). (3) To ease requirements
for model re-training when using DRL for localization, this paper uses RSS
measurements together with agent location to construct DRL inputs. The proposed
method was tested by using field testing data from multiple Bluetooth 5 smart
ear tags in a pasture. Meanwhile, the experimental verification process
reflected the advantages and challenges for using DRL in wireless localization."
"Modern model-free reinforcement learning methods have recently demonstrated
impressive results on a number of problems. However, complex domains like
dexterous manipulation remain a challenge due to the high sample complexity. To
address this, current approaches employ expert demonstrations in the form of
state-action pairs, which are difficult to obtain for real-world settings such
as learning from videos. In this paper, we move toward a more realistic setting
and explore state-only imitation learning. To tackle this setting, we train an
inverse dynamics model and use it to predict actions for state-only
demonstrations. The inverse dynamics model and the policy are trained jointly.
Our method performs on par with state-action approaches and considerably
outperforms RL alone. By not relying on expert actions, we are able to learn
from demonstrations with different dynamics, morphologies, and objects. Videos
available at https://people.eecs.berkeley.edu/~ilija/soil ."
"It is well understood that a system built from individually fair components
may not itself be individually fair. In this work, we investigate individual
fairness under pipeline composition. Pipelines differ from ordinary sequential
or repeated composition in that individuals may drop out at any stage, and
classification in subsequent stages may depend on the remaining ""cohort"" of
individuals. As an example, a company might hire a team for a new project and
at a later point promote the highest performer on the team. Unlike other
repeated classification settings, where the degree of unfairness degrades
gracefully over multiple fair steps, the degree of unfairness in pipelines can
be arbitrary, even in a pipeline with just two stages.
  Guided by a panoply of real-world examples, we provide a rigorous framework
for evaluating different types of fairness guarantees for pipelines. We show
that na\""{i}ve auditing is unable to uncover systematic unfairness and that, in
order to ensure fairness, some form of dependence must exist between the design
of algorithms at different stages in the pipeline. Finally, we provide
constructions that permit flexibility at later stages, meaning that there is no
need to lock in the entire pipeline at the time that the early stage is
constructed."
"We study cost-effective communication strategies that can be used to improve
the performance of distributed learning systems in resource-constrained
environments. For distributed learning in sequential decision making, we
propose a new cost-effective partial communication protocol. We illustrate that
with this protocol the group obtains the same order of performance that it
obtains with full communication. Moreover, we prove that under the proposed
partial communication protocol the communication cost is $O(\log T)$, where $T$
is the time horizon of the decision-making process. This improves significantly
on protocols with full communication, which incur a communication cost that is
$O(T)$. We validate our theoretical results using numerical simulations."
"We consider the matrix completion problem of recovering a structured low rank
matrix with partially observed entries with mixed data types. Vast majority of
the solutions have proposed computationally feasible estimators with strong
statistical guarantees for the case where the underlying distribution of data
in the matrix is continuous. A few recent approaches have extended using
similar ideas these estimators to the case where the underlying distributions
belongs to the exponential family. Most of these approaches assume that there
is only one underlying distribution and the low rank constraint is regularized
by the matrix Schatten Norm. We propose a computationally feasible statistical
approach with strong recovery guarantees along with an algorithmic framework
suited for parallelization to recover a low rank matrix with partially observed
entries for mixed data types in one step. We also provide extensive simulation
evidence that corroborate our theoretical results."
"We consider the fundamental problem of ReLU regression, where the goal is to
output the best fitting ReLU with respect to square loss given access to draws
from some unknown distribution. We give the first efficient, constant-factor
approximation algorithm for this problem assuming the underlying distribution
satisfies some weak concentration and anti-concentration conditions (and
includes, for example, all log-concave distributions). This solves the main
open problem of Goel et al., who proved hardness results for any exact
algorithm for ReLU regression (up to an additive $\epsilon$). Using more
sophisticated techniques, we can improve our results and obtain a
polynomial-time approximation scheme for any subgaussian distribution. Given
the aforementioned hardness results, these guarantees can not be substantially
improved.
  Our main insight is a new characterization of surrogate losses for nonconvex
activations. While prior work had established the existence of convex
surrogates for monotone activations, we show that properties of the underlying
distribution actually induce strong convexity for the loss, allowing us to
relate the global minimum to the activation's Chow parameters."
"Graph Convolutional Network (GCN) has achieved extraordinary success in
learning effective task-specific representations of nodes in graphs. However,
regarding Heterogeneous Information Network (HIN), existing HIN-oriented GCN
methods still suffer from two deficiencies: (1) they cannot flexibly explore
all possible meta-paths and extract the most useful ones for a target object,
which hinders both effectiveness and interpretability; (2) they often need to
generate intermediate meta-path based dense graphs, which leads to high
computational complexity. To address the above issues, we propose an
interpretable and efficient Heterogeneous Graph Convolutional Network (ie-HGCN)
to learn the representations of objects in HINs. It is designed as a
hierarchical aggregation architecture, i.e., object-level aggregation first,
followed by type-level aggregation. The novel architecture can automatically
extract useful meta-paths for each object from all possible meta-paths (within
a length limit), which brings good model interpretability. It can also reduce
the computational cost by avoiding intermediate HIN transformation and
neighborhood attention. We provide theoretical analysis about the proposed
ie-HGCN in terms of evaluating the usefulness of all possible meta-paths, its
connection to the spectral graph convolution on HINs, and its quasi-linear time
complexity. Extensive experiments on three real network datasets demonstrate
the superiority of ie-HGCN over the state-of-the-art methods."
"Cells regulate themselves via dizzyingly complex biochemical processes called
signaling pathways. These are usually depicted as a network, where nodes
represent proteins and edges indicate their influence on each other. In order
to understand diseases and therapies at the cellular level, it is crucial to
have an accurate understanding of the signaling pathways at work. Since
signaling pathways can be modified by disease, the ability to infer signaling
pathways from condition- or patient-specific data is highly valuable. A variety
of techniques exist for inferring signaling pathways. We build on past works
that formulate signaling pathway inference as a Dynamic Bayesian Network
structure estimation problem on phosphoproteomic time course data. We take a
Bayesian approach, using Markov Chain Monte Carlo to estimate a posterior
distribution over possible Dynamic Bayesian Network structures. Our primary
contributions are (i) a novel proposal distribution that efficiently samples
sparse graphs and (ii) the relaxation of common restrictive modeling
assumptions. We implement our method, named Sparse Signaling Pathway Sampling,
in Julia using the Gen probabilistic programming language. Probabilistic
programming is a powerful methodology for building statistical models. The
resulting code is modular, extensible, and legible. The Gen language, in
particular, allows us to customize our inference procedure for biological
graphs and ensure efficient sampling. We evaluate our algorithm on simulated
data and the HPN-DREAM pathway reconstruction challenge, comparing our
performance against a variety of baseline methods. Our results demonstrate the
vast potential for probabilistic programming, and Gen specifically, for
biological network inference. Find the full codebase at
https://github.com/gitter-lab/ssps"
"Transfer learning (TL) utilizes data or knowledge from one or more source
domains to facilitate the learning in a target domain. It is particularly
useful when the target domain has very few or no labeled data, due to
annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of
TL is not always guaranteed. Negative transfer (NT), i.e., leveraging source
domain data/knowledge undesirably reduces the learning performance in the
target domain, has been a long-standing and challenging problem in TL. Various
approaches have been proposed in the literature to handle it. However, there
does not exist a systematic survey on the formulation of NT, the factors
leading to NT, and the algorithms that mitigate NT. This paper fills this gap,
by first introducing the definition of NT and its factors, then reviewing about
fifty representative approaches for overcoming NT, according to four
categories: secure transfer, domain similarity estimation, distant transfer,
and NT mitigation. NT in related fields, e.g., multi-task learning, lifelong
learning, and adversarial attacks, are also discussed."
"Prior works have found it beneficial to combine provably noise-robust loss
functions e.g., mean absolute error (MAE) with standard categorical loss
function e.g. cross entropy (CE) to improve their learnability. Here, we
propose to use Jensen-Shannon divergence as a noise-robust loss function and
show that it interestingly interpolate between CE and MAE with a controllable
mixing parameter. Furthermore, we make a crucial observation that CE exhibit
lower consistency around noisy data points. Based on this observation, we adopt
a generalized version of the Jensen-Shannon divergence for multiple
distributions to encourage consistency around data points. Using this loss
function, we show state-of-the-art results on both synthetic (CIFAR), and
real-world (e.g., WebVision) noise with varying noise rates."
"We present the first provable Least-Squares Value Iteration (LSVI) algorithms
that have runtime complexity sublinear in the number of actions. We formulate
the value function estimation procedure in value iteration as an approximate
maximum inner product search problem and propose a locality sensitive hashing
(LSH) [Indyk and Motwani STOC'98, Andoni and Razenshteyn STOC'15, Andoni,
Laarhoven, Razenshteyn and Waingarten SODA'17] type data structure to solve
this problem with sublinear time complexity. Moreover, we build the connections
between the theory of approximate maximum inner product search and the regret
analysis of reinforcement learning. We prove that, with our choice of
approximation factor, our Sublinear LSVI algorithms maintain the same regret as
the original LSVI algorithms while reducing the runtime complexity to sublinear
in the number of actions. To the best of our knowledge, this is the first work
that combines LSH with reinforcement learning resulting in provable
improvements. We hope that our novel way of combining data-structures and
iterative algorithm will open the door for further study into cost reduction in
optimization."
"Meta-learning synthesizes and leverages the knowledge from a given set of
tasks to rapidly learn new tasks using very little data. Meta-learning of
linear regression tasks, where the regressors lie in a low-dimensional
subspace, is an extensively-studied fundamental problem in this domain.
However, existing results either guarantee highly suboptimal estimation errors,
or require $\Omega(d)$ samples per task (where $d$ is the data dimensionality)
thus providing little gain over separately learning each task. In this work, we
study a simple alternating minimization method (MLLAM), which alternately
learns the low-dimensional subspace and the regressors. We show that, for a
constant subspace dimension MLLAM obtains nearly-optimal estimation error,
despite requiring only $\Omega(\log d)$ samples per task. However, the number
of samples required per task grows logarithmically with the number of tasks. To
remedy this in the low-noise regime, we propose a novel task subset selection
scheme that ensures the same strong statistical guarantee as MLLAM, even with
bounded number of samples per task for arbitrarily large number of tasks."
"We consider the statistical analysis of heterogeneous data for prediction in
situations where the observations include functions, typically time series. We
extend the modeling with Mixtures-of-Experts (ME), as a framework of choice in
modeling heterogeneity in data for prediction with vectorial observations, to
this functional data analysis context. We first present a new family of ME
models, named functional ME (FME) in which the predictors are potentially noisy
observations, from entire functions. Furthermore, the data generating process
of the predictor and the real response, is governed by a hidden discrete
variable representing an unknown partition. Second, by imposing sparsity on
derivatives of the underlying functional parameters via Lasso-like
regularizations, we provide sparse and interpretable functional representations
of the FME models called iFME. We develop dedicated expectation--maximization
algorithms for Lasso-like (EM-Lasso) regularized maximum-likelihood parameter
estimation strategies to fit the models. The proposed models and algorithms are
studied in simulated scenarios and in applications to two real data sets, and
the obtained results demonstrate their performance in accurately capturing
complex nonlinear relationships and in clustering the heterogeneous regression
data."
"Rapid progress in representation learning has led to a proliferation of
embedding models, and to associated challenges of model selection and practical
application. It is non-trivial to assess a model's generalizability to new,
candidate datasets and failure to generalize may lead to poor performance on
downstream tasks. Distribution shifts are one cause of reduced
generalizability, and are often difficult to detect in practice. In this paper,
we use the embedding space geometry to propose a non-parametric framework for
detecting distribution shifts, and specify two tests. The first test detects
shifts by establishing a robustness boundary, determined by an intelligible
performance criterion, for comparing reference and candidate datasets. The
second test detects shifts by featurizing and classifying multiple subsamples
of two datasets as in-distribution and out-of-distribution. In evaluation, both
tests detect model-impacting distribution shifts, in various shift scenarios,
for both synthetic and real-world datasets."
"Understanding the asymptotic behavior of gradient-descent training of deep
neural networks is essential for revealing inductive biases and improving
network performance. We derive the infinite-time training limit of a
mathematically tractable class of deep nonlinear neural networks, gated linear
networks (GLNs), and generalize these results to gated networks described by
general homogeneous polynomials. We study the implications of our results,
focusing first on two-layer GLNs. We then apply our theoretical predictions to
GLNs trained on MNIST and show how architectural constraints and the implicit
bias of gradient descent affect performance. Finally, we show that our theory
captures a substantial portion of the inductive bias of ReLU networks. By
making the inductive bias explicit, our framework is poised to inform the
development of more efficient, biologically plausible, and robust learning
algorithms."
"Distributed Gaussian process (DGP) is a popular approach to scale GP to big
data which divides the training data into some subsets, performs local
inference for each partition, and aggregates the results to acquire global
prediction. To combine the local predictions, the conditional independence
assumption is used which basically means there is a perfect diversity between
the subsets. Although it keeps the aggregation tractable, it is often violated
in practice and generally yields poor results. In this paper, we propose a
novel approach for aggregating the Gaussian experts' predictions by Gaussian
graphical model (GGM) where the target aggregation is defined as an unobserved
latent variable and the local predictions are the observed variables. We first
estimate the joint distribution of latent and observed variables using the
Expectation-Maximization (EM) algorithm. The interaction between experts can be
encoded by the precision matrix of the joint distribution and the aggregated
predictions are obtained based on the property of conditional Gaussian
distribution. Using both synthetic and real datasets, our experimental
evaluations illustrate that our new method outperforms other state-of-the-art
DGP approaches."
"Previous partial permutation synchronization (PPS) algorithms, which are
commonly used for multi-object matching, often involve computation-intensive
and memory-demanding matrix operations. These operations become intractable for
large scale structure-from-motion datasets. For pure permutation
synchronization, the recent Cycle-Edge Message Passing (CEMP) framework
suggests a memory-efficient and fast solution. Here we overcome the restriction
of CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for
estimating the corruption levels of the observed partial permutations. It
allows us to subsequently implement a nonconvex weighted projected power method
without the need of spectral initialization. The resulting new PPS algorithm,
MatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse
matrix operations, and thus enjoys lower time and space complexities in
comparison to previous PPS algorithms. We prove that under adversarial
corruption, though without additive noise and with certain assumptions,
CEMP-Partial is able to exactly classify corrupted and clean partial
permutations. We demonstrate the state-of-the-art accuracy, speed and memory
efficiency of our method on both synthetic and real datasets."
"Images have become one of the most popular types of media through which users
convey their emotions within online social networks. Although vast amount of
research is devoted to sentiment analysis of textual data, there has been very
limited work that focuses on analyzing sentiment of image data. In this work,
we propose a novel visual sentiment prediction framework that performs image
understanding with Deep Convolutional Neural Networks (CNN). Specifically, the
proposed sentiment prediction framework performs transfer learning from a CNN
with millions of parameters, which is pre-trained on large-scale data for
object recognition. Experiments conducted on two real-world datasets from
Twitter and Tumblr demonstrate the effectiveness of the proposed visual
sentiment analysis framework."
"We consider the following general hidden hubs model: an $n \times n$ random
matrix $A$ with a subset $S$ of $k$ special rows (hubs): entries in rows
outside $S$ are generated from the probability distribution $p_0 \sim
N(0,\sigma_0^2)$; for each row in $S$, some $k$ of its entries are generated
from $p_1 \sim N(0,\sigma_1^2)$, $\sigma_1>\sigma_0$, and the rest of the
entries from $p_0$. The problem is to identify the high-degree hubs
efficiently. This model includes and significantly generalizes the planted
Gaussian Submatrix Model, where the special entries are all in a $k \times k$
submatrix. There are two well-known barriers: if $k\geq c\sqrt{n\ln n}$, just
the row sums are sufficient to find $S$ in the general model. For the submatrix
problem, this can be improved by a $\sqrt{\ln n}$ factor to $k \ge c\sqrt{n}$
by spectral methods or combinatorial methods. In the variant with $p_0=\pm 1$
(with probability $1/2$ each) and $p_1\equiv 1$, neither barrier has been
broken.
  We give a polynomial-time algorithm to identify all the hidden hubs with high
probability for $k \ge n^{0.5-\delta}$ for some $\delta >0$, when
$\sigma_1^2>2\sigma_0^2$. The algorithm extends to the setting where planted
entries might have different variances each at least as large as $\sigma_1^2$.
We also show a nearly matching lower bound: for $\sigma_1^2 \le 2\sigma_0^2$,
there is no polynomial-time Statistical Query algorithm for distinguishing
between a matrix whose entries are all from $N(0,\sigma_0^2)$ and a matrix with
$k=n^{0.5-\delta}$ hidden hubs for any $\delta >0$. The lower bound as well as
the algorithm are related to whether the chi-squared distance of the two
distributions diverges. At the critical value $\sigma_1^2=2\sigma_0^2$, we show
that the general hidden hubs problem can be solved for $k\geq c\sqrt n(\ln
n)^{1/4}$, improving on the naive row sum-based method."
"In cardiac magnetic resonance imaging, fully-automatic segmentation of the
heart enables precise structural and functional measurements to be taken, e.g.
from short-axis MR images of the left-ventricle. In this work we propose a
recurrent fully-convolutional network (RFCN) that learns image representations
from the full stack of 2D slices and has the ability to leverage inter-slice
spatial dependences through internal memory units. RFCN combines anatomical
detection and segmentation into a single architecture that is trained
end-to-end thus significantly reducing computational time, simplifying the
segmentation pipeline, and potentially enabling real-time applications. We
report on an investigation of RFCN using two datasets, including the publicly
available MICCAI 2009 Challenge dataset. Comparisons have been carried out
between fully convolutional networks and deep restricted Boltzmann machines,
including a recurrent version that leverages inter-slice spatial correlation.
Our studies suggest that RFCN produces state-of-the-art results and can
substantially improve the delineation of contours near the apex of the heart."
"Model-based collaborative filtering analyzes user-item interactions to infer
latent factors that represent user preferences and item characteristics in
order to predict future interactions. Most collaborative filtering algorithms
assume that these latent factors are static, although it has been shown that
user preferences and item perceptions drift over time. In this paper, we
propose a conjugate and numerically stable dynamic matrix factorization (DCPF)
based on compound Poisson matrix factorization that models the smoothly
drifting latent factors using Gamma-Markov chains. We propose a numerically
stable Gamma chain construction, and then present a stochastic variational
inference approach to estimate the parameters of our model. We apply our model
to time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF
achieves a higher predictive accuracy than state-of-the-art static and dynamic
factorization models."
"Across many scientific domains, there is a common need to automatically
extract a simplified view or coarse-graining of how a complex system's
components interact. This general task is called community detection in
networks and is analogous to searching for clusters in independent vector data.
It is common to evaluate the performance of community detection algorithms by
their ability to find so-called ""ground truth"" communities. This works well in
synthetic networks with planted communities because such networks' links are
formed explicitly based on those known communities. However, there are no
planted communities in real world networks. Instead, it is standard practice to
treat some observed discrete-valued node attributes, or metadata, as ground
truth. Here, we show that metadata are not the same as ground truth, and that
treating them as such induces severe theoretical and practical problems. We
prove that no algorithm can uniquely solve community detection, and we prove a
general No Free Lunch theorem for community detection, which implies that there
can be no algorithm that is optimal for all possible community detection tasks.
However, community detection remains a powerful tool and node metadata still
have value so a careful exploration of their relationship with network
structure can yield insights of genuine worth. We illustrate this point by
introducing two statistical techniques that can quantify the relationship
between metadata and community structure for a broad class of models. We
demonstrate these techniques using both synthetic and real-world networks, and
for multiple types of metadata and community structure."
"Spectral clustering methods which are frequently used in clustering and
community detection applications are sensitive to the specific graph
constructions particularly when imbalanced clusters are present. We show that
ratio cut (RCut) or normalized cut (NCut) objectives are not tailored to
imbalanced cluster sizes since they tend to emphasize cut sizes over cut
values. We propose a graph partitioning problem that seeks minimum cut
partitions under minimum size constraints on partitions to deal with imbalanced
cluster sizes. Our approach parameterizes a family of graphs by adaptively
modulating node degrees on a fixed node set, yielding a set of parameter
dependent cuts reflecting varying levels of imbalance. The solution to our
problem is then obtained by optimizing over these parameters. We present
rigorous limit cut analysis results to justify our approach and demonstrate the
superiority of our method through experiments on synthetic and real datasets
for data clustering, semi-supervised learning and community detection."
"Early results in using convolutional neural networks (CNNs) on x-rays to
diagnose disease have been promising, but it has not yet been shown that models
trained on x-rays from one hospital or one group of hospitals will work equally
well at different hospitals. Before these tools are used for computer-aided
diagnosis in real-world clinical settings, we must verify their ability to
generalize across a variety of hospital systems. A cross-sectional design was
used to train and evaluate pneumonia screening CNNs on 158,323 chest x-rays
from NIH (n=112,120 from 30,805 patients), Mount Sinai (42,396 from 12,904
patients), and Indiana (n=3,807 from 3,683 patients). In 3 / 5 natural
comparisons, performance on chest x-rays from outside hospitals was
significantly lower than on held-out x-rays from the original hospital systems.
CNNs were able to detect where an x-ray was acquired (hospital system, hospital
department) with extremely high accuracy and calibrate predictions accordingly.
The performance of CNNs in diagnosing diseases on x-rays may reflect not only
their ability to identify disease-specific imaging findings on x-rays, but also
their ability to exploit confounding information. Estimates of CNN performance
based on test data from hospital systems used for model training may overstate
their likely real-world performance."
"The task of translating between programming languages differs from the
challenge of translating natural languages in that programming languages are
designed with a far more rigid set of structural and grammatical rules.
Previous work has used a tree-to-tree encoder/decoder model to take advantage
of the inherent tree structure of programs during translation. Neural decoders,
however, by default do not exploit known grammar rules of the target language.
In this paper, we describe a tree decoder that leverages knowledge of a
language's grammar rules to exclusively generate syntactically correct
programs. We find that this grammar-based tree-to-tree model outperforms the
state of the art tree-to-tree model in translating between two programming
languages on a previously used synthetic task."
"Recommender systems can be formulated as a matrix completion problem,
predicting ratings from user and item parameter vectors. Optimizing these
parameters by subsampling data becomes difficult as the number of users and
items grows. We develop a novel approach to generate all latent variables on
demand from the ratings matrix itself and a fixed pool of parameters. We
estimate missing ratings using chains of evidence that link them to a small set
of prototypical users and items. Our model automatically addresses the
cold-start and online learning problems by combining information across both
users and items. We investigate the scaling behavior of this model, and
demonstrate competitive results with respect to current matrix factorization
techniques in terms of accuracy and convergence speed."
"This master thesis focuses on practical application of Convolutional Neural
Network models on the task of road labeling with bike attractivity score. We
start with an abstraction of real world locations into nodes and scored edges
in partially annotated dataset. We enhance information available about each
edge with photographic data from Google Street View service and with additional
neighborhood information from Open Street Map database. We teach a model on
this enhanced dataset and experiment with ImageNet Large Scale Visual
Recognition Competition. We try different dataset enhancing techniques as well
as various model architectures to improve road scoring. We also make use of
transfer learning to use features from a task with rich dataset of ImageNet
into our task with smaller number of images, to prevent model overfitting."
"Jets from boosted heavy particles have a typical angular scale which can be
used to distinguish them from QCD jets. We introduce a machine learning
strategy for jet substructure analysis using a spectral function on the angular
scale. The angular spectrum allows us to scan energy deposits over the angle
between a pair of particles in a highly visual way. We set up an artificial
neural network (ANN) to find out characteristic shapes of the spectra of the
jets from heavy particle decays. By taking the Higgs jets and QCD jets as
examples, we show that the ANN of the angular spectrum input has similar
performance to existing taggers. In addition, some improvement is seen when
additional extra radiations occur. Notably, the new algorithm automatically
combines the information of the multi-point correlations in the jet."
"Seismic phase association is a fundamental task in seismology that pertains
to linking together phase detections on different sensors that originate from a
common earthquake. It is widely employed to detect earthquakes on permanent and
temporary seismic networks, and underlies most seismicity catalogs produced
around the world. This task can be challenging because the number of sources is
unknown, events frequently overlap in time, or can occur simultaneously in
different parts of a network. We present PhaseLink, a framework based on recent
advances in deep learning for grid-free earthquake phase association. Our
approach learns to link phases together that share a common origin, and is
trained entirely on tens of millions of synthetic sequences of P- and S-wave
arrival times generated using a simple 1D velocity model. Our approach is
simple to implement for any tectonic regime, suitable for real-time processing,
and can naturally incorporate errors in arrival time picks. Rather than tuning
a set of ad hoc hyperparameters to improve performance, PhaseLink can be
improved by simply adding examples of problematic cases to the training
dataset. We demonstrate the state-of-the-art performance of PhaseLink on a
challenging recent sequence from southern California, and synthesized sequences
from Japan designed to test the point at which the method fails. For the
examined datasets, PhaseLink can precisely associate P- and S-picks to events
that are separated by ~12 seconds in origin time. This approach is expected to
improve the resolution of seismicity catalogs, add stability to real-time
seismic monitoring, and streamline automated processing of large seismic
datasets."
"PARyOpt is a python based implementation of the Bayesian optimization routine
designed for remote and asynchronous function evaluations. Bayesian
optimization is especially attractive for computational optimization due to its
low cost function footprint as well as the ability to account for uncertainties
in data. A key challenge to efficiently deploy any optimization strategy on
distributed computing systems is the synchronization step, where data from
multiple function calls is assimilated to identify the next campaign of
function calls. Bayesian optimization provides an elegant approach to overcome
this issue via asynchronous updates. We formulate, develop and implement a
parallel, asynchronous variant of Bayesian optimization. The framework is
robust and resilient to external failures. We show how such asynchronous
evaluations help reduce the total optimization wall clock time for a suite of
test problems. Additionally, we show how the software design of the framework
allows easy extension to response surface reconstruction (Kriging), providing a
high performance software for autonomous exploration. The software is available
on PyPI, with examples and documentation."
"We consider the problem of generating automatic code given sample
input-output pairs. We train a neural network to map from the current state and
the outputs to the program's next statement. The neural network optimizes
multiple tasks concurrently: the next operation out of a set of high level
commands, the operands of the next statement, and which variables can be
dropped from memory. Using our method we are able to create programs that are
more than twice as long as existing state-of-the-art solutions, while improving
the success rate for comparable lengths, and cutting the run-time by two orders
of magnitude. Our code, including an implementation of various literature
baselines, is publicly available at https://github.com/amitz25/PCCoder"
"Ever since the proof of asymptotic normality of maximum likelihood estimator
by Cramer (1946), it has been understood that a basic technique of the Taylor
series expansion suffices for asymptotics of $M$-estimators with
smooth/differentiable loss function. Although the Taylor series expansion is a
purely deterministic tool, the realization that the asymptotic normality
results can also be made deterministic (and so finite sample) received far less
attention. With the advent of big data and high-dimensional statistics, the
need for finite sample results has increased. In this paper, we use the
(well-known) Banach fixed point theorem to derive various deterministic
inequalities that lead to the classical results when studied under randomness.
In addition, we provide applications of these deterministic inequalities for
crossvalidation/subsampling, marginal screening and uniform-in-submodel results
that are very useful for post-selection inference and in the study of
post-regularization estimators. Our results apply to many classical estimators,
in particular, generalized linear models, non-linear regression and cox
proportional hazards model. Extensions to non-smooth and constrained problems
are also discussed."
"Talent search and recommendation systems at LinkedIn strive to match the
potential candidates to the hiring needs of a recruiter or a hiring manager
expressed in terms of a search query or a job posting. Recent work in this
domain has mainly focused on linear models, which do not take complex
relationships between features into account, as well as ensemble tree models,
which introduce non-linearity but are still insufficient for exploring all the
potential feature interactions, and strictly separate feature generation from
modeling. In this paper, we present the results of our application of deep and
representation learning models on LinkedIn Recruiter. Our key contributions
include: (i) Learning semantic representations of sparse entities within the
talent search domain, such as recruiter ids, candidate ids, and skill entity
ids, for which we utilize neural network models that take advantage of LinkedIn
Economic Graph, and (ii) Deep models for learning recruiter engagement and
candidate response in talent search applications. We also explore learning to
rank approaches applied to deep models, and show the benefits for the talent
search use case. Finally, we present offline and online evaluation results for
LinkedIn talent search and recommendation systems, and discuss potential
challenges along the path to a fully deep model architecture. The challenges
and approaches discussed generalize to any multi-faceted search engine."
"The Kak family of neural networks is able to learn patterns quickly, and this
speed of learning can be a decisive advantage over other competing models in
many applications. Amongst the implementations of these networks are those
using reconfigurable networks, FPGAs and optical networks. In some
applications, it is useful to use complex data, and it is with that in mind
that this introduction to the basic Kak network with complex inputs is being
presented. The training algorithm is prescriptive and the network weights are
assigned simply upon examining the inputs. The input is mapped using quaternary
encoding for purpose of efficienty. This network family is part of a larger
hierarchy of learning schemes that include quantum models."
"A primary motivation for research in Digital Ecosystems is the desire to
exploit the self-organising properties of natural ecosystems. Ecosystems arc
thought to be robust, scalable architectures that can automatically solve
complex, dynamic problems. However, the biological processes that contribute to
these properties have not been made explicit in Digital Ecosystem research.
Here, we introduce how biological properties contribute to the self-organising
features of natural ecosystems. These properties include populations of
evolving agents, a complex dynamic environment, and spatial distributions which
generate local interactions. The potential for exploiting these properties in
artificial systems is then considered."
"With this paper, we contribute to the understanding of ant colony
optimization (ACO) algorithms by formally analyzing their runtime behavior. We
study simple MAX-MIN ant systems on the class of linear pseudo-Boolean
functions defined on binary strings of length 'n'. Our investigations point out
how the progress according to function values is stored in pheromone. We
provide a general upper bound of O((n^3 \log n)/ \rho) for two ACO variants on
all linear functions, where (\rho) determines the pheromone update strength.
Furthermore, we show improved bounds for two well-known linear pseudo-Boolean
functions called OneMax and BinVal and give additional insights using an
experimental study."
"This paper presents some properties of unary coding of significance for
biological learning and instantaneously trained neural networks."
"The article presents new results on the use of variable thresholds to
increase the capacity of a feedback neural network. Non-binary networks are
also considered in this analysis."
"We present an analysis of the performance of an elitist Evolutionary
algorithm using a recombination operator known as 1-Bit-Swap on the Royal Roads
test function based on a population. We derive complete, approximate and
asymptotic convergence rates for the algorithm. The complete model shows the
benefit of the size of the population and re- combination pool."
"The analysis of randomized search heuristics on classes of functions is
fundamental for the understanding of the underlying stochastic process and the
development of suitable proof techniques. Recently, remarkable progress has
been made in bounding the expected optimization time of the simple (1+1) EA on
the class of linear functions. We improve the best known bound in this setting
from $(1.39+o(1))en\ln n$ to $en\ln n+O(n)$ in expectation and with high
probability, which is tight up to lower-order terms. Moreover, upper and lower
bounds for arbitrary mutations probabilities $p$ are derived, which imply
expected polynomial optimization time as long as $p=O((\ln n)/n)$ and which are
tight if $p=c/n$ for a constant $c$. As a consequence, the standard mutation
probability $p=1/n$ is optimal for all linear functions, and the (1+1) EA is
found to be an optimal mutation-based algorithm. The proofs are based on
adaptive drift functions and the recent multiplicative drift theorem."
"Power dissipation in sequential circuits is due to increased toggling count
of Circuit under Test, which depends upon test vectors applied. If successive
test vectors sequences have more toggling nature then it is sure that toggling
rate of flip flops is higher. Higher toggling for flip flops results more power
dissipation. To overcome this problem, one method is to use GA to have test
vectors of high fault coverage in short interval, followed by Hamming distance
management on test patterns. This approach is time consuming and needs more
efforts. Another method which is purposed in this paper is a PSO based Frame
Work to optimize power dissipation. Here target is to set the entire test
vector in a frame for time period 'T', so that the frame consists of all those
vectors strings which not only provide high fault coverage but also arrange
vectors in frame to produce minimum toggling."
"Evolutionary computation techniques have mostly been used to solve various
optimization and learning problems successfully. Evolutionary algorithm is more
effective to gain optimal solution(s) to solve complex problems than
traditional methods. In case of problems with large set of parameters,
evolutionary computation technique incurs a huge computational burden for a
single processing unit. Taking this limitation into account, this paper
presents a new distributed evolutionary computation technique, which decomposes
decision vectors into smaller components and achieves optimal solution in a
short time. In this technique, a Jacobi-based Time Variant Adaptive (JBTVA)
Hybrid Evolutionary Algorithm is distributed incorporating cluster computation.
Moreover, two new selection methods named Best All Selection (BAS) and Twin
Selection (TS) are introduced for selecting best fit solution vector.
Experimental results show that optimal solution is achieved for different kinds
of problems having huge parameters and a considerable speedup is obtained in
proposed distributed system."
"Prediction of annual rice production in all the 31 districts of Tamilnadu is
an important decision for the Government of Tamilnadu. Rice production is a
complex process and non linear problem involving soil, crop, weather, pest,
disease, capital, labour and management parameters. ANN software was designed
and developed with Feed Forward Back Propagation (FFBP) network to predict rice
production. The input layer has six independent variables like area of
cultivation and rice production in three seasons like Kuruvai, Samba and Kodai.
The popular sigmoid activation function was adopted to convert input data into
sigmoid values. The hidden layer computes the summation of six sigmoid values
with six sets of weightages. The final output was converted into sigmoid values
using a sigmoid transfer function. ANN outputs are the predicted results. The
error between original data and ANN output values were computed. A threshold
value of 10-9 was used to test whether the error is greater than the threshold
level. If the error is greater than threshold then updating of weights was done
all summations were done by back propagation. This process was repeated until
error equal to zero. The predicted results were printed and it was found to be
exactly matching with the expected values. It shows that the ANN prediction was
100% accurate."
"For simple digital circuits, conventional method of designing circuits can
easily be applied. But for complex digital circuits, the conventional method of
designing circuits is not fruitfully applicable because it is time-consuming.
On the contrary, Genetic Programming is used mostly for automatic program
generation. The modern approach for designing Arithmetic circuits, commonly
digital circuits, is based on Graphs. This graph-based evolutionary design of
arithmetic circuits is a method of optimized designing of arithmetic circuits.
In this paper, a new technique for evolutionary design of digital circuits is
proposed using Genetic Programming (GP) with Subtree Mutation in place of
Graph-based design. The results obtained using this technique demonstrates the
potential capability of genetic programming in digital circuit design with
limited computer algorithms. The proposed technique, helps to simplify and
speed up the process of designing digital circuits, discovers a variation in
the field of digital circuit design where optimized digital circuits can be
successfully and effectively designed."
"In last decades optimization and control of complex systems that possessed
various conflicted objectives simultaneously attracted an incremental interest
of scientists. This is because of the vast applications of these systems in
various fields of real life engineering phenomena that are generally multi
modal, non convex and multi criterion. Hence, many researchers utilized
versatile intelligent models such as Pareto based techniques, game theory
(cooperative and non cooperative games), neuro evolutionary systems, fuzzy
logic and advanced neural networks for handling these types of problems. In
this paper a novel method called Synchronous Self Learning Pareto Strategy
Algorithm (SSLPSA) is presented which utilizes Evolutionary Computing (EC),
Swarm Intelligence (SI) techniques and adaptive Classical Self Organizing Map
(CSOM) simultaneously incorporating with a data shuffling behavior.
Evolutionary Algorithms (EA) which attempt to simulate the phenomenon of
natural evolution are powerful numerical optimization algorithms that reach an
approximate global maximum of a complex multi variable function over a wide
search space and swarm base technique can improved the intensity and the
robustness in EA. CSOM is a neural network capable of learning and can improve
the quality of obtained optimal Pareto front. To prove the efficient
performance of proposed algorithm, authors utilized some well known benchmark
test functions. Obtained results indicate that the cited method is best suit in
the case of vector optimization."
"Block matching (BM) motion estimation plays a very important role in video
coding. In a BM approach, image frames in a video sequence are divided into
blocks. For each block in the current frame, the best matching block is
identified inside a region of the previous frame, aiming to minimize the sum of
absolute differences (SAD). Unfortunately, the SAD evaluation is
computationally expensive and represents the most consuming operation in the BM
process. Therefore, BM motion estimation can be approached as an optimization
problem, where the goal is to find the best matching block within a search
space. The simplest available BM method is the full search algorithm (FSA)
which finds the most accurate motion vector through an exhaustive computation
of SAD values for all elements of the search window. Recently, several fast BM
algorithms have been proposed to reduce the number of SAD operations by
calculating only a fixed subset of search locations at the price of poor
accuracy. In this paper, a new algorithm based on Artificial Bee Colony (ABC)
optimization is proposed to reduce the number of search locations in the BM
process. In our algorithm, the computation of search locations is drastically
reduced by considering a fitness calculation strategy which indicates when it
is feasible to calculate or only estimate new search locations. Since the
proposed algorithm does not consider any fixed search pattern or any other
movement assumption as most of other BM approaches do, a high probability for
finding the true minimum (accurate motion vector) is expected. Conducted
simulations show that the proposed method achieves the best balance over other
fast BM algorithms, in terms of both estimation accuracy and computational
cost."
"Echo state networks (ESN), a type of reservoir computing (RC) architecture,
are efficient and accurate artificial neural systems for time series processing
and learning. An ESN consists of a core of recurrent neural networks, called a
reservoir, with a small number of tunable parameters to generate a
high-dimensional representation of an input, and a readout layer which is
easily trained using regression to produce a desired output from the reservoir
states. Certain computational tasks involve real-time calculation of high-order
time correlations, which requires nonlinear transformation either in the
reservoir or the readout layer. Traditional ESN employs a reservoir with
sigmoid or tanh function neurons. In contrast, some types of biological neurons
obey response curves that can be described as a product unit rather than a sum
and threshold. Inspired by this class of neurons, we introduce a RC
architecture with a reservoir of product nodes for time series computation. We
find that the product RC shows many properties of standard ESN such as
short-term memory and nonlinear capacity. On standard benchmarks for chaotic
prediction tasks, the product RC maintains the performance of a standard
nonlinear ESN while being more amenable to mathematical analysis. Our study
provides evidence that such networks are powerful in highly nonlinear tasks
owing to high-order statistics generated by the recurrent product node
reservoir."
"We present a hardware architecture that uses the Neural Engineering Framework
(NEF) to implement large-scale neural networks on Field Programmable Gate
Arrays (FPGAs) for performing pattern recognition in real time. NEF is a
framework that is capable of synthesising large-scale cognitive systems from
subnetworks. We will first present the architecture of the proposed neural
network implemented using fixed-point numbers and demonstrate a routine that
computes the decoding weights by using the online pseudoinverse update method
(OPIUM) in a parallel and distributed manner. The proposed system is
efficiently implemented on a compact digital neural core. This neural core
consists of 64 neurons that are instantiated by a single physical neuron using
a time-multiplexing approach. As a proof of concept, we combined 128 identical
neural cores together to build a handwritten digit recognition system using the
MNIST database and achieved a recognition rate of 96.55%. The system is
implemented on a state-of-the-art FPGA and can process 5.12 million digits per
second. The architecture is not limited to handwriting recognition, but is
generally applicable as an extremely fast pattern recognition processor for
various kinds of patterns such as speech and images."
"There has been significant research over the past two decades in developing
new platforms for spiking neural computation. Current neural computers are
primarily developed to mimick biology. They use neural networks which can be
trained to perform specific tasks to mainly solve pattern recognition problems.
These machines can do more than simulate biology, they allow us to re-think our
current paradigm of computation. The ultimate goal is to develop brain inspired
general purpose computation architectures that can breach the current
bottleneck introduced by the Von Neumann architecture. This work proposes a new
framework for such a machine. We show that the use of neuron like units with
precise timing representation, synaptic diversity, and temporal delays allows
us to set a complete, scalable compact computation framework. The presented
framework provides both linear and non linear operations, allowing us to
represent and solve any function. We show usability in solving real use cases
from simple differential equations to sets of non-linear differential equations
leading to chaotic attractors."
"Because stochastic gradient descent (SGD) has shown promise optimizing neural
networks with millions of parameters and few if any alternatives are known to
exist, it has moved to the heart of leading approaches to reinforcement
learning (RL). For that reason, the recent result from OpenAI showing that a
particular kind of evolution strategy (ES) can rival the performance of
SGD-based deep RL methods with large neural networks provoked surprise. This
result is difficult to interpret in part because of the lingering ambiguity on
how ES actually relates to SGD. The aim of this paper is to significantly
reduce this ambiguity through a series of MNIST-based experiments designed to
uncover their relationship. As a simple supervised problem without domain noise
(unlike in most RL), MNIST makes it possible (1) to measure the correlation
between gradients computed by ES and SGD and (2) then to develop an SGD-based
proxy that accurately predicts the performance of different ES population
sizes. These innovations give a new level of insight into the real capabilities
of ES, and lead also to some unconventional means for applying ES to supervised
problems that shed further light on its differences from SGD. Incorporating
these lessons, the paper concludes by demonstrating that ES can achieve 99%
accuracy on MNIST, a number higher than any previously published result for any
evolutionary method. While not by any means suggesting that ES should
substitute for SGD in supervised learning, the suite of experiments herein
enables more informed decisions on the application of ES within RL and other
paradigms."
"It is often the case that the performance of a neural network can be improved
by adding layers. In real-world practices, we always train dozens of neural
network architectures in parallel which is a wasteful process. We explored
$CompNet$, in which case we morph a well-trained neural network to a deeper one
where network function can be preserved and the added layer is compact. The
work of the paper makes two contributions: a). The modified network can
converge fast and keep the same functionality so that we do not need to train
from scratch again; b). The layer size of the added layer in the neural network
is controlled by removing the redundant parameters with sparse optimization.
This differs from previous network morphism approaches which tend to add more
neurons or channels beyond the actual requirements and result in redundance of
the model. The method is illustrated using several neural network structures on
different data sets including MNIST and CIFAR10."
"This paper proposes an efficient unsupervised method for detecting relevant
changes between two temporally different images of the same scene. A
convolutional neural network (CNN) for semantic segmentation is implemented to
extract compressed image features, as well as to classify the detected changes
into the correct semantic classes. A difference image is created using the
feature map information generated by the CNN, without explicitly training on
target difference images. Thus, the proposed change detection method is
unsupervised, and can be performed using any CNN model pre-trained for semantic
segmentation."
"We review neural network architectures which were motivated by Fourier series
and integrals and which are referred to as Fourier neural networks. These
networks are empirically evaluated in synthetic and real-world tasks. Neither
of them outperforms the standard neural network with sigmoid activation
function in the real-world tasks. All neural networks, both Fourier and the
standard one, empirically demonstrate lower approximation error than the
truncated Fourier series when it comes to an approximation of a known function
of multiple variables."
"Multidimensional genetic programming represents candidate solutions as sets
of programs, and thereby provides an interesting framework for exploiting
building block identification. Towards this goal, we investigate the use of
machine learning as a way to bias which components of programs are promoted,
and propose two semantic operators to choose where useful building blocks are
placed during crossover. A forward stagewise crossover operator we propose
leads to significant improvements on a set of regression problems, and produces
state-of-the-art results in a large benchmark study. We discuss this
architecture and others in terms of their propensity for allowing heuristic
search to utilize information during the evolutionary process. Finally, we look
at the collinearity and complexity of the data representations that result from
these architectures, with a view towards disentangling factors of variation in
application."
"The main feature of the Dynamic Multi-objective Optimization Problems (DMOPs)
is that optimization objective functions will change with times or
environments. One of the promising approaches for solving the DMOPs is reusing
the obtained Pareto optimal set (POS) to train prediction models via machine
learning approaches. In this paper, we train an Incremental Support Vector
Machine (ISVM) classifier with the past POS, and then the solutions of the DMOP
we want to solve at the next moment are filtered through the trained ISVM
classifier. A high-quality initial population will be generated by the ISVM
classifier, and a variety of different types of population-based dynamic
multi-objective optimization algorithms can benefit from the population. To
verify this idea, we incorporate the proposed approach into three evolutionary
algorithms, the multi-objective particle swarm optimization(MOPSO),
Nondominated Sorting Genetic Algorithm II (NSGA-II), and the Regularity
Model-based multi-objective estimation of distribution algorithm(RE-MEDA). We
employ experiments to test these algorithms, and experimental results show the
effectiveness."
"Estimation of Distribution Algorithms (EDAs) are one branch of Evolutionary
Algorithms (EAs) in the broad sense that they evolve a probabilistic model
instead of a population. Many existing algorithms fall into this category.
Analogous to genetic drift in EAs, EDAs also encounter the phenomenon that
updates of the probabilistic model not justified by the fitness move the
sampling frequencies to the boundary values. This can result in a considerable
performance loss.
  This paper proves the first sharp estimates of the boundary hitting time of
the sampling frequency of a neutral bit for several univariate EDAs. For the
UMDA that selects $\mu$ best individuals from $\lambda$ offspring each
generation, we prove that the expected first iteration when the frequency of
the neutral bit leaves the middle range $[\tfrac 14, \tfrac 34]$ and the
expected first time it is absorbed in 0 or 1 are both $\Theta(\mu)$. The
corresponding hitting times are $\Theta(K^2)$ for the cGA with hypothetical
population size $K$. This paper further proves that for PBIL with parameters
$\mu$, $\lambda$, and $\rho$, in an expected number of $\Theta(\mu/\rho^2)$
iterations the sampling frequency of a neutral bit leaves the interval
$[\Theta(\rho/\mu),1-\Theta(\rho/\mu)]$ and then always the same value is
sampled for this bit, that is, the frequency approaches the corresponding
boundary value with maximum speed.
  For the lower bounds implicit in these statements, we also show exponential
tail bounds. If a bit is not neutral, but neutral or has a preference for ones,
then the lower bounds on the times to reach a low frequency value still hold.
An analogous statement holds for bits that are neutral or prefer the value
zero."
"This contribution introduces the GTOPX space mission benchmark collection,
which is an extension of GTOP database published by the European Space Agency
(ESA). GTOPX consists of ten individual benchmark instances representing
real-world interplanetary space trajectory design problems. In regard to the
original GTOP collection, GTOPX includes three new problem instances featuring
mixed-integer and multi-objective properties. GTOPX enables a simplified user
handling, unified benchmark function call and some minor bug corrections to the
original GTOP implementation. Furthermore, GTOPX is linked from it's original
C++ source code to Python and Matlab based on dynamic link libraries, assuring
computationally fast and accurate reproduction of the benchmark results in all
three programming languages. Space mission trajectory design problems as those
represented in GTOPX are known to be highly non-linear and difficult to solve.
The GTOPX collection, therefore, aims particularly at researchers wishing to
put advanced (meta)heuristic and hybrid optimization algorithms to the test.
The goal of this paper is to provide researchers with a manual and reference to
the newly available GTOPX benchmark software."
"NM-landscapes have been recently introduced as a class of tunable rugged
models. They are a subset of the general interaction models where all the
interactions are of order less or equal $M$. The Boltzmann distribution has
been extensively applied in single-objective evolutionary algorithms to
implement selection and study the theoretical properties of model-building
algorithms. In this paper we propose the combination of the multi-objective
NM-landscape model and the Boltzmann distribution to obtain Pareto-front
approximations. We investigate the joint effect of the parameters of the
NM-landscapes and the probabilistic factorizations in the shape of the Pareto
front approximations."
"How to maintain relative high diversity is important to avoid premature
convergence in population-based optimization methods. Island model is widely
considered as a major approach to achieve this because of its flexibility and
high efficiency. The model maintains a group of sub-populations on different
islands and allows sub-populations to interact with each other via predefined
migration policies. However, current island model has some drawbacks. One is
that after a certain number of generations, different islands may retain quite
similar, converged sub-populations thereby losing diversity and decreasing
efficiency. Another drawback is that determining the number of islands to
maintain is also very challenging. Meanwhile initializing many sub-populations
increases the randomness of island model. To address these issues, we proposed
a dynamic island model~(DIM-SP) which can force each island to maintain
different sub-populations, control the number of islands dynamically and starts
with one sub-population. The proposed island model outperforms the other three
state-of-the-art island models in three baseline optimization problems
including job shop scheduler problem, travelling salesmen problem and quadratic
multiple knapsack problem."
"Random weight change (RWC) algorithm is extremely component and robust for
the hardware implementation of neural networks. RWC and Genetic algorithm (GA)
are well known methodologies used for optimizing and learning the neural
network (NN). Individually, each of these two algorithms has its strength and
weakness along with separate objectives. However, recently, researchers combine
these two algorithms for better learning and optimization of NN. In this paper,
we proposed a methodology by combining the RWC and GA, namely Genetic Random
Weight Change (GRWC), as well as demonstrate a seminal way to reduce the
complexity of the neural network by removing weak weights of GRWC. In contrast
to RWC and GA, GRWC contains an effective optimization procedure which is
worthy at exploring a large and complex space in intellectual strategies
influenced by the GA/RWC synergy. The learning behavior of the proposed
algorithm was tested on MNIST dataset and it was able to prove its performance."
"Recently, the discretization of decision and objective spaces has been
discussed in the literature. In some studies, it is shown that the decision
space discretization improves the performance of evolutionary multi-objective
optimization (EMO) algorithms on continuous multi-objective test problems. In
other studies, it is shown that the objective space discretization improves the
performance on combinatorial multi-objective problems. However, the effect of
the simultaneous discretization of both spaces has not been examined in the
literature. In this paper, we examine the effects of the decision space
discretization, objective space discretization and simultaneous discretization
on the performance of NSGA-II through computational experiments on the DTLZ and
WFG problems. Using various settings about the number of decision variables and
the number of objectives, our experiments are performed on four types of
problems: standard problems, large-scale problems, many-objective problems, and
large-scale many-objective problems. We show that the decision space
discretization has a positive effect for large-scale problems and the objective
space discretization has a positive effect for many-objective problems. We also
show the discretization of both spaces is useful for large-scale many-objective
problems."
"A recent metaheuristic algorithm, such as Whale Optimization Algorithm (WOA),
was proposed. The idea of proposing this algorithm belongs to the hunting
behavior of the humpback whale. However, WOA suffers from poor performance in
the exploitation phase and stagnates in the local best solution. Grey Wolf
Optimization (GWO) is a very competitive algorithm comparing to other common
metaheuristic algorithms as it has a super performance in the exploitation
phase while it is tested on unimodal benchmark functions. Therefore, the aim of
this paper is to hybridize GWO with WOA to overcome the problems. GWO can
perform well in exploiting optimal solutions. In this paper, a hybridized WOA
with GWO which is called WOAGWO is presented. The proposed hybridized model
consists of two steps. Firstly, the hunting mechanism of GWO is embedded into
the WOA exploitation phase with a new condition which is related to GWO.
Secondly, a new technique is added to the exploration phase to improve the
solution after each iteration. Experimentations are tested on three different
standard test functions which are called benchmark functions: 23 common
functions, 25 CEC2005 functions and 10 CEC2019 functions. The proposed WOAGWO
is also evaluated against original WOA, GWO and three other commonly used
algorithms. Results show that WOAGWO outperforms other algorithms depending on
the Wilcoxon rank-sum test. Finally, WOAGWO is likewise applied to solve an
engineering problem such as pressure vessel design. Then the results prove that
WOAGWO achieves optimum solution which is better than WOA and Fitness Dependent
Optimizer (FDO)."
"A multi-modal multi-objective optimization problem is a special kind of
multi-objective optimization problem with multiple Pareto subsets. In this
paper, we propose an efficient multi-modal multi-objective optimization
algorithm based on the widely used MOEA/D algorithm. In our proposed algorithm,
each weight vector has its own sub-population. With a clearing mechanism and a
greedy removal strategy, our proposed algorithm can effectively preserve
equivalent Pareto optimal solutions (i.e., different Pareto optimal solutions
with same objective values). Experimental results show that our proposed
algorithm can effectively preserve the diversity of solutions in the decision
space when handling large-scale multi-modal multi-objective optimization
problems."
"Evolutionary algorithms (EAs) are general-purpose problem solvers that
usually perform an unbiased search. This is reasonable and desirable in a
black-box scenario. For combinatorial optimization problems, often more
knowledge about the structure of optimal solutions is given, which can be
leveraged by means of biased search operators. We consider the Minimum Spanning
Tree (MST) problem in a single- and multi-objective version, and introduce a
biased mutation, which puts more emphasis on the selection of edges of low rank
in terms of low domination number. We present example graphs where the biased
mutation can significantly speed up the expected runtime until (Pareto-)optimal
solutions are found. On the other hand, we demonstrate that bias can lead to
exponential runtime if heavy edges are necessarily part of an optimal solution.
However, on general graphs in the single-objective setting, we show that a
combined mutation operator which decides for unbiased or biased edge selection
in each step with equal probability exhibits a polynomial upper bound -- as
unbiased mutation -- in the worst case and benefits from bias if the
circumstances are favorable."
"Many real-world optimisation problems involve dynamic and stochastic
components. While problems with multiple interacting components are omnipresent
in inherently dynamic domains like supply-chain optimisation and logistics,
most research on dynamic problems focuses on single-component problems. With
this article, we define a number of scenarios based on the Travelling Thief
Problem to enable research on the effect of dynamic changes to sub-components.
Our investigations of 72 scenarios and seven algorithms show that -- depending
on the instance, the magnitude of the change, and the algorithms in the
portfolio -- it is preferable to either restart the optimisation from scratch
or to continue with the previously valid solutions."
"Backpropagation algorithms on recurrent artificial neural networks require an
unfolding of accumulated states over time. These states must be kept in memory
for an undefined period of time which is task-dependent. This paper uses the
reservoir computing paradigm where an untrained recurrent neural network layer
is used as a preprocessor stage to learn temporal and limited data. These
so-called reservoirs require either extensive fine-tuning or neuroplasticity
with unsupervised learning rules. We propose a new local plasticity rule named
P-CRITICAL designed for automatic reservoir tuning that translates well to
Intel's Loihi research chip, a recent neuromorphic processor. We compare our
approach on well-known datasets from the machine learning community while using
a spiking neuronal architecture. We observe an improved performance on tasks
coming from various modalities without the need to tune parameters. Such
algorithms could be a key to end-to-end energy-efficient neuromorphic-based
machine learning on edge devices."
"In previous work I proposed a framework for thinking about open-ended
evolution. The framework characterised the basic processes required for
Darwinian evolution as: (1) the generation of a phenotype from a genetic
description; (2) the evaluation of that phenotype; and (3) the reproduction
with variation of successful genotype-phenotypes. My treatment emphasized the
potential influence of the biotic and abiotic environment, and of the laws of
physics/chemistry, on each of these processes. I demonstrated the conditions
under which these processes can allow for ongoing exploration of a space of
possible phenotypes (which I labelled exploratory open-endedness). However,
these processes by themselves cannot expand the space of possible phenotypes
and therefore cannot account for the more interesting and unexpected kinds of
evolutionary innovation (such as those I labelled expansive and
transformational open-endedness). In the previous work I looked at ways in
which expansive and transformational innovations could arise. I proposed
transdomain bridges and non-additive compositional systems as two mechanisms by
which these kinds of innovations could arise. In the current paper I wish to
generalise and expand upon these two concepts. I do this by adopting the
Parameter Space-Organisation Space-Action Space (POA) perspective, as suggested
at in my previous work, and proposing that all evolutionary innovations can be
viewed as either capturing some novel physical phenomena that had previously
been unused, or as the creation of new persistent systems within the
environment."
"According to the published papers and books since the turn of the century,
Pareto optimization is the dominating assessment method for multi-objective
nonlinear optimization problems treated by population-based optimizers like
Evolutionary Algorithms. However, is it always the method of choice for
real-world applications, where either more than four objectives have to be
considered, or the same type of task is repeated again and again with only
minor modifications, in an automated optimization or planning process? This
paper presents a classification of application scenarios and compares the
Pareto approach with an extended version of the weighted sum, called cascaded
weighted sum, for the different scenarios. Its range of application within the
field of multi-objective optimization is discussed as well as its strengths and
weaknesses."
"This Paper will deal with a combination of Ant Colony and Genetic Programming
Algorithm to optimize Travelling Salesmen problem (NP-Hard). However, the
complexity of the algorithm requires considerable computational time and
resources. Parallel implementation can reduce the computational time. In this
paper, emphasis in the parallelizing section is given to Multi-core
architecture and Multi-Processor Systems which is developed and used almost
everywhere today and hence, multi-core parallelization to the combination of
algorithm is achieved by OpenMP library by Intel Corporation."
"This paper examines the use of artificial neural network approach in
identifying the origin of clove buds based on metabolites composition.
Generally, large data sets are critical for accurate identification. Machine
learning with large data sets lead to precise identification based on origins.
However, clove buds uses small data sets due to lack of metabolites composition
and their high cost of extraction. The results show that backpropagation and
resilient propagation with one and two hidden layers identifies clove buds
origin accurately. The backpropagation with one hidden layer offers 99.91% and
99.47% for training and testing data sets, respectively. The resilient
propagation with two hidden layers offers 99.96% and 97.89% accuracy for
training and testing data sets, respectively."
"Mutation and recombination operators play a key role in determining the speed
and quality of Genetic and Evolutionary Algorithms (GEAs). Prior work has
analyzed the effects of these operators on genotypic variation, often using
locality metrics that measure the sensitivity and stability of
genotype-phenotype representations to these operators.
  In this paper, we focus on an important subset of representations, namely
nonredundant bitstring-to-integer representations, and analyze them through the
lens of Rothlauf's widely used locality metrics. We first define locality
metrics equivalent to Rothlauf's that are tailored to our domain: the
\textit{point locality} for single-bit mutation and \textit{general locality}
for recombination. With these definitions, we derive tight bounds and a closed
form expected value for point locality. For general locality we show that it is
asymptotically equivalent across all representations and operators. We also
recreate three established GEA experiments to understand the predictive power
of point locality on GEA performance, focusing on two popular and often
juxtaposed representations: standard binary and binary reflected Gray.
  We show that standard binary has provably no worse locality than any Gray
encoding, including binary reflected Gray. We discuss this result in the
context of previous studies that found binary reflected Gray to outperform
standard binary, and we argue that locality cannot be the explanation for
strong performance. Finally, we provide empirical evidence that weak point
locality representations can be beneficial to performance in the exploration
phase of the GEA, while strong point locality representations are more
beneficial in the exploitation phase."
"During the outbreak of the novel coronavirus pneumonia (COVID-19), there is a
huge demand for medical masks. A mask manufacturer often receives a large
amount of orders that are beyond its capability. Therefore, it is of critical
importance for the manufacturer to schedule mask production tasks as
efficiently as possible. However, existing scheduling methods typically require
a considerable amount of computational resources and, therefore, cannot
effectively cope with the surge of orders. In this paper, we propose an
end-to-end neural network for scheduling real-time production tasks. The neural
network takes a sequence of production tasks as inputs to predict a
distribution over different schedules, employs reinforcement learning to
optimize network parameters using the negative total tardiness as the reward
signal, and finally produces a high-quality solution to the scheduling problem.
We applied the proposed approach to schedule emergency production tasks for a
medical mask manufacturer during the peak of COVID-19 in China. Computational
results show that the neural network scheduler can solve problem instances with
hundreds of tasks within seconds. The objective function value (i.e., the total
weighted tardiness) produced by the neural network scheduler is significantly
better than those of existing constructive heuristics, and is very close to
those of the state-of-the-art metaheuristics whose computational time is
unaffordable in practice."
"Recently a mechanism called stagnation detection was proposed that
automatically adjusts the mutation rate of evolutionary algorithms when they
encounter local optima. The so-called $SD-(1+1)EA$ introduced by Rajabi and
Witt (GECCO 2020) adds stagnation detection to the classical $(1+1)EA$ with
standard bit mutation, which flips each bit independently with some mutation
rate, and raises the mutation rate when the algorithm is likely to have
encountered local optima.
  In this paper, we investigate stagnation detection in the context of the
$k$-bit flip operator of randomized local search that flips $k$ bits chosen
uniformly at random and let stagnation detection adjust the parameter $k$. We
obtain improved runtime results compared to the $SD-(1+1)EA$ amounting to a
speed-up of up to $e=2.71\dots$ Moreover, we propose additional schemes that
prevent infinite optimization times even if the algorithm misses a working
choice of $k$ due to unlucky events. Finally, we present an example where
standard bit mutation still outperforms the local $k$-bit flip with stagnation
detection."
"We call a finite family of activation functions superexpressive if any
multivariate continuous function can be approximated by a neural network that
uses these activations and has a fixed architecture only depending on the
number of input variables (i.e., to achieve any accuracy we only need to adjust
the weights, without increasing the number of neurons). Previously, it was
known that superexpressive activations exist, but their form was quite complex.
We give examples of very simple superexpressive families: for example, we prove
that the family {sin, arcsin} is superexpressive. We also show that most
practical activations (not involving periodic functions) are not
superexpressive."
"Non-pharmaceutical interventions (NPIs) are effective measures to contain a
pandemic. Yet, such control measures commonly have a negative effect on the
economy. Here, we propose a macro-level approach to support resolving this
Health-Economy Dilemma (HED). First, an extension to the well-known SEIR model
is suggested which includes an economy model. Second, a bi-objective
optimization problem is defined to study optimal control policies in view of
the HED problem. Next, several multi-objective evolutionary algorithms are
applied to perform a study on the health-economy performance trade-offs that
are inherent to the obtained optimal policies. Finally, the results from the
applied algorithms are compared to select a preferred algorithm for future
studies. As expected, for the proposed models and strategies, a clear conflict
between the health and economy performances is found. Furthermore, the results
suggest that the guided usage of NPIs is preferable as compared to refraining
from employing such strategies at all. This study contributes to pandemic
modeling and simulation by providing a novel concept that elaborates on
integrating economic aspects while exploring the optimal moment to enable NPIs."
"As a vital cognitive function of animals, the navigation skill is first built
on the accurate perception of the directional heading in the environment. Head
direction cells (HDCs), found in the limbic system of animals, are proven to
play an important role in identifying the directional heading allocentrically
in the horizontal plane, independent of the animal's location and the ambient
conditions of the environment. However, practical HDC models that can be
implemented in robotic applications are rarely investigated, especially those
that are biologically plausible and yet applicable to the real world. In this
paper, we propose a computational HDC network which is consistent with several
neurophysiological findings concerning biological HDCs, and then implement it
in robotic navigation tasks. The HDC network keeps a representation of the
directional heading only relying on the angular velocity as an input. We
examine the proposed HDC model in extensive simulations and real-world
experiments and demonstrate its excellent performance in terms of accuracy and
real-time capability."
"Evolutionary transfer multiobjective optimization (ETMO) has been becoming a
hot research topic in the field of evolutionary computation, which is based on
the fact that knowledge learning and transfer across the related optimization
exercises can improve the efficiency of others. Besides, the potential for
transfer optimization is deemed invaluable from the standpoint of human-like
problem-solving capabilities where knowledge gather and reuse are instinctive.
To promote the research on ETMO, benchmark problems are of great importance to
ETMO algorithm analysis, which helps designers or practitioners to understand
the merit and demerit better of ETMO algorithms. Therefore, a total number of
40 benchmark functions are proposed in this report, covering diverse types and
properties in the case of knowledge transfer, such as various formulation
models, various PS geometries and PF shapes, large-scale of variables,
dynamically changed environment, and so on. All the benchmark functions have
been implemented in JAVA code, which can be downloaded on the following
website: https://github.com/songbai-liu/etmo."
"Quality diversity (QD) algorithms have been shown to be very successful when
dealing with problems in areas such as robotics, games and combinatorial
optimization. They aim to maximize the quality of solutions for different
regions of the so-called behavioural space of the underlying problem. In this
paper, we apply the QD paradigm to simulate dynamic programming behaviours on
knapsack problem, and provide a first runtime analysis of QD algorithms. We
show that they are able to compute an optimal solution within expected
pseudo-polynomial time, and reveal parameter settings that lead to a fully
polynomial randomised approximation scheme (FPRAS). Our experimental
investigations evaluate the different approaches on classical benchmark sets in
terms of solutions constructed in the behavioural space as well as the runtime
needed to obtain an optimal solution."
"With the increasing integration of neural networks as components in
mission-critical systems, there is an increasing need to ensure that they
satisfy various safety and liveness requirements. In recent years, numerous
sound and complete verification methods have been proposed towards that end,
but these typically suffer from severe scalability limitations. Recent work has
proposed enhancing such verification techniques with abstraction-refinement
capabilities, which have been shown to boost scalability: instead of verifying
a large and complex network, the verifier constructs and then verifies a much
smaller network, whose correctness implies the correctness of the original
network. A shortcoming of such a scheme is that if verifying the smaller
network fails, the verifier needs to perform a refinement step that increases
the size of the network being verified, and then start verifying the new
network from scratch - effectively ""wasting"" its earlier work on verifying the
smaller network. In this paper, we present an enhancement to abstraction-based
verification of neural networks, by using residual reasoning: the process of
utilizing information acquired when verifying an abstract network, in order to
expedite the verification of a refined network. In essence, the method allows
the verifier to store information about parts of the search space in which the
refined network is guaranteed to behave correctly, and allows it to focus on
areas where bugs might be discovered. We implemented our approach as an
extension to the Marabou verifier, and obtained promising results."
"Linear functions play a key role in the runtime analysis of evolutionary
algorithms and studies have provided a wide range of new insights and
techniques for analyzing evolutionary computation methods. Motivated by studies
on separable functions and the optimization behaviour of evolutionary
algorithms as well as objective functions from the area of chance constrained
optimization, we study the class of objective functions that are weighted sums
of two transformed linear functions. Our results show that the (1+1) EA, with a
mutation rate depending on the number of overlapping bits of the functions,
obtains an optimal solution for these functions in expected time O(n log n),
thereby generalizing a well-known result for linear functions to a much wider
range of problems."
"Gastro-Intestinal Tract cancer is considered a fatal malignant condition of
the organs in the GI tract. Due to its fatality, there is an urgent need for
medical image segmentation techniques to segment organs to reduce the treatment
time and enhance the treatment. Traditional segmentation techniques rely upon
handcrafted features and are computationally expensive and inefficient. Vision
Transformers have gained immense popularity in many image classification and
segmentation tasks. To address this problem from a transformers' perspective,
we introduced a hybrid CNN-transformer architecture to segment the different
organs from an image. The proposed solution is robust, scalable, and
computationally efficient, with a Dice and Jaccard coefficient of 0.79 and
0.72, respectively. The proposed solution also depicts the essence of deep
learning-based automation to improve the effectiveness of the treatment"
"This paper presents a Neuromorphic Starter Kit, which has been designed to
help a variety of research groups perform research, exploration and real-world
demonstrations of brain-based, neuromorphic processors and hardware
environments. A prototype kit has been built and tested. We explain the
motivation behind the kit, its design and composition, and a prototype physical
demonstration."
"Evolutionary Reinforcement Learning (ERL) that applying Evolutionary
Algorithms (EAs) to optimize the weight parameters of Deep Neural Network (DNN)
based policies has been widely regarded as an alternative to traditional
reinforcement learning methods. However, the evaluation of the iteratively
generated population usually requires a large amount of computational time and
can be prohibitively expensive, which may potentially restrict the
applicability of ERL. Surrogate is often used to reduce the computational
burden of evaluation in EAs. Unfortunately, in ERL, each individual of policy
usually represents millions of weights parameters of DNN. This high-dimensional
representation of policy has introduced a great challenge to the application of
surrogates into ERL to speed up training. This paper proposes a PE-SAERL
Framework to at the first time enable surrogate-assisted evolutionary
reinforcement learning via policy embedding (PE). Empirical results on 5 Atari
games show that the proposed method can perform more efficiently than the four
state-of-the-art algorithms. The training process is accelerated up to 7x on
tested games, comparing to its counterpart without the surrogate and PE."
"Vector Symbolic Architectures (VSAs) are a powerful framework for
representing compositional reasoning. They lend themselves to neural-network
implementations, allowing us to create neural networks that can perform
cognitive functions, like spatial reasoning, arithmetic, symbol binding, and
logic. But the vectors involved can be quite large, hence the alternative label
Hyperdimensional (HD) computing. Advances in neuromorphic hardware hold the
promise of reducing the running time and energy footprint of neural networks by
orders of magnitude. In this paper, we extend some pioneering work to run VSA
algorithms on a substrate of spiking neurons that could be run efficiently on
neuromorphic hardware."
"We propose a novel evolutionary algorithm on bit vectors which derives from
the principles of information theory. The information-theoretic evolutionary
algorithm (it-EA) iteratively updates a search distribution with two
parameters, the center, that is the bit vector at which standard bit mutation
is applied, and the mutation rate. The mutation rate is updated by means of
information-geometric optimization and the center is updated by means of a
maximum likelihood principle. Standard elitist and non elitist updates of the
center are also considered. Experiments illustrate the dynamics of the mutation
rate and the influence of hyperparameters. In an empirical runtime analysis, on
OneMax and LeadingOnes, the elitist and non elitist it-EAs obtain promising
results."
"Among the array of neural network architectures, the Vision Transformer (ViT)
stands out as a prominent choice, acclaimed for its exceptional expressiveness
and consistent high performance in various vision applications. Recently, the
emerging Spiking ViT approach has endeavored to harness spiking neurons, paving
the way for a more brain-inspired transformer architecture that thrives in
ultra-low power operations on dedicated neuromorphic hardware. Nevertheless,
this approach remains confined to spatial self-attention and doesn't fully
unlock the potential of spiking neural networks. We introduce DISTA, a
Denoising Spiking Transformer with Intrinsic Plasticity and SpatioTemporal
Attention, designed to maximize the spatiotemporal computational prowess of
spiking neurons, particularly for vision applications. DISTA explores two types
of spatiotemporal attentions: intrinsic neuron-level attention and
network-level attention with explicit memory. Additionally, DISTA incorporates
an efficient nonlinear denoising mechanism to quell the noise inherent in
computed spatiotemporal attention maps, thereby resulting in further
performance gains. Our DISTA transformer undergoes joint training involving
synaptic plasticity (i.e., weight tuning) and intrinsic plasticity (i.e.,
membrane time constant tuning) and delivers state-of-the-art performances
across several static image and dynamic neuromorphic datasets. With only 6 time
steps, DISTA achieves remarkable top-1 accuracy on CIFAR10 (96.26%) and
CIFAR100 (79.15%), as well as 79.1% on CIFAR10-DVS using 10 time steps."
"The fitness level method is an easy-to-use tool for estimating the hitting
time of elitist evolutionary algorithms. Recently, linear lower and upper
bounds by fitness levels have been constructed. But these bounds require
recursive computation, which makes them difficult to use in practice. We
address this shortcoming with a new directed graph (digraph) method that does
not require recursive computation and significantly simplifies the calculation
of coefficients in the lower bound. In the method, we select a sub-digraph and
divide it into fitness levels, then construct an explicit formula for computing
the linear lower bound coefficients using transition probabilities restricted
to the subdigraph. A major advantage of the new method is the derivation of
tight lower bounds on fitness functions with shortcuts, which are difficult to
achieve using previous fitness methods. We use three examples (FullyDeceptive,
TwoMax1 and Deceptive) to demonstrate that each new lower bound is tight, but
previous lower bounds are not. Our work significantly extends the fitness level
method from addressing simple fitness functions without shortcuts to more
complex functions with shortcuts."
"Co-evolution is a powerful problem-solving approach. However, fitness
evaluation in co-evolutionary algorithms can be computationally expensive, as
the quality of an individual in one population is defined by its interactions
with many (or all) members of one or more other populations. To accelerate
co-evolutionary systems, we introduce phylogeny-informed interaction
estimation, which uses runtime phylogenetic analysis to estimate interaction
outcomes between individuals based on how their relatives performed against
each other. We test our interaction estimation method with three distinct
co-evolutionary systems: two systems focused on measuring problem-solving
success and one focused on measuring evolutionary open-endedness. We find that
phylogeny-informed estimation can substantially reduce the computation required
to solve problems, particularly at the beginning of long-term evolutionary
runs. Additionally, we find that our estimation method initially jump-starts
the evolution of neural complexity in our open-ended domain, but
estimation-free systems eventually ""catch-up"" if given enough time. More
broadly, continued refinements to these phylogeny-informed interaction
estimation methods offers a promising path to reducing the computational cost
of running co-evolutionary systems while maintaining their open-endedness."
"Gene expression programming is an evolutionary optimization algorithm with
the potential to generate interpretable and easily implementable equations for
regression problems. Despite knowledge gained from previous optimizations being
potentially available, the initial candidate solutions are typically generated
randomly at the beginning and often only include features or terms based on
preliminary user assumptions. This random initial guess, which lacks
constraints on the search space, typically results in higher computational
costs in the search for an optimal solution. Meanwhile, transfer learning, a
technique to reuse parts of trained models, has been successfully applied to
neural networks. However, no generalized strategy for its use exists for
symbolic regression in the context of evolutionary algorithms. In this work, we
propose an approach for integrating transfer learning with gene expression
programming applied to symbolic regression. The constructed framework
integrates Natural Language Processing techniques to discern correlations and
recurring patterns from equations explored during previous optimizations. This
integration facilitates the transfer of acquired knowledge from similar tasks
to new ones. Through empirical evaluation of the extended framework across a
range of univariate problems from an open database and from the field of
computational fluid dynamics, our results affirm that initial solutions derived
via a transfer learning mechanism enhance the algorithm's convergence rate
towards improved solutions."
"Neuromorphic models take inspiration from the human brain by adopting
bio-plausible neuron models to build alternatives to traditional Machine
Learning (ML) and Deep Learning (DL) solutions. The scarce availability of
dedicated hardware able to actualize the emulation of brain-inspired
computation, which is otherwise only simulated, yet still hinders the wide
adoption of neuromorphic computing for edge devices and embedded systems. With
this premise, we adopt the perspective of neuromorphic computing for
conventional hardware and we present the L2MU, a natively neuromorphic Legendre
Memory Unit (LMU) which entirely relies on Leaky Integrate-and-Fire (LIF)
neurons. Specifically, the original recurrent architecture of LMU has been
redesigned by modelling every constituent element with neural populations made
of LIF or Current-Based (CuBa) LIF neurons. To couple neuromorphic computing
and off-the-shelf edge devices, we equipped the L2MU with an input module for
the conversion of real values into spikes, which makes it an encoding-free
implementation of a Recurrent Spiking Neural Network (RSNN) able to directly
work with raw sensor signals on non-dedicated hardware. As a use case to
validate our network, we selected the task of Human Activity Recognition (HAR).
We benchmarked our L2MU on smartwatch signals from hand-oriented activities,
deploying it on three different commercial edge devices in compressed versions
too. The reported results remark the possibility of considering neuromorphic
models not only in an exclusive relationship with dedicated hardware but also
as a suitable choice to work with common sensors and devices."
"We present a Spiking Neural Network (SNN) model that incorporates learnable
synaptic delays through two approaches: per-synapse delay learning via Dilated
Convolutions with Learnable Spacings (DCLS) and a dynamic pruning strategy that
also serves as a form of delay learning. In the latter approach, the network
dynamically selects and prunes connections, optimizing the delays in sparse
connectivity settings. We evaluate both approaches on the Raw Heidelberg Digits
keyword spotting benchmark using Backpropagation Through Time with surrogate
gradients.
  Our analysis of the spatio-temporal structure of synaptic interactions
reveals that, after training, excitation and inhibition group together in space
and time. Notably, the dynamic pruning approach, which employs DEEP R for
connection removal and RigL for reconnection, not only preserves these
spatio-temporal patterns but outperforms per-synapse delay learning in sparse
networks.
  Our results demonstrate the potential of combining delay learning with
dynamic pruning to develop efficient SNN models for temporal data processing.
Moreover, the preservation of spatio-temporal dynamics throughout pruning and
rewiring highlights the robustness of these features, providing a solid
foundation for future neuromorphic computing applications."
"Numerical optimization techniques are widely used in a broad area of science
and technology, from finding the minimal energy of systems in Physics or
Chemistry to finding optimal routes in logistics or optimal strategies for high
speed trading. In general, a set of parameters (parameter space) is tuned to
find the lowest value of a function depending on these parameters (cost
function). In most cases the parameter space is too big to be completely
searched and the most efficient techniques combine stochastic elements
(randomness included in the starting setting and decision making during the
optimization process) with well designed deterministic process. Thus there is
nothing like a universal best optimization method; rather than that, different
methods and their settings are more or less efficient in different contexts.
Here we present a method that integrates Particle Swarm Optimization (PSO), a
highly effective and successful algorithm inspired by the collective behavior
of a flock of birds searching for food, with the principles of Harmonic
Oscillators. This physics-based approach introduces the concept of energy,
enabling a smoother and a more controlled convergence throughout the
optimization process. We test our method on a standard set of test functions
and show that in most cases it can outperform its natural competitors including
the original PSO as well as the broadly used COBYLA and Differential Evolution
optimization methods."
"In response to the persistent safety challenges within coal mines, this study
proposes a novel approach integrating a three-layer feedforward backpropagation
artificial neural network with a genetic algorithm (GA-BP) for establishing a
safety early warning system. Focused on a coal mine in Shandong, China, the
model's effectiveness is evaluated using relevant data for training and
analysis. Results indicate the superiority of the GA-BP model over traditional
BP neural networks, offering enhanced capability for identifying potential
safety risks promptly. This advancement enables coal mine management to
implement timely interventions, ensuring the safety of miners. The findings
present valuable insights for engineering applications in similar contexts."
"In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP)
aims to optimize a sequence of operations and assign each to an eligible
machine with varying processing times. For integration of the workforce, each
machine also requires a worker to be present to process an operation which
additionally affects the processing times. The resulting problem is called
Flexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W). The
FJSSP has been approached with various problem representations, including Mixed
Integer Linear Programming (MILP), Constrained Programming (CP), and
Simulation-based Optimization (SBO). In the latter area in particular, there
exists a large number of specialized Evolutionary Algorithms (EA) like Particle
Swarm Optimization (PSO) or Genetic Algorithms (GA). Yet, the solvers are often
developed for single use cases only, and validated on a few selected test
instances, let alone compared with results from solvers using other problem
representations. While suitable approaches do also exist, the design of the
FJSSP-W instances is not standardized and the algorithms are hardly comparable.
This calls for a systematic benchmarking environment that provides a
comprehensive set of FJSSP(-W) instances and supports targeted algorithm
development. It will facilitate the comparison of algorithmic performance in
the face of different problem characteristics. The present paper presents a
collection of 402 commonly accepted FJSSP instances and proposes an approach to
extend these with worker flexibility. In addition, we present a detailed
procedure for the evaluation of scheduling algorithms on these problem sets and
provide suitable model representations for this purpose. We provide complexity
characteristics for all presented instances as well as baseline results of
common commercial solvers to facilitate the validation of new algorithmic
developments."
"Robustness to a wide variety of negative factors and the ability to
self-repair is an inherent and natural characteristic of all life forms on
earth. As opposed to nature, man-made systems are in most cases not inherently
robust and a significant effort has to be made in order to make them resistant
against failures. This can be done in a wide variety of ways and on various
system levels. In the field of digital systems, for example, techniques such as
triple modular redundancy (TMR) are frequently used, which results in a
considerable hardware overhead. Biologically-inspired computing by means of
bio-chemical metaphors offers alternative paradigms, which need to be explored
and evaluated.
  Here, we are interested to evaluate the potential of nature-inspired
artificial chemistries and membrane systems as an alternative information
representing and processing paradigm in order to obtain robust and spatially
extended Boolean computing systems in a distributed environment. We investigate
conceptual approaches inspired by artificial chemistries and membrane systems
and compare proof-of-concepts. First, we show, that elementary logical
functions can be implemented. Second, we illustrate how they can be made more
robust and how they can be assembled to larger-scale systems. Finally, we
discuss the implications for and paths to possible genuine implementations.
Compared to the main body of work in artificial chemistries, we take a very
pragmatic and implementation-oriented approach and are interested in realizing
Boolean computations only. The results emphasize that artificial chemistries
can be used to implement Boolean logic in a spatially extended and distributed
environment and can also be made highly robust, but at a significant price."
"In order to analyze and extract different structural properties of
distributions, one can introduce different coordinate systems over the manifold
of distributions. In Evolutionary Computation, the Walsh bases and the Building
Block Bases are often used to describe populations, which simplifies the
analysis of evolutionary operators applying on populations. Quite independent
from these approaches, information geometry has been developed as a geometric
way to analyze different order dependencies between random variables (e.g.,
neural activations or genes).
  In these notes I briefly review the essentials of various coordinate bases
and of information geometry. The goal is to give an overview and make the
approaches comparable. Besides introducing meaningful coordinate bases,
information geometry also offers an explicit way to distinguish different order
interactions and it offers a geometric view on the manifold and thereby also on
operators that apply on the manifold. For instance, uniform crossover can be
interpreted as an orthogonal projection of a population along an m-geodesic,
monotonously reducing the theta-coordinates that describe interactions between
genes."
"Nous montrons comment il est possible d'utiliser l'algorithme d'auto
organisation de Kohonen pour traiter des donn\'ees avec valeurs manquantes et
estimer ces derni\`eres. Apr\`es un rappel m\'ethodologique, nous illustrons
notre propos \`a partir de trois applications \`a des donn\'ees r\'eelles.
  -----
  We show how it is possible to use the Kohonen self-organizing algorithm to
deal with data which contain missing values and to estimate them. After a
methodological recall, we illustrate our purpose from three real databases
applications."
"Network intrusion detection sensors are usually built around low level models
of network traffic. This means that their output is of a similarly low level
and as a consequence, is difficult to analyze. Intrusion alert correlation is
the task of automating some of this analysis by grouping related alerts
together. Attack graphs provide an intuitive model for such analysis.
Unfortunately alert flooding attacks can still cause a loss of service on
sensors, and when performing attack graph correlation, there can be a large
number of extraneous alerts included in the output graph. This obscures the
fine structure of genuine attacks and makes them more difficult for human
operators to discern. This paper explores modified correlation algorithms which
attempt to minimize the impact of this attack."
"This paper contributes to the human-machine interface community in two ways:
as a critique of the closed-loop AC (augmented cognition) approach, and as a
way to introduce concepts from complex systems and systems physiology into the
field. Of particular relevance is a comparison of the inverted-U (or Gaussian)
model of optimal performance and multidimensional fitness landscape model.
Hypothetical examples will be given from human physiology and learning and
memory. In particular, a four-step model will be introduced that is proposed as
a better means to characterize multivariate systems during behavioral processes
with complex dynamics such as learning. Finally, the alternate approach
presented herein is considered as a preferable design alternate in
human-machine systems. It is within this context that future directions are
discussed."
"A primary motivation for our research in digital ecosystems is the desire to
exploit the self-organising properties of biological ecosystems. Ecosystems are
thought to be robust, scalable architectures that can automatically solve
complex, dynamic problems. However, the computing technologies that contribute
to these properties have not been made explicit in digital ecosystems research.
Here, we discuss how different computing technologies can contribute to
providing the necessary self-organising features, including Multi-Agent
Systems, Service-Oriented Architectures, and distributed evolutionary
computing. The potential for exploiting these properties in digital ecosystems
is considered, suggesting how several key features of biological ecosystems can
be exploited in Digital Ecosystems, and discussing how mimicking these features
may assist in developing robust, scalable self-organising architectures. An
example architecture, the Digital Ecosystem, is considered in detail. The
Digital Ecosystem is then measured experimentally through simulations,
considering the self-organised diversity of its evolving agent populations
relative to the user request behaviour."
"This article presents a spiking neuroevolutionary system which implements
memristors as plastic connections, i.e. whose weights can vary during a trial.
The evolutionary design process exploits parameter self-adaptation and variable
topologies, allowing the number of neurons, connection weights, and
inter-neural connectivity pattern to emerge. By comparing two phenomenological
real-world memristor implementations with networks comprised of (i) linear
resistors (ii) constant-valued connections, we demonstrate that this approach
allows the evolution of networks of appropriate complexity to emerge whilst
exploiting the memristive properties of the connections to reduce learning
time. We extend this approach to allow for heterogeneous mixtures of memristors
within the networks; our approach provides an in-depth analysis of network
structure. Our networks are evaluated on simulated robotic navigation tasks;
results demonstrate that memristive plasticity enables higher performance than
constant-weighted connections in both static and dynamic reward scenarios, and
that mixtures of memristive elements provide performance advantages when
compared to homogeneous memristive networks."
"Enhancing the robustness and accuracy of time series forecasting models is an
active area of research. Recently, Artificial Neural Networks (ANNs) have found
extensive applications in many practical forecasting problems. However, the
standard backpropagation ANN training algorithm has some critical issues, e.g.
it has a slow convergence rate and often converges to a local minimum, the
complex pattern of error surfaces, lack of proper training parameters selection
methods, etc. To overcome these drawbacks, various improved training methods
have been developed in literature; but, still none of them can be guaranteed as
the best for all problems. In this paper, we propose a novel weighted ensemble
scheme which intelligently combines multiple training algorithms to increase
the ANN forecast accuracies. The weight for each training algorithm is
determined from the performance of the corresponding ANN model on the
validation dataset. Experimental results on four important time series depicts
that our proposed technique reduces the mentioned shortcomings of individual
ANN training algorithms to a great extent. Also it achieves significantly
better forecast accuracies than two other popular statistical models."
"Recently recurrent neural networks (RNN) has been very successful in handling
sequence data. However, understanding RNN and finding the best practices for
RNN is a difficult task, partly because there are many competing and complex
hidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as
Minimal Gated Unit (MGU), since it only contains one gate, which is a minimal
design among all gated hidden units. The design of MGU benefits from evaluation
results on LSTM and GRU in the literature. Experiments on various sequence data
show that MGU has comparable accuracy with GRU, but has a simpler structure,
fewer parameters, and faster training. Hence, MGU is suitable in RNN's
applications. Its simple architecture also means that it is easier to evaluate
and tune, and in principle it is easier to study MGU's properties theoretically
and empirically."
"In this paper, we propose a principled deep reinforcement learning (RL)
approach that is able to accelerate the convergence rate of general deep neural
networks (DNNs). With our approach, a deep RL agent (synonym for optimizer in
this work) is used to automatically learn policies about how to schedule
learning rates during the optimization of a DNN. The state features of the
agent are learned from the weight statistics of the optimizee during training.
The reward function of this agent is designed to learn policies that minimize
the optimizee's training time given a certain performance goal. The actions of
the agent correspond to changing the learning rate for the optimizee during
training. As far as we know, this is the first attempt to use deep RL to learn
how to optimize a large-sized DNN. We perform extensive experiments on a
standard benchmark dataset and demonstrate the effectiveness of the policies
learned by our approach."
"Autism Spectrum Disorders are associated with atypical movements, of which
stereotypical motor movements (SMMs) interfere with learning and social
interaction. The automatic SMM detection using inertial measurement units (IMU)
remains complex due to the strong intra and inter-subject variability,
especially when handcrafted features are extracted from the signal. We propose
a new application of the deep learning to facilitate automatic SMM detection
using multi-axis IMUs. We use a convolutional neural network (CNN) to learn a
discriminative feature space from raw data. We show how the CNN can be used for
parameter transfer learning to enhance the detection rate on longitudinal data.
We also combine the long short-term memory (LSTM) with CNN to model the
temporal patterns in a sequence of multi-axis signals. Further, we employ
ensemble learning to combine multiple LSTM learners into a more robust SMM
detector. Our results show that: 1) feature learning outperforms handcrafted
features; 2) parameter transfer learning is beneficial in longitudinal
settings; 3) using LSTM to learn the temporal dynamic of signals enhances the
detection rate especially for skewed training data; 4) an ensemble of LSTMs
provides more accurate and stable detectors. These findings provide a
significant step toward accurate SMM detection in real-time scenarios."
"Lexicase selection achieves very good solution quality by introducing ordered
test cases. However, the computational complexity of lexicase selection can
prohibit its use in many applications. In this paper, we introduce Batch
Tournament Selection (BTS), a hybrid of tournament and lexicase selection which
is approximately one order of magnitude faster than lexicase selection while
achieving a competitive quality of solutions. Tests on a number of regression
datasets show that BTS compares well with lexicase selection in terms of mean
absolute error while having a speed-up of up to 25 times. Surprisingly, BTS and
lexicase selection have almost no difference in both diversity and performance.
This reveals that batches and ordered test cases are completely different
mechanisms which share the same general principle fostering the specialization
of individuals. This work introduces an efficient algorithm that sheds light
onto the main principles behind the success of lexicase, potentially opening up
a new range of possibilities for algorithms to come."
"Deep Reservoir Computing has emerged as a new paradigm for deep learning,
which is based around the reservoir computing principle of maintaining random
pools of neurons combined with hierarchical deep learning. The reservoir
paradigm reflects and respects the high degree of recurrence in biological
brains, and the role that neuronal dynamics play in learning. However, one
issue hampering deep reservoir network development is that one cannot
backpropagate through the reservoir layers. Recent deep reservoir architectures
do not learn hidden or hierarchical representations in the same manner as deep
artificial neural networks, but rather concatenate all hidden reservoirs
together to perform traditional regression. Here we present a novel Deep
Reservoir Network for time series prediction and classification that learns
through the non-differentiable hidden reservoir layers using a
biologically-inspired backpropagation alternative called Direct Feedback
Alignment, which resembles global dopamine signal broadcasting in the brain. We
demonstrate its efficacy on two real world multidimensional time series
datasets."
"A novel multi-agent evolutionary robotics (ER) based framework, inspired by
competitive evolutionary environments in nature, is demonstrated for training
Spiking Neural Networks (SNN). The weights of a population of SNNs along with
morphological parameters of bots they control in the ER environment are treated
as phenotypes. Rules of the framework select certain bots and their SNNs for
reproduction and others for elimination based on their efficacy in capturing
food in a competitive environment. While the bots and their SNNs are given no
explicit reward to survive or reproduce via any loss function, these drives
emerge implicitly as they evolve to hunt food and survive within these rules.
Their efficiency in capturing food as a function of generations exhibit the
evolutionary signature of punctuated equilibria. Two evolutionary inheritance
algorithms on the phenotypes, Mutation and Crossover with Mutation, are
demonstrated. Performances of these algorithms are compared using ensembles of
100 experiments for each algorithm. We find that Crossover with Mutation
promotes 40% faster learning in the SNN than mere Mutation with a statistically
significant margin."
"Differential evolution was developed for reliable and versatile function
optimization. It has also become interesting for other domains because of its
ease to use. In this paper, we posed the question of whether differential
evolution can also be used by solving of the combinatorial optimization
problems, and in particular, for the graph coloring problem. Therefore, a
hybrid self-adaptive differential evolution algorithm for graph coloring was
proposed that is comparable with the best heuristics for graph coloring today,
i.e. Tabucol of Hertz and de Werra and the hybrid evolutionary algorithm of
Galinier and Hao. We have focused on the graph 3-coloring. Therefore, the
evolutionary algorithm with method SAW of Eiben et al., which achieved
excellent results for this kind of graphs, was also incorporated into this
study. The extensive experiments show that the differential evolution could
become a competitive tool for the solving of graph coloring problem in the
future."
"A customized multi-objective evolutionary algorithm (MOEA) is proposed for
the multi-objective flexible job shop scheduling problem (FJSP). It uses smart
initialization approaches to enrich the first generated population, and
proposes various crossover operators to create a better diversity of offspring.
Especially, the MIP-EGO configurator, which can tune algorithm parameters, is
adopted to automatically tune operator probabilities. Furthermore, different
local search strategies are employed to explore the neighborhood for better
solutions. In general, the algorithm enhancement strategy can be integrated
with any standard EMO algorithm. In this paper, it has been combined with
NSGA-III to solve benchmark multi-objective FJSPs, whereas an off-the-shelf
implementation of NSGA-III is not capable of solving the FJSP. The experimental
results show excellent performance with less computing budget."
"Recent works have shown that the input domain of any machine learning
classifier is bound to contain adversarial examples. Thus we can no longer hope
to immune classifiers against adversarial examples and instead can only aim to
achieve the following two defense goals: 1) making adversarial examples harder
to find, or 2) weakening their adversarial nature by pushing them further away
from correctly classified data points. Most if not all the previously suggested
defense mechanisms attend to just one of those two goals, and as such, could be
bypassed by adaptive attacks that take the defense mechanism into
consideration. In this work we suggest a novel defense mechanism that
simultaneously addresses both defense goals: We flatten the gradients of the
loss surface, making adversarial examples harder to find, using a novel
stochastic regularization term that explicitly decreases the sensitivity of
individual neurons to small input perturbations. In addition, we push the
decision boundary away from correctly classified inputs by leveraging Jacobian
regularization. We present a solid theoretical basis and an empirical testing
of our suggested approach, demonstrate its superiority over previously
suggested defense mechanisms, and show that it is effective against a wide
range of adaptive attacks."
"Computing optical flow is a fundamental problem in computer vision. However,
deep learning-based optical flow techniques do not perform well for non-rigid
movements such as those found in faces, primarily due to lack of the training
data representing the fine facial motion. We hypothesize that learning optical
flow on face motion data will improve the quality of predicted flow on faces.
The aim of this work is threefold: (1) exploring self-supervised techniques to
generate optical flow ground truth for face images; (2) computing baseline
results on the effects of using face data to train Convolutional Neural
Networks (CNN) for predicting optical flow; and (3) using the learned optical
flow in micro-expression recognition to demonstrate its effectiveness. We
generate optical flow ground truth using facial key-points in the
BP4D-Spontaneous dataset. The generated optical flow is used to train the
FlowNetS architecture to test its performance on the generated dataset. The
performance of FlowNetS trained on face images surpassed that of other optical
flow CNN architectures, demonstrating its usefulness. Our optical flow features
are further compared with other methods using the STSTNet micro-expression
classifier, and the results indicate that the optical flow obtained using this
work has promising applications in facial expression analysis."
"Image content is a predominant factor in marketing campaigns, websites and
banners. Today, marketers and designers spend considerable time and money in
generating such professional quality content. We take a step towards
simplifying this process using Generative Adversarial Networks (GANs). We
propose a simple and novel conditioning strategy which allows generation of
images conditioned on given semantic attributes using a generator trained for
an unconditional image generation task. Our approach is based on modifying
latent vectors, using directional vectors of relevant semantic attributes in
latent space. Our method is designed to work with both discrete (binary and
multi-class) and continuous image attributes. We show the applicability of our
proposed approach, named Directional GAN, on multiple public datasets, with an
average accuracy of 86.4% across different attributes."
"Classifiers learnt from data are increasingly being used as components in
systems where safety is a critical concern. In this work, we present a formal
notion of safety for classifiers via constraints called safe-ordering
constraints. These constraints relate requirements on the order of the classes
output by a classifier to conditions on its input, and are expressive enough to
encode various interesting examples of classifier safety specifications from
the literature. For classifiers implemented using neural networks, we also
present a run-time mechanism for the enforcement of safe-ordering constraints.
Our approach is based on a self-correcting layer, which provably yields safe
outputs regardless of the characteristics of the classifier input. We compose
this layer with an existing neural network classifier to construct a
self-correcting network (SC-Net), and show that in addition to providing safe
outputs, the SC-Net is guaranteed to preserve the classification accuracy of
the original network whenever possible. Our approach is independent of the size
and architecture of the neural network used for classification, depending only
on the specified property and the dimension of the network's output; thus it is
scalable to large state-of-the-art networks. We show that our approach can be
optimized for a GPU, introducing run-time overhead of less than 1ms on current
hardware -- even on large, widely-used networks containing hundreds of
thousands of neurons and millions of parameters."
"Spiking and Quantized Neural Networks (NNs) are becoming exceedingly
important for hyper-efficient implementations of Deep Learning (DL) algorithms.
However, these networks face challenges when trained using error
backpropagation, due to the absence of gradient signals when applying hard
thresholds. The broadly accepted trick to overcoming this is through the use of
biased gradient estimators: surrogate gradients which approximate thresholding
in Spiking Neural Networks (SNNs), and Straight-Through Estimators (STEs),
which completely bypass thresholding in Quantized Neural Networks (QNNs). While
noisy gradient feedback has enabled reasonable performance on simple supervised
learning tasks, it is thought that such noise increases the difficulty of
finding optima in loss landscapes, especially during the later stages of
optimization. By periodically boosting the Learning Rate (LR) during training,
we expect the network can navigate unexplored solution spaces that would
otherwise be difficult to reach due to local minima, barriers, or flat
surfaces. This paper presents a systematic evaluation of a cosine-annealed LR
schedule coupled with weight-independent adaptive moment estimation as applied
to Quantized SNNs (QSNNs). We provide a rigorous empirical evaluation of this
technique on high precision and 4-bit quantized SNNs across three datasets,
demonstrating (close to) state-of-the-art performance on the more complex
datasets. Our source code is available at this link:
https://github.com/jeshraghian/QSNNs."
"We introduce a Genetic Algorithm (GA) based, open-source project to solve
multi-objective optimization problems of materials characterization data
analysis including EXAFS, XPS and nanoindentation. The modular design and
multiple crossover and mutation options make the software extensible for
additional materials characterization applications too. This automation of the
analysis is crucial in the era when instrumentation acquires data orders of
magnitude more rapidly than it can be analyzed by hand. Our results
demonstrated good fitness scores with minimal human intervention."
"In the field of artificial intelligence, neuromorphic computing has been
around for several decades. Deep learning has however made much recent progress
such that it consistently outperforms neuromorphic learning algorithms in
classification tasks in terms of accuracy. Specifically in the field of image
classification, neuromorphic computing has been traditionally using either the
temporal or rate code for encoding static images in datasets into spike trains.
It is only till recently, that neuromorphic vision sensors are widely used by
the neuromorphic research community, and provides an alternative to such
encoding methods. Since then, several neuromorphic datasets as obtained by
applying such sensors on image datasets (e.g. the neuromorphic CALTECH 101)
have been introduced. These data are encoded in spike trains and hence seem
ideal for benchmarking of neuromorphic learning algorithms. Specifically, we
train a deep learning framework used for image classification on the CALTECH
101 and a collapsed version of the neuromorphic CALTECH 101 datasets. We
obtained an accuracy of 91.66% and 78.01% for the CALTECH 101 and neuromorphic
CALTECH 101 datasets respectively. For CALTECH 101, our accuracy is close to
the best reported accuracy, while for neuromorphic CALTECH 101, it outperforms
the last best reported accuracy by over 10%. This raises the question of the
suitability of such datasets as benchmarks for neuromorphic learning
algorithms."
"The massively parallel nature of biological information processing plays an
important role for its superiority to human-engineered computing devices. In
particular, it may hold the key to overcoming the von Neumann bottleneck that
limits contemporary computer architectures. Physical-model neuromorphic devices
seek to replicate not only this inherent parallelism, but also aspects of its
microscopic dynamics in analog circuits emulating neurons and synapses.
However, these machines require network models that are not only adept at
solving particular tasks, but that can also cope with the inherent
imperfections of analog substrates. We present a spiking network model that
performs Bayesian inference through sampling on the BrainScaleS neuromorphic
platform, where we use it for generative and discriminative computations on
visual data. By illustrating its functionality on this platform, we implicitly
demonstrate its robustness to various substrate-specific distortive effects, as
well as its accelerated capability for computation. These results showcase the
advantages of brain-inspired physical computation and provide important
building blocks for large-scale neuromorphic applications."
"Recent theoretical research proposes that computational complexity can be
seen as an ultimate constraint that allows for open-ended biological evolution
on finite static fitness landscapes. Whereas on easy fitness landscapes,
evolution will quickly converge to a local fitness peaks, on hard fitness
landscapes this computational constraints prevents evolution from reaching any
local fitness peak in polynomial time. Valued constraint satisfaction problems
(VCSPs) can be used to represent both easy and hard fitness landscapes. Thus
VCSPS can be seen as a natural way of linking the theory of evolution with
notions of computer science to better understand the features that make
landscapes hard. However, there are currently no simulators that study
VCSP-structured fitness landscapes.
  This report describes the design and build of an evolution simulator for
VCSP-structured fitness landscapes. The platform is used for simulating various
instances of easy and hard fitness landscapes. In particular, we look at
evolution under more realistic assumptions than fittest mutant strong-selection
weak mutation dynamics on the winding semismooth fitness landscape. The results
obtained match with the theoretical expectations, while also providing new
information about the limits of evolution. The last part of the report
introduces a mathematical model for smooth fitness landscapes and uses it to
better understand why these landscapes are easy."
"Premature convergence can be detrimental to the performance of search
methods, which is why many search algorithms include restart strategies to deal
with it. While it is common to perturb the incumbent solution with
diversification steps of various sizes with the hope that the search method
will find a new basin of attraction leading to a better local optimum, it is
usually not clear how big the perturbation step should be. We introduce a new
property of fitness landscapes termed ""Neighbours with Similar Fitness"" and we
demonstrate that the effectiveness of a restart strategy depends on this
property."
"There is now significant historical data available on decision making in
organizations, consisting of the decision problem, what decisions were made,
and how desirable the outcomes were. Using this data, it is possible to learn a
surrogate model, and with that model, evolve a decision strategy that optimizes
the outcomes. This paper introduces a general such approach, called
Evolutionary Surrogate-Assisted Prescription, or ESP. The surrogate is, for
example, a random forest or a neural network trained with gradient descent, and
the strategy is a neural network that is evolved to maximize the predictions of
the surrogate model. ESP is further extended in this paper to sequential
decision-making tasks, which makes it possible to evaluate the framework in
reinforcement learning (RL) benchmarks. Because the majority of evaluations are
done on the surrogate, ESP is more sample efficient, has lower variance, and
lower regret than standard RL approaches. Surprisingly, its solutions are also
better because both the surrogate and the strategy network regularize the
decision-making behavior. ESP thus forms a promising foundation to decision
optimization in real-world problems."
"Recently introduced EASE algorithm presents a simple and elegant way, how to
solve the top-N recommendation task. In this paper, we introduce Neural EASE to
further improve the performance of this algorithm by incorporating techniques
for training modern neural networks. Also, there is a growing interest in the
recsys community to utilize variational autoencoders (VAE) for this task. We
introduce deep autoencoder FLVAE benefiting from multiple non-linear layers
without an information bottleneck while not overfitting towards the identity.
We show how to learn FLVAE in parallel with Neural EASE and achieve the state
of the art performance on the MovieLens 20M dataset and competitive results on
the Netflix Prize dataset."
"If machine failures can be detected preemptively, then maintenance and
repairs can be performed more efficiently, reducing production costs. Many
machine learning techniques for performing early failure detection using
vibration data have been proposed; however, these methods are often power and
data-hungry, susceptible to noise, and require large amounts of data
preprocessing. Also, training is usually only performed once before inference,
so they do not learn and adapt as the machine ages. Thus, we propose a method
of performing online, real-time anomaly detection for predictive maintenance
using Hierarchical Temporal Memory (HTM). Inspired by the human neocortex, HTMs
learn and adapt continuously and are robust to noise. Using the Numenta Anomaly
Benchmark, we empirically demonstrate that our approach outperforms
state-of-the-art algorithms at preemptively detecting real-world cases of
bearing failures and simulated 3D printer failures. Our approach achieves an
average score of 64.71, surpassing state-of-the-art deep-learning (49.38) and
statistical (61.06) methods."
"Public charging station occupancy prediction plays key importance in
developing a smart charging strategy to reduce electric vehicle (EV) operator
and user inconvenience. However, existing studies are mainly based on
conventional econometric or time series methodologies with limited accuracy. We
propose a new mixed long short-term memory neural network incorporating both
historical charging state sequences and time-related features for multistep
discrete charging occupancy state prediction. Unlike the existing LSTM
networks, the proposed model separates different types of features and handles
them differently with mixed neural network architecture. The model is compared
to a number of state-of-the-art machine learning and deep learning approaches
based on the EV charging data obtained from the open data portal of the city of
Dundee, UK. The results show that the proposed method produces very accurate
predictions (99.99% and 81.87% for 1 step (10 minutes) and 6 steps (1 hour)
ahead, respectively, and outperforms the benchmark approaches significantly
(+22.4% for one-step-ahead prediction and +6.2% for 6 steps ahead). A
sensitivity analysis is conducted to evaluate the impact of the model
parameters on prediction accuracy."
"We propose a new learning algorithm to train spiking neural networks (SNN)
using conventional artificial neural networks (ANN) as proxy. We couple two SNN
and ANN networks, respectively, made of integrate-and-fire (IF) and ReLU
neurons with the same network architectures and shared synaptic weights. The
forward passes of the two networks are totally independent. By assuming IF
neuron with rate-coding as an approximation of ReLU, we backpropagate the error
of the SNN in the proxy ANN to update the shared weights, simply by replacing
the ANN final output with that of the SNN. We applied the proposed proxy
learning to deep convolutional SNNs and evaluated it on two benchmarked
datasets of Fashion-MNIST and Cifar10 with 94.56% and 93.11% classification
accuracy, respectively. The proposed networks could outperform other deep SNNs
trained with tandem learning, surrogate gradient learning, or converted from
deep ANNs. Converted SNNs require long simulation times to reach reasonable
accuracies while our proxy learning leads to efficient SNNs with much smaller
simulation times. The source codes of the proposed method are publicly
available at https://github.com/SRKH/ProxyLearning."
"Multi-task optimization (MTO) studies how to simultaneously solve multiple
optimization problems for the purpose of obtaining better performance on each
problem. Over the past few years, evolutionary MTO (EMTO) was proposed to
handle MTO problems via evolutionary algorithms. So far, many EMTO algorithms
have been developed and demonstrated well performance on solving real-world
problems. However, there remain many works to do in adapting knowledge transfer
to task relatedness in EMTO. Different from the existing works, we develop a
self-adaptive multi-task particle swarm optimization (SaMTPSO) through the
developed knowledge transfer adaptation strategy, the focus search strategy and
the knowledge incorporation strategy. In the knowledge transfer adaptation
strategy, each task has a knowledge source pool that consists of all knowledge
sources. Each source (task) outputs knowledge to the task. And knowledge
transfer adapts to task relatedness via individuals' choice on different
sources of a pool, where the chosen probabilities for different sources are
computed respectively according to task's success rate in generating improved
solutions via these sources. In the focus search strategy, if there is no
knowledge source benefit the optimization of a task, then all knowledge sources
in the task's pool are forbidden to be utilized except the task, which helps to
improve the performance of the proposed algorithm. Note that the task itself is
as a knowledge source of its own. In the knowledge incorporation strategy, two
different forms are developed to help the SaMTPSO explore and exploit the
transferred knowledge from a chosen source, each leading to a version of the
SaMTPSO. Several experiments are conducted on two test suites. The results of
the SaMTPSO are comparing to that of 3 popular EMTO algorithms and a particle
swarm algorithm, which demonstrates the superiority of the SaMTPSO."
"Non-Volatile Memory (NVM) cells are used in neuromorphic hardware to store
model parameters, which are programmed as resistance states. NVMs suffer from
the read disturb issue, where the programmed resistance state drifts upon
repeated access of a cell during inference. Resistance drifts can lower the
inference accuracy. To address this, it is necessary to periodically reprogram
model parameters (a high overhead operation). We study read disturb failures of
an NVM cell. Our analysis show both a strong dependency on model
characteristics such as synaptic activation and criticality, and on the voltage
used to read resistance states during inference. We propose a system software
framework to incorporate such dependencies in programming model parameters on
NVM cells of a neuromorphic hardware. Our framework consists of a convex
optimization formulation which aims to implement synaptic weights that have
more activations and are critical, i.e., those that have high impact on
accuracy on NVM cells that are exposed to lower voltages during inference. In
this way, we increase the time interval between two consecutive reprogramming
of model parameters. We evaluate our system software with many emerging
inference models on a neuromorphic hardware simulator and show a significant
reduction in the system overhead."
"While the field of Quality-Diversity (QD) has grown into a distinct branch of
stochastic optimization, a few problems, in particular locomotion and
navigation tasks, have become de facto standards. Are such benchmarks
sufficient? Are they representative of the key challenges faced by QD
algorithms? Do they provide the ability to focus on one particular challenge by
properly disentangling it from others? Do they have much predictive power in
terms of scalability and generalization? Existing benchmarks are not
standardized, and there is currently no MNIST equivalent for QD. Inspired by
recent works on Reinforcement Learning benchmarks, we argue that the
identification of challenges faced by QD methods and the development of
targeted, challenging, scalable but affordable benchmarks is an important step.
As an initial effort, we identify three problems that are challenging in sparse
reward settings, and propose associated benchmarks: (1) Behavior metric bias,
which can result from the use of metrics that do not match the structure of the
behavior space. (2) Behavioral Plateaus, with varying characteristics, such
that escaping them would require adaptive QD algorithms and (3) Evolvability
Traps, where small variations in genotype result in large behavioral changes.
The environments that we propose satisfy the properties listed above."
"Hyperdimensional Computing (HDC) has obtained abundant attention as an
emerging non von Neumann computing paradigm. Inspired by the way human brain
functions, HDC leverages high dimensional patterns to perform learning tasks.
Compared to neural networks, HDC has shown advantages such as energy efficiency
and smaller model size, but sub-par learning capabilities in sophisticated
applications. Recently, researchers have observed when combined with neural
network components, HDC can achieve better performance than conventional HDC
models. This motivates us to explore the deeper insights behind theoretical
foundations of HDC, particularly the connection and differences with neural
networks. In this paper, we make a comparative study between HDC and neural
network to provide a different angle where HDC can be derived from an extremely
compact neural network trained upfront. Experimental results show such neural
network-derived HDC model can achieve up to 21% and 5% accuracy increase from
conventional and learning-based HDC models respectively. This paper aims to
provide more insights and shed lights on future directions for researches on
this popular emerging learning scheme."
"Spiking neural network is a kind of neuromorphic computing that is believed
to improve the level of intelligence and provide advantages for quantum
computing. In this work, we address this issue by designing an optical spiking
neural network and find that it can be used to accelerate the speed of
computation, especially on combinatorial optimization problems. Here the
spiking neural network is constructed by the antisymmetrically coupled
degenerate optical parametric oscillator pulses and dissipative pulses. A
nonlinear transfer function is chosen to mitigate amplitude inhomogeneities and
destabilize the resulting local minima according to the dynamical behavior of
spiking neurons. It is numerically shown that the spiking neural
network-coherent Ising machines have excellent performance on combinatorial
optimization problems, which is expected to offer new applications for neural
computing and optical computing."
"Automated feature extraction capability and significant performance of Deep
Neural Networks (DNN) make them suitable for Internet of Things (IoT)
applications. However, deploying DNN on edge devices becomes prohibitive due to
the colossal computation, energy, and storage requirements. This paper presents
a novel approach for designing and training lightweight DNN using large-size
DNN. The approach considers the available storage, processing speed, and
maximum allowable processing time to execute the task on edge devices. We
present a knowledge distillation based training procedure to train the
lightweight DNN to achieve adequate accuracy. During the training of
lightweight DNN, we introduce a novel early halting technique, which preserves
network resources; thus, speedups the training procedure. Finally, we present
the empirically and real-world evaluations to verify the effectiveness of the
proposed approach under different constraints using various edge devices."
"SNNs are an active research domain towards energy efficient machine
intelligence. Compared to conventional ANNs, SNNs use temporal spike data and
bio-plausible neuronal activation functions such as Leaky-Integrate
Fire/Integrate Fire (LIF/IF) for data processing. However, SNNs incur
significant dot-product operations causing high memory and computation overhead
in standard von-Neumann computing platforms. Today, In-Memory Computing (IMC)
architectures have been proposed to alleviate the ""memory-wall bottleneck""
prevalent in von-Neumann architectures. Although recent works have proposed
IMC-based SNN hardware accelerators, the following have been overlooked- 1) the
adverse effects of crossbar non-ideality on SNN performance due to repeated
analog dot-product operations over multiple time-steps, 2) hardware overheads
of essential SNN-specific components such as the LIF/IF and data communication
modules. To this end, we propose SpikeSim, a tool that can perform realistic
performance, energy, latency and area evaluation of IMC-mapped SNNs. SpikeSim
consists of a practical monolithic IMC architecture called SpikeFlow for
mapping SNNs. Additionally, the non-ideality computation engine (NICE) and
energy-latency-area (ELA) engine performs hardware-realistic evaluation of
SpikeFlow-mapped SNNs. Based on 65nm CMOS implementation and experiments on
CIFAR10, CIFAR100 and TinyImagenet datasets, we find that the LIF/IF neuronal
module has significant area contribution (>11% of the total hardware area). We
propose SNN topological modifications leading to 1.24x and 10x reduction in the
neuronal module's area and the overall energy-delay-product value,
respectively. Furthermore, in this work, we perform a holistic comparison
between IMC implemented ANN and SNNs and conclude that lower number of
time-steps are the key to achieve higher throughput and energy-efficiency for
SNNs compared to 4-bit ANNs."
"Spiking neural networks (SNNs) attract great attention due to their low power
consumption, low latency, and biological plausibility. As they are widely
deployed in neuromorphic devices for low-power brain-inspired computing,
security issues become increasingly important. However, compared to deep neural
networks (DNNs), SNNs currently lack specifically designed defense methods
against adversarial attacks. Inspired by neural membrane potential oscillation,
we propose a novel neural model that incorporates the bio-inspired oscillation
mechanism to enhance the security of SNNs. Our experiments show that SNNs with
neural oscillation neurons have better resistance to adversarial attacks than
ordinary SNNs with LIF neurons on kinds of architectures and datasets.
Furthermore, we propose a defense method that changes model's gradients by
replacing the form of oscillation, which hides the original training gradients
and confuses the attacker into using gradients of 'fake' neurons to generate
invalid adversarial samples. Our experiments suggest that the proposed defense
method can effectively resist both single-step and iterative attacks with
comparable defense effectiveness and much less computational costs than
adversarial training methods on DNNs. To the best of our knowledge, this is the
first work that establishes adversarial defense through masking surrogate
gradients on SNNs."
"Most existing Spiking Neural Network (SNN) works state that SNNs may utilize
temporal information dynamics of spikes. However, an explicit analysis of
temporal information dynamics is still missing. In this paper, we ask several
important questions for providing a fundamental understanding of SNNs: What are
temporal information dynamics inside SNNs? How can we measure the temporal
information dynamics? How do the temporal information dynamics affect the
overall learning performance? To answer these questions, we estimate the Fisher
Information of the weights to measure the distribution of temporal information
during training in an empirical manner. Surprisingly, as training goes on,
Fisher information starts to concentrate in the early timesteps. After
training, we observe that information becomes highly concentrated in earlier
few timesteps, a phenomenon we refer to as temporal information concentration.
We observe that the temporal information concentration phenomenon is a common
learning feature of SNNs by conducting extensive experiments on various
configurations such as architecture, dataset, optimization strategy, time
constant, and timesteps. Furthermore, to reveal how temporal information
concentration affects the performance of SNNs, we design a loss function to
change the trend of temporal information. We find that temporal information
concentration is crucial to building a robust SNN but has little effect on
classification accuracy. Finally, we propose an efficient iterative pruning
method based on our observation on temporal information concentration. Code is
available at
https://github.com/Intelligent-Computing-Lab-Yale/Exploring-Temporal-Information-Dynamics-in-Spiking-Neural-Networks."
"Clustering is a fundamental problem in many areas, which aims to partition a
given data set into groups based on some distance measure, such that the data
points in the same group are similar while that in different groups are
dissimilar. Due to its importance and NP-hardness, a lot of methods have been
proposed, among which evolutionary algorithms are a class of popular ones.
Evolutionary clustering has found many successful applications, but all the
results are empirical, lacking theoretical support. This paper fills this gap
by proving that the approximation performance of the GSEMO (a simple
multi-objective evolutionary algorithm) for solving four formulations of
clustering, i.e., $k$-tMM, $k$-center, discrete $k$-median and $k$-means, can
be theoretically guaranteed. Furthermore, we consider clustering under
fairness, which tries to avoid algorithmic bias, and has recently been an
important research topic in machine learning. We prove that for discrete
$k$-median clustering under individual fairness, the approximation performance
of the GSEMO can be theoretically guaranteed with respect to both the objective
function and the fairness constraint."
"A long-standing and difficult problem in, e.g., condensed matter physics is
how to find the ground state of a complex many-body system where the potential
energy surface has a large number of local minima. Spin systems containing
complex and/or topological textures, for example spin spirals or magnetic
skyrmions, are prime examples of such systems. We propose here a
genetic-tunneling-driven variance-controlled optimization approach, and apply
it to two-dimensional magnetic skyrmionic systems. The approach combines a
local energy-minimizer backend and a metaheuristic global search frontend. The
algorithm is naturally concurrent, resulting in short user execution time. We
find that the method performs significantly better than simulated annealing
(SA). Specifically, we demonstrate that for the Pd/Fe/Ir(111) system, our
method correctly and efficiently identifies the experimentally observed spin
spiral, skyrmion lattice and ferromagnetic ground states as a function of
external magnetic field. To our knowledge, no other optimization method has
until now succeeded in doing this. We envision that our findings will pave the
way for evolutionary computing in mapping out phase diagrams for spin systems
in general."
"Inspired by neuronal diversity in the biological neural system, a plethora of
studies proposed to design novel types of artificial neurons and introduce
neuronal diversity into artificial neural networks. Recently proposed quadratic
neuron, which replaces the inner-product operation in conventional neurons with
a quadratic one, have achieved great success in many essential tasks. Despite
the promising results of quadratic neurons, there is still an unresolved issue:
\textit{Is the superior performance of quadratic networks simply due to the
increased parameters or due to the intrinsic expressive capability?} Without
clarifying this issue, the performance of quadratic networks is always
suspicious. Additionally, resolving this issue is reduced to finding killer
applications of quadratic networks. In this paper, with theoretical and
empirical studies, we show that quadratic networks enjoy parametric efficiency,
thereby confirming that the superior performance of quadratic networks is due
to the intrinsic expressive capability. This intrinsic expressive ability comes
from that quadratic neurons can easily represent nonlinear interaction, while
it is hard for conventional neurons. Theoretically, we derive the approximation
efficiency of the quadratic network over conventional ones in terms of real
space and manifolds. Moreover, from the perspective of the Barron space, we
demonstrate that there exists a functional space whose functions can be
approximated by quadratic networks in a dimension-free error, but the
approximation error of conventional networks is dependent on dimensions.
Empirically, experimental results on synthetic data, classic benchmarks, and
real-world applications show that quadratic models broadly enjoy parametric
efficiency, and the gain of efficiency depends on the task."
"Predicting body fat can provide medical practitioners and users with
essential information for preventing and diagnosing heart diseases. Hybrid
machine learning models offer better performance than simple regression
analysis methods by selecting relevant body measurements and capturing complex
nonlinear relationships among selected features in modelling body fat
prediction problems. There are, however, some disadvantages to them. Current
machine learning. Modelling body fat prediction as a combinatorial single- and
multi-objective optimisation problem often gets stuck in local optima. When
multiple feature subsets produce similar or close predictions, avoiding local
optima becomes more complex. Evolutionary feature selection has been used to
solve several machine-learning-based optimisation problems. A fuzzy set theory
determines appropriate levels of exploration and exploitation while managing
parameterisation and computational costs. A weighted-sum body fat prediction
approach was explored using evolutionary feature selection, fuzzy set theory,
and machine learning algorithms, integrating contradictory metrics into a
single composite goal optimised by fuzzy adaptive evolutionary feature
selection. Hybrid fuzzy adaptive global learning local search universal
diversity-based feature selection is applied to this single-objective feature
selection-machine learning framework (FAGLSUD-based FS-ML). While using fewer
features, this model achieved a more accurate and stable estimate of body fat
percentage than other hybrid and state-of-the-art machine learning models. A
multi-objective FAGLSUD-based FS-MLP is also proposed to analyse accuracy,
stability, and dimensionality conflicts simultaneously. To make informed
decisions about fat deposits in the most vital body parts and blood lipid
levels, medical practitioners and users can use a well-distributed Pareto set
of trade-off solutions."
"$\partial\mathbb{B}$ nets are differentiable neural networks that learn
discrete boolean-valued functions by gradient descent. $\partial\mathbb{B}$
nets have two semantically equivalent aspects: a differentiable soft-net, with
real weights, and a non-differentiable hard-net, with boolean weights. We train
the soft-net by backpropagation and then `harden' the learned weights to yield
boolean weights that bind with the hard-net. The result is a learned discrete
function. `Hardening' involves no loss of accuracy, unlike existing approaches
to neural network binarization. Preliminary experiments demonstrate that
$\partial\mathbb{B}$ nets achieve comparable performance on standard machine
learning problems yet are compact (due to 1-bit weights) and interpretable (due
to the logical nature of the learnt functions)."
"We present a simple, sample-efficient algorithm for introducing large but
directed learning steps in reinforcement learning (RL), through the use of
evolutionary operators. The methodology uses a population of RL agents training
with a common experience buffer, with occasional crossovers and mutations of
the agents in order to search efficiently through the policy space. Unlike
prior literature on combining evolutionary search (ES) with RL, this work does
not generate a distribution of agents from a common mean and covariance matrix.
Neither does it require the evaluation of the entire population of policies at
every time step. Instead, we focus on gradient-based training throughout the
life of every policy (individual), with a sparse amount of evolutionary
exploration. The resulting algorithm is shown to be robust to hyperparameter
variations. As a surprising corollary, we show that simply initialising and
training multiple RL agents with a common memory (with no further evolutionary
updates) outperforms several standard RL baselines."
"Coupled oscillators are being increasingly used as the basis of machine
learning (ML) architectures, for instance in sequence modeling, graph
representation learning and in physical neural networks that are used in analog
ML devices. We introduce an abstract class of neural oscillators that
encompasses these architectures and prove that neural oscillators are
universal, i.e, they can approximate any continuous and casual operator mapping
between time-varying functions, to desired accuracy. This universality result
provides theoretical justification for the use of oscillator based ML systems.
The proof builds on a fundamental result of independent interest, which shows
that a combination of forced harmonic oscillators with a nonlinear read-out
suffices to approximate the underlying operators."
"In both natural and artificial studies, evolution is often seen as synonymous
to natural selection. Individuals evolve under pressures set by environments
that are either reset or do not carry over significant changes from previous
generations. Thus, niche construction (NC), the reciprocal process to natural
selection where individuals incur inheritable changes to their environment, is
ignored. Arguably due to this lack of study, the dynamics of NC are today
little understood, especially in real-world settings. In this work, we study NC
in simulation environments that consist of multiple, diverse niches and
populations that evolve their plasticity, evolvability and niche-constructing
behaviors. Our empirical analysis reveals many interesting dynamics, with
populations experiencing mass extinctions, arms races and oscillations. To
understand these behaviors, we analyze the interaction between NC and
adaptability and the effect of NC on the population's genomic diversity and
dispersal, observing that NC diversifies niches. Our study suggests that
complexifying the simulation environments studying NC, by considering multiple
and diverse niches, is necessary for understanding its dynamics and can lend
testable hypotheses to future studies of both natural and artificial systems."
"Compositionality is a hallmark of human language that not only enables
linguistic generalization, but also potentially facilitates acquisition. When
simulating language emergence with neural networks, compositionality has been
shown to improve communication performance; however, its impact on imitation
learning has yet to be investigated. Our work explores the link between
compositionality and imitation in a Lewis game played by deep neural agents.
Our contributions are twofold: first, we show that the learning algorithm used
to imitate is crucial: supervised learning tends to produce more average
languages, while reinforcement learning introduces a selection pressure toward
more compositional languages. Second, our study reveals that compositional
languages are easier to imitate, which may induce the pressure toward
compositional languages in RL imitation settings."
"AI Alignment research seeks to align human and AI goals to ensure independent
actions by a machine are always ethical. This paper argues empathy is necessary
for this task, despite being often neglected in favor of more deductive
approaches. We offer an inside-out approach that grounds morality within the
context of the brain as a basis for algorithmically understanding ethics and
empathy. These arguments are justified via a survey of relevant literature. The
paper concludes with a suggested experimental approach to future research and
some initial experimental observations."
"Automated machine learning (AutoML) systems propose an end-to-end solution to
a given machine learning problem, creating either fixed or flexible pipelines.
Fixed pipelines are task independent constructs: their general composition
remains the same, regardless of the data. In contrast, the structure of
flexible pipelines varies depending on the input, making them finely tailored
to individual tasks. However, flexible pipelines can be structurally
overcomplicated and have poor explainability. We propose the EVOSA approach
that compensates for the negative points of flexible pipelines by incorporating
a sensitivity analysis which increases the robustness and interpretability of
the flexible solutions. EVOSA quantitatively estimates positive and negative
impact of an edge or a node on a pipeline graph, and feeds this information to
the evolutionary AutoML optimizer. The correctness and efficiency of EVOSA was
validated in tabular, multimodal and computer vision tasks, suggesting
generalizability of the proposed approach across domains."
"In this paper, we present NSGA-II-SVM (Non-dominated Sorting Genetic
Algorithm with Support Vector Machine Guidance), a novel learnable evolutionary
and search-based testing algorithm that leverages Support Vector Machine (SVM)
classification models to direct the search towards failure-revealing test
inputs. Supported by genetic search, NSGA-II-SVM creates iteratively SVM-based
models of the test input space, learning which regions in the search space are
promising to be explored. A subsequent sampling and repetition of evolutionary
search iterations allow to refine and make the model more accurate in the
prediction. Our preliminary evaluation of NSGA-II-SVM by testing an Automated
Valet Parking system shows that NSGA-II-SVM is more effective in identifying
more critical test cases than a state of the art learnable evolutionary testing
technique as well as naive random search."
"Symbolic regression (SR) searches for parametric models that accurately fit a
dataset, prioritizing simplicity and interpretability. Despite this secondary
objective, studies point out that the models are often overly complex due to
redundant operations, introns, and bloat that arise during the iterative
process, and can hinder the search with repeated exploration of bloated
segments. Applying a fast heuristic algebraic simplification may not fully
simplify the expression and exact methods can be infeasible depending on size
or complexity of the expressions. We propose a novel agnostic simplification
and bloat control for SR employing an efficient memoization with
locality-sensitive hashing (LHS). The idea is that expressions and their
sub-expressions traversed during the iterative simplification process are
stored in a dictionary using LHS, enabling efficient retrieval of similar
structures. We iterate through the expression, replacing subtrees with others
of same hash if they result in a smaller expression. Empirical results shows
that applying this simplification during evolution performs equal or better
than without simplification in minimization of error, significantly reducing
the number of nonlinear functions. This technique can learn simplification
rules that work in general or for a specific problem, and improves convergence
while reducing model complexity."
"Solving multimodal optimization problems (MMOP) requires finding all optimal
solutions, which is challenging in limited function evaluations. Although
existing works strike the balance of exploration and exploitation through
hand-crafted adaptive strategies, they require certain expert knowledge, hence
inflexible to deal with MMOP with different properties. In this paper, we
propose RLEMMO, a Meta-Black-Box Optimization framework, which maintains a
population of solutions and incorporates a reinforcement learning agent for
flexibly adjusting individual-level searching strategies to match the
up-to-date optimization status, hence boosting the search performance on MMOP.
Concretely, we encode landscape properties and evolution path information into
each individual and then leverage attention networks to advance population
information sharing. With a novel reward mechanism that encourages both quality
and diversity, RLEMMO can be effectively trained using a policy gradient
algorithm. The experimental results on the CEC2013 MMOP benchmark underscore
the competitive optimization performance of RLEMMO against several strong
baselines."
"Automated design of metaheuristic algorithms offers an attractive avenue to
reduce human effort and gain enhanced performance beyond human intuition.
Current automated methods design algorithms within a fixed structure and
operate from scratch. This poses a clear gap towards fully discovering
potentials over the metaheuristic family and fertilizing from prior design
experience. To bridge the gap, this paper proposes an autoregressive
learning-based designer for automated design of metaheuristic algorithms. Our
designer formulates metaheuristic algorithm design as a sequence generation
task, and harnesses an autoregressive generative network to handle the task.
This offers two advances. First, through autoregressive inference, the designer
generates algorithms with diverse lengths and structures, enabling to fully
discover potentials over the metaheuristic family. Second, prior design
knowledge learned and accumulated in neurons of the designer can be retrieved
for designing algorithms for future problems, paving the way to continual
design of algorithms for open-ended problem-solving. Extensive experiments on
numeral benchmarks and real-world problems reveal that the proposed designer
generates algorithms that outperform all human-created baselines on 24 out of
25 test problems. The generated algorithms display various structures and
behaviors, reasonably fitting for different problem-solving contexts. Code will
be released after paper publication."
"Working memory $\unicode{x2013}$ the ability to remember recent events as
they recede continuously into the past $\unicode{x2013}$ requires the ability
to represent any stimulus at any time delay. This property requires neurons
coding working memory to show mixed selectivity, with conjunctive receptive
fields (RFs) for stimuli and time, forming a representation of 'what' $\times$
'when'. We study the properties of such a working memory in simple experiments
where a single stimulus must be remembered for a short time. The requirement of
conjunctive receptive fields allows the covariance matrix of the network to
decouple neatly, allowing an understanding of the low-dimensional dynamics of
the population. Different choices of temporal basis functions lead to
qualitatively different dynamics. We study a specific choice $\unicode{x2013}$
a Laplace space with exponential basis functions for time coupled to an
""Inverse Laplace"" space with circumscribed basis functions in time. We refer to
this choice with basis functions that evenly tile log time as a Laplace Neural
Manifold. Despite the fact that they are related to one another by a linear
projection, the Laplace population shows a stable stimulus-specific subspace
whereas the Inverse Laplace population shows rotational dynamics. The growth of
the rank of the covariance matrix with time depends on the density of the
temporal basis set; logarithmic tiling shows good agreement with data. We
sketch a continuous attractor CANN that constructs a Laplace Neural Manifold.
The attractor in the Laplace space appears as an edge; the attractor for the
inverse space appears as a bump. This work provides a map for going from more
abstract cognitive models of WM to circuit-level implementation using
continuous attractor neural networks, and places constraints on the types of
neural dynamics that support working memory."
"Spiking Neural Networks (SNNs) are amenable to deployment on edge devices and
neuromorphic hardware due to their lower dissipation. Recently, SNN-based
transformers have garnered significant interest, incorporating attention
mechanisms akin to their counterparts in Artificial Neural Networks (ANNs)
while demonstrating excellent performance. However, deploying large spiking
transformer models on resource-constrained edge devices such as mobile phones,
still poses significant challenges resulted from the high computational demands
of large uncompressed high-precision models. In this work, we introduce a novel
heterogeneous quantization method for compressing spiking transformers through
layer-wise quantization. Our approach optimizes the quantization of each layer
using one of two distinct quantization schemes, i.e., uniform or power-of-two
quantification, with mixed bit resolutions. Our heterogeneous quantization
demonstrates the feasibility of maintaining high performance for spiking
transformers while utilizing an average effective resolution of 3.14-3.67 bits
with less than a 1% accuracy drop on DVS Gesture and CIFAR10-DVS datasets. It
attains a model compression rate of 8.71x-10.19x for standard floating-point
spiking transformers. Moreover, the proposed approach achieves a significant
energy reduction of 5.69x, 8.72x, and 10.2x while maintaining high accuracy
levels of 85.3%, 97.57%, and 80.4% on N-Caltech101, DVS-Gesture, and
CIFAR10-DVS datasets, respectively."
"Decision trees are widely used in machine learning due to their simplicity
and interpretability, but they often lack robustness to adversarial attacks and
data perturbations. The paper proposes a novel island-based coevolutionary
algorithm (ICoEvoRDF) for constructing robust decision tree ensembles. The
algorithm operates on multiple islands, each containing populations of decision
trees and adversarial perturbations. The populations on each island evolve
independently, with periodic migration of top-performing decision trees between
islands. This approach fosters diversity and enhances the exploration of the
solution space, leading to more robust and accurate decision tree ensembles.
ICoEvoRDF utilizes a popular game theory concept of mixed Nash equilibrium for
ensemble weighting, which further leads to improvement in results. ICoEvoRDF is
evaluated on 20 benchmark datasets, demonstrating its superior performance
compared to state-of-the-art methods in optimizing both adversarial accuracy
and minimax regret. The flexibility of ICoEvoRDF allows for the integration of
decision trees from various existing methods, providing a unified framework for
combining diverse solutions. Our approach offers a promising direction for
developing robust and interpretable machine learning models"
"Optimization techniques are pivotal in neural network training, shaping both
predictive performance and convergence efficiency. This study introduces
Foxtsage, a novel hybrid optimisation approach that integrates the Hybrid
FOX-TSA with Stochastic Gradient Descent for training Multi-Layer Perceptron
models. The proposed Foxtsage method is benchmarked against the widely adopted
Adam optimizer across multiple standard datasets, focusing on key performance
metrics such as training loss, accuracy, precision, recall, F1-score, and
computational time. Experimental results demonstrate that Foxtsage achieves a
42.03% reduction in loss mean (Foxtsage: 9.508, Adam: 16.402) and a 42.19%
improvement in loss standard deviation (Foxtsage: 20.86, Adam: 36.085),
reflecting enhanced consistency and robustness. Modest improvements in accuracy
mean (0.78%), precision mean (0.91%), recall mean (1.02%), and F1-score mean
(0.89%) further underscore its predictive performance. However, these gains are
accompanied by an increased computational cost, with a 330.87% rise in time
mean (Foxtsage: 39.541 seconds, Adam: 9.177 seconds). By effectively combining
the global search capabilities of FOX-TSA with the stability and adaptability
of SGD, Foxtsage presents itself as a robust and viable alternative for neural
network optimization tasks."
"We classify twenty-one Indo-European languages starting from written text. We
use neural networks in order to define a distance among different languages,
construct a dendrogram and analyze the ultrametric structure that emerges. Four
or five subgroups of languages are identified, according to the ""cut"" of the
dendrogram, drawn with an entropic criterion. The results and the method are
discussed."
"This work investigates how the traditional image classification pipelines can
be extended into a deep architecture, inspired by recent successes of deep
neural networks. We propose a deep boosting framework based on layer-by-layer
joint feature boosting and dictionary learning. In each layer, we construct a
dictionary of filters by combining the filters from the lower layer, and
iteratively optimize the image representation with a joint
discriminative-generative formulation, i.e. minimization of empirical
classification error plus regularization of analysis image generation over
training images. For optimization, we perform two iterating steps: i) to
minimize the classification error, select the most discriminative features
using the gentle adaboost algorithm; ii) according to the feature selection,
update the filters to minimize the regularization on analysis image
representation using the gradient descent method. Once the optimization is
converged, we learn the higher layer representation in the same way. Our model
delivers several distinct advantages. First, our layer-wise optimization
provides the potential to build very deep architectures. Second, the generated
image representation is compact and meaningful. In several visual recognition
tasks, our framework outperforms existing state-of-the-art approaches."
"Anthropomimetic robots are robots that sense, behave, interact and feel like
humans. By this definition, anthropomimetic robots require human-like physical
hardware and actuation, but also brain-like control and sensing. The most
self-evident realization to meet those requirements would be a human-like
musculoskeletal robot with a brain-like neural controller. While both
musculoskeletal robotic hardware and neural control software have existed for
decades, a scalable approach that could be used to build and control an
anthropomimetic human-scale robot has not been demonstrated yet. Combining
Myorobotics, a framework for musculoskeletal robot development, with SpiNNaker,
a neuromorphic computing platform, we present the proof-of-principle of a
system that can scale to dozens of neurally-controlled, physically compliant
joints. At its core, it implements a closed-loop cerebellar model which
provides real-time low-level neural control at minimal power consumption and
maximal extensibility: higher-order (e.g., cortical) neural networks and
neuromorphic sensors like silicon-retinae or -cochleae can naturally be
incorporated."
"We study the segmental recurrent neural network for end-to-end acoustic
modelling. This model connects the segmental conditional random field (CRF)
with a recurrent neural network (RNN) used for feature extraction. Compared to
most previous CRF-based acoustic models, it does not rely on an external system
to provide features or segmentation boundaries. Instead, this model
marginalises out all the possible segmentations, and features are extracted
from the RNN trained together with the segmental CRF. In essence, this model is
self-contained and can be trained end-to-end. In this paper, we discuss
practical training and decoding issues as well as the method to speed up the
training in the context of speech recognition. We performed experiments on the
TIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass
decoding --- the best reported result using CRFs, despite the fact that we only
used a zeroth-order CRF and without using any language model."
"Common nonlinear activation functions used in neural networks can cause
training difficulties due to the saturation behavior of the activation
function, which may hide dependencies that are not visible to vanilla-SGD
(using first order gradients only). Gating mechanisms that use softly
saturating activation functions to emulate the discrete switching of digital
logic circuits are good examples of this. We propose to exploit the injection
of appropriate noise so that the gradients may flow easily, even if the
noiseless application of the activation function would yield zero gradient.
Large noise will dominate the noise-free gradient and allow stochastic gradient
descent toexplore more. By adding noise only to the problematic parts of the
activation function, we allow the optimization procedure to explore the
boundary between the degenerate (saturating) and the well-behaved parts of the
activation function. We also establish connections to simulated annealing, when
the amount of noise is annealed down, making it easier to optimize hard
objective functions. We find experimentally that replacing such saturating
activation functions by noisy variants helps training in many contexts,
yielding state-of-the-art or competitive results on different datasets and
task, especially when training seems to be the most difficult, e.g., when
curriculum learning is necessary to obtain good results."
"The paper systematically studies the impact of a range of recent advances in
CNN architectures and learning methods on the object categorization (ILSVRC)
problem. The evalution tests the influence of the following choices of the
architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch
normalization), pooling variants (stochastic, max, average, mixed), network
width, classifier design (convolutional, fully-connected, SPP), image
pre-processing, and of learning parameters: learning rate, batch size,
cleanliness of the data, etc.
  The performance gains of the proposed modifications are first tested
individually and then in combination. The sum of individual gains is bigger
than the observed improvement when all modifications are introduced, but the
""deficit"" is small suggesting independence of their benefits. We show that the
use of 128x128 pixel images is sufficient to make qualitative conclusions about
optimal network structure that hold for the full size Caffe and VGG nets. The
results are obtained an order of magnitude faster than with the standard 224
pixel images."
"Convolutional Neural Networks (CNNs) were recently shown to provide
state-of-the-art results for object category viewpoint estimation. However
different ways of formulating this problem have been proposed and the competing
approaches have been explored with very different design choices. This paper
presents a comparison of these approaches in a unified setting as well as a
detailed analysis of the key factors that impact performance. Followingly, we
present a new joint training method with the detection task and demonstrate its
benefit. We also highlight the superiority of classification approaches over
regression approaches, quantify the benefits of deeper architectures and
extended training data, and demonstrate that synthetic data is beneficial even
when using ImageNet training data. By combining all these elements, we
demonstrate an improvement of approximately 5% mAVP over previous
state-of-the-art results on the Pascal3D+ dataset. In particular for their most
challenging 24 view classification task we improve the results from 31.1% to
36.1% mAVP."
"Recent research in the deep learning field has produced a plethora of new
architectures. At the same time, a growing number of groups are applying deep
learning to new applications. Some of these groups are likely to be composed of
inexperienced deep learning practitioners who are baffled by the dizzying array
of architecture choices and therefore opt to use an older architecture (i.e.,
Alexnet). Here we attempt to bridge this gap by mining the collective knowledge
contained in recent deep learning research to discover underlying principles
for designing neural network architectures. In addition, we describe several
architectural innovations, including Fractal of FractalNet network, Stagewise
Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt
files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope
others are inspired to build on our preliminary work."
"Neural networks are powerful and flexible models that work well for many
difficult learning tasks in image, speech and natural language understanding.
Despite their success, neural networks are still hard to design. In this paper,
we use a recurrent network to generate the model descriptions of neural
networks and train this RNN with reinforcement learning to maximize the
expected accuracy of the generated architectures on a validation set. On the
CIFAR-10 dataset, our method, starting from scratch, can design a novel network
architecture that rivals the best human-invented architecture in terms of test
set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is
0.09 percent better and 1.05x faster than the previous state-of-the-art model
that used a similar architectural scheme. On the Penn Treebank dataset, our
model can compose a novel recurrent cell that outperforms the widely-used LSTM
cell, and other state-of-the-art baselines. Our cell achieves a test set
perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than
the previous state-of-the-art model. The cell can also be transferred to the
character language modeling task on PTB and achieves a state-of-the-art
perplexity of 1.214."
"The paper presents a parallel math library, dMath, that demonstrates leading
scaling when using intranode, internode, and hybrid-parallelism for deep
learning (DL). dMath provides easy-to-use distributed primitives and a variety
of domain-specific algorithms including matrix multiplication, convolutions,
and others allowing for rapid development of scalable applications like deep
neural networks (DNNs). Persistent data stored in GPU memory and advanced
memory management techniques avoid costly transfers between host and device.
dMath delivers performance, portability, and productivity to its specific
domain of support."
"In recent years, machine learning techniques based on neural networks for
mobile computing become increasingly popular. Classical multi-layer neural
networks require matrix multiplications at each stage. Multiplication operation
is not an energy efficient operation and consequently it drains the battery of
the mobile device. In this paper, we propose a new energy efficient neural
network with the universal approximation property over space of Lebesgue
integrable functions. This network, called, additive neural network, is very
suitable for mobile computing. The neural structure is based on a novel vector
product definition, called ef-operator, that permits a multiplier-free
implementation. In ef-operation, the ""product"" of two real numbers is defined
as the sum of their absolute values, with the sign determined by the sign of
the product of the numbers. This ""product"" is used to construct a vector
product in $R^N$. The vector product induces the $l_1$ norm. The proposed
additive neural network successfully solves the XOR problem. The experiments on
MNIST dataset show that the classification performances of the proposed
additive neural networks are very similar to the corresponding multi-layer
perceptron and convolutional neural networks (LeNet)."
"Reason and inference require process as well as memory skills by humans.
Neural networks are able to process tasks like image recognition (better than
humans) but in memory aspects are still limited (by attention mechanism, size).
Recurrent Neural Network (RNN) and it's modified version LSTM are able to solve
small memory contexts, but as context becomes larger than a threshold, it is
difficult to use them. The Solution is to use large external memory. Still, it
poses many challenges like, how to train neural networks for discrete memory
representation, how to describe long term dependencies in sequential data etc.
Most prominent neural architectures for such tasks are Memory networks:
inference components combined with long term memory and Neural Turing Machines:
neural networks using external memory resources. Also, additional techniques
like attention mechanism, end to end gradient descent on discrete memory
representation are needed to support these solutions. Preliminary results of
above neural architectures on simple algorithms (sorting, copying) and Question
Answering (based on story, dialogs) application are comparable with the state
of the art. In this paper, I explain these architectures (in general), the
additional techniques used and the results of their application."
"Unsupervised learning in a generalized Hopfield associative-memory network is
investigated in this work. First, we prove that the (generalized) Hopfield
model is equivalent to a semi-restricted Boltzmann machine with a layer of
visible neurons and another layer of hidden binary neurons, so it could serve
as the building block for a multilayered deep-learning system. We then
demonstrate that the Hopfield network can learn to form a faithful internal
representation of the observed samples, with the learned memory patterns being
prototypes of the input data. Furthermore, we propose a spectral method to
extract a small set of concepts (idealized prototypes) as the most concise
summary or abstraction of the empirical data."
"We introduce a hybrid machine-learning algorithm for designing quantum optics
experiments that produce specific quantum states. Our algorithm successfully
found experimental schemes to produce all 5 states we asked it to, including
Schr\""odinger cat states and cubic phase states, all to a fidelity of over
$96\%$. Here we specifically focus on designing realistic experiments, and
hence all of the algorithm's designs only contain experimental elements that
are available with current technology. The core of our algorithm is a genetic
algorithm that searches for optimal arrangements of the experimental elements,
but to speed up the initial search we incorporate a neural network that
classifies quantum states. The latter is of independent interest, as it quickly
learned to accurately classify quantum states given their photon-number
distributions."
"We investigate properties of a classifier applied to the measurements of the
CP state of the Higgs boson in $H\rightarrow\tau\tau$ decays. The problem is
framed as binary classifier applied to individual instances. Then the prior
knowledge that the instances belong to the same class is used to define the
multi-instance classifier. Its final score is calculated as multiplication of
single instance scores for a given series of instances. In the paper we discuss
properties of such classifier, notably its dependence on the number of
instances in the series. This classifier exhibits very strong random dependence
on the number of epochs used for training and requires careful tuning of the
classification threshold. We derive formula for this optimal threshold."
"Recommender systems help users find relevant items of interest, for example
on e-commerce or media streaming sites. Most academic research is concerned
with approaches that personalize the recommendations according to long-term
user profiles. In many real-world applications, however, such long-term
profiles often do not exist and recommendations therefore have to be made
solely based on the observed behavior of a user during an ongoing session.
Given the high practical relevance of the problem, an increased interest in
this problem can be observed in recent years, leading to a number of proposals
for session-based recommendation algorithms that typically aim to predict the
user's immediate next actions. In this work, we present the results of an
in-depth performance comparison of a number of such algorithms, using a variety
of datasets and evaluation measures. Our comparison includes the most recent
approaches based on recurrent neural networks like GRU4REC, factorized Markov
model approaches such as FISM or FOSSIL, as well as simpler methods based,
e.g., on nearest neighbor schemes. Our experiments reveal that algorithms of
this latter class, despite their sometimes almost trivial nature, often perform
equally well or significantly better than today's more complex approaches based
on deep neural networks. Our results therefore suggest that there is
substantial room for improvement regarding the development of more
sophisticated session-based recommendation algorithms."
"Training large-scale image recognition models is computationally expensive.
This raises the question of whether there might be simple ways to improve the
test performance of an already trained model without having to re-train or
fine-tune it with new data. Here, we show that, surprisingly, this is indeed
possible. The key observation we make is that the layers of a deep network
close to the output layer contain independent, easily extractable
class-relevant information that is not contained in the output layer itself. We
propose to extract this extra class-relevant information using a simple
key-value cache memory to improve the classification performance of the model
at test time. Our cache memory is directly inspired by a similar cache model
previously proposed for language modeling (Grave et al., 2017). This cache
component does not require any training or fine-tuning; it can be applied to
any pre-trained model and, by properly setting only two hyper-parameters, leads
to significant improvements in its classification performance. Improvements are
observed across several architectures and datasets. In the cache component,
using features extracted from layers close to the output (but not from the
output layer itself) as keys leads to the largest improvements. Concatenating
features from multiple layers to form keys can further improve performance over
using single-layer features as keys. The cache component also has a
regularizing effect, a simple consequence of which is that it substantially
increases the robustness of models against adversarial attacks."
"Deep Neural Networks have been shown to be beneficial for a variety of tasks,
in particular allowing for end-to-end learning and reducing the requirement for
manual design decisions. However, still many parameters have to be chosen in
advance, also raising the need to optimize them. One important, but often
ignored system parameter is the selection of a proper activation function.
Thus, in this paper we target to demonstrate the importance of activation
functions in general and show that for different tasks different activation
functions might be meaningful. To avoid the manual design or selection of
activation functions, we build on the idea of genetic algorithms to learn the
best activation function for a given task. In addition, we introduce two new
activation functions, ELiSH and HardELiSH, which can easily be incorporated in
our framework. In this way, we demonstrate for three different image
classification benchmarks that different activation functions are learned, also
showing improved results compared to typically used baselines."
"To improve how neural networks function it is crucial to understand their
learning process. The information bottleneck theory of deep learning proposes
that neural networks achieve good generalization by compressing their
representations to disregard information that is not relevant to the task.
However, empirical evidence for this theory is conflicting, as compression was
only observed when networks used saturating activation functions. In contrast,
networks with non-saturating activation functions achieved comparable levels of
task performance but did not show compression. In this paper we developed more
robust mutual information estimation techniques, that adapt to hidden activity
of neural networks and produce more sensitive measurements of activations from
all functions, especially unbounded functions. Using these adaptive estimation
techniques, we explored compression in networks with a range of different
activation functions. With two improved methods of estimation, firstly, we show
that saturation of the activation function is not required for compression, and
the amount of compression varies between different activation functions. We
also find that there is a large amount of variation in compression between
different network initializations. Secondary, we see that L2 regularization
leads to significantly increased compression, while preventing overfitting.
Finally, we show that only compression of the last layer is positively
correlated with generalization."
"Training a Neural Network (NN) with lots of parameters or intricate
architectures creates undesired phenomena that complicate the optimization
process. To address this issue we propose a first modular approach to NN
design, wherein the NN is decomposed into a control module and several
functional modules, implementing primitive operations. We illustrate the
modular concept by comparing performances between a monolithic and a modular NN
on a list sorting problem and show the benefits in terms of training speed,
training stability and maintainability. We also discuss some questions that
arise in modular NNs."
"Gravitational-wave detection strategies are based on a signal analysis
technique known as matched filtering. Despite the success of matched filtering,
due to its computational cost, there has been recent interest in developing
deep convolutional neural networks (CNNs) for signal detection. Designing these
networks remains a challenge as most procedures adopt a trial and error
strategy to set the hyperparameter values. We propose a new method for
hyperparameter optimization based on genetic algorithms (GAs). We compare six
different GA variants and explore different choices for the GA-optimized
fitness score. We show that the GA can discover high-quality architectures when
the initial hyperparameter seed values are far from a good solution as well as
refining already good networks. For example, when starting from the
architecture proposed by George and Huerta, the network optimized over the
20-dimensional hyperparameter space has 78% fewer trainable parameters while
obtaining an 11% increase in accuracy for our test problem. Using genetic
algorithm optimization to refine an existing network should be especially
useful if the problem context (e.g. statistical properties of the noise, signal
model, etc) changes and one needs to rebuild a network. In all of our
experiments, we find the GA discovers significantly less complicated networks
as compared to the seed network, suggesting it can be used to prune wasteful
network structures. While we have restricted our attention to CNN classifiers,
our GA hyperparameter optimization strategy can be applied within other machine
learning settings."
"In this work, we present a lightweight pipeline for robust behavioral cloning
of a human driver using end-to-end imitation learning. The proposed pipeline
was employed to train and deploy three distinct driving behavior models onto a
simulated vehicle. The training phase comprised of data collection, balancing,
augmentation, preprocessing and training a neural network, following which, the
trained model was deployed onto the ego vehicle to predict steering commands
based on the feed from an onboard camera. A novel coupled control law was
formulated to generate longitudinal control commands on-the-go based on the
predicted steering angle and other parameters such as actual speed of the ego
vehicle and the prescribed constraints for speed and steering. We analyzed
computational efficiency of the pipeline and evaluated robustness of the
trained models through exhaustive experimentation during the deployment phase.
We also compared our approach against state-of-the-art implementation in order
to comment on its validity."
"In today's clinical practice, magnetic resonance imaging (MRI) is routinely
accelerated through subsampling of the associated Fourier domain. Currently,
the construction of these subsampling strategies - known as experimental design
- relies primarily on heuristics. We propose to learn experimental design
strategies for accelerated MRI with policy gradient methods. Unexpectedly, our
experiments show that a simple greedy approximation of the objective leads to
solutions nearly on-par with the more general non-greedy approach. We offer a
partial explanation for this phenomenon rooted in greater variance in the
non-greedy objective's gradient estimates, and experimentally verify that this
variance hampers non-greedy models in adapting their policies to individual MR
images. We empirically show that this adaptivity is key to improving
subsampling designs."
"Recent work has shown that deep neural networks are capable of approximating
both value functions and policies in reinforcement learning domains featuring
continuous state and action spaces. However, to the best of our knowledge no
previous work has succeeded at using deep neural networks in structured
(parameterized) continuous action spaces. To fill this gap, this paper focuses
on learning within the domain of simulated RoboCup soccer, which features a
small set of discrete action types, each of which is parameterized with
continuous variables. The best learned agent can score goals more reliably than
the 2012 RoboCup champion agent. As such, this paper represents a successful
extension of deep reinforcement learning to the class of parameterized action
space MDPs."
"Graph-structured data appears frequently in domains including chemistry,
natural language semantics, social networks, and knowledge bases. In this work,
we study feature learning techniques for graph-structured inputs. Our starting
point is previous work on Graph Neural Networks (Scarselli et al., 2009), which
we modify to use gated recurrent units and modern optimization techniques and
then extend to output sequences. The result is a flexible and broadly useful
class of neural network models that has favorable inductive biases relative to
purely sequence-based models (e.g., LSTMs) when the problem is
graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and
graph algorithm learning tasks. We then show it achieves state-of-the-art
performance on a problem from program verification, in which subgraphs need to
be matched to abstract data structures."
"We propose a novel method of regularization for recurrent neural networks
called suprisal-driven zoneout. In this method, states zoneout (maintain their
previous value rather than updating), when the suprisal (discrepancy between
the last state's prediction and target) is small. Thus regularization is
adaptive and input-driven on a per-neuron basis. We demonstrate the
effectiveness of this idea by achieving state-of-the-art bits per character of
1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to
the best known highly-engineered compression methods."
"Vision-based object detection is one of the fundamental functions in numerous
traffic scene applications such as self-driving vehicle systems and advance
driver assistance systems (ADAS). However, it is also a challenging task due to
the diversity of traffic scene and the storage, power and computing source
limitations of the platforms for traffic scene applications. This paper
presents a generalized Haar filter based deep network which is suitable for the
object detection tasks in traffic scene. In this approach, we first decompose a
object detection task into several easier local regression tasks. Then, we
handle the local regression tasks by using several tiny deep networks which
simultaneously output the bounding boxes, categories and confidence scores of
detected objects. To reduce the consumption of storage and computing resources,
the weights of the deep networks are constrained to the form of generalized
Haar filter in training phase. Additionally, we introduce the strategy of
sparse windows generation to improve the efficiency of the algorithm. Finally,
we perform several experiments to validate the performance of our proposed
approach. Experimental results demonstrate that the proposed approach is both
efficient and effective in traffic scene compared with the state-of-the-art."
"Artificial Neural Networks (ANNs) have received increasing attention in
recent years with applications that span a wide range of disciplines including
vital domains such as medicine, network security and autonomous transportation.
However, neural network architectures are becoming increasingly complex and
with an increasing need to obtain real-time results from such models, it has
become pivotal to use parallelization as a mechanism for speeding up network
training and deployment. In this work we propose an implementation of Network
Parallel Training through Cannon's Algorithm for matrix multiplication. We show
that increasing the number of processes speeds up training until the point
where process communication costs become prohibitive; this point varies by
network complexity. We also show through empirical efficiency calculations that
the speedup obtained is superlinear."
"In the past decades, the rapid growth of computer and database technologies
has led to the rapid growth of large-scale datasets. On the other hand, data
mining applications with high dimensional datasets that require high speed and
accuracy are rapidly increasing. An important issue with these applications is
the curse of dimensionality, where the number of features is much higher than
the number of patterns. One of the dimensionality reduction approaches is
feature selection that can increase the accuracy of the data mining task and
reduce its computational complexity. The feature selection method aims at
selecting a subset of features with the lowest inner similarity and highest
relevancy to the target class. It reduces the dimensionality of the data by
eliminating irrelevant, redundant, or noisy data. In this paper, a comparative
analysis of different feature selection methods is presented, and a general
categorization of these methods is performed. Moreover, in this paper,
state-of-the-art swarm intelligence are studied, and the recent feature
selection methods based on these algorithms are reviewed. Furthermore, the
strengths and weaknesses of the different studied swarm intelligence-based
feature selection methods are evaluated."
"Deep convolutional neural networks (CNN) have achieved the unwavering
confidence in its performance on image processing tasks. The CNN architecture
constitutes a variety of different types of layers including the convolution
layer and the max-pooling layer. CNN practitioners widely understand the fact
that the stability of learning depends on how to initialize the model
parameters in each layer. Nowadays, no one doubts that the de facto standard
scheme for initialization is the so-called Kaiming initialization that has been
developed by He et al. The Kaiming scheme was derived from a much simpler model
than the currently used CNN structure having evolved since the emergence of the
Kaiming scheme. The Kaiming model consists only of the convolution and fully
connected layers, ignoring the max-pooling layer and the global average pooling
layer. In this study, we derived the initialization scheme again not from the
simplified Kaiming model, but precisely from the modern CNN architectures, and
empirically investigated how the new initialization method performs compared to
the de facto standard ones that are widely used today."
"We propose PermaKey, a novel approach to representation learning based on
object keypoints. It leverages the predictability of local image regions from
spatial neighborhoods to identify salient regions that correspond to object
parts, which are then converted to keypoints. Unlike prior approaches, it
utilizes predictability to discover object keypoints, an intrinsic property of
objects. This ensures that it does not overly bias keypoints to focus on
characteristics that are not unique to objects, such as movement, shape, colour
etc. We demonstrate the efficacy of PermaKey on Atari where it learns keypoints
corresponding to the most salient object parts and is robust to certain visual
distractors. Further, on downstream RL tasks in the Atari domain we demonstrate
how agents equipped with our keypoints outperform those using competing
alternatives, even on challenging environments with moving backgrounds or
distractor objects."
"Although deep learning models have taken on commercial and political
relevance, key aspects of their training and operation remain poorly
understood. This has sparked interest in science of deep learning projects,
many of which require large amounts of time, money, and electricity. But how
much of this research really needs to occur at scale? In this paper, we
introduce MNIST-1D: a minimalist, procedurally generated, low-memory, and
low-compute alternative to classic deep learning benchmarks. Although the
dimensionality of MNIST-1D is only 40 and its default training set size only
4000, MNIST-1D can be used to study inductive biases of different deep
architectures, find lottery tickets, observe deep double descent, metalearn an
activation function, and demonstrate guillotine regularization in
self-supervised learning. All these experiments can be conducted on a GPU or
often even on a CPU within minutes, allowing for fast prototyping, educational
use cases, and cutting-edge research on a low budget."
"This paper introduces a novel perspective about error in machine learning and
proposes inverse feature learning (IFL) as a representation learning approach
that learns a set of high-level features based on the representation of error
for classification or clustering purposes. The proposed perspective about error
representation is fundamentally different from current learning methods, where
in classification approaches they interpret the error as a function of the
differences between the true labels and the predicted ones or in clustering
approaches, in which the clustering objective functions such as compactness are
used. Inverse feature learning method operates based on a deep clustering
approach to obtain a qualitative form of the representation of error as
features. The performance of the proposed IFL method is evaluated by applying
the learned features along with the original features, or just using the
learned features in different classification and clustering techniques for
several data sets. The experimental results show that the proposed method leads
to promising results in classification and especially in clustering. In
classification, the proposed features along with the primary features improve
the results of most of the classification methods on several popular data sets.
In clustering, the performance of different clustering methods is considerably
improved on different data sets. There are interesting results that show some
few features of the representation of error capture highly informative aspects
of primary features. We hope this paper helps to utilize the error
representation learning in different feature learning domains."
"We propose a hardware learning rule for unsupervised clustering within a
novel spintronic computing architecture. The proposed approach leverages the
three-terminal structure of domain-wall magnetic tunnel junction devices to
establish a feedback loop that serves to train such devices when they are used
as synapses in a neuromorphic computing architecture."
"Knowledge Distillation (KD) is a common method for transferring the
``knowledge'' learned by one machine learning model (the \textit{teacher}) into
another model (the \textit{student}), where typically, the teacher has a
greater capacity (e.g., more parameters or higher bit-widths). To our
knowledge, existing methods overlook the fact that although the student absorbs
extra knowledge from the teacher, both models share the same input data -- and
this data is the only medium by which the teacher's knowledge can be
demonstrated. Due to the difference in model capacities, the student may not
benefit fully from the same data points on which the teacher is trained. On the
other hand, a human teacher may demonstrate a piece of knowledge with
individualized examples adapted to a particular student, for instance, in terms
of her cultural background and interests. Inspired by this behavior, we design
data augmentation agents with distinct roles to facilitate knowledge
distillation. Our data augmentation agents generate distinct training data for
the teacher and student, respectively. We find empirically that specially
tailored data points enable the teacher's knowledge to be demonstrated more
effectively to the student. We compare our approach with existing KD methods on
training popular neural architectures and demonstrate that role-wise data
augmentation improves the effectiveness of KD over strong prior approaches. The
code for reproducing our results can be found at
https://github.com/bigaidream-projects/role-kd"
"Binarization is an attractive strategy for implementing lightweight Deep
Convolutional Neural Networks (CNNs). Despite the unquestionable savings
offered, memory footprint above all, it may induce an excessive accuracy loss
that prevents a widespread use. This work elaborates on this aspect introducing
TentacleNet, a new template designed to improve the predictive performance of
binarized CNNs via parallelization. Inspired by the ensemble learning theory,
it consists of a compact topology that is end-to-end trainable and organized to
minimize memory utilization. Experimental results collected over three
realistic benchmarks show TentacleNet fills the gap left by classical binary
models, ensuring substantial memory savings w.r.t. state-of-the-art binary
ensemble methods."
"In this paper, we have extended the well-established universal approximator
theory to neural networks that use the unbounded ReLU activation function and a
nonlinear softmax output layer. We have proved that a sufficiently large neural
network using the ReLU activation function can approximate any function in
$L^1$ up to any arbitrary precision. Moreover, our theoretical results have
shown that a large enough neural network using a nonlinear softmax output layer
can also approximate any indicator function in $L^1$, which is equivalent to
mutually-exclusive class labels in any realistic multiple-class pattern
classification problems. To the best of our knowledge, this work is the first
theoretical justification for using the softmax output layers in neural
networks for pattern classification."
"The choice of activation function can have a large effect on the performance
of a neural network. While there have been some attempts to hand-engineer novel
activation functions, the Rectified Linear Unit (ReLU) remains the most
commonly-used in practice. This paper shows that evolutionary algorithms can
discover novel activation functions that outperform ReLU. A tree-based search
space of candidate activation functions is defined and explored with mutation,
crossover, and exhaustive search. Experiments on training wide residual
networks on the CIFAR-10 and CIFAR-100 image datasets show that this approach
is effective. Replacing ReLU with evolved activation functions results in
statistically significant increases in network accuracy. Optimal performance is
achieved when evolution is allowed to customize activation functions to a
particular task; however, these novel activation functions are shown to
generalize, achieving high performance across tasks. Evolutionary optimization
of activation functions is therefore a promising new dimension of metalearning
in neural networks."
"On a periodic basis, publicly traded companies report fundamentals, financial
data including revenue, earnings, debt, among others. Quantitative finance
research has identified several factors, functions of the reported data that
historically correlate with stock market performance. In this paper, we first
show through simulation that if we could select stocks via factors calculated
on future fundamentals (via oracle), that our portfolios would far outperform
standard factor models. Motivated by this insight, we train deep nets to
forecast future fundamentals from a trailing 5-year history. We propose
lookahead factor models which plug these predicted future fundamentals into
traditional factors. Finally, we incorporate uncertainty estimates from both
neural heteroscedastic regression and a dropout-based heuristic, improving
performance by adjusting our portfolios to avert risk. In retrospective
analysis, we leverage an industry-grade portfolio simulator (backtester) to
show simultaneous improvement in annualized return and Sharpe ratio.
Specifically, the simulated annualized return for the uncertainty-aware model
is 17.7% (vs 14.0% for a standard factor model) and the Sharpe ratio is 0.84
(vs 0.52)."
"Despite recent improvements in computer vision, artificial visual systems'
design is still daunting since an explanation of visual computing algorithms
remains elusive. Salient object detection is one problem that is still open due
to the difficulty of understanding the brain's inner workings. Progress on this
research area follows the traditional path of hand-made designs using
neuroscience knowledge. In recent years two different approaches based on
genetic programming appear to enhance their technique. One follows the idea of
combining previous hand-made methods through genetic programming and fuzzy
logic. The other approach consists of improving the inner computational
structures of basic hand-made models through artificial evolution. This
research work proposes expanding the artificial dorsal stream using a recent
proposal to solve salient object detection problems. This approach uses the
benefits of the two main aspects of this research area: fixation prediction and
detection of salient objects. We decided to apply the fusion of visual saliency
and image segmentation algorithms as a template. The proposed methodology
discovers several critical structures in the template through artificial
evolution. We present results on a benchmark designed by experts with
outstanding results in comparison with the state-of-the-art."
"Features extracted from Deep Neural Networks (DNNs) have proven to be very
effective in the context of Content Based Image Retrieval (CBIR). In recent
work, biologically inspired \textit{Hebbian} learning algorithms have shown
promises for DNN training. In this contribution, we study the performance of
such algorithms in the development of feature extractors for CBIR tasks.
Specifically, we consider a semi-supervised learning strategy in two steps:
first, an unsupervised pre-training stage is performed using Hebbian learning
on the image dataset; second, the network is fine-tuned using supervised
Stochastic Gradient Descent (SGD) training. For the unsupervised pre-training
stage, we explore the nonlinear Hebbian Principal Component Analysis (HPCA)
learning rule. For the supervised fine-tuning stage, we assume sample
efficiency scenarios, in which the amount of labeled samples is just a small
fraction of the whole dataset. Our experimental analysis, conducted on the
CIFAR10 and CIFAR100 datasets shows that, when few labeled samples are
available, our Hebbian approach provides relevant improvements compared to
various alternative methods."
"Vision transformer (ViT) and its variants have achieved remarkable successes
in various visual tasks. The key characteristic of these ViT models is to adopt
different aggregation strategies of spatial patch information within the
artificial neural networks (ANNs). However, there is still a key lack of
unified representation of different ViT architectures for systematic
understanding and assessment of model representation performance. Moreover, how
those well-performing ViT ANNs are similar to real biological neural networks
(BNNs) is largely unexplored. To answer these fundamental questions, we, for
the first time, propose a unified and biologically-plausible relational graph
representation of ViT models. Specifically, the proposed relational graph
representation consists of two key sub-graphs: aggregation graph and affine
graph. The former one considers ViT tokens as nodes and describes their spatial
interaction, while the latter one regards network channels as nodes and
reflects the information communication between channels. Using this unified
relational graph representation, we found that: a) a sweet spot of the
aggregation graph leads to ViTs with significantly improved predictive
performance; b) the graph measures of clustering coefficient and average path
length are two effective indicators of model prediction performance, especially
when applying on the datasets with small samples; c) our findings are
consistent across various ViT architectures and multiple datasets; d) the
proposed relational graph representation of ViT has high similarity with real
BNNs derived from brain science data. Overall, our work provides a novel
unified and biologically-plausible paradigm for more interpretable and
effective representation of ViT ANNs."
"There is mounting evidence of emergent phenomena in the capabilities of deep
learning methods as we scale up datasets, model sizes, and training times.
While there are some accounts of how these resources modulate statistical
capacity, far less is known about their effect on the computational problem of
model training. This work conducts such an exploration through the lens of
learning a $k$-sparse parity of $n$ bits, a canonical discrete search problem
which is statistically easy but computationally hard. Empirically, we find that
a variety of neural networks successfully learn sparse parities, with
discontinuous phase transitions in the training curves. On small instances,
learning abruptly occurs at approximately $n^{O(k)}$ iterations; this nearly
matches SQ lower bounds, despite the apparent lack of a sparse prior. Our
theoretical analysis shows that these observations are not explained by a
Langevin-like mechanism, whereby SGD ""stumbles in the dark"" until it finds the
hidden set of features (a natural algorithm which also runs in $n^{O(k)}$
time). Instead, we show that SGD gradually amplifies the sparse solution via a
Fourier gap in the population gradient, making continual progress that is
invisible to loss and error metrics."
"In recent years, Generative Adversarial Networks (GANs) have become a hot
topic among researchers and engineers that work with deep learning. It has been
a ground-breaking technique which can generate new pieces of content of data in
a consistent way. The topic of GANs has exploded in popularity due to its
applicability in fields like image generation and synthesis, and music
production and composition. GANs have two competing neural networks: a
generator and a discriminator. The generator is used to produce new samples or
pieces of content, while the discriminator is used to recognize whether the
piece of content is real or generated. What makes it different from other
generative models is its ability to learn unlabeled samples. In this review
paper, we will discuss the evolution of GANs, several improvements proposed by
the authors and a brief comparison between the different models. Index Terms
generative adversarial networks, unsupervised learning, deep learning."
"Fine-tuning can be vulnerable to adversarial attacks. Existing works about
black-box attacks on fine-tuned models (BAFT) are limited by strong
assumptions. To fill the gap, we propose two novel BAFT settings, cross-domain
and cross-domain cross-architecture BAFT, which only assume that (1) the target
model for attacking is a fine-tuned model, and (2) the source domain data is
known and accessible. To successfully attack fine-tuned models under both
settings, we propose to first train an adversarial generator against the source
model, which adopts an encoder-decoder architecture and maps a clean input to
an adversarial example. Then we search in the low-dimensional latent space
produced by the encoder of the adversarial generator. The search is conducted
under the guidance of the surrogate gradient obtained from the source model.
Experimental results on different domains and different network architectures
demonstrate that the proposed attack method can effectively and efficiently
attack the fine-tuned models."
"Over past few years afterward the birth of ResNet, skip connection has become
the defacto standard for the design of modern architectures due to its
widespread adoption, easy optimization and proven performance. Prior work has
explained the effectiveness of the skip connection mechanism from different
perspectives. In this work, we deep dive into the model's behaviors with skip
connections which can be formulated as a learnable Markov chain. An efficient
Markov chain is preferred as it always maps the input data to the target domain
in a better way. However, while a model is explained as a Markov chain, it is
not guaranteed to be optimized following an efficient Markov chain by existing
SGD-based optimizers which are prone to get trapped in local optimal points. In
order to towards a more efficient Markov chain, we propose a simple routine of
penal connection to make any residual-like model become a learnable Markov
chain. Aside from that, the penal connection can also be viewed as a particular
model regularization and can be easily implemented with one line of code in the
most popular deep learning frameworks~\footnote{Source code:
\url{https://github.com/densechen/penal-connection}}. The encouraging
experimental results in multi-modal translation and image recognition
empirically confirm our conjecture of the learnable Markov chain view and
demonstrate the superiority of the proposed penal connection."
"In this paper, we allocate IoT devices as resources for smart services with
time-constrained resource requirements. The allocation method named as BRAD can
work under multiple resource scenarios with diverse resource richnesses,
availabilities and costs, such as the intelligent healthcare system deployed by
Harbin Institute of Technology (HIT-IHC). The allocation aims for
bimetric-balancing under the multi-scenario case, i.e., the profit and cost
associated with service satisfaction are jointly optimised and balanced wisely.
Besides, we abstract IoT devices as digital objects (DO) to make them easier to
interact with during resource allocation. Considering that the problem is
NP-Hard and the optimisation objective is not differentiable, we utilise Grey
Wolf Optimisation (GWO) algorithm as the model optimiser. Specifically, we
tackle the deficiencies of GWO and significantly improve its performance by
introducing three new mechanisms to form the BRAD-GWA algorithm. Comprehensive
experiments are conducted on realistic HIT-IHC IoT testbeds and several
algorithms are compared, including the allocation method originally used by
HIT-IHC system to verify the effectiveness of the BRAD-GWA. The BRAD-GWA
achieves a 3.14 times and 29.6% objective reduction compared with the HIT-IHC
and the original GWO algorithm, respectively."
"Aiming at highly accurate object detection for connected and automated
vehicles (CAVs), this paper presents a Deep Neural Network based 3D object
detection model that leverages a three-stage feature extractor by developing a
novel LIDAR-Camera fusion scheme. The proposed feature extractor extracts
high-level features from two input sensory modalities and recovers the
important features discarded during the convolutional process. The novel fusion
scheme effectively fuses features across sensory modalities and convolutional
layers to find the best representative global features. The fused features are
shared by a two-stage network: the region proposal network (RPN) and the
detection head (DH). The RPN generates high-recall proposals, and the DH
produces final detection results. The experimental results show the proposed
model outperforms more recent research on the KITTI 2D and 3D detection
benchmark, particularly for distant and highly occluded instances."
"Machine learning methods have been used to accelerate the molecule
optimization process. However, efficient search for optimized molecules
satisfying several properties with scarce labeled data remains a challenge for
machine learning molecule optimization. In this study, we propose MOMO, a
multi-objective molecule optimization framework to address the challenge by
combining learning of chemical knowledge with Pareto-based multi-objective
evolutionary search. To learn chemistry, it employs a self-supervised codec to
construct an implicit chemical space and acquire the continues representation
of molecules. To explore the established chemical space, MOMO uses
multi-objective evolution to comprehensively and efficiently search for similar
molecules with multiple desirable properties. We demonstrate the high
performance of MOMO on four multi-objective property and similarity
optimization tasks, and illustrate the search capability of MOMO through case
studies. Remarkably, our approach significantly outperforms previous approaches
in optimizing three objectives simultaneously. The results show the
optimization capability of MOMO, suggesting to improve the success rate of lead
molecule optimization."
"Models of sensory processing and learning in the cortex need to efficiently
assign credit to synapses in all areas. In deep learning, a known solution is
error backpropagation, which however requires biologically implausible weight
transport from feed-forward to feedback paths.
  We introduce Phaseless Alignment Learning (PAL), a bio-plausible method to
learn efficient feedback weights in layered cortical hierarchies. This is
achieved by exploiting the noise naturally found in biophysical systems as an
additional carrier of information. In our dynamical system, all weights are
learned simultaneously with always-on plasticity and using only information
locally available to the synapses. Our method is completely phase-free (no
forward and backward passes or phased learning) and allows for efficient error
propagation across multi-layer cortical hierarchies, while maintaining
biologically plausible signal transport and learning.
  Our method is applicable to a wide class of models and improves on previously
known biologically plausible ways of credit assignment: compared to random
synaptic feedback, it can solve complex tasks with less neurons and learn more
useful latent representations. We demonstrate this on various classification
tasks using a cortical microcircuit model with prospective coding."
"Multiplication layers are a key component in various influential neural
network modules, including self-attention and hypernetwork layers. In this
paper, we investigate the approximation capabilities of deep neural networks
with intermediate neurons connected by simple multiplication operations. We
consider two classes of target functions: generalized bandlimited functions,
which are frequently used to model real-world signals with finite bandwidth,
and Sobolev-Type balls, which are embedded in the Sobolev Space
$\mathcal{W}^{r,2}$. Our results demonstrate that multiplicative neural
networks can approximate these functions with significantly fewer layers and
neurons compared to standard ReLU neural networks, with respect to both input
dimension and approximation error. These findings suggest that multiplicative
gates can outperform standard feed-forward layers and have potential for
improving neural network design."
"Evolutionary algorithms have been used to evolve a population of actors to
generate diverse experiences for training reinforcement learning agents, which
helps to tackle the temporal credit assignment problem and improves the
exploration efficiency. However, when adapting this approach to address
constrained problems, balancing the trade-off between the reward and constraint
violation is hard. In this paper, we propose a novel evolutionary constrained
reinforcement learning (ECRL) algorithm, which adaptively balances the reward
and constraint violation with stochastic ranking, and at the same time,
restricts the policy's behaviour by maintaining a set of Lagrange relaxation
coefficients with a constraint buffer. Extensive experiments on robotic control
benchmarks show that our ECRL achieves outstanding performance compared to
state-of-the-art algorithms. Ablation analysis shows the benefits of
introducing stochastic ranking and constraint buffer."
"Extending a recent suggestion to generate new instances for numerical
black-box optimization benchmarking by interpolating pairs of the
well-established BBOB functions from the COmparing COntinuous Optimizers (COCO)
platform, we propose in this work a further generalization that allows multiple
affine combinations of the original instances and arbitrarily chosen locations
of the global optima. We demonstrate that the MA-BBOB generator can help fill
the instance space, while overall patterns in algorithm performance are
preserved. By combining the landscape features of the problems with the
performance data, we pose the question of whether these features are as useful
for algorithm selection as previous studies suggested. MA-BBOB is built on the
publicly available IOHprofiler platform, which facilitates standardized
experimentation routines, provides access to the interactive IOHanalyzer module
for performance analysis and visualization, and enables comparisons with the
rich and growing data collection available for the (MA-)BBOB functions."
"We quantify the impact of thermo-optic and free-carrier effects on time-delay
reservoir computing using a silicon microring resonator. We identify pump power
and frequency detuning ranges with NMSE less than 0.05 for the NARMA-10 task
depending on the time constants of the two considered effects."
"In this article, we present a novel data assimilation strategy in pore-scale
imaging and demonstrate that this makes it possible to robustly address
reactive inverse problems incorporating Uncertainty Quantification (UQ).
Pore-scale modeling of reactive flow offers a valuable opportunity to
investigate the evolution of macro-scale properties subject to dynamic
processes. Yet, they suffer from imaging limitations arising from the
associated X-ray microtomography (X-ray microCT) process, which induces
discrepancies in the properties estimates. Assessment of the kinetic parameters
also raises challenges, as reactive coefficients are critical parameters that
can cover a wide range of values. We account for these two issues and ensure
reliable calibration of pore-scale modeling, based on dynamical microCT images,
by integrating uncertainty quantification in the workflow.
  The present method is based on a multitasking formulation of reactive inverse
problems combining data-driven and physics-informed techniques in calcite
dissolution. This allows quantifying morphological uncertainties on the
porosity field and estimating reactive parameter ranges through prescribed PDE
models with a latent concentration field and dynamical microCT. The data
assimilation strategy relies on sequential reinforcement incorporating
successively additional PDE constraints. We guarantee robust and unbiased
uncertainty quantification by straightforward adaptive weighting of Bayesian
Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity
changes during geochemical transformations. We demonstrate successful Bayesian
Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT
images with meaningful posterior distribution on the reactive parameters and
dimensionless numbers."
"Traveling waves of neural activity have been observed throughout the brain at
a diversity of regions and scales; however, their precise computational role is
still debated. One physically inspired hypothesis suggests that the cortical
sheet may act like a wave-propagating system capable of invertibly storing a
short-term memory of sequential stimuli through induced waves traveling across
the cortical surface, and indeed many experimental results from neuroscience
correlate wave activity with memory tasks. To date, however, the computational
implications of this idea have remained hypothetical due to the lack of a
simple recurrent neural network architecture capable of exhibiting such waves.
In this work, we introduce a model to fill this gap, which we denote the
Wave-RNN (wRNN), and demonstrate how such an architecture indeed efficiently
encodes the recent past through a suite of synthetic memory tasks where wRNNs
learn faster and reach significantly lower error than wave-free counterparts.
We further explore the implications of this memory storage system on more
complex sequence modeling tasks such as sequential image classification and
find that wave-based models not only again outperform comparable wave-free RNNs
while using significantly fewer parameters, but additionally perform comparably
to more complex gated architectures such as LSTMs and GRUs."
"Prompt engineering is crucial for deploying LLMs but is poorly understood
mathematically. We formalize LLM systems as a class of discrete stochastic
dynamical systems to explore prompt engineering through the lens of control
theory. We offer a mathematical analysis of the limitations on the
controllability of self-attention as a function of the singular values of the
parameter matrices. We present complementary empirical results on the
controllability of a panel of LLMs, including Falcon-7b, Llama-7b, and
Falcon-40b. Given initial state $\mathbf x_0$ from Wikitext and prompts of
length $k \leq 10$ tokens, we find that the ""correct"" next token is reachable
at least 97% of the time, and that the top 75 most likely next tokens are
reachable at least 85% of the time. Intriguingly, short prompt sequences can
dramatically alter the likelihood of specific outputs, even making the least
likely tokens become the most likely ones. This control-theoretic analysis of
LLMs demonstrates the significant and poorly understood role of input sequences
in steering output probabilities, offering a foundational perspective for
enhancing language model system capabilities."
"In this paper, we show that the Kolmogorov two hidden layer neural network
model with a continuous, discontinuous bounded or unbounded activation function
in the second hidden layer can precisely represent continuous, discontinuous
bounded and all unbounded multivariate functions, respectively."
"Prior attacks on graph neural networks have mostly focused on graph poisoning
and evasion, neglecting the network's weights and biases. Traditional
weight-based fault injection attacks, such as bit flip attacks used for
convolutional neural networks, do not consider the unique properties of graph
neural networks. We propose the Injectivity Bit Flip Attack, the first bit flip
attack designed specifically for graph neural networks. Our attack targets the
learnable neighborhood aggregation functions in quantized message passing
neural networks, degrading their ability to distinguish graph structures and
losing the expressivity of the Weisfeiler-Lehman test. Our findings suggest
that exploiting mathematical properties specific to certain graph neural
network architectures can significantly increase their vulnerability to bit
flip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive
Graph Isomorphism Networks trained on various graph property prediction
datasets to random output by flipping only a small fraction of the network's
bits, demonstrating its higher destructive power compared to a bit flip attack
transferred from convolutional neural networks. Our attack is transparent and
motivated by theoretical insights which are confirmed by extensive empirical
results."
"In power systems, the incorporation of capacitors offers a wide range of
established advantages. These benefits encompass the enhancement of the systems
power factor, optimization of voltage profiles, increased capacity for current
flow through cables and transformers, and the mitigation of losses attributed
to the compensation of reactive power components. Different techniques have
been applied to enhance the performance of the distribution system by reducing
line losses. This paper focuses on reducing line losses through the optimal
placement and sizing of capacitors. Optimal capacitor placement is analysed
using load flow analysis with the Newton Raphson method. The placement of
capacitor optimization is related to the sensitivity of the buses, which
depends on the loss sensitivity factor. The optimal capacitor size is
determined using Particle Swarm Optimization (PSO). The analysis is conducted
using the IEEE 14 bus system in MATLAB. The results reveal that placing
capacitors at the most sensitive bus locations leads to a significant reduction
in line losses. Additionally, the optimal capacitor size has a substantial
impact on improving the voltage profile and the power loss is reduced by 21.02
percent through the proposed method."
"With the growth of machine learning techniques, privacy of data of users has
become a major concern. Most of the machine learning algorithms rely heavily on
large amount of data which may be collected from various sources. Collecting
these data yet maintaining privacy policies has become one of the most
challenging tasks for the researchers. To combat this issue, researchers have
introduced federated learning, where a prediction model is learnt by ensuring
the privacy of data of clients data. However, the prevalent federated learning
algorithms possess an accuracy and efficiency trade-off, especially for non-IID
data. In this research, we propose a centralized, neural network-based
federated learning system. The centralized algorithm incorporates micro-level
parallel processing inspired by the traditional mini-batch algorithm where the
client devices and the server handle the forward and backward propagation
respectively. We also devise a semi-centralized version of our proposed
algorithm. This algorithm takes advantage of edge computing for minimizing the
load from the central server, where clients handle both the forward and
backward propagation while sacrificing the overall train time to some extent.
We evaluate our proposed systems on five well-known benchmark datasets and
achieve satisfactory performance in a reasonable time across various data
distribution settings as compared to some existing benchmark algorithms."
"What has an Artificial Neural Network (ANN) learned after being successfully
trained to solve a task - the set of training items or the relations between
them? This question is difficult to answer for modern applied ANNs because of
their enormous size and complexity. Therefore, here we consider a
low-dimensional network and a simple task, i.e., the network has to reproduce a
set of training items identically. We construct the family of solutions
analytically and use standard learning algorithms to obtain numerical
solutions. These numerical solutions differ depending on the optimization
algorithm and the weight initialization and are shown to be particular members
of the family of analytical solutions. In this simple setting, we observe that
the general structure of the network weights represents the training set's
symmetry group, i.e., the relations between training items. As a consequence,
linear networks generalize, i.e., reproduce items that were not part of the
training set but are consistent with the symmetry of the training set. In
contrast, non-linear networks tend to learn individual training items and show
associative memory. At the same time, their ability to generalize is limited. A
higher degree of generalization is obtained for networks whose activation
function contains a linear regime, such as tanh. Our results suggest ANN's
ability to generalize - instead of learning items - could be improved by
generating a sufficiently big set of elementary operations to represent
relations and strongly depends on the applied non-linearity."
"To guide the design of better iterative optimisation heuristics, it is
imperative to understand how inherent structural biases within algorithm
components affect the performance on a wide variety of search landscapes. This
study explores the impact of structural bias in the modular Covariance Matrix
Adaptation Evolution Strategy (modCMA), focusing on the roles of various
modulars within the algorithm. Through an extensive investigation involving
435,456 configurations of modCMA, we identified key modules that significantly
influence structural bias of various classes. Our analysis utilized the
Deep-BIAS toolbox for structural bias detection and classification,
complemented by SHAP analysis for quantifying module contributions. The
performance of these configurations was tested on a sequence of
affine-recombined functions, maintaining fixed optimum locations while
gradually varying the landscape features. Our results demonstrate an interplay
between module-induced structural bias and algorithm performance across
different landscape characteristics."
"Using OpenCL-based high-level synthesis, we create a number of spiking neural
network (SNN) simulators for the Potjans-Diesmann cortical microcircuit for a
high-end Field-Programmable Gate Array (FPGA). Our best simulators simulate the
circuit 25\% faster than real-time, require less than 21 nJ per synaptic event,
and are bottle-necked by the device's on-chip memory. Speed-wise they compare
favorably to the state-of-the-art GPU-based simulators and their energy usage
is lower than any other published result. This result is the first for
simulating the circuit on a single hardware accelerator. We also extensively
analyze the techniques and algorithms we implement our simulators with, many of
which can be realized on other types of hardware. Thus, this article is of
interest to any researcher or practitioner interested in efficient SNN
simulation, whether they target FPGAs or not."
"Towards energy-efficient artificial intelligence similar to the human brain,
the bio-inspired spiking neural networks (SNNs) have advantages of biological
plausibility, event-driven sparsity, and binary activation. Recently,
large-scale language models exhibit promising generalization capability, making
it a valuable issue to explore more general spike-driven models. However, the
binary spikes in existing SNNs fail to encode adequate semantic information,
placing technological challenges for generalization. This work proposes the
first fully spiking mechanism for general language tasks, including both
discriminative and generative ones. Different from previous spikes with {0,1}
levels, we propose a more general spike formulation with bi-directional,
elastic amplitude, and elastic frequency encoding, while still maintaining the
addition nature of SNNs. In a single time step, the spike is enhanced by
direction and amplitude information; in spike frequency, a strategy to control
spike firing rate is well designed. We plug this elastic bi-spiking mechanism
in language modeling, named SpikeLM. It is the first time to handle general
language tasks with fully spike-driven models, which achieve much higher
accuracy than previously possible. SpikeLM also greatly bridges the performance
gap between SNNs and ANNs in language modeling. Our code is available at
https://github.com/Xingrun-Xing/SpikeLM."
"Activity recognition is a challenging task due to the large scale of
trajectory data and the need for prompt and efficient processing. Existing
methods have attempted to mitigate this problem by employing traditional LSTM
architectures, but these approaches often suffer from inefficiencies in
processing large datasets. In response to this challenge, we propose VecLSTM, a
novel framework that enhances the performance and efficiency of LSTM-based
neural networks. Unlike conventional approaches, VecLSTM incorporates
vectorization layers, leveraging optimized mathematical operations to process
input sequences more efficiently. We have implemented VecLSTM and incorporated
it into the MySQL database. To evaluate the effectiveness of VecLSTM, we
compare its performance against a conventional LSTM model using a dataset
comprising 1,467,652 samples with seven unique labels. Experimental results
demonstrate superior accuracy and efficiency compared to the state-of-the-art,
with VecLSTM achieving a validation accuracy of 85.57\%, a test accuracy of
85.47\%, and a weighted F1-score of 0.86. Furthermore, VecLSTM significantly
reduces training time, offering a 26.2\% reduction compared to traditional LSTM
models."
"Graph neural architecture search (GNAS) can customize high-performance graph
neural network architectures for specific graph tasks or datasets. However,
existing GNAS methods begin searching for architectures from a zero-knowledge
state, ignoring the prior knowledge that may improve the search efficiency. The
available knowledge base (e.g. NAS-Bench-Graph) contains many rich
architectures and their multiple performance metrics, such as the accuracy
(#Acc) and number of parameters (#Params). This study proposes exploiting such
prior knowledge to accelerate the multi-objective evolutionary search on a new
graph dataset, named knowledge-aware evolutionary GNAS (KEGNAS). KEGNAS employs
the knowledge base to train a knowledge model and a deep multi-output Gaussian
process (DMOGP) in one go, which generates and evaluates transfer architectures
in only a few GPU seconds. The knowledge model first establishes a
dataset-to-architecture mapping, which can quickly generate candidate transfer
architectures for a new dataset. Subsequently, the DMOGP with architecture and
dataset encodings is designed to predict multiple performance metrics for
candidate transfer architectures on the new dataset. According to the predicted
metrics, non-dominated candidate transfer architectures are selected to
warm-start the multi-objective evolutionary algorithm for optimizing the #Acc
and #Params on a new dataset. Empirical studies on NAS-Bench-Graph and five
real-world datasets show that KEGNAS swiftly generates top-performance
architectures, achieving 4.27% higher accuracy than advanced evolutionary
baselines and 11.54% higher accuracy than advanced differentiable baselines. In
addition, ablation studies demonstrate that the use of prior knowledge
significantly improves the search performance."
"When regarding the suffering of others, we often experience personal distress
and feel compelled to help. Inspired by living systems, we investigate the
emergence of prosocial behavior among autonomous agents that are motivated by
homeostatic self-regulation. We perform multi-agent reinforcement learning,
treating each agent as a vulnerable homeostat charged with maintaining its own
well-being. We introduce an empathy-like mechanism to share homeostatic states
between agents: an agent can either \emph{observe} their partner's internal
state (cognitive empathy) or the agent's internal state can be \emph{directly
coupled} to that of their partner's (affective empathy). In three simple
multi-agent environments, we show that prosocial behavior arises only under
homeostatic coupling - when the distress of a partner can affect one's own
well-being. Our findings specify the type and role of empathy in artificial
agents capable of prosocial behavior."
"This paper addresses theory in evolutionary multiobjective optimisation (EMO)
and focuses on the role of crossover operators in many-objective optimisation.
The advantages of using crossover are hardly understood and rigorous runtime
analyses with crossover are lagging far behind its use in practice,
specifically in the case of more than two objectives. We present a
many-objective problem class together with a theoretical runtime analysis of
the widely used NSGA-III to demonstrate that crossover can yield an exponential
speedup on the runtime. In particular, this algorithm can find the Pareto set
in expected polynomial time when using crossover while without crossover it
requires exponential time to even find a single Pareto-optimal point. To our
knowledge, this is the first rigorous runtime analysis in many-objective
optimisation demonstrating an exponential performance gap when using crossover
for more than two objectives."
"The security issue of large language models (LLMs) has gained significant
attention recently, with various defense mechanisms developed to prevent
harmful outputs, among which safeguards based on text embedding models serve as
a fundamental defense. Through testing, we discover that the distribution of
text embedding model outputs is significantly biased with a large mean.
Inspired by this observation, we propose novel efficient methods to search for
universal magic words that can attack text embedding models. The universal
magic words as suffixes can move the embedding of any text towards the bias
direction, therefore manipulate the similarity of any text pair and mislead
safeguards. By appending magic words to user prompts and requiring LLMs to end
answers with magic words, attackers can jailbreak the safeguard. To eradicate
this security risk, we also propose defense mechanisms against such attacks,
which can correct the biased distribution of text embeddings in a train-free
manner."
"We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive
Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal
exploration mechanism which allows active learning of inverse models in
high-dimensional redundant robots. This allows a robot to efficiently and
actively learn distributions of parameterized motor skills/policies that solve
a corresponding distribution of parameterized tasks/goals. The architecture
makes the robot sample actively novel parameterized tasks in the task space,
based on a measure of competence progress, each of which triggers low-level
goal-directed learning of the motor policy pa- rameters that allow to solve it.
For both learning and generalization, the system leverages regression
techniques which allow to infer the motor policy parameters corresponding to a
given novel parameterized task, and based on the previously learnt
correspondences between policy and task parameters. We present experiments with
high-dimensional continuous sensorimotor spaces in three different robotic
setups: 1) learning the inverse kinematics in a highly-redundant robotic arm,
2) learning omnidirectional locomotion with motor primitives in a quadruped
robot, 3) an arm learning to control a fishing rod with a flexible wire. We
show that 1) exploration in the task space can be a lot faster than exploration
in the actuator space for learning inverse models in redundant robots; 2)
selecting goals maximizing competence progress creates developmental
trajectories driving the robot to progressively focus on tasks of increasing
complexity and is statistically significantly more efficient than selecting
tasks randomly, as well as more efficient than different standard active motor
babbling methods; 3) this architecture allows the robot to actively discover
which parts of its task space it can learn to reach and which part it cannot."
"Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not."
"Several papers have recently contained reports on applying machine learning
(ML) to the automation of software engineering (SE) tasks, such as project
management, modeling and development. However, there appear to be no approaches
comparing how software engineers fare against machine-learning algorithms as
applied to specific software development tasks. Such a comparison is essential
to gain insight into which tasks are better performed by humans and which by
machine learning and how cooperative work or human-in-the-loop processes can be
implemented more effectively. In this paper, we present an empirical study that
compares how software engineers and machine-learning algorithms perform and
reuse tasks. The empirical study involves the synthesis of the control
structure of an autonomous streetlight application. Our approach consists of
four steps. First, we solved the problem using machine learning to determine
specific performance and reuse tasks. Second, we asked software engineers with
different domain knowledge levels to provide a solution to the same tasks.
Third, we compared how software engineers fare against machine-learning
algorithms when accomplishing the performance and reuse tasks based on criteria
such as energy consumption and safety. Finally, we analyzed the results to
understand which tasks are better performed by either humans or algorithms so
that they can work together more effectively. Such an understanding and the
resulting human-in-the-loop approaches, which take into account the strengths
and weaknesses of humans and machine-learning algorithms, are fundamental not
only to provide a basis for cooperative work in support of software
engineering, but also, in other areas."
"In symbolic regression, the search for analytic models is typically driven
purely by the prediction error observed on the training data samples. However,
when the data samples do not sufficiently cover the input space, the prediction
error does not provide sufficient guidance toward desired models. Standard
symbolic regression techniques then yield models that are partially incorrect,
for instance, in terms of their steady-state characteristics or local behavior.
If these properties were considered already during the search process, more
accurate and relevant models could be produced. We propose a multi-objective
symbolic regression approach that is driven by both the training data and the
prior knowledge of the properties the desired model should manifest. The
properties given in the form of formal constraints are internally represented
by a set of discrete data samples on which candidate models are exactly
checked. The proposed approach was experimentally evaluated on three test
problems with results clearly demonstrating its capability to evolve realistic
models that fit the training data well while complying with the prior knowledge
of the desired model characteristics at the same time. It outperforms standard
symbolic regression by several orders of magnitude in terms of the mean squared
deviation from a reference model."
"We propose a class of trainable deep learning-based geometries called Neural
Spacetimes (NSTs), which can universally represent nodes in weighted directed
acyclic graphs (DAGs) as events in a spacetime manifold. While most works in
the literature focus on undirected graph representation learning or causality
embedding separately, our differentiable geometry can encode both graph edge
weights in its spatial dimensions and causality in the form of edge
directionality in its temporal dimensions. We use a product manifold that
combines a quasi-metric (for space) and a partial order (for time). NSTs are
implemented as three neural networks trained in an end-to-end manner: an
embedding network, which learns to optimize the location of nodes as events in
the spacetime manifold, and two other networks that optimize the space and time
geometries in parallel, which we call a neural (quasi-)metric and a neural
partial order, respectively. The latter two networks leverage recent ideas at
the intersection of fractal geometry and deep learning to shape the geometry of
the representation space in a data-driven fashion, unlike other works in the
literature that use fixed spacetime manifolds such as Minkowski space or De
Sitter space to embed DAGs. Our main theoretical guarantee is a universal
embedding theorem, showing that any $k$-point DAG can be embedded into an NST
with $1+\mathcal{O}(\log(k))$ distortion while exactly preserving its causal
structure. The total number of parameters defining the NST is sub-cubic in $k$
and linear in the width of the DAG. If the DAG has a planar Hasse diagram, this
is improved to $\mathcal{O}(\log(k)) + 2)$ spatial and 2 temporal dimensions.
We validate our framework computationally with synthetic weighted DAGs and
real-world network embeddings; in both cases, the NSTs achieve lower embedding
distortions than their counterparts using fixed spacetime geometries."
"This paper introduces a novel framework that leverages large language models
(LLMs) for machine translation (MT). We start with one conjecture: an ideal
translation should contain complete and accurate information for a strong
enough LLM to recover the original sentence. We generate multiple translation
candidates from a source language A to a target language B, and subsequently
translate these candidates back to the original language A. By evaluating the
cycle consistency between the original and back-translated sentences using
metrics such as token-level precision and accuracy, we implicitly estimate the
translation quality in language B, without knowing its ground-truth. This also
helps to evaluate the LLM translation capability, only with monolingual
corpora. For each source sentence, we identify the translation candidate with
optimal cycle consistency with the original sentence as the final answer. Our
experiments demonstrate that larger LLMs, or the same LLM with more forward
passes during inference, exhibit increased cycle consistency, aligning with the
LLM model size scaling law and test-time computation scaling law. This work
provide methods for, 1) to implicitly evaluate translation quality of a
sentence in the target language, 2), to evaluate capability of LLM for
any-to-any-language translation, and 3), how to generate a better translation
for a specific LLM."
"Neural networks based on soft and biological matter constitute an interesting
potential alternative to traditional implementations based on electric
circuits. DNA is a particularly promising system in this context due its
natural ability to store information. In recent years, researchers have started
to construct neural networks that are based on DNA. In this chapter, I provide
a very basic introduction to the concept of DNA neural networks, aiming at an
audience that is not familiar with biochemistry."
"The NWO Priority Programme Language and Speech Technology is a 5-year
research programme aiming at the development of spoken language information
systems. In the Programme, two alternative natural language processing (NLP)
modules are developed in parallel: a grammar-based (conventional, rule-based)
module and a data-oriented (memory-based, stochastic, DOP) module. In order to
compare the NLP modules, a formal evaluation has been carried out three years
after the start of the Programme. This paper describes the evaluation procedure
and the evaluation results. The grammar-based component performs much better
than the data-oriented one in this comparison."
"Transformation-based learning has been successfully employed to solve many
natural language processing problems. It achieves state-of-the-art performance
on many natural language processing tasks and does not overtrain easily.
However, it does have a serious drawback: the training time is often
intorelably long, especially on the large corpora which are often used in NLP.
In this paper, we present a novel and realistic method for speeding up the
training time of a transformation-based learner without sacrificing
performance. The paper compares and contrasts the training time needed and
performance achieved by our modified learner with two other systems: a standard
transformation-based learner, and the ICA system \cite{hepple00:tbl}. The
results of these experiments show that our system is able to achieve a
significant improvement in training time while still achieving the same
performance as a standard transformation-based learner. This is a valuable
contribution to systems and algorithms which utilize transformation-based
learning at any part of the execution."
"This article studies the problem of assessing relevance to each of the rules
of a reference resolution system. The reference solver described here stems
from a formal model of reference and is integrated in a reference processing
workbench. Evaluation of the reference resolution is essential, as it enables
differential evaluation of individual rules. Numerical values of these measures
are given, and discussed, for simple selection rules and other processing
rules; such measures are then studied for numerical parameters."
"This paper presents an ""elitist approach"" for extracting automatically
well-realized speech sounds with high confidence. The elitist approach uses a
speech recognition system based on Hidden Markov Models (HMM). The HMM are
trained on speech sounds which are systematically well-detected in an iterative
procedure. The results show that, by using the HMM models defined in the
training phase, the speech recognizer detects reliably specific speech sounds
with a small rate of errors."
"In this paper we present an automated method for the classification of the
origin of non-native speakers. The origin of non-native speakers could be
identified by a human listener based on the detection of typical pronunciations
for each nationality. Thus we suppose the existence of several phoneme
sequences that might allow the classification of the origin of non-native
speakers. Our new method is based on the extraction of discriminative sequences
of phonemes from a non-native English speech database. These sequences are used
to construct a probabilistic classifier for the speakers' origin. The existence
of discriminative phone sequences in non-native speech is a significant result
of this work. The system that we have developed achieved a significant correct
classification rate of 96.3% and a significant error reduction compared to some
other tested techniques."
"In this paper, we present an open-source parsing environment (Tuebingen
Linguistic Parsing Architecture, TuLiPA) which uses Range Concatenation Grammar
(RCG) as a pivot formalism, thus opening the way to the parsing of several
mildly context-sensitive formalisms. This environment currently supports
tree-based grammars (namely Tree-Adjoining Grammars, TAG) and Multi-Component
Tree-Adjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not
only of syntactic structures, but also of the corresponding semantic
representations. It is used for the development of a tree-based grammar for
German."
"Researchers in textual entailment have begun to consider inferences involving
'downward-entailing operators', an interesting and important class of lexical
items that change the way inferences are made. Recent work proposed a method
for learning English downward-entailing operators that requires access to a
high-quality collection of 'negative polarity items' (NPIs). However, English
is one of the very few languages for which such a list exists. We propose the
first approach that can be applied to the many languages for which there is no
pre-existing high-precision database of NPIs. As a case study, we apply our
method to Romanian and show that our method yields good results. Also, we
perform a cross-linguistic analysis that suggests interesting connections to
some findings in linguistic typology."
"A temporal analysis of emoticon use in Swedish, Italian, German and English
asynchronous electronic communication is reported. Emoticons are classified as
positive, negative and neutral. Postings to newsgroups over a 66 week period
are considered. The aggregate analysis of emoticon use in newsgroups for
science and politics tend on the whole to be consistent over the entire time
period. Where possible, events that coincide with divergences from trends in
language-subject pairs are noted. Political discourse in Italian over the
period shows marked use of negative emoticons, and in Swedish, positive
emoticons."
"This paper presents a novel approach to machine translation by combining the
state of art name entity translation scheme. Improper translation of name
entities lapse the quality of machine translated output. In this work, name
entities are transliterated by using statistical rule based approach. This
paper describes the translation and transliteration of name entities from
English to Punjabi. We have experimented on four types of name entities which
are: Proper names, Location names, Organization names and miscellaneous.
Various rules for the purpose of syllabification have been constructed.
Transliteration of name entities is accomplished with the help of Probability
calculation. N-Gram probabilities for the extracted syllables have been
calculated using statistical machine translation toolkit MOSES."
"The Cornell Semantic Parsing Framework (SPF) is a learning and inference
framework for mapping natural language to formal representation of its meaning."
"Machine translation evaluation is a very important activity in machine
translation development. Automatic evaluation metrics proposed in literature
are inadequate as they require one or more human reference translations to
compare them with output produced by machine translation. This does not always
give accurate results as a text can have several different translations. Human
evaluation metrics, on the other hand, lacks inter-annotator agreement and
repeatability. In this paper we have proposed a new human evaluation metric
which addresses these issues. Moreover this metric also provides solid grounds
for making sound assumptions on the quality of the text produced by a machine
translation."
"This article reports the evaluation of the integration of data from a
syntactic-semantic lexicon, the Lexicon-Grammar of French, into a syntactic
parser. We show that by changing the set of labels for verbs and predicational
nouns, we can improve the performance on French of a non-lexicalized
probabilistic parser."
"Word sense disambiguation (WSD) is a problem in the field of computational
linguistics given as finding the intended sense of a word (or a set of words)
when it is activated within a certain context. WSD was recently addressed as a
combinatorial optimization problem in which the goal is to find a sequence of
senses that maximize the semantic relatedness among the target words. In this
article, a novel algorithm for solving the WSD problem called D-Bees is
proposed which is inspired by bee colony optimization (BCO)where artificial bee
agents collaborate to solve the problem. The D-Bees algorithm is evaluated on a
standard dataset (SemEval 2007 coarse-grained English all-words task corpus)and
is compared to simulated annealing, genetic algorithms, and two ant colony
optimization techniques (ACO). It will be observed that the BCO and ACO
approaches are on par."
"Development of a proper names pronunciation lexicon is usually a manual
effort which can not be avoided. Grapheme to phoneme (G2P) conversion modules,
in literature, are usually rule based and work best for non-proper names in a
particular language. Proper names are foreign to a G2P module. We follow an
optimization approach to enable automatic construction of proper names
pronunciation lexicon. The idea is to construct a small orthogonal set of words
(basis) which can span the set of names in a given database. We propose two
algorithms for the construction of this basis. The transcription lexicon of all
the proper names in a database can be produced by the manual transcription of
only the small set of basis words. We first construct a cost function and show
that the minimization of the cost function results in a basis. We derive
conditions for convergence of this cost function and validate them
experimentally on a very large proper name database. Experiments show the
transcription can be achieved by transcribing a set of small number of basis
words. The algorithms proposed are generic and independent of language; however
performance is better if the proper names have same origin, namely, same
language or geographical region."
"We describe a technique for attributing parts of a written text to a set of
unknown authors. Nothing is assumed to be known a priori about the writing
styles of potential authors. We use multiple independent clusterings of an
input text to identify parts that are similar and dissimilar to one another. We
describe algorithms necessary to combine the multiple clusterings into a
meaningful output. We show results of the application of the technique on texts
having multiple writing styles."
"Our dictionary-based lemmatizer for the Bulgarian language presented here is
distributed as free software, publicly available to download and use under the
GPL v3 license. The presented software is written entirely in Java and is
distributed as a GATE plugin. To our best knowledge, at the time of writing
this article, there are not any other free lemmatization tools specifically
targeting the Bulgarian language. The presented lemmatizer is a work in
progress and currently yields an accuracy of about 95% in comparison to the
manually annotated corpus BulTreeBank-Morph, which contains 273933 tokens."
"A metonym is a word with a figurative meaning, similar to a metaphor. Because
metonyms are closely related to metaphors, we apply features that are used
successfully for metaphor recognition to the task of detecting metonyms. On the
ACL SemEval 2007 Task 8 data with gold standard metonym annotations, our system
achieved 86.45% accuracy on the location metonyms. Our code can be found on
GitHub."
"This paper presents a novel approach for enhancing the multiple sets of
acoustic patterns automatically discovered from a given corpus. In a previous
work it was proposed that different HMM configurations (number of states per
model, number of distinct models) for the acoustic patterns form a
two-dimensional space. Multiple sets of acoustic patterns automatically
discovered with the HMM configurations properly located on different points
over this two-dimensional space were shown to be complementary to one another,
jointly capturing the characteristics of the given corpus. By representing the
given corpus as sequences of acoustic patterns on different HMM sets, the
pattern indices in these sequences can be relabeled considering the context
consistency across the different sequences. Good improvements were observed in
preliminary experiments of pattern spoken term detection (STD) performed on
both TIMIT and Mandarin Broadcast News with such enhanced patterns."
"As the name suggests, type-logical grammars are a grammar formalism based on
logic and type theory. From the prespective of grammar design, type-logical
grammars develop the syntactic and semantic aspects of linguistic phenomena
hand-in-hand, letting the desired semantics of an expression inform the
syntactic type and vice versa. Prototypical examples of the successful
application of type-logical grammars to the syntax-semantics interface include
coordination, quantifier scope and extraction.This chapter describes the Grail
theorem prover, a series of tools for designing and testing grammars in various
modern type-logical grammars which functions as a tool . All tools described in
this chapter are freely available."
"Readability is defined as the reading level of the speech from grade 1 to
grade 12. It results from the use of the REAP readability analysis (vocabulary
- Collins-Thompson and Callan, 2004; syntax - Heilman et al ,2006, 2007), which
use the lexical contents and grammatical structure of the sentences in a
document to predict the reading level. After analysis, results were grouped
into the average readability of each candidate, the evolution of the
candidate's speeches' readability over time and the standard deviation, or how
much each candidate varied their speech from one venue to another. For
comparison, one speech from four past presidents and the Gettysburg Address
were also analyzed."
"We can often detect from a person's utterances whether he/she is in favor of
or against a given target entity -- their stance towards the target. However, a
person may express the same stance towards a target by using negative or
positive language. Here for the first time we present a dataset of
tweet--target pairs annotated for both stance and sentiment. The targets may or
may not be referred to in the tweets, and they may or may not be the target of
opinion in the tweets. Partitions of this dataset were used as training and
test sets in a SemEval-2016 shared task competition. We propose a simple stance
detection system that outperforms submissions from all 19 teams that
participated in the shared task. Additionally, access to both stance and
sentiment annotations allows us to explore several research questions. We show
that while knowing the sentiment expressed by a tweet is beneficial for stance
classification, it alone is not sufficient. Finally, we use additional
unlabeled data through distant supervision techniques and word embeddings to
further improve stance classification."
"Any natural language can be considered as a tool for producing large
databases (consisting of texts, written, or discursive). This tool for its
description in turn requires other large databases (dictionaries, grammars
etc.). Nowadays, the notion of database is associated with computer processing
and computer memory. However, a natural language resides also in human brains
and functions in human communication, from interpersonal to intergenerational
one. We discuss in this survey/research paper mathematical, in particular
geometric, constructions, which help to bridge these two worlds. In particular,
in this paper we consider the Vector Space Model of semantics based on
frequency matrices, as used in Natural Language Processing. We investigate
underlying geometries, formulated in terms of Grassmannians, projective spaces,
and flag varieties. We formulate the relation between vector space models and
semantic spaces based on semic axes in terms of projectability of subvarieties
in Grassmannians and projective spaces. We interpret Latent Semantics as a
geometric flow on Grassmannians. We also discuss how to formulate G\""ardenfors'
notion of ""meeting of minds"" in our geometric setting."
"This paper investigates two different neural architectures for the task of
relation classification: convolutional neural networks and recurrent neural
networks. For both models, we demonstrate the effect of different architectural
choices. We present a new context representation for convolutional neural
networks for relation classification (extended middle context). Furthermore, we
propose connectionist bi-directional recurrent neural networks and introduce
ranking loss for their optimization. Finally, we show that combining
convolutional and recurrent neural networks using a simple voting scheme is
accurate enough to improve results. Our neural models achieve state-of-the-art
results on the SemEval 2010 relation classification task."
"Deep neural networks have achieved remarkable results across many language
processing tasks, however these methods are highly sensitive to noise and
adversarial attacks. We present a regularization based method for limiting
network sensitivity to its inputs, inspired by ideas from computer vision, thus
learning models that are more robust. Empirical evaluation over a range of
sentiment datasets with a convolutional neural network shows that, compared to
a baseline model and the dropout method, our method achieves superior
performance over noisy inputs and out-of-domain data."
"Neural network based models have achieved impressive results on various
specific tasks. However, in previous works, most models are learned separately
based on single-task supervised objectives, which often suffer from
insufficient training data. In this paper, we propose two deep architectures
which can be trained jointly on multiple related tasks. More specifically, we
augment neural model with an external memory, which is shared by several tasks.
Experiments on two groups of text classification tasks show that our proposed
architectures can improve the performance of a task with the help of other
related tasks."
"Acoustic word embeddings --- fixed-dimensional vector representations of
variable-length spoken word segments --- have begun to be considered for tasks
such as speech recognition and query-by-example search. Such embeddings can be
learned discriminatively so that they are similar for speech segments
corresponding to the same word, while being dissimilar for segments
corresponding to different words. Recent work has found that acoustic word
embeddings can outperform dynamic time warping on query-by-example search and
related word discrimination tasks. However, the space of embedding models and
training approaches is still relatively unexplored. In this paper we present
new discriminative embedding models based on recurrent neural networks (RNNs).
We consider training losses that have been successful in prior work, in
particular a cross entropy loss for word classification and a contrastive loss
that explicitly aims to separate same-word and different-word pairs in a
""Siamese network"" training setting. We find that both classifier-based and
Siamese RNN embeddings improve over previously reported results on a word
discrimination task, with Siamese RNNs outperforming classification models. In
addition, we present analyses of the learned embeddings and the effects of
variables such as dimensionality and network structure."
"Previous machine comprehension (MC) datasets are either too small to train
end-to-end deep learning models, or not difficult enough to evaluate the
ability of current MC techniques. The newly released SQuAD dataset alleviates
these limitations, and gives us a chance to develop more realistic MC models.
Based on this dataset, we propose a Multi-Perspective Context Matching (MPCM)
model, which is an end-to-end system that directly predicts the answer
beginning and ending points in a passage. Our model first adjusts each
word-embedding vector in the passage by multiplying a relevancy weight computed
against the question. Then, we encode the question and weighted passage by
using bi-directional LSTMs. For each point in the passage, our model matches
the context of this point against the encoded question from multiple
perspectives and produces a matching vector. Given those matched vectors, we
employ another bi-directional LSTM to aggregate all the information and predict
the beginning and ending points. Experimental result on the test set of SQuAD
shows that our model achieves a competitive result on the leaderboard."
"This paper presents a novel reranking model, future reward reranking, to
re-score the actions in a transition-based parser by using a global scorer.
Different to conventional reranking parsing, the model searches for the best
dependency tree in all feasible trees constraining by a sequence of actions to
get the future reward of the sequence. The scorer is based on a first-order
graph-based parser with bidirectional LSTM, which catches different parsing
view compared with the transition-based parser. Besides, since context
enhancement has shown substantial improvement in the arc-stand transition-based
parsing over the parsing accuracy, we implement context enhancement on an
arc-eager transition-base parser with stack LSTMs, the dynamic oracle and
dropout supporting and achieve further improvement. With the global scorer and
context enhancement, the results show that UAS of the parser increases as much
as 1.20% for English and 1.66% for Chinese, and LAS increases as much as 1.32%
for English and 1.63% for Chinese. Moreover, we get state-of-the-art LASs,
achieving 87.58% for Chinese and 93.37% for English."
"In this paper we present a novel Neural Network algorithm for conducting
semi-supervised learning for sequence labeling tasks arranged in a
linguistically motivated hierarchy. This relationship is exploited to
regularise the representations of supervised tasks by backpropagating the error
of the unsupervised task through the supervised tasks. We introduce a neural
network where lower layers are supervised by junior downstream tasks and the
final layer task is an auxiliary unsupervised task. The architecture shows
improvements of up to two percentage points F1 for Chunking compared to a
plausible baseline."
"Acoustic unit discovery (AUD) is a process of automatically identifying a
categorical acoustic unit inventory from speech and producing corresponding
acoustic unit tokenizations. AUD provides an important avenue for unsupervised
acoustic model training in a zero resource setting where expert-provided
linguistic knowledge and transcribed speech are unavailable. Therefore, to
further facilitate zero-resource AUD process, in this paper, we demonstrate
acoustic feature representations can be significantly improved by (i)
performing linear discriminant analysis (LDA) in an unsupervised self-trained
fashion, and (ii) leveraging resources of other languages through building a
multilingual bottleneck (BN) feature extractor to give effective cross-lingual
generalization. Moreover, we perform comprehensive evaluations of AUD efficacy
on multiple downstream speech applications, and their correlated performance
suggests that AUD evaluations are feasible using different alternative language
resources when only a subset of these evaluation resources can be available in
typical zero resource applications."
"Prepositions are highly polysemous, and their variegated senses encode
significant semantic information. In this paper we match each preposition's
complement and attachment and their interplay crucially to the geometry of the
word vectors to the left and right of the preposition. Extracting such features
from the vast number of instances of each preposition and clustering them makes
for an efficient preposition sense disambigution (PSD) algorithm, which is
comparable to and better than state-of-the-art on two benchmark datasets. Our
reliance on no external linguistic resource allows us to scale the PSD
algorithm to a large WikiCorpus and learn sense-specific preposition
representations -- which we show to encode semantic relations and paraphrasing
of verb particle compounds, via simple vector operations."
"We present opinion recommendation, a novel task of jointly predicting a
custom review with a rating score that a certain user would give to a certain
product or service, given existing reviews and rating scores to the product or
service by other users, and the reviews that the user has given to other
products and services. A characteristic of opinion recommendation is the
reliance of multiple data sources for multi-task joint learning, which is the
strength of neural models. We use a single neural network to model users and
products, capturing their correlation and generating customised product
representations using a deep memory network, from which customised ratings and
reviews are constructed jointly. Results show that our opinion recommendation
system gives ratings that are closer to real user ratings on Yelp.com data
compared with Yelp's own ratings, and our methods give better results compared
to several pipelines baselines using state-of-the-art sentiment rating and
summarization systems."
"Today when many practitioners run basic NLP on the entire web and
large-volume traffic, faster methods are paramount to saving time and energy
costs. Recent advances in GPU hardware have led to the emergence of
bi-directional LSTMs as a standard method for obtaining per-token vector
representations serving as input to labeling tasks such as NER (often followed
by prediction in a linear-chain CRF). Though expressive and accurate, these
models fail to fully exploit GPU parallelism, limiting their computational
efficiency. This paper proposes a faster alternative to Bi-LSTMs for NER:
Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better
capacity than traditional CNNs for large context and structured prediction.
Unlike LSTMs whose sequential processing on sentences of length N requires O(N)
time even in the face of parallelism, ID-CNNs permit fixed-depth convolutions
to run in parallel across entire documents. We describe a distinct combination
of network structure, parameter sharing and training procedures that enable
dramatic 14-20x test-time speedups while retaining accuracy comparable to the
Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire
document are even more accurate while maintaining 8x faster test time speeds."
"Concept maps can be used to concisely represent important information and
bring structure into large document collections. Therefore, we study a variant
of multi-document summarization that produces summaries in the form of concept
maps. However, suitable evaluation datasets for this task are currently
missing. To close this gap, we present a newly created corpus of concept maps
that summarize heterogeneous collections of web documents on educational
topics. It was created using a novel crowdsourcing approach that allows us to
efficiently determine important elements in large document collections. We
release the corpus along with a baseline system and proposed evaluation
protocol to enable further research on this variant of summarization."
"We present SwellShark, a framework for building biomedical named entity
recognition (NER) systems quickly and without hand-labeled data. Our approach
views biomedical resources like lexicons as function primitives for
autogenerating weak supervision. We then use a generative model to unify and
denoise this supervision and construct large-scale, probabilistically labeled
datasets for training high-accuracy NER taggers. In three biomedical NER tasks,
SwellShark achieves competitive scores with state-of-the-art supervised
benchmarks using no hand-labeled training data. In a drug name extraction task
using patient medical records, one domain expert using SwellShark achieved
within 5.1% of a crowdsourced annotation approach -- which originally utilized
20 teams over the course of several weeks -- in 24 hours."
"Sarcasm is a form of speech in which speakers say the opposite of what they
truly mean in order to convey a strong sentiment. In other words, ""Sarcasm is
the giant chasm between what I say, and the person who doesn't get it."". In
this paper we present the novel task of sarcasm interpretation, defined as the
generation of a non-sarcastic utterance conveying the same message as the
original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets,
each interpreted by five human judges. Addressing the task as monolingual
machine translation (MT), we experiment with MT algorithms and evaluation
measures. We then present SIGN: an MT based sarcasm interpretation algorithm
that targets sentiment words, a defining element of textual sarcasm. We show
that while the scores of n-gram based automatic measures are similar for all
interpretation models, SIGN's interpretations are scored higher by humans for
adequacy and sentiment polarity. We conclude with a discussion on future
research directions for our new task."
"Language models are typically applied at the sentence level, without access
to the broader document context. We present a neural language model that
incorporates document context in the form of a topic model-like architecture,
thus providing a succinct representation of the broader document context
outside of the current sentence. Experiments over a range of datasets
demonstrate that our model outperforms a pure sentence-based model in terms of
language model perplexity, and leads to topics that are potentially more
coherent than those produced by a standard LDA topic model. Our model also has
the ability to generate related sentences for a topic, providing another way to
interpret topics."
"In this work, we explore multiple neural architectures adapted for the task
of automatic post-editing of machine translation output. We focus on neural
end-to-end models that combine both inputs $mt$ (raw MT output) and $src$
(source language input) in a single neural architecture, modeling $\{mt, src\}
\rightarrow pe$ directly. Apart from that, we investigate the influence of
hard-attention models which seem to be well-suited for monolingual tasks, as
well as combinations of both ideas. We report results on data sets provided
during the WMT-2016 shared task on automatic post-editing and can demonstrate
that dual-attention models that incorporate all available data in the APE
scenario in a single model improve on the best shared task system and on all
other published results after the shared task. Dual-attention models that are
combined with hard attention remain competitive despite applying fewer changes
to the input."
"For years, recursive neural networks (RvNNs) have been shown to be suitable
for representing text into fixed-length vectors and achieved good performance
on several natural language processing tasks. However, the main drawback of
RvNNs is that they require structured input, which makes data preparation and
model implementation hard. In this paper, we propose Gumbel Tree-LSTM, a novel
tree-structured long short-term memory architecture that learns how to compose
task-specific tree structures only from plain text data efficiently. Our model
uses Straight-Through Gumbel-Softmax estimator to decide the parent node among
candidates dynamically and to calculate gradients of the discrete decision. We
evaluate the proposed model on natural language inference and sentiment
analysis, and show that our model outperforms or is at least comparable to
previous models. We also find that our model converges significantly faster
than other models."
"We present the first open-set language identification experiments using
one-class classification. We first highlight the shortcomings of traditional
feature extraction methods and propose a hashing-based feature vectorization
approach as a solution. Using a dataset of 10 languages from different writing
systems, we train a One- Class Support Vector Machine using only a monolingual
corpus for each language. Each model is evaluated against a test set of data
from all 10 languages and we achieve an average F-score of 0.99, highlighting
the effectiveness of this approach for open-set language identification."
"Selectional preferences have long been claimed to be essential for
coreference resolution. However, they are mainly modeled only implicitly by
current coreference resolvers. We propose a dependency-based embedding model of
selectional preferences which allows fine-grained compatibility judgments with
high coverage. We show that the incorporation of our model improves coreference
resolution performance on the CoNLL dataset, matching the state-of-the-art
results of a more complex system. However, it comes with a cost that makes it
debatable how worthwhile such improvements are."
"Emotions are physiological states generated in humans in reaction to internal
or external events. They are complex and studied across numerous fields
including computer science. As humans, on reading ""Why don't you ever text me!""
we can either interpret it as a sad or angry emotion and the same ambiguity
exists for machines. Lack of facial expressions and voice modulations make
detecting emotions from text a challenging problem. However, as humans
increasingly communicate using text messaging applications, and digital agents
gain popularity in our society, it is essential that these digital agents are
emotion aware, and respond accordingly.
  In this paper, we propose a novel approach to detect emotions like happy, sad
or angry in textual conversations using an LSTM based Deep Learning model. Our
approach consists of semi-automated techniques to gather training data for our
model. We exploit advantages of semantic and sentiment based embeddings and
propose a solution combining both. Our work is evaluated on real-world
conversations and significantly outperforms traditional Machine Learning
baselines as well as other off-the-shelf Deep Learning models."
"In NLP, convolutional neural networks (CNNs) have benefited less than
recurrent neural networks (RNNs) from attention mechanisms. We hypothesize that
this is because the attention in CNNs has been mainly implemented as attentive
pooling (i.e., it is applied to pooling) rather than as attentive convolution
(i.e., it is integrated into convolution). Convolution is the differentiator of
CNNs in that it can powerfully model the higher-level representation of a word
by taking into account its local fixed-size context in the input text t^x. In
this work, we propose an attentive convolution network, ATTCONV. It extends the
context scope of the convolution operation, deriving higher-level features for
a word not only from local context, but also information extracted from
nonlocal context by the attention mechanism commonly used in RNNs. This
nonlocal context can come (i) from parts of the input text t^x that are distant
or (ii) from extra (i.e., external) contexts t^y. Experiments on sentence
modeling with zero-context (sentiment analysis), single-context (textual
entailment) and multiple-context (claim verification) demonstrate the
effectiveness of ATTCONV in sentence representation learning with the
incorporation of context. In particular, attentive convolution outperforms
attentive pooling and is a strong competitor to popular attentive RNNs."
"Modeling hypernymy, such as poodle is-a dog, is an important generalization
aid to many NLP tasks, such as entailment, coreference, relation extraction,
and question answering. Supervised learning from labeled hypernym sources, such
as WordNet, limits the coverage of these models, which can be addressed by
learning hypernyms from unlabeled text. Existing unsupervised methods either do
not scale to large vocabularies or yield unacceptably poor accuracy. This paper
introduces distributional inclusion vector embedding (DIVE), a
simple-to-implement unsupervised method of hypernym discovery via per-word
non-negative vector embeddings which preserve the inclusion property of word
contexts in a low-dimensional and interpretable space. In experimental
evaluations more comprehensive than any previous literature of which we are
aware-evaluating on 11 datasets using multiple existing as well as newly
proposed scoring functions-we find that our method provides up to double the
precision of previous unsupervised embeddings, and the highest average
performance, using a much more compact word representation, and yielding many
new state-of-the-art results."
"Sentence representation at the semantic level is a challenging task for
Natural Language Processing and Artificial Intelligence. Despite the advances
in word embeddings (i.e. word vector representations), capturing sentence
meaning is an open question due to complexities of semantic interactions among
words. In this paper, we present an embedding method, which is aimed at
learning unsupervised sentence representations from unlabeled text. We propose
an unsupervised method that models a sentence as a weighted series of word
embeddings. The weights of the word embeddings are fitted by using Shannon's
word entropies provided by the Term Frequency--Inverse Document Frequency
(TF--IDF) transform. The hyperparameters of the model can be selected according
to the properties of data (e.g. sentence length and textual gender).
Hyperparameter selection involves word embedding methods and dimensionalities,
as well as weighting schemata. Our method offers advantages over existing
methods: identifiable modules, short-term training, online inference of
(unseen) sentence representations, as well as independence from domain,
external knowledge and language resources. Results showed that our model
outperformed the state of the art in well-known Semantic Textual Similarity
(STS) benchmarks. Moreover, our model reached state-of-the-art performance when
compared to supervised and knowledge-based STS systems."
"In this paper we describe the Japanese-English Subtitle Corpus (JESC). JESC
is a large Japanese-English parallel corpus covering the underrepresented
domain of conversational dialogue. It consists of more than 3.2 million
examples, making it the largest freely available dataset of its kind. The
corpus was assembled by crawling and aligning subtitles found on the web. The
assembly process incorporates a number of novel preprocessing elements to
ensure high monolingual fluency and accurate bilingual alignments. We summarize
its contents and evaluate its quality using human experts and baseline machine
translation (MT) systems."
"We describe a new challenge aimed at discovering subword and word units from
raw speech. This challenge is the followup to the Zero Resource Speech
Challenge 2015. It aims at constructing systems that generalize across
languages and adapt to new speakers. The design features and evaluation metrics
of the challenge are presented and the results of seventeen models are
discussed."
"Word sense induction (WSI), which addresses polysemy by unsupervised
discovery of multiple word senses, resolves ambiguities for downstream NLP
tasks and also makes word representations more interpretable. This paper
proposes an accurate and efficient graph-based method for WSI that builds a
global non-negative vector embedding basis (which are interpretable like
topics) and clusters the basis indexes in the ego network of each polysemous
word. By adopting distributional inclusion vector embeddings as our basis
formation model, we avoid the expensive step of nearest neighbor search that
plagues other graph-based methods without sacrificing the quality of sense
clusters. Experiments on three datasets show that our proposed method produces
similar or better sense clusters and embeddings compared with previous
state-of-the-art methods while being significantly more efficient."
"Social media user geolocation is vital to many applications such as event
detection. In this paper, we propose GCN, a multiview geolocation model based
on Graph Convolutional Networks, that uses both text and network context. We
compare GCN to the state-of-the-art, and to two baselines we propose, and show
that our model achieves or is competitive with the state- of-the-art over three
benchmark geolocation datasets when sufficient supervision is available. We
also evaluate GCN under a minimal supervision scenario, and show it outperforms
baselines. We find that highway network gates are essential for controlling the
amount of useful neighbourhood expansion in GCN."
"We present an empirical study of gender bias in coreference resolution
systems. We first introduce a novel, Winograd schema-style set of minimal pair
sentences that differ only by pronoun gender. With these ""Winogender schemas,""
we evaluate and confirm systematic gender bias in three publicly-available
coreference resolution systems, and correlate this bias with real-world and
textual gender statistics."
"Distantly-supervised Relation Extraction (RE) methods train an extractor by
automatically aligning relation instances in a Knowledge Base (KB) with
unstructured text. In addition to relation instances, KBs often contain other
relevant side information, such as aliases of relations (e.g., founded and
co-founded are aliases for the relation founderOfCompany). RE models usually
ignore such readily available side information. In this paper, we propose
RESIDE, a distantly-supervised neural relation extraction method which utilizes
additional side information from KBs for improved relation extraction. It uses
entity type and relation alias information for imposing soft constraints while
predicting relations. RESIDE employs Graph Convolution Networks (GCN) to encode
syntactic information from text and improves performance even when limited side
information is available. Through extensive experiments on benchmark datasets,
we demonstrate RESIDE's effectiveness. We have made RESIDE's source code
available to encourage reproducible research."
"The e-commerce has started a new trend in natural language processing through
sentiment analysis of user-generated reviews. Different consumers have
different concerns about various aspects of a specific product or service.
Aspect category detection, as a subtask of aspect-based sentiment analysis,
tackles the problem of categorizing a given review sentence into a set of
pre-defined aspect categories. In recent years, deep learning approaches have
brought revolutionary advances in multiple branches of natural language
processing including sentiment analysis. In this paper, we propose a deep
neural network method based on attention mechanism to identify different aspect
categories of a given review sentence. Our model utilizes several attentions
with different topic contexts, enabling it to attend to different parts of a
review sentence based on different topics. Experimental results on two datasets
in the restaurant domain released by SemEval workshop demonstrates that our
approach outperforms existing methods on both datasets. Visualization of the
topic attention weights shows the effectiveness of our model in identifying
words related to different topics."
"Word vector representations are well developed tools for various NLP and
Machine Learning tasks and are known to retain significant semantic and
syntactic structure of languages. But they are prone to carrying and amplifying
bias which can perpetrate discrimination in various applications. In this work,
we explore new simple ways to detect the most stereotypically gendered words in
an embedding and remove the bias from them. We verify how names are masked
carriers of gender bias and then use that as a tool to attenuate bias in
embeddings. Further, we extend this property of names to show how names can be
used to detect other types of bias in the embeddings such as bias based on
race, ethnicity, and age."
"We present our system for semantic frame induction that showed the best
performance in Subtask B.1 and finished as the runner-up in Subtask A of the
SemEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et
al., 2019). Our approach separates this task into two independent steps: verb
clustering using word and their context embeddings and role labeling by
combining these embeddings with syntactical features. A simple combination of
these steps shows very competitive results and can be extended to process other
datasets and languages."
"Biomedical Named Entity Recognition (NER) is a challenging problem in
biomedical information processing due to the widespread ambiguity of out of
context terms and extensive lexical variations. Performance on bioNER
benchmarks continues to improve due to advances like BERT, GPT, and XLNet.
FLAIR (1) is an alternative embedding model which is less computationally
intensive than the others mentioned. We test FLAIR and its pretrained PubMed
embeddings (which we term BioFLAIR) on a variety of bio NER tasks and compare
those with results from BERT-type networks. We also investigate the effects of
a small amount of additional pretraining on PubMed content, and of combining
FLAIR and ELMO models. We find that with the provided embeddings, FLAIR
performs on-par with the BERT networks - even establishing a new state of the
art on one benchmark. Additional pretraining did not provide a clear benefit,
although this might change with even more pretraining being done. Stacking the
FLAIR embeddings with others typically does provide a boost in the benchmark
results."
"With increasing information from social media, there are more and more videos
available. Therefore, the ability to reason on a video is important and
deserves to be discussed. TheDialog System Technology Challenge (DSTC7)
(Yoshino et al. 2018) proposed an Audio Visual Scene-aware Dialog (AVSD) task,
which contains five modalities including video, dialogue history, summary, and
caption, as a scene-aware environment. In this paper, we propose the
entropy-enhanced dynamic memory network (DMN) to effectively model video
modality. The attention-based GRU in the proposed model can improve the model's
ability to comprehend and memorize sequential information. The entropy
mechanism can control the attention distribution higher, so each to-be-answered
question can focus more specifically on a small set of video segments. After
the entropy-enhanced DMN secures the video context, we apply an attention model
that in-corporates summary and caption to generate an accurate answer given the
question about the video. In the official evaluation, our system can achieve
improved performance against the released baseline model for both subjective
and objective evaluation metrics."
"Computer scientists working on natural language processing, native speakers
of endangered languages, and field linguists to discuss ways to harness
Automatic Speech Recognition, especially neural networks, to automate
annotation, speech tagging, and text parsing on endangered languages."
"Domain adaptation has been well-studied in supervised neural machine
translation (SNMT). However, it has not been well-studied for unsupervised
neural machine translation (UNMT), although UNMT has recently achieved
remarkable results in several domain-specific language pairs. Besides the
inconsistent domains between training data and test data for SNMT, there
sometimes exists an inconsistent domain between two monolingual training data
for UNMT. In this work, we empirically show different scenarios for
unsupervised neural machine translation. Based on these scenarios, we revisit
the effect of the existing domain adaptation methods including batch weighting
and fine tuning methods in UNMT. Finally, we propose modified methods to
improve the performances of domain-specific UNMT systems."
"The recent success of neural machine translation models relies on the
availability of high quality, in-domain data. Domain adaptation is required
when domain-specific data is scarce or nonexistent. Previous unsupervised
domain adaptation strategies include training the model with in-domain copied
monolingual or back-translated data. However, these methods use generic
representations for text regardless of domain shift, which makes it infeasible
for translation models to control outputs conditional on a specific domain. In
this work, we propose an approach that adapts models with domain-aware feature
embeddings, which are learned via an auxiliary language modeling task. Our
approach allows the model to assign domain-specific representations to words
and output sentences in the desired domain. Our empirical results demonstrate
the effectiveness of the proposed strategy, achieving consistent improvements
in multiple experimental settings. In addition, we show that combining our
method with back translation can further improve the performance of the model."
"In recent years, natural language processing (NLP) has got great development
with deep learning techniques. In the sub-field of machine translation, a new
approach named Neural Machine Translation (NMT) has emerged and got massive
attention from both academia and industry. However, with a significant number
of researches proposed in the past several years, there is little work in
investigating the development process of this new technology trend. This
literature survey traces back the origin and principal development timeline of
NMT, investigates the important branches, categorizes different research
orientations, and discusses some future research trends in this field."
"Text articles with false claims, especially news, have recently become
aggravating for the Internet users. These articles are in wide circulation and
readers face difficulty discerning fact from fiction. Previous work on
credibility assessment has focused on factual analysis and linguistic features.
The task's main challenge is the distinction between the features of true and
false articles. In this paper, we propose a novel approach called Credibility
Outcome (CREDO) which aims at scoring the credibility of an article in an open
domain setting.
  CREDO consists of different modules for capturing various features
responsible for the credibility of an article. These features includes
credibility of the article's source and author, semantic similarity between the
article and related credible articles retrieved from a knowledge base, and
sentiments conveyed by the article. A neural network architecture learns the
contribution of each of these modules to the overall credibility of an article.
Experiments on Snopes dataset reveals that CREDO outperforms the
state-of-the-art approaches based on linguistic features."
"In neural machine translation (NMT), researchers face the challenge of
un-seen (or out-of-vocabulary OOV) words translation. To solve this, some
researchers propose the splitting of western languages such as English and
German into sub-words or compounds. In this paper, we try to address this OOV
issue and improve the NMT adequacy with a harder language Chinese whose
characters are even more sophisticated in composition. We integrate the Chinese
radicals into the NMT model with different settings to address the unseen words
challenge in Chinese to English translation. On the other hand, this also can
be considered as semantic part of the MT system since the Chinese radicals
usually carry the essential meaning of the words they are constructed in.
Meaningful radicals and new characters can be integrated into the NMT systems
with our models. We use an attention-based NMT system as a strong baseline
system. The experiments on standard Chinese-to-English NIST translation shared
task data 2006 and 2008 show that our designed models outperform the baseline
model in a wide range of state-of-the-art evaluation metrics including LEPOR,
BEER, and CharacTER, in addition to BLEU and NIST scores, especially on the
adequacy-level translation. We also have some interesting findings from the
results of our various experiment settings about the performance of words and
characters in Chinese NMT, which is different with other languages. For
instance, the fully character level NMT may perform well or the state of the
art in some other languages as researchers demonstrated recently, however, in
the Chinese NMT model, word boundary knowledge is important for the model
learning."
"Neural machine translation (NMT) has been accelerated by deep learning neural
networks over statistical-based approaches, due to the plethora and
programmability of commodity heterogeneous computing architectures such as
FPGAs and GPUs and the massive amount of training corpuses generated from news
outlets, government agencies and social media. Training a learning classifier
for neural networks entails tuning hyper-parameters that would yield the best
performance. Unfortunately, the number of parameters for machine translation
include discrete categories as well as continuous options, which makes for a
combinatorial explosive problem. This research explores optimizing
hyper-parameters when training deep learning neural networks for machine
translation. Specifically, our work investigates training a language model with
Marian NMT. Results compare NMT under various hyper-parameter settings across a
variety of modern GPU architecture generations in single node and multi-node
settings, revealing insights on which hyper-parameters matter most in terms of
performance, such as words processed per second, convergence rates, and
translation accuracy, and provides insights on how to best achieve
high-performing NMT systems."
"Word Sense Disambiguation (WSD) aims to identify the correct meaning of
polysemous words in the particular context. Lexical resources like WordNet
which are proved to be of great help for WSD in the knowledge-based methods.
However, previous neural networks for WSD always rely on massive labeled data
(context), ignoring lexical resources like glosses (sense definitions). In this
paper, we integrate the context and glosses of the target word into a unified
framework in order to make full use of both labeled data and lexical knowledge.
Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly
encodes the context and glosses of the target word. GAS models the semantic
relationship between the context and the gloss in an improved memory network
framework, which breaks the barriers of the previous supervised methods and
knowledge-based methods. We further extend the original gloss of word sense via
its semantic relations in WordNet to enrich the gloss information. The
experimental results show that our model outperforms the state-of-theart
systems on several English all-words WSD datasets."
"This paper describes the submissions to the efficiency track for GPUs at the
Workshop for Neural Machine Translation and Generation by members of the
University of Edinburgh, Adam Mickiewicz University, Tilde and University of
Alicante. We focus on efficient implementation of the recurrent deep-learning
model as implemented in Amun, the fast inference engine for neural machine
translation. We improve the performance with an efficient mini-batching
algorithm, and by fusing the softmax operation with the k-best extraction
algorithm. Submissions using Amun were first, second and third fastest in the
GPU efficiency track."
"When parsing morphologically-rich languages with neural models, it is
beneficial to model input at the character level, and it has been claimed that
this is because character-level models learn morphology. We test these claims
by comparing character-level models to an oracle with access to explicit
morphological analysis on twelve languages with varying morphological
typologies. Our results highlight many strengths of character-level models, but
also show that they are poor at disambiguating some words, particularly in the
face of case syncretism. We then demonstrate that explicitly modeling
morphological case improves our best model, showing that character-level models
can benefit from targeted forms of explicit morphological modeling."
"Attention-based models are successful when trained on large amounts of data.
In this paper, we demonstrate that even in the low-resource scenario, attention
can be learned effectively. To this end, we start with discrete human-annotated
rationales and map them into continuous attention. Our central hypothesis is
that this mapping is general across domains, and thus can be transferred from
resource-rich domains to low-resource ones. Our model jointly learns a
domain-invariant representation and induces the desired mapping between
rationales and attention. Our empirical results validate this hypothesis and
show that our approach delivers significant gains over state-of-the-art
baselines, yielding over 15% average error reduction on benchmark datasets."
"Scripts define knowledge about how everyday scenarios (such as going to a
restaurant) are expected to unfold. One of the challenges to learning scripts
is the hierarchical nature of the knowledge. For example, a suspect arrested
might plead innocent or guilty, and a very different track of events is then
expected to happen. To capture this type of information, we propose an
autoencoder model with a latent space defined by a hierarchy of categorical
variables. We utilize a recently proposed vector quantization based approach,
which allows continuous embeddings to be associated with each latent variable
value. This permits the decoder to softly decide what portions of the latent
hierarchy to condition on by attending over the value embeddings for a given
setting. Our model effectively encodes and generates scripts, outperforming a
recent language modeling-based method on several standard tasks, and allowing
the autoencoder model to achieve substantially lower perplexity scores compared
to the previous language modeling-based method."
"Humans have entered the age of algorithms. Each minute, algorithms shape
countless preferences from suggesting a product to a potential life partner. In
the marketplace algorithms are trained to learn consumer preferences from
customer reviews because user-generated reviews are considered the voice of
customers and a valuable source of information to firms. Insights mined from
reviews play an indispensable role in several business activities ranging from
product recommendation, targeted advertising, promotions, segmentation etc. In
this research, we question whether reviews might hold stereotypic gender bias
that algorithms learn and propagate Utilizing data from millions of
observations and a word embedding approach, GloVe, we show that algorithms
designed to learn from human language output also learn gender bias. We also
examine why such biases occur: whether the bias is caused because of a negative
bias against females or a positive bias for males. We examine the impact of
gender bias in reviews on choice and conclude with policy implications for
female consumers, especially when they are unaware of the bias, and the ethical
implications for firms."
"Cross-lingual word embeddings (CLEs) enable multilingual modeling of meaning
and facilitate cross-lingual transfer of NLP models. Despite their ubiquitous
usage in downstream tasks, recent increasingly popular projection-based CLE
models are almost exclusively evaluated on a single task only: bilingual
lexicon induction (BLI). Even BLI evaluations vary greatly, hindering our
ability to correctly interpret performance and properties of different CLE
models. In this work, we make the first step towards a comprehensive evaluation
of cross-lingual word embeddings. We thoroughly evaluate both supervised and
unsupervised CLE models on a large number of language pairs in the BLI task and
three downstream tasks, providing new insights concerning the ability of
cutting-edge CLE models to support cross-lingual NLP. We empirically
demonstrate that the performance of CLE models largely depends on the task at
hand and that optimizing CLE models for BLI can result in deteriorated
downstream performance. We indicate the most robust supervised and unsupervised
CLE models and emphasize the need to reassess existing baselines, which still
display competitive performance across the board. We hope that our work will
catalyze further work on CLE evaluation and model analysis."
"In this paper we present a novel lemmatization method based on a
sequence-to-sequence neural network architecture and morphosyntactic context
representation. In the proposed method, our context-sensitive lemmatizer
generates the lemma one character at a time based on the surface form
characters and its morphosyntactic features obtained from a morphological
tagger. We argue that a sliding window context representation suffers from
sparseness, while in majority of cases the morphosyntactic features of a word
bring enough information to resolve lemma ambiguities while keeping the context
representation dense and more practical for machine learning systems.
Additionally, we study two different data augmentation methods utilizing
autoencoder training and morphological transducers especially beneficial for
low resource languages. We evaluate our lemmatizer on 52 different languages
and 76 different treebanks, showing that our system outperforms all latest
baseline systems. Compared to the best overall baseline, UDPipe Future, our
system outperforms it on 62 out of 76 treebanks reducing errors on average by
19% relative. The lemmatizer together with all trained models is made available
as a part of the Turku-neural-parsing-pipeline under the Apache 2.0 license."
"Nowadays, the automatic detection of emotions is employed by many
applications in different fields like security informatics, e-learning, humor
detection, targeted advertising, etc. Many of these applications focus on
social media and treat this problem as a classification problem, which requires
preparing training data. The typical method for annotating the training data by
human experts is considered time consuming, labor intensive and sometimes prone
to error. Moreover, such an approach is not easily extensible to new
domains/languages since such extensions require annotating new training data.
In this study, we propose a distant supervised learning approach where the
training sentences are automatically annotated based on the emojis they have.
Such training data would be very cheap to produce compared with the manually
created training data, thus, much larger training data can be easily obtained.
On the other hand, this training data would naturally have lower quality as it
may contain some errors in the annotation. Nonetheless, we experimentally show
that training classifiers on cheap, large and possibly erroneous data annotated
using this approach leads to more accurate results compared with training the
same classifiers on the more expensive, much smaller and error-free manually
annotated training data. Our experiments are conducted on an in-house dataset
of emotional Arabic tweets and the classifiers we consider are: Support Vector
Machine (SVM), Multinomial Naive Bayes (MNB) and Random Forest (RF). In
addition to experimenting with single classifiers, we also consider using an
ensemble of classifiers. The results show that using an automatically annotated
training data (that is only one order of magnitude larger than the manually
annotated one) gives better results in almost all settings considered."
"An experimental approach to studying the properties of word embeddings is
proposed. Controlled experiments, achieved through modifications of the
training corpus, permit the demonstration of direct relations between word
properties and word vector direction and length. The approach is demonstrated
using the word2vec CBOW model with experiments that independently vary word
frequency and word co-occurrence noise. The experiments reveal that word vector
length depends more or less linearly on both word frequency and the level of
noise in the co-occurrence distribution of the word. The coefficients of
linearity depend upon the word. The special point in feature space, defined by
the (artificial) word with pure noise in its co-occurrence distribution, is
found to be small but non-zero."
"We propose a recurrent neural model that generates natural-language questions
from documents, conditioned on answers. We show how to train the model using a
combination of supervised and reinforcement learning. After teacher forcing for
standard maximum likelihood training, we fine-tune the model using policy
gradient techniques to maximize several rewards that measure question quality.
Most notably, one of these rewards is the performance of a question-answering
system. We motivate question generation as a means to improve the performance
of question answering systems. Our model is trained and evaluated on the recent
question-answering dataset SQuAD."
"Tree-structured neural networks have proven to be effective in learning
semantic representations by exploiting syntactic information. In spite of their
success, most existing models suffer from the underfitting problem: they
recursively use the same shared compositional function throughout the whole
compositional process and lack expressive power due to inability to capture the
richness of compositionality. In this paper, we address this issue by
introducing the dynamic compositional neural networks over tree structure
(DC-TreeNN), in which the compositional function is dynamically generated by a
meta network. The role of meta-network is to capture the metaknowledge across
the different compositional rules and formulate them. Experimental results on
two typical tasks show the effectiveness of the proposed models."
"Measuring the salience of a word is an essential step in numerous NLP tasks.
Heuristic approaches such as tfidf have been used so far to estimate the
salience of words. We propose \emph{Neural Word Salience} (NWS) scores, unlike
heuristics, are learnt from a corpus. Specifically, we learn word salience
scores such that, using pre-trained word embeddings as the input, can
accurately predict the words that appear in a sentence, given the words that
appear in the sentences preceding or succeeding that sentence. Experimental
results on sentence similarity prediction show that the learnt word salience
scores perform comparably or better than some of the state-of-the-art
approaches for representing sentences on benchmark datasets for sentence
similarity, while using only a fraction of the training and prediction times
required by prior methods. Moreover, our NWS scores positively correlate with
psycholinguistic measures such as concreteness, and imageability implying a
close connection to the salience as perceived by humans."
"Multimodal machine translation is an attractive application of neural machine
translation (NMT). It helps computers to deeply understand visual objects and
their relations with natural languages. However, multimodal NMT systems suffer
from a shortage of available training data, resulting in poor performance for
translating rare words. In NMT, pretrained word embeddings have been shown to
improve NMT of low-resource domains, and a search-based approach is proposed to
address the rare word problem. In this study, we effectively combine these two
approaches in the context of multimodal NMT and explore how we can take full
advantage of pretrained word embeddings to better translate rare words. We
report overall performance improvements of 1.24 METEOR and 2.49 BLEU and
achieve an improvement of 7.67 F-score for rare word translation."
"In domain adaptation for neural machine translation, translation performance
can benefit from separating features into domain-specific features and common
features. In this paper, we propose a method to explicitly model the two kinds
of information in the encoder-decoder framework so as to exploit out-of-domain
data in in-domain training. In our method, we maintain a private encoder and a
private decoder for each domain which are used to model domain-specific
information. In the meantime, we introduce a common encoder and a common
decoder shared by all the domains which can only have domain-independent
information flow through. Besides, we add a discriminator to the shared encoder
and employ adversarial training for the whole model to reinforce the
performance of information separation and machine translation simultaneously.
Experiment results show that our method can outperform competitive baselines
greatly on multiple data sets."
"In this work, we train an Automatic Post-Editing (APE) model and use it to
reveal biases in standard Machine Translation (MT) evaluation procedures. The
goal of our APE model is to correct typical errors introduced by the
translation process, and convert the ""translationese"" output into natural text.
Our APE model is trained entirely on monolingual data that has been round-trip
translated through English, to mimic errors that are similar to the ones
introduced by NMT. We apply our model to the output of existing NMT systems,
and demonstrate that, while the human-judged quality improves in all cases,
BLEU scores drop with forward-translated test sets. We verify these results for
the WMT18 English to German, WMT15 English to French, and WMT16 English to
Romanian tasks. Furthermore, we selectively apply our APE model on the output
of the top submissions of the most recent WMT evaluation campaigns. We see
quality improvements on all tasks of up to 2.5 BLEU points."
"To understand diverse natural language commands, virtual assistants today are
trained with numerous labor-intensive, manually annotated sentences. This paper
presents a methodology and the Genie toolkit that can handle new compound
commands with significantly less manual effort. We advocate formalizing the
capability of virtual assistants with a Virtual Assistant Programming Language
(VAPL) and using a neural semantic parser to translate natural language into
VAPL code. Genie needs only a small realistic set of input sentences for
validating the neural model. Developers write templates to synthesize data;
Genie uses crowdsourced paraphrases and data augmentation, along with the
synthesized data, to train a semantic parser. We also propose design principles
that make VAPL languages amenable to natural language translation. We apply
these principles to revise ThingTalk, the language used by the Almond virtual
assistant. We use Genie to build the first semantic parser that can support
compound virtual assistants commands with unquoted free-form parameters. Genie
achieves a 62% accuracy on realistic user inputs. We demonstrate Genie's
generality by showing a 19% and 31% improvement over the previous state of the
art on a music skill, aggregate functions, and access control."
"Programmers typically organize executable source code using high-level coding
patterns or idiomatic structures such as nested loops, exception handlers and
recursive blocks, rather than as individual code tokens. In contrast, state of
the art (SOTA) semantic parsers still map natural language instructions to
source code by building the code syntax tree one node at a time. In this paper,
we introduce an iterative method to extract code idioms from large source code
corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax
trees, and train semantic parsers to apply these idioms during decoding.
Applying idiom-based decoding on a recent context-dependent semantic parsing
task improves the SOTA by 2.2\% BLEU score while reducing training time by more
than 50\%. This improved speed enables us to scale up the model by training on
an extended training set that is 5$\times$ larger, to further move up the SOTA
by an additional 2.3\% BLEU and 0.9\% exact match. Finally, idioms also
significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL
dataset, when training data is limited."
"Several studies have shown that speech and language features, automatically
extracted from clinical interviews or spontaneous discourse, have diagnostic
value for mental disorders such as schizophrenia and bipolar disorder. They
typically make use of a large feature set to train a classifier for
distinguishing between two groups of interest, i.e. a clinical and control
group. However, a purely data-driven approach runs the risk of overfitting to a
particular data set, especially when sample sizes are limited. Here, we first
down-select the set of language features to a small subset that is related to a
well-validated test of functional ability, the Social Skills Performance
Assessment (SSPA). This helps establish the concurrent validity of the selected
features. We use only these features to train a simple classifier to
distinguish between groups of interest. Linear regression reveals that a subset
of language features can effectively model the SSPA, with a correlation
coefficient of 0.75. Furthermore, the same feature set can be used to build a
strong binary classifier to distinguish between healthy controls and a clinical
group (AUC = 0.96) and also between patients within the clinical group with
schizophrenia and bipolar I disorder (AUC = 0.83)."
"The slot filling task aims at extracting answers for queries about entities
from text, such as ""Who founded Apple"". In this paper, we focus on the relation
classification component of a slot filling system. We propose type-aware
convolutional neural networks to benefit from the mutual dependencies between
entity and relation classification. In particular, we explore different ways of
integrating the named entity types of the relation arguments into a neural
network for relation classification, including a joint training and a
structured prediction approach. To the best of our knowledge, this is the first
study on type-aware neural networks for slot filling. The type-aware models
lead to the best results of our slot filling pipeline. Joint training performs
comparable to structured prediction. To understand the impact of the different
components of the slot filling pipeline, we perform a recall analysis, a manual
error analysis and several ablation studies. Such analyses are of particular
importance to other slot filling researchers since the official slot filling
evaluations only assess pipeline outputs. The analyses show that especially
coreference resolution and our convolutional neural networks have a large
positive impact on the final performance of the slot filling pipeline. The
presented models, the source code of our system as well as our coreference
resource is publicy available."
"Several computational models have been developed to detect and analyze
dialect variation in recent years. Most of these models assume a predefined set
of geographical regions over which they detect and analyze dialectal variation.
However, dialect variation occurs at multiple levels of geographic resolution
ranging from cities within a state, states within a country, and between
countries across continents. In this work, we propose a model that enables
detection of dialectal variation at multiple levels of geographic resolution
obviating the need for a-priori definition of the resolution level. Our method
DialectGram, learns dialect-sensitive word embeddings while being agnostic of
the geographic resolution. Specifically it only requires one-time training and
enables analysis of dialectal variation at a chosen resolution post-hoc -- a
significant departure from prior models which need to be re-trained whenever
the pre-defined set of regions changes. Furthermore, DialectGram explicitly
models senses thus enabling one to estimate the proportion of each sense usage
in any given region. Finally, we quantitatively evaluate our model against
other baselines on a new evaluation dataset DialectSim (in English) and show
that DialectGram can effectively model linguistic variation."
"This paper describes our recursive system for SemEval-2019 \textit{ Task 1:
Cross-lingual Semantic Parsing with UCCA}. Each recursive step consists of two
parts. We first perform semantic parsing using a sequence tagger to estimate
the probabilities of the UCCA categories in the sentence. Then, we apply a
decoding policy which interprets these probabilities and builds the graph
nodes. Parsing is done recursively, we perform a first inference on the
sentence to extract the main scenes and links and then we recursively apply our
model on the sentence using a masking feature that reflects the decisions made
in previous steps. Process continues until the terminal nodes are reached. We
choose a standard neural tagger and we focused on our recursive parsing
strategy and on the cross lingual transfer problem to develop a robust model
for the French language, using only few training samples."
"We consider the problem of conversational question answering over a
large-scale knowledge base. To handle huge entity vocabulary of a large-scale
knowledge base, recent neural semantic parsing based approaches usually
decompose the task into several subtasks and then solve them sequentially,
which leads to following issues: 1) errors in earlier subtasks will be
propagated and negatively affect downstream ones; and 2) each subtask cannot
naturally share supervision signals with others. To tackle these issues, we
propose an innovative multi-task learning framework where a pointer-equipped
semantic parsing model is designed to resolve coreference in conversations, and
naturally empower joint learning with a novel type-aware entity detection
model. The proposed framework thus enables shared supervisions and alleviates
the effect of error propagation. Experiments on a large-scale conversational
question answering dataset containing 1.6M question answering pairs over 12.8M
entities show that the proposed framework improves overall F1 score from 67% to
79% compared with previous state-of-the-art work."
"For language documentation initiatives, transcription is an expensive
resource: one minute of audio is estimated to take one hour and a half on
average of a linguist's work (Austin and Sallabank, 2013). Recently, collecting
aligned translations in well-resourced languages became a popular solution for
ensuring posterior interpretability of the recordings (Adda et al. 2016). In
this paper we investigate language-related impact in automatic approaches for
computational language documentation. We translate the bilingual Mboshi-French
parallel corpus (Godard et al. 2017) into four other languages, and we perform
bilingual-rooted unsupervised word discovery. Our results hint towards an
impact of the well-resourced language in the quality of the output. However, by
combining the information learned by different bilingual models, we are only
able to marginally increase the quality of the segmentation."
"It is challenging for current one-step retrieve-and-read question answering
(QA) systems to answer questions like ""Which novel by the author of 'Armada'
will be adapted as a feature film by Steven Spielberg?"" because the question
seldom contains retrievable clues about the missing entity (here, the author).
Answering such a question requires multi-hop reasoning where one must gather
information about the missing entity (or facts) to proceed with further
reasoning. We present GoldEn (Gold Entity) Retriever, which iterates between
reading context and retrieving more supporting documents to answer open-domain
multi-hop questions. Instead of using opaque and computationally expensive
neural retrieval models, GoldEn Retriever generates natural language search
queries given the question and available context, and leverages off-the-shelf
information retrieval systems to query for missing entities. This allows GoldEn
Retriever to scale up efficiently for open-domain multi-hop reasoning while
maintaining interpretability. We evaluate GoldEn Retriever on the recently
proposed open-domain multi-hop QA dataset, HotpotQA, and demonstrate that it
outperforms the best previously published model despite not using pretrained
language models such as BERT."
"We present a simple methods to leverage the table content for the BERT-based
model to solve the text-to-SQL problem. Based on the observation that some of
the table content match some words in question string and some of the table
header also match some words in question string, we encode two addition feature
vector for the deep model. Our methods also benefit the model inference in
testing time as the tables are almost the same in training and testing time. We
test our model on the WikiSQL dataset and outperform the BERT-based baseline by
3.7% in logic form and 3.7% in execution accuracy and achieve state-of-the-art."
"In Machine Translation, considering the document as a whole can help to
resolve ambiguities and inconsistencies. In this paper, we propose a simple yet
promising approach to add contextual information in Neural Machine Translation.
We present a method to add source context that capture the whole document with
accurate boundaries, taking every word into account. We provide this additional
information to a Transformer model and study the impact of our method on three
language pairs. The proposed approach obtains promising results in the
English-German, English-French and French-English document-level translation
tasks. We observe interesting cross-sentential behaviors where the model learns
to use document-level information to improve translation coherence."
"In this study, we examined the possibility to extract personality traits from
a text. We created an extensive dataset by having experts annotate personality
traits in a large number of texts from multiple online sources. From these
annotated texts, we selected a sample and made further annotations ending up in
a large low-reliability dataset and a small high-reliability dataset. We then
used the two datasets to train and test several machine learning models to
extract personality from text, including a language model. Finally, we
evaluated our best models in the wild, on datasets from different domains. Our
results show that the models based on the small high-reliability dataset
performed better (in terms of $\textrm{R}^2$) than models based on large
low-reliability dataset. Also, language model based on small high-reliability
dataset performed better than the random baseline. Finally, and more
importantly, the results showed our best model did not perform better than the
random baseline when tested in the wild. Taken together, our results show that
determining personality traits from a text remains a challenge and that no firm
conclusions can be made on model performance before testing in the wild."
"Word embeddings are high dimensional vector representations of words that
capture their semantic similarity in the vector space. There exist several
algorithms for learning such embeddings both for a single language as well as
for several languages jointly. In this work we propose to evaluate collections
of embeddings by adapting downstream natural language tasks to the optimal
transport framework. We show how the family of Wasserstein distances can be
used to solve cross-lingual document retrieval and the cross-lingual document
classification problems. We argue on the advantages of this approach compared
to more traditional evaluation methods of embeddings like bilingual lexical
induction. Our experimental results suggest that using Wasserstein distances on
these problems out-performs several strong baselines and performs on par with
state-of-the-art models."
"Currently used metrics for assessing summarization algorithms do not account
for whether summaries are factually consistent with source documents. We
propose a weakly-supervised, model-based approach for verifying factual
consistency and identifying conflicts between source documents and a generated
summary. Training data is generated by applying a series of rule-based
transformations to the sentences of source documents. The factual consistency
model is then trained jointly for three tasks: 1) identify whether sentences
remain factually consistent after transformation, 2) extract a span in the
source documents to support the consistency prediction, 3) extract a span in
the summary sentence that is inconsistent if one exists. Transferring this
model to summaries generated by several state-of-the art models reveals that
this highly scalable approach substantially outperforms previous models,
including those trained with strong supervision using standard datasets for
natural language inference and fact checking. Additionally, human evaluation
shows that the auxiliary span extraction tasks provide useful assistance in the
process of verifying factual consistency."
"Human dialogues are scenario-based and appropriate responses generally relate
to the latent context knowledge entailed by the specific scenario. To enable
responses that are more meaningful and context-specific, we propose to improve
generative dialogue systems from the scenario perspective, where both dialogue
history and future conversation are taken into account to implicitly
reconstruct the scenario knowledge. More importantly, the conversation
scenarios are further internalized using imitation learning framework, where
the conventional dialogue model that has no access to future conversations is
effectively regularized by transferring the scenario knowledge contained in
hierarchical supervising signals from the scenario-based dialogue model, so
that the future conversation is not required in actual inference. Extensive
evaluations show that our approach significantly outperforms state-of-the-art
baselines on diversity and relevance, and expresses scenario-specific
knowledge."
"Multiple-Choice Question Answering (MCQA) is a challenging task in machine
reading comprehension. The main challenge in MCQA is to extract ""evidence"" from
the given context that supports the correct answer. In the OpenbookQA dataset,
the requirement of extracting ""evidence"" is particularly important due to the
mutual independence of sentences in the context. Existing work tackles this
problem by annotated evidence or distant supervision with rules which overly
rely on human efforts. To address the challenge, we propose a simple yet
effective approach termed evidence filtering to model the relationships between
the encoded contexts with respect to different options collectively and to
potentially highlight the evidence sentences and filter out unrelated
sentences. In addition to the effective reduction of human efforts of our
approach compared, through extensive experiments on OpenbookQA, we show that
the proposed approach outperforms the models that use the same backbone and
more training data; and our parameter analysis also demonstrates the
interpretability of our approach."
"The FPT.AI team participated in the SHINRA2020-ML subtask of the NTCIR-15
SHINRA task. This paper describes our method to solving the problem and
discusses the official results. Our method focuses on learning cross-lingual
representations, both on the word level and document level for page
classification. We propose a three-stage approach including multilingual model
pre-training, monolingual model fine-tuning and cross-lingual voting. Our
system is able to achieve the best scores for 25 out of 30 languages; and its
accuracy gaps to the best performing systems of the other five languages are
relatively small."
"Natural language understanding (NLU) and Natural language generation (NLG)
tasks hold a strong dual relationship, where NLU aims at predicting semantic
labels based on natural language utterances and NLG does the opposite. The
prior work mainly focused on exploiting the duality in model training in order
to obtain the models with better performance. However, regarding the
fast-growing scale of models in the current NLP area, sometimes we may have
difficulty retraining whole NLU and NLG models. To better address the issue,
this paper proposes to leverage the duality in the inference stage without the
need of retraining. The experiments on three benchmark datasets demonstrate the
effectiveness of the proposed method in both NLU and NLG, providing the great
potential of practical usage."
"We study the problem of learning neural text classifiers without using any
labeled data, but only easy-to-provide rules as multiple weak supervision
sources. This problem is challenging because rule-induced weak labels are often
noisy and incomplete. To address these two challenges, we design a label
denoiser, which estimates the source reliability using a conditional soft
attention mechanism and then reduces label noise by aggregating rule-annotated
weak labels. The denoised pseudo labels then supervise a neural classifier to
predicts soft labels for unmatched samples, which address the rule coverage
issue. We evaluate our model on five benchmarks for sentiment, topic, and
relation classifications. The results show that our model outperforms
state-of-the-art weakly-supervised and semi-supervised methods consistently,
and achieves comparable performance with fully-supervised methods even without
any labeled data. Our code can be found at
https://github.com/weakrules/Denoise-multi-weak-sources."
"We present a differentiable stack data structure that simultaneously and
tractably encodes an exponential number of stack configurations, based on
Lang's algorithm for simulating nondeterministic pushdown automata. We call the
combination of this data structure with a recurrent neural network (RNN)
controller a Nondeterministic Stack RNN. We compare our model against existing
stack RNNs on various formal languages, demonstrating that our model converges
more reliably to algorithmic behavior on deterministic tasks, and achieves
lower cross-entropy on inherently nondeterministic tasks."
"We present LemMED, a character-level encoder-decoder for contextual
morphological analysis (combined lemmatization and tagging). LemMED extends and
is named after two other attention-based models, namely Lematus, a contextual
lemmatizer, and MED, a morphological (re)inflection model. Our approach does
not require training separate lemmatization and tagging models, nor does it
need additional resources and tools, such as morphological dictionaries or
transducers. Moreover, LemMED relies solely on character-level representations
and on local context. Although the model can, in principle, account for global
context on sentence level, our experiments show that using just a single word
of context around each target word is not only more computationally feasible,
but yields better results as well. We evaluate LemMED in the framework of the
SIMGMORPHON-2019 shared task on combined lemmatization and tagging. In terms of
average performance LemMED ranks 5th among 13 systems and is bested only by the
submissions that use contextualized embeddings."
"Transformers are widely used in state-of-the-art machine translation, but the
key to their success is still unknown. To gain insight into this, we consider
three groups of parameters: embeddings, attention, and feed forward neural
network (FFN) layers. We examine the relative importance of each by performing
an ablation study where we initialise them at random and freeze them, so that
their weights do not change over the course of the training. Through this, we
show that the attention and FFN are equally important and fulfil the same
functionality in a model. We show that the decision about whether a component
is frozen or allowed to train is at least as important for the final model
performance as its number of parameters. At the same time, the number of
parameters alone is not indicative of a component's importance. Finally, while
the embedding layer is the least essential for machine translation tasks, it is
the most important component for language modelling tasks."
"The ability to rank creative natural language provides an important general
tool for downstream language understanding and generation. However, current
deep ranking models require substantial amounts of labeled data that are
difficult and expensive to obtain for different domains, languages and creative
characteristics. A recent neural approach, the DirectRanker, promises to reduce
the amount of training data needed but its application to text isn't fully
explored. We therefore adapt the DirectRanker to provide a new deep model for
ranking creative language with small data. We compare DirectRanker with a
Bayesian approach, Gaussian process preference learning (GPPL), which has
previously been shown to work well with sparse data. Our experiments with
sparse training data show that while the performance of standard neural ranking
approaches collapses with small training datasets, DirectRanker remains
effective. We find that combining DirectRanker with GPPL increases performance
across different settings by leveraging the complementary benefits of both
models. Our combined approach outperforms the previous state-of-the-art on
humor and metaphor novelty tasks, increasing Spearman's $\rho$ by 14% and 16%
on average."
"There is a multitude of novel generative models for open-domain
conversational systems; however, there is no systematic evaluation of different
systems. Systematic comparisons require consistency in experimental design,
evaluation sets, conversational systems and their outputs, and statistical
analysis. We lay out a protocol for the evaluation of conversational models
using head-to-head pairwise comparison. We analyze ten recent models that claim
state-of-the-art performance using a paired head-to-head performance
(win-loss-tie) on five evaluation datasets. Our findings show that DialoGPT and
Blender are superior systems using Bradley-Terry model and TrueSkill ranking
methods. These findings demonstrate the feasibility of our protocol to evaluate
conversational agents and evaluation sets. Finally, we make all code and
evaluations publicly available for researchers to compare their model to other
state-of-the-art dialog models."
"We consider a new perspective on dialog state tracking (DST), the task of
estimating a user's goal through the course of a dialog. By formulating DST as
a semantic parsing task over hierarchical representations, we can incorporate
semantic compositionality, cross-domain knowledge sharing and co-reference. We
present TreeDST, a dataset of 27k conversations annotated with tree-structured
dialog states and system acts. We describe an encoder-decoder framework for DST
with hierarchical representations, which leads to 20% improvement over
state-of-the-art DST approaches that operate on a flat meaning space of
slot-value pairs."
"Approaches for mitigating bias in supervised models are designed to reduce
models' dependence on specific sensitive features of the input data, e.g.,
mentioned social groups. However, in the case of hate speech detection, it is
not always desirable to equalize the effects of social groups because of their
essential role in distinguishing outgroup-derogatory hate, such that particular
types of hateful rhetoric carry the intended meaning only when contextualized
around certain social group tokens. Counterfactual token fairness for a
mentioned social group evaluates the model's predictions as to whether they are
the same for (a) the actual sentence and (b) a counterfactual instance, which
is generated by changing the mentioned social group in the sentence. Our
approach assures robust model predictions for counterfactuals that imply
similar meaning as the actual sentence. To quantify the similarity of a
sentence and its counterfactual, we compare their likelihood score calculated
by generative language models. By equalizing model behaviors on each sentence
and its counterfactuals, we mitigate bias in the proposed model while
preserving the overall classification performance."
"The present study is focused on the automatic identification and description
of frozen similes in British and French novels written between the 19 th
century and the beginning of the 20 th century. Two main patterns of frozen
similes were considered: adjectival ground + simile marker + nominal vehicle
(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.
sleep like a top). All potential similes and their components were first
extracted using a rule-based algorithm. Then, frozen similes were identified
based on reference lists of existing similes and semantic distance between the
tenor and the vehicle. The results obtained tend to confirm the fact that
frozen similes are not used haphazardly in literary texts. In addition,
contrary to how they are often presented, frozen similes often go beyond the
ground or the eventuality and the vehicle to also include the tenor."
"Text-to-Speech synthesis in Indian languages has a seen lot of progress over
the decade partly due to the annual Blizzard challenges. These systems assume
the text to be written in Devanagari or Dravidian scripts which are nearly
phonemic orthography scripts. However, the most common form of computer
interaction among Indians is ASCII written transliterated text. Such text is
generally noisy with many variations in spelling for the same word. In this
paper we evaluate three approaches to synthesize speech from such noisy ASCII
text: a naive Uni-Grapheme approach, a Multi-Grapheme approach, and a
supervised Grapheme-to-Phoneme (G2P) approach. These methods first convert the
ASCII text to a phonetic script, and then learn a Deep Neural Network to
synthesize speech from that. We train and test our models on Blizzard Challenge
datasets that were transliterated to ASCII using crowdsourcing. Our experiments
on Hindi, Tamil and Telugu demonstrate that our models generate speech of
competetive quality from ASCII text compared to the speech synthesized from the
native scripts. All the accompanying transliterated datasets are released for
public access."
"This paper describes the extension and optimization of our previous work on
very deep convolutional neural networks (CNNs) for effective recognition of
noisy speech in the Aurora 4 task. The appropriate number of convolutional
layers, the sizes of the filters, pooling operations and input feature maps are
all modified: the filter and pooling sizes are reduced and dimensions of input
feature maps are extended to allow adding more convolutional layers.
Furthermore appropriate input padding and input feature map selection
strategies are developed. In addition, an adaptation framework using joint
training of very deep CNN with auxiliary features i-vector and fMLLR features
is developed. These modifications give substantial word error rate reductions
over the standard CNN used as baseline. Finally the very deep CNN is combined
with an LSTM-RNN acoustic model and it is shown that state-level weighted log
likelihood score combination in a joint acoustic model decoding scheme is very
effective. On the Aurora 4 task, the very deep CNN achieves a WER of 8.81%,
further 7.99% with auxiliary feature joint training, and 7.09% with LSTM-RNN
joint decoding."
"Deep learning techniques are increasingly popular in the textual entailment
task, overcoming the fragility of traditional discrete models with hard
alignments and logics. In particular, the recently proposed attention models
(Rockt\""aschel et al., 2015; Wang and Jiang, 2015) achieves state-of-the-art
accuracy by computing soft word alignments between the premise and hypothesis
sentences. However, there remains a major limitation: this line of work
completely ignores syntax and recursion, which is helpful in many traditional
efforts. We show that it is beneficial to extend the attention model to tree
nodes between premise and hypothesis. More importantly, this subtree-level
attention reveals information about entailment relation. We study the recursive
composition of this subtree-level entailment relation, which can be viewed as a
soft version of the Natural Logic framework (MacCartney and Manning, 2009).
Experiments show that our structured attention and entailment composition model
can correctly identify and infer entailment relations from the bottom up, and
bring significant improvements in accuracy."
"In this paper, we describe a research method that generates Bangla word
clusters on the basis of relating to meaning in language and contextual
similarity. The importance of word clustering is in parts of speech (POS)
tagging, word sense disambiguation, text classification, recommender system,
spell checker, grammar checker, knowledge discover and for many others Natural
Language Processing (NLP) applications. In the history of word clustering,
English and some other languages have already implemented some methods on word
clustering efficiently. But due to lack of the resources, word clustering in
Bangla has not been still implemented efficiently. Presently, its
implementation is in the beginning stage. In some research of word clustering
in English based on preceding and next five words of a key word they found an
efficient result. Now, we are trying to implement the tri-gram, 4-gram and
5-gram model of word clustering for Bangla to observe which one is the best
among them. We have started our research with quite a large corpus of
approximate 1 lakh Bangla words. We are using a machine learning technique in
this research. We will generate word clusters and analyze the clusters by
testing some different threshold values."
"The science of happiness is an area of positive psychology concerned with
understanding what behaviors make people happy in a sustainable fashion.
Recently, there has been interest in developing technologies that help
incorporate the findings of the science of happiness into users' daily lives by
steering them towards behaviors that increase happiness. With the goal of
building technology that can understand how people express their happy moments
in text, we crowd-sourced HappyDB, a corpus of 100,000 happy moments that we
make publicly available. This paper describes HappyDB and its properties, and
outlines several important NLP problems that can be studied with the help of
the corpus. We also apply several state-of-the-art analysis techniques to
analyze HappyDB. Our results demonstrate the need for deeper NLP techniques to
be developed which makes HappyDB an exciting resource for follow-on research."
"We show that generating English Wikipedia articles can be approached as a
multi- document summarization of source documents. We use extractive
summarization to coarsely identify salient information and a neural abstractive
model to generate the article. For the abstractive model, we introduce a
decoder-only architecture that can scalably attend to very long sequences, much
longer than typical encoder- decoder architectures used in sequence
transduction. We show that this model can generate fluent, coherent
multi-sentence paragraphs and even whole Wikipedia articles. When given
reference documents, we show it can extract relevant factual information as
reflected in perplexity, ROUGE scores and human evaluations."
"Emotion detection in conversations is a necessary step for a number of
applications, including opinion mining over chat history, social media threads,
debates, argumentation mining, understanding consumer feedback in live
conversations, etc. Currently, systems do not treat the parties in the
conversation individually by adapting to the speaker of each utterance. In this
paper, we describe a new method based on recurrent neural networks that keeps
track of the individual party states throughout the conversation and uses this
information for emotion classification. Our model outperforms the state of the
art by a significant margin on two different datasets."
"Relation extraction is the task of identifying predefined relationship
between entities, and plays an essential role in information extraction,
knowledge base construction, question answering and so on. Most existing
relation extractors make predictions for each entity pair locally and
individually, while ignoring implicit global clues available across different
entity pairs and in the knowledge base, which often leads to conflicts among
local predictions from different entity pairs. This paper proposes a joint
inference framework that employs such global clues to resolve disagreements
among local predictions. We exploit two kinds of clues to generate constraints
which can capture the implicit type and cardinality requirements of a relation.
Those constraints can be examined in either hard style or soft style, both of
which can be effectively explored in an integer linear program formulation.
Experimental results on both English and Chinese datasets show that our
proposed framework can effectively utilize those two categories of global clues
and resolve the disagreements among local predictions, thus improve various
relation extractors when such clues are applicable to the datasets. Our
experiments also indicate that the clues learnt automatically from existing
knowledge bases perform comparably to or better than those refined by human."
"Opinion mining mainly involves three elements: feature and feature-of
relations, opinion expressions and the related opinion attributes (e.g.
Polarity), and feature-opinion relations. Although many works have emerged to
achieve its aim of gaining information, the previous researches typically
handled each of the three elements in isolation, which cannot give sufficient
information extraction results; hence, the complexity and the running time of
information extraction is increased. In this paper, we propose an opinion
mining extraction algorithm to jointly discover the main opinion mining
elements. Specifically, the algorithm automatically builds kernels to combine
closely related words into new terms from word level to phrase level based on
dependency relations; and we ensure the accuracy of opinion expressions and
polarity based on: fuzzy measurements, opinion degree intensifiers, and opinion
patterns. The 3458 analyzed reviews show that the proposed algorithm can
effectively identify the main elements simultaneously and outperform the
baseline methods. The proposed algorithm is used to analyze the features among
heterogeneous products in the same category. The feature-by-feature comparison
can help to select the weaker features and recommend the correct specifications
from the beginning life of a product. From this comparison, some interesting
observations are revealed. For example, the negative polarity of video
dimension is higher than the product usability dimension for a product. Yet,
enhancing the dimension of product usability can more effectively improve the
product (C) 2015 Elsevier Ltd. All rights reserved."
"Named entity recognition (NER) is one of the tasks in natural language
processing that can greatly benefit from the use of external knowledge sources.
We propose a named entity recognition framework composed of knowledge-based
feature extractors and a deep learning model including contextual word
embeddings, long short-term memory (LSTM) layers and conditional random fields
(CRF) inference layer. We use an entity linking module to integrate our system
with Wikipedia. The combination of effective neural architecture and external
resources allows us to obtain state-of-the-art results on recognition of Polish
proper names. We evaluate our model on data from PolEval 2018 NER challenge on
which it outperforms other methods, reducing the error rate by 22.4% compared
to the winning solution. Our work shows that combining neural NER model and
entity linking model with a knowledge base is more effective in recognizing
named entities than using NER model alone."
"When answering natural language questions over knowledge bases (KBs),
different question components and KB aspects play different roles. However,
most existing embedding-based methods for knowledge base question answering
(KBQA) ignore the subtle inter-relationships between the question and the KB
(e.g., entity types, relation paths and context). In this work, we propose to
directly model the two-way flow of interactions between the questions and the
KB via a novel Bidirectional Attentive Memory Network, called BAMnet. Requiring
no external resources and only very few hand-crafted features, on the
WebQuestions benchmark, our method significantly outperforms existing
information-retrieval based methods, and remains competitive with
(hand-crafted) semantic parsing based methods. Also, since we use attention
mechanisms, our method offers better interpretability compared to other
baselines."
"Long Short Term Memory Connectionist Temporal Classification (LSTM-CTC) based
end-to-end models are widely used in speech recognition due to its simplicity
in training and efficiency in decoding. In conventional LSTM-CTC based models,
a bottleneck projection matrix maps the hidden feature vectors obtained from
LSTM to softmax output layer. In this paper, we propose to use a high rank
projection layer to replace the projection matrix. The output from the high
rank projection layer is a weighted combination of vectors that are projected
from the hidden feature vectors via different projection matrices and
non-linear activation function. The high rank projection layer is able to
improve the expressiveness of LSTM-CTC models. The experimental results show
that on Wall Street Journal (WSJ) corpus and LibriSpeech data set, the proposed
method achieves 4%-6% relative word error rate (WER) reduction over the
baseline CTC system. They outperform other published CTC based end-to-end (E2E)
models under the condition that no external data or data augmentation is
applied. Code has been made available at https://github.com/mobvoi/lstm_ctc."
"Recognizing arrow of time in short stories is a challenging task. i.e., given
only two paragraphs, determining which comes first and which comes next is a
difficult task even for humans. In this paper, we have collected and curated a
novel dataset for tackling this challenging task. We have shown that a
pre-trained BERT architecture achieves reasonable accuracy on the task, and
outperforms RNN-based architectures."
"The use of Deep Neural Network architectures for Language Modeling has
recently seen a tremendous increase in interest in the field of NLP with the
advent of transfer learning and the shift in focus from rule-based and
predictive models (supervised learning) to generative or unsupervised models to
solve the long-standing problems in NLP like Information Extraction or Question
Answering. While this shift has worked greatly for languages lacking in
inflectional morphology, such as English, challenges still arise when trying to
build similar systems for morphologically-rich languages, since their
individual words shift forms in context more often. In this paper we
investigate the extent to which these new unsupervised or generative techniques
can serve to alleviate the type-token ratio disparity in morphologically rich
languages. We apply an off-the-shelf neural language modeling library to the
newly introduced task of unsupervised inflection generation in the nominal
domain of three morphologically rich languages: Romanian, German, and Finnish.
We show that this neural language model architecture can successfully generate
the full inflection table of nouns without needing any pre-training on large,
wikipedia-sized corpora, as long as the model is shown enough inflection
examples. In fact, our experiments show that pre-training hinders the
generation performance."
"Although modern named entity recognition (NER) systems show impressive
performance on standard datasets, they perform poorly when presented with noisy
data. In particular, capitalization is a strong signal for entities in many
languages, and even state of the art models overfit to this feature, with
drastically lower performance on uncapitalized text. In this work, we address
the problem of robustness of NER systems in data with noisy or uncertain
casing, using a pretraining objective that predicts casing in text, or a
truecaser, leveraging unlabeled data. The pretrained truecaser is combined with
a standard BiLSTM-CRF model for NER by appending output distributions to
character embeddings. In experiments over several datasets of varying domain
and casing quality, we show that our new model improves performance in uncased
text, even adding value to uncased BERT embeddings. Our method achieves a new
state of the art on the WNUT17 shared task dataset."
"Lexical normalization, the translation of non-canonical data to standard
language, has shown to improve the performance of manynatural language
processing tasks on social media. Yet, using multiple languages in one
utterance, also called code-switching (CS), is frequently overlooked by these
normalization systems, despite its common use in social media. In this paper,
we propose three normalization models specifically designed to handle
code-switched data which we evaluate for two language pairs: Indonesian-English
(Id-En) and Turkish-German (Tr-De). For the latter, we introduce novel
normalization layers and their corresponding language ID and POS tags for the
dataset, and evaluate the downstream effect of normalization on POS tagging.
Results show that our CS-tailored normalization models outperform Id-En state
of the art and Tr-De monolingual models, and lead to 5.4% relative performance
increase for POS tagging as compared to unnormalized input."
"Many natural language processing (NLP) tasks involve reasoning with textual
spans, including question answering, entity recognition, and coreference
resolution. While extensive research has focused on functional architectures
for representing words and sentences, there is less work on representing
arbitrary spans of text within sentences. In this paper, we conduct a
comprehensive empirical evaluation of six span representation methods using
eight pretrained language representation models across six tasks, including two
tasks that we introduce. We find that, although some simple span
representations are fairly reliable across tasks, in general the optimal span
representation varies by task, and can also vary within different facets of
individual tasks. We also find that the choice of span representation has a
bigger impact with a fixed pretrained encoder than with a fine-tuned encoder."
"The Iranian Persian language has two varieties: standard and colloquial. Most
natural language processing tools for Persian assume that the text is in
standard form: this assumption is wrong in many real applications especially
web content. This paper describes a simple and effective standardization
approach based on sequence-to-sequence translation. We design an algorithm for
generating artificial parallel colloquial-to-standard data for learning a
sequence-to-sequence model. Moreover, we annotate a publicly available
evaluation data consisting of 1912 sentences from a diverse set of domains. Our
intrinsic evaluation shows a higher BLEU score of 62.8 versus 61.7 compared to
an off-the-shelf rule-based standardization model in which the original text
has a BLEU score of 46.4. We also show that our model improves
English-to-Persian machine translation in scenarios for which the training data
is from colloquial Persian with 1.4 absolute BLEU score difference in the
development data, and 0.8 in the test data."
"Conditional text generation has been a challenging task that is yet to see
human-level performance from state-of-the-art models. In this work, we
specifically focus on the Commongen benchmark, wherein the aim is to generate a
plausible sentence for a given set of input concepts. Despite advances in other
tasks, large pre-trained language models that are fine-tuned on this dataset
often produce sentences that are syntactically correct but qualitatively
deviate from a human understanding of common sense. Furthermore, generated
sequences are unable to fulfill such lexical requirements as matching
part-of-speech and full concept coverage. In this paper, we explore how
commonsense knowledge graphs can enhance model performance, with respect to
commonsense reasoning and lexically-constrained decoding. We propose strategies
for enhancing the semantic correctness of the generated text, which we
accomplish through: extracting commonsense relations from Conceptnet, injecting
these relations into the Unified Language Model (UniLM) through attention
mechanisms, and enforcing the aforementioned lexical requirements through
output constraints. By performing several ablations, we find that commonsense
injection enables the generation of sentences that are more aligned with human
understanding, while remaining compliant with lexical requirements."
"Research in Natural Language Processing is making rapid advances, resulting
in the publication of a large number of research papers. Finding relevant
research papers and their contribution to the domain is a challenging problem.
In this paper, we address this challenge via the SemEval 2021 Task 11:
NLPContributionGraph, by developing a system for a research paper
contributions-focused knowledge graph over Natural Language Processing
literature. The task is divided into three sub-tasks: extracting contribution
sentences that show important contributions in the research article, extracting
phrases from the contribution sentences, and predicting the information units
in the research article together with triplet formation from the phrases. The
proposed system is agnostic to the subject domain and can be applied for
building a knowledge graph for any area. We found that transformer-based
language models can significantly improve existing techniques and utilized the
SciBERT-based model. Our first sub-task uses Bidirectional LSTM (BiLSTM)
stacked on top of SciBERT model layers, while the second sub-task uses
Conditional Random Field (CRF) on top of SciBERT with BiLSTM. The third
sub-task uses a combined SciBERT based neural approach with heuristics for
information unit prediction and triplet formation from the phrases. Our system
achieved F1 score of 0.38, 0.63 and 0.76 in end-to-end pipeline testing, phrase
extraction testing and triplet extraction testing respectively."
"Previous work for text summarization in scientific domain mainly focused on
the content of the input document, but seldom considering its citation network.
However, scientific papers are full of uncommon domain-specific terms, making
it almost impossible for the model to understand its true meaning without the
help of the relevant research community. In this paper, we redefine the task of
scientific papers summarization by utilizing their citation graph and propose a
citation graph-based summarization model CGSum which can incorporate the
information of both the source paper and its references. In addition, we
construct a novel scientific papers summarization dataset Semantic Scholar
Network (SSN) which contains 141K research papers in different domains and 661K
citation relationships. The entire dataset constitutes a large connected
citation graph. Extensive experiments show that our model can achieve
competitive performance when compared with the pretrained models even with a
simple architecture. The results also indicates the citation graph is crucial
to better understand the content of papers and generate high-quality summaries."
"We present Samanantar, the largest publicly available parallel corpora
collection for Indic languages. The collection contains a total of 49.7 million
sentence pairs between English and 11 Indic languages (from two language
families). Specifically, we compile 12.4 million sentence pairs from existing,
publicly-available parallel corpora, and additionally mine 37.4 million
sentence pairs from the web, resulting in a 4x increase. We mine the parallel
sentences from the web by combining many corpora, tools, and methods: (a)
web-crawled monolingual corpora, (b) document OCR for extracting sentences from
scanned documents, (c) multilingual representation models for aligning
sentences, and (d) approximate nearest neighbor search for searching in a large
collection of sentences. Human evaluation of samples from the newly mined
corpora validate the high quality of the parallel sentences across 11
languages. Further, we extract 83.4 million sentence pairs between all 55 Indic
language pairs from the English-centric parallel corpus using English as the
pivot language. We trained multilingual NMT models spanning all these languages
on Samanantar, which outperform existing models and baselines on publicly
available benchmarks, such as FLORES, establishing the utility of Samanantar.
Our data and models are available publicly at
https://ai4bharat.iitm.ac.in/samanantar and we hope they will help advance
research in NMT and multilingual NLP for Indic languages."
"Many crowdsourced NLP datasets contain systematic gaps and biases that are
identified only after data collection is complete. Identifying these issues
from early data samples during crowdsourcing should make mitigation more
efficient, especially when done iteratively. We take natural language inference
as a test case and ask whether it is beneficial to put a linguist `in the loop'
during data collection to dynamically identify and address gaps in the data by
introducing novel constraints on the task. We directly compare three data
collection protocols: (i) a baseline protocol, (ii) a linguist-in-the-loop
intervention with iteratively-updated constraints on the task, and (iii) an
extension of linguist-in-the-loop that provides direct interaction between
linguists and crowdworkers via a chatroom. The datasets collected with linguist
involvement are more reliably challenging than baseline, without loss of
quality. But we see no evidence that using this data in training leads to
better out-of-domain model performance, and the addition of a chat platform has
no measurable effect on the resulting dataset. We suggest integrating expert
analysis \textit{during} data collection so that the expert can dynamically
address gaps and biases in the dataset."
"While vector-based language representations from pretrained language models
have set a new standard for many NLP tasks, there is not yet a complete
accounting of their inner workings. In particular, it is not entirely clear
what aspects of sentence-level syntax are captured by these representations,
nor how (if at all) they are built along the stacked layers of the network. In
this paper, we aim to address such questions with a general class of
interventional, input perturbation-based analyses of representations from
pretrained language models. Importing from computational and cognitive
neuroscience the notion of representational invariance, we perform a series of
probes designed to test the sensitivity of these representations to several
kinds of structure in sentences. Each probe involves swapping words in a
sentence and comparing the representations from perturbed sentences against the
original. We experiment with three different perturbations: (1) random
permutations of n-grams of varying width, to test the scale at which a
representation is sensitive to word position; (2) swapping of two spans which
do or do not form a syntactic phrase, to test sensitivity to global phrase
structure; and (3) swapping of two adjacent words which do or do not break
apart a syntactic phrase, to test sensitivity to local phrase structure.
  Results from these probes collectively suggest that Transformers build
sensitivity to larger parts of the sentence along their layers, and that
hierarchical phrase structure plays a role in this process. More broadly, our
results also indicate that structured input perturbations widens the scope of
analyses that can be performed on often-opaque deep learning systems, and can
serve as a complement to existing tools (such as supervised linear probes) for
interpreting complex black-box models."
"Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new
languages is hard. We propose a novel architecture to facilitate it for
multiple languages while using data less than 3% of the size of the data used
by the state of the art results on English. We treat TN as a sequence
classification problem and propose a granular tokenization mechanism that
enables the system to learn majority of the classes and their normalizations
from the training data itself. This is further combined with minimal precoded
linguistic knowledge for other classes. We publish the first results on TN for
TTS in Spanish and Tamil and also demonstrate that the performance of the
approach is comparable with the previous work done on English. All annotated
datasets used for experimentation will be released at
https://github.com/amazon-research/proteno."
"Understanding how events are semantically related to each other is the
essence of reading comprehension. Recent event-centric reading comprehension
datasets focus mostly on event arguments or temporal relations. While these
tasks partially evaluate machines' ability of narrative understanding,
human-like reading comprehension requires the capability to process event-based
information beyond arguments and temporal reasoning. For example, to understand
causality between events, we need to infer motivation or purpose; to establish
event hierarchy, we need to understand the composition of events. To facilitate
these tasks, we introduce ESTER, a comprehensive machine reading comprehension
(MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages
natural language queries to reason about the five most common event semantic
relations, provides more than 6K questions and captures 10.1K event relation
pairs. Experimental results show that the current SOTA systems achieve 22.1%,
63.3%, and 83.5% for token-based exact-match, F1, and event-based HIT@1 scores,
which are all significantly below human performances (36.0%, 79.6%, 100%
respectively), highlighting our dataset as a challenging benchmark."
"Deep learning techniques have achieved great success in many fields, while at
the same time deep learning models are getting more complex and expensive to
compute. It severely hinders the wide applications of these models. In order to
alleviate this problem, model distillation emerges as an effective means to
compress a large model into a smaller one without a significant drop in
accuracy. In this paper, we study a related but orthogonal issue, data
distillation, which aims to distill the knowledge from a large training dataset
down to a smaller and synthetic one. It has the potential to address the large
and growing neural network training problem based on the small dataset. We
develop a novel data distillation method for text classification. We evaluate
our method on eight benchmark datasets. The results that the distilled data
with the size of 0.1% of the original text data achieves approximately 90%
performance of the original is rather impressive."
"Dialogue systems powered by large pre-trained language models (LM) exhibit an
innate ability to deliver fluent and natural-looking responses. Despite their
impressive generation performance, these models can often generate factually
incorrect statements impeding their widespread adoption. In this paper, we
focus on the task of improving the faithfulness -- and thus reduce
hallucination -- of Neural Dialogue Systems to known facts supplied by a
Knowledge Graph (KG). We propose Neural Path Hunter which follows a
generate-then-refine strategy whereby a generated response is amended using the
k-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level
fact critic to identify plausible sources of hallucination followed by a
refinement stage consisting of a chain of two neural LM's that retrieves
correct entities by crafting a query signal that is propagated over the k-hop
subgraph. Our proposed model can easily be applied to any dialogue generated
responses without retraining the model. We empirically validate our proposed
approach on the OpenDialKG dataset against a suite of metrics and report a
relative improvement of faithfulness over dialogue responses by 20.35% based on
FeQA (Durmus et al., 2020)."
"WordNet lexical-database groups English words into sets of synonyms called
""synsets."" Synsets are utilized for several applications in the field of
text-mining. However, they were also open to criticism because although, in
reality, not all the members of a synset represent the meaning of that synset
with the same degree, in practice, they are considered as members of the
synset, identically. Thus, the fuzzy version of synsets, called fuzzy-synsets
(or fuzzy word-sense classes) were proposed and studied. In this study, we
discuss why (type-1) fuzzy synsets (T1 F-synsets) do not properly model the
membership uncertainty, and propose an upgraded version of fuzzy synsets in
which membership degrees of word-senses are represented by intervals, similar
to what in Interval Type 2 Fuzzy Sets (IT2 FS) and discuss that IT2 FS
theoretical framework is insufficient for analysis and design of such synsets,
and propose a new concept, called Interval Probabilistic Fuzzy (IPF) sets. Then
we present an algorithm for constructing the IPF synsets in any language, given
a corpus and a word-sense-disambiguation system. Utilizing our algorithm and
the open-American-online-corpus (OANC) and UKB word-sense-disambiguation, we
constructed and published the IPF synsets of WordNet for English language."
"Contextual word representation models have shown massive improvements on a
multitude of NLP tasks, yet their word sense disambiguation capabilities remain
poorly explained. To address this gap, we assess whether contextual word
representations extracted from deep pretrained language models create
distinguishable representations for different senses of a given word. We
analyze the representation geometry and find that most layers of deep
pretrained language models create highly anisotropic representations, pointing
towards the existence of representation degeneration problem in contextual word
representations. After accounting for anisotropy, our study further reveals
that there is variability in sense learning capabilities across different
language models. Finally, we propose LASeR, a 'Low Anisotropy Sense
Retrofitting' approach that renders off-the-shelf representations isotropic and
semantically more meaningful, resolving the representation degeneration problem
as a post-processing step, and conducting sense-enrichment of contextualized
representations extracted from deep neural language models."
"We introduce the problem of proficiency modeling: Given a user's posts on a
social media platform, the task is to identify the subset of posts or topics
for which the user has some level of proficiency. This enables the filtering
and ranking of social media posts on a given topic as per user proficiency.
Unlike experts on a given topic, proficient users may not have received formal
training and possess years of practical experience, but may be autodidacts,
hobbyists, and people with sustained interest, enabling them to make genuine
and original contributions to discourse. While predicting whether a user is an
expert on a given topic imposes strong constraints on who is a true positive,
proficiency modeling implies a graded scoring, relaxing these constraints. Put
another way, many active social media users can be assumed to possess, or
eventually acquire, some level of proficiency on topics relevant to their
community. We tackle proficiency modeling in an unsupervised manner by
utilizing user embeddings to model engagement with a given topic, as indicated
by a user's preference for authoring related content. We investigate five
alternative approaches to model proficiency, ranging from basic ones to an
advanced, tailored user modeling approach, applied within two real-world
benchmarks for evaluation."
"Fact-checking is an essential tool to mitigate the spread of misinformation
and disinformation. We introduce the task of fact-checking in dialogue, which
is a relatively unexplored area. We construct DialFact, a testing benchmark
dataset of 22,245 annotated conversational claims, paired with pieces of
evidence from Wikipedia. There are three sub-tasks in DialFact: 1) Verifiable
claim detection task distinguishes whether a response carries verifiable
factual information; 2) Evidence retrieval task retrieves the most relevant
Wikipedia snippets as evidence; 3) Claim verification task predicts a dialogue
response to be supported, refuted, or not enough information. We found that
existing fact-checking models trained on non-dialogue data like FEVER fail to
perform well on our task, and thus, we propose a simple yet data-efficient
solution to effectively improve fact-checking performance in dialogue. We point
out unique challenges in DialFact such as handling the colloquialisms,
coreferences and retrieval ambiguities in the error analysis to shed light on
future research in this direction."
"Idiomatic expressions are an integral part of natural language and constantly
being added to a language. Owing to their non-compositionality and their
ability to take on a figurative or literal meaning depending on the sentential
context, they have been a classical challenge for NLP systems. To address this
challenge, we study the task of detecting whether a sentence has an idiomatic
expression and localizing it. Prior art for this task had studied specific
classes of idiomatic expressions offering limited views of their
generalizability to new idioms. We propose a multi-stage neural architecture
with the attention flow mechanism for identifying these expressions. The
network effectively fuses contextual and lexical information at different
levels using word and sub-word representations. Empirical evaluations on three
of the largest benchmark datasets with idiomatic expressions of varied
syntactic patterns and degrees of non-compositionality show that our proposed
model achieves new state-of-the-art results. A salient feature of the model is
its ability to identify idioms unseen during training with gains from 1.4% to
30.8% over competitive baselines on the largest dataset."
"Over the recent years, large pretrained language models (LM) have
revolutionized the field of natural language processing (NLP). However, while
pretraining on general language has been shown to work very well for common
language, it has been observed that niche language poses problems. In
particular, climate-related texts include specific language that common LMs can
not represent accurately. We argue that this shortcoming of today's LMs limits
the applicability of modern NLP to the broad field of text processing of
climate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based
language model that is further pretrained on over 2 million paragraphs of
climate-related texts, crawled from various sources such as common news,
research articles, and climate reporting of companies. We find that CLIMATEBERT
leads to a 48% improvement on a masked language model objective which, in turn,
leads to lowering error rates by 3.57% to 35.71% for various climate-related
downstream tasks like text classification, sentiment analysis, and
fact-checking."
"Dialogue disentanglement aims to group utterances in a long and
multi-participant dialogue into threads. This is useful for discourse analysis
and downstream applications such as dialogue response selection, where it can
be the first step to construct a clean context/response set. Unfortunately,
labeling all~\emph{reply-to} links takes quadratic effort w.r.t the number of
utterances: an annotator must check all preceding utterances to identify the
one to which the current utterance is a reply. In this paper, we are the first
to propose a~\textbf{zero-shot} dialogue disentanglement solution. Firstly, we
train a model on a multi-participant response selection dataset harvested from
the web which is not annotated; we then apply the trained model to perform
zero-shot dialogue disentanglement. Without any labeled data, our model can
achieve a cluster F1 score of 25. We also fine-tune the model using various
amounts of labeled data. Experiments show that with only 10\% of the data, we
achieve nearly the same performance of using the full dataset\footnote{Code is
released at
\url{https://github.com/chijames/zero_shot_dialogue_disentanglement}}."
"To consider Hawrami and Zaza (Zazaki) standalone languages or dialects of a
language have been discussed and debated for a while among linguists active in
studying Iranian languages. The question of whether those languages/dialects
belong to the Kurdish language or if they are independent descendants of
Iranian languages was answered by MacKenzie (1961). However, a majority of
people who speak the dialects are against that answer. Their disapproval mainly
seems to be based on the sociological, cultural, and historical relationship
among the speakers of the dialects. While the case of Hawrami and Zaza has
remained unexplored and under-examined, an almost unanimous agreement exists
about the classification of Kurmanji and Sorani as Kurdish dialects. The
related studies to address the mentioned cases are primarily qualitative.
However, computational linguistics could approach the question from a
quantitative perspective. In this research, we look into three questions from a
linguistic distance point of view. First, how similar or dissimilar Hawrami and
Zaza are, considering no common geographical coexistence between the two.
Second, what about Kurmanji and Sorani that have geographical overlap. Finally,
what is the distance among all these dialects, pair by pair? We base our
computation on phonetic presentations of these dialects (languages), and we
calculate various linguistic distances among the pairs. We analyze the data and
discuss the results to conclude."
"Pretrained language models (PLMs) often fail to fairly represent target users
from certain world regions because of the under-representation of those regions
in training datasets. With recent PLMs trained on enormous data sources,
quantifying their potential biases is difficult, due to their black-box nature
and the sheer scale of the data sources. In this work, we devise an approach to
study the geographic bias (and knowledge) present in PLMs, proposing a
Geographic-Representation Probing Framework adopting a self-conditioning method
coupled with entity-country mappings. Our findings suggest PLMs'
representations map surprisingly well to the physical world in terms of
country-to-country associations, but this knowledge is unequally shared across
languages. Last, we explain how large PLMs despite exhibiting notions of
geographical proximity, over-amplify geopolitical favouritism at inference
time."
"Abstractive summarization has enjoyed renewed interest in recent years,
thanks to pre-trained language models and the availability of large-scale
datasets. Despite promising results, current models still suffer from
generating factually inconsistent summaries, reducing their utility for
real-world application. Several recent efforts attempt to address this by
devising models that automatically detect factual inconsistencies in machine
generated summaries. However, they focus exclusively on English, a language
with abundant resources. In this work, we leverage factual consistency
evaluation models to improve multilingual summarization. We explore two
intuitive approaches to mitigate hallucinations based on the signal provided by
a multilingual NLI model, namely data filtering and controlled generation.
Experimental results in the 45 languages from the XLSum dataset show gains over
strong baselines in both automatic and human evaluation."
"Machine Translation for Indian languages is an emerging research area.
Transliteration is one such module that we design while designing a translation
system. Transliteration means mapping of source language text into the target
language. Simple mapping decreases the efficiency of overall translation
system. We propose the use of stemming and part-of-speech tagging for
transliteration. The effectiveness of translation can be improved if we use
part-of-speech tagging and stemming assisted transliteration.We have shown that
much of the content in Gujarati gets transliterated while being processed for
translation to Hindi language."
"The problem of word search in Sanskrit is inseparable from complexities that
include those caused by euphonic conjunctions and case-inflections. The
case-inflectional forms of a noun normally number 24 owing to the fact that in
Sanskrit there are eight cases and three numbers-singular, dual and plural. The
traditional method of generating these inflectional forms is rather elaborate
owing to the fact that there are differences in the forms generated between
even very similar words and there are subtle nuances involved. Further, it
would be a cumbersome exercise to generate and search for 24 forms of a word
during a word search in a large text, using the currently available
case-inflectional form generators. This study presents a new approach to
generating case-inflectional forms that is simpler to compute. Further, an
optimized model that is sufficient for generating only those word forms that
are required in a word search and is more than 80% efficient compared to the
complete case-inflectional forms generator, is presented in this study for the
first time."
"This work studies comparatively two typical sentence pair classification
tasks: textual entailment (TE) and answer selection (AS), observing that phrase
alignments of different intensities contribute differently in these tasks. We
address the problems of identifying phrase alignments of flexible granularity
and pooling alignments of different intensities for these tasks. Examples for
flexible granularity are alignments between two single words, between a single
word and a phrase and between a short phrase and a long phrase. By intensity we
roughly mean the degree of match, it ranges from identity over surface-form
co-occurrence, rephrasing and other semantic relatedness to unrelated words as
in lots of parenthesis text. Prior work (i) has limitations in phrase
generation and representation, or (ii) conducts alignment at word and phrase
levels by handcrafted features or (iii) utilizes a single attention mechanism
over alignment intensities without considering the characteristics of specific
tasks, which limits the system's effectiveness across tasks. We propose an
architecture based on Gated Recurrent Unit that supports (i) representation
learning of phrases of arbitrary granularity and (ii) task-specific focusing of
phrase alignments between two sentences by attention pooling. Experimental
results on TE and AS match our observation and are state-of-the-art."
"We study the impact of big models (in terms of the degree of lexicalization)
and big data (in terms of the training corpus size) on dependency grammar
induction. We experimented with L-DMV, a lexicalized version of Dependency
Model with Valence and L-NDMV, our lexicalized extension of the Neural
Dependency Model with Valence. We find that L-DMV only benefits from very small
degrees of lexicalization and moderate sizes of training corpora. L-NDMV can
benefit from big training data and lexicalization of greater degrees,
especially when enhanced with good model initialization, and it achieves a
result that is competitive with the current state-of-the-art."
"Traditional automatic evaluation measures for natural language generation
(NLG) use costly human-authored references to estimate the quality of a system
output. In this paper, we propose a referenceless quality estimation (QE)
approach based on recurrent neural networks, which predicts a quality score for
a NLG system output by comparing it to the source meaning representation only.
Our method outperforms traditional metrics and a constant baseline in most
respects; we also show that synthetic data helps to increase correlation
results by 21% compared to the base system. Our results are comparable to
results obtained in similar QE tasks despite the more challenging setting."
"In this paper, the problem of recovery of morphological information lost in
abbreviated forms is addressed with a focus on highly inflected languages.
Evidence is presented that the correct inflected form of an expanded
abbreviation can in many cases be deduced solely from the morphosyntactic tags
of the context. The prediction model is a deep bidirectional LSTM network with
tag embedding. The training and evaluation data are gathered by finding the
words which could have been abbreviated and using their corresponding
morphosyntactic tags as the labels, while the tags of the context words are
used as the input features for classification. The network is trained on over
10 million words from the Polish Sejm Corpus and achieves 74.2% prediction
accuracy on a smaller, but more general National Corpus of Polish. The analysis
of errors suggests that performance in this task may improve if some prior
knowledge about the abbreviated word is incorporated into the model."
"This paper presents an empirical study of two widely-used sequence prediction
models, Conditional Random Fields (CRFs) and Long Short-Term Memory Networks
(LSTMs), on two fundamental tasks for Vietnamese text processing, including
part-of-speech tagging and named entity recognition. We show that a strong
lower bound for labeling accuracy can be obtained by relying only on simple
word-based features with minimal hand-crafted feature engineering, of 90.65\%
and 86.03\% performance scores on the standard test sets for the two tasks
respectively. In particular, we demonstrate empirically the surprising
efficiency of word embeddings in both of the two tasks, with both of the two
models. We point out that the state-of-the-art LSTMs model does not always
outperform significantly the traditional CRFs model, especially on
moderate-sized data sets. Finally, we give some suggestions and discussions for
efficient use of sequence labeling models in practical applications."
"Croatian is poorly resourced and highly inflected language from Slavic
language family. Nowadays, research is focusing mostly on English. We created a
new word analogy corpus based on the original English Word2vec word analogy
corpus and added some of the specific linguistic aspects from Croatian
language. Next, we created Croatian WordSim353 and RG65 corpora for a basic
evaluation of word similarities. We compared created corpora on two popular
word representation models, based on Word2Vec tool and fastText tool. Models
has been trained on 1.37B tokens training data corpus and tested on a new
robust Croatian word analogy corpus. Results show that models are able to
create meaningful word representation. This research has shown that free word
order and the higher morphological complexity of Croatian language influences
the quality of resulting word embeddings."
"Inspired by the principles of speed reading, we introduce Skim-RNN, a
recurrent neural network (RNN) that dynamically decides to update only a small
fraction of the hidden state for relatively unimportant input tokens. Skim-RNN
gives computational advantage over an RNN that always updates the entire hidden
state. Skim-RNN uses the same input and output interfaces as a standard RNN and
can be easily used instead of RNNs in existing models. In our experiments, we
show that Skim-RNN can achieve significantly reduced computational cost without
losing accuracy compared to standard RNNs across five different natural
language tasks. In addition, we demonstrate that the trade-off between accuracy
and speed of Skim-RNN can be dynamically controlled during inference time in a
stable manner. Our analysis also shows that Skim-RNN running on a single CPU
offers lower latency compared to standard RNNs on GPUs."
"This paper describes a neural semantic parser that maps natural language
utterances onto logical forms which can be executed against a task-specific
environment, such as a knowledge base or a database, to produce a response. The
parser generates tree-structured logical forms with a transition-based approach
which combines a generic tree-generation algorithm with domain-general
operations defined by the logical language. The generation process is modeled
by structured recurrent neural networks, which provide a rich encoding of the
sentential context and generation history for making predictions. To tackle
mismatches between natural language and logical form tokens, various attention
mechanisms are explored. Finally, we consider different training settings for
the neural semantic parser, including a fully supervised training where
annotated logical forms are given, weakly-supervised training where denotations
are provided, and distant supervision where only unlabeled sentences and a
knowledge base are available. Experiments across a wide range of datasets
demonstrate the effectiveness of our parser."
"Research on customer satisfaction has increased substantially in recent
years. However, the relative importance and relationships between different
determinants of satisfaction remains uncertain. Moreover, quantitative studies
to date tend to test for significance of pre-determined factors thought to have
an influence with no scalable means to identify other causes of user
satisfaction. The gaps in knowledge make it difficult to use available
knowledge on user preference for public service improvement. Meanwhile, digital
technology development has enabled new methods to collect user feedback, for
example through online forums where users can comment freely on their
experience. New tools are needed to analyze large volumes of such feedback. Use
of topic models is proposed as a feasible solution to aggregate open-ended user
opinions that can be easily deployed in the public sector. Generated insights
can contribute to a more inclusive decision-making process in public service
provision. This novel methodological approach is applied to a case of service
reviews of publicly-funded primary care practices in England. Findings from the
analysis of 145,000 reviews covering almost 7,700 primary care centers indicate
that the quality of interactions with staff and bureaucratic exigencies are the
key issues driving user satisfaction across England."
"Social media platforms have recently seen an increase in the occurrence of
hate speech discourse which has led to calls for improved detection methods.
Most of these rely on annotated data, keywords, and a classification technique.
While this approach provides good coverage, it can fall short when dealing with
new terms produced by online extremist communities which act as original
sources of words which have alternate hate speech meanings. These code words
(which can be both created and adopted words) are designed to evade automatic
detection and often have benign meanings in regular discourse. As an example,
""skypes"", ""googles"", and ""yahoos"" are all instances of words which have an
alternate meaning that can be used for hate speech. This overlap introduces
additional challenges when relying on keywords for both the collection of data
that is specific to hate speech, and downstream classification. In this work,
we develop a community detection approach for finding extremist hate speech
communities and collecting data from their members. We also develop a word
embedding model that learns the alternate hate speech meaning of words and
demonstrate the candidacy of our code words with several annotation
experiments, designed to determine if it is possible to recognize a word as
being used for hate speech without knowing its alternate meaning. We report an
inter-annotator agreement rate of K=0.871, and K=0.676 for data drawn from our
extremist community and the keyword approach respectively, supporting our claim
that hate speech detection is a contextual task and does not depend on a fixed
list of keywords. Our goal is to advance the domain by providing a high quality
hate speech dataset in addition to learned code words that can be fed into
existing classification approaches, thus improving the accuracy of automated
detection."
"We present a novel deep learning architecture to address the natural language
inference (NLI) task. Existing approaches mostly rely on simple reading
mechanisms for independent encoding of the premise and hypothesis. Instead, we
propose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) to
efficiently model the relationship between a premise and a hypothesis during
encoding and inference. We also introduce a sophisticated ensemble strategy to
combine our proposed models, which noticeably improves final predictions.
Finally, we demonstrate how the results can be improved further with an
additional preprocessing step. Our evaluation shows that DR-BiLSTM obtains the
best single model and ensemble model results achieving the new state-of-the-art
scores on the Stanford NLI dataset."
"This paper describes our system that has been used in Task1 Affect in Tweets.
We combine two different approaches. The first one called N-Stream ConvNets,
which is a deep learning approach where the second one is XGboost regresseor
based on a set of embedding and lexicons based features. Our system was
evaluated on the testing sets of the tasks outperforming all other approaches
for the Arabic version of valence intensity regression task and valence ordinal
classification task."
"Common-sense reasoning is becoming increasingly important for the advancement
of Natural Language Processing. While word embeddings have been very
successful, they cannot explain which aspects of 'coffee' and 'tea' make them
similar, or how they could be related to 'shop'. In this paper, we propose an
explicit word representation that builds upon the Distributional Hypothesis to
represent meaning from semantic roles, and allow inference of relations from
their meshing, as supported by the affordance-based Indexical Hypothesis. We
find that our model improves the state-of-the-art on unsupervised word
similarity tasks while allowing for direct inference of new relations from the
same vector space."
"Past shared tasks on emotions use data with both overt expressions of
emotions (I am so happy to see you!) as well as subtle expressions where the
emotions have to be inferred, for instance from event descriptions. Further,
most datasets do not focus on the cause or the stimulus of the emotion. Here,
for the first time, we propose a shared task where systems have to predict the
emotions in a large automatically labeled dataset of tweets without access to
words denoting emotions. Based on this intention, we call this the Implicit
Emotion Shared Task (IEST) because the systems have to infer the emotion mostly
from the context. Every tweet has an occurrence of an explicit emotion word
that is masked. The tweets are collected in a manner such that they are likely
to include a description of the cause of the emotion - the stimulus.
Altogether, 30 teams submitted results which range from macro F1 scores of 21 %
to 71 %. The baseline (MaxEnt bag of words and bigrams) obtains an F1 score of
60 % which was available to the participants during the development phase. A
study with human annotators suggests that automatic methods outperform human
predictions, possibly by honing into subtle textual clues not used by humans.
Corpora, resources, and results are available at the shared task website at
http://implicitemotions.wassa2018.com."
"Semantic parsing from denotations faces two key challenges in model training:
(1) given only the denotations (e.g., answers), search for good candidate
semantic parses, and (2) choose the best model update algorithm. We propose
effective and general solutions to each of them. Using policy shaping, we bias
the search procedure towards semantic parses that are more compatible to the
text, which provide better supervision signals for training. In addition, we
propose an update equation that generalizes three different families of
learning algorithms, which enables fast model exploration. When experimented on
a recently proposed sequential question answering dataset, our framework leads
to a new state-of-the-art model that outperforms previous work by 5.0% absolute
on exact match accuracy."
"Neural machine translation (NMT) systems operate primarily on words (or
sub-words), ignoring lower-level patterns of morphology. We present a
character-aware decoder designed to capture such patterns when translating into
morphologically rich languages. We achieve character-awareness by augmenting
both the softmax and embedding layers of an attention-based encoder-decoder
model with convolutional neural networks that operate on the spelling of a
word. To investigate performance on a wide variety of morphological phenomena,
we translate English into 14 typologically diverse target languages using the
TED multi-target dataset. In this low-resource setting, the character-aware
decoder provides consistent improvements with BLEU score gains of up to
$+3.05$. In addition, we analyze the relationship between the gains obtained
and properties of the target language and find evidence that our model does
indeed exploit morphological patterns."
"Recent neural machine translation (NMT) systems have been greatly improved by
encoder-decoder models with attention mechanisms and sub-word units. However,
important differences between languages with logographic and alphabetic writing
systems have long been overlooked. This study focuses on these differences and
uses a simple approach to improve the performance of NMT systems utilizing
decomposed sub-character level information for logographic languages. Our
results indicate that our approach not only improves the translation
capabilities of NMT systems between Chinese and English, but also further
improves NMT systems between Chinese and Japanese, because it utilizes the
shared information brought by similar sub-character units."
"Sequential neural networks models are powerful tools in a variety of Natural
Language Processing (NLP) tasks. The sequential nature of these models raises
the questions: to what extent can these models implicitly learn hierarchical
structures typical to human language, and what kind of grammatical phenomena
can they acquire?
  We focus on the task of agreement prediction in Basque, as a case study for a
task that requires implicit understanding of sentence structure and the
acquisition of a complex but consistent morphological system. Analyzing
experimental results from two syntactic prediction tasks -- verb number
prediction and suffix recovery -- we find that sequential models perform worse
on agreement prediction in Basque than one might expect on the basis of a
previous agreement prediction work in English. Tentative findings based on
diagnostic classifiers suggest the network makes use of local heuristics as a
proxy for the hierarchical structure of the sentence. We propose the Basque
agreement prediction task as challenging benchmark for models that attempt to
learn regularities in human language."
"In this paper, we present a method of automatic catchphrase extracting from
legal case documents. We utilize deep neural networks for constructing scoring
model of our extraction system. We achieve comparable performance with systems
using corpus-wide and citation information which we do not use in our system."
"Readmission after discharge from a hospital is disruptive and costly,
regardless of the reason. However, it can be particularly problematic for
psychiatric patients, so predicting which patients may be readmitted is
critically important but also very difficult. Clinical narratives in
psychiatric electronic health records (EHRs) span a wide range of topics and
vocabulary; therefore, a psychiatric readmission prediction model must begin
with a robust and interpretable topic extraction component. We created a data
pipeline for using document vector similarity metrics to perform topic
extraction on psychiatric EHR data in service of our long-term goal of creating
a readmission risk classifier. We show initial results for our topic extraction
model and identify additional features we will be incorporating in the future."
"Generative adversarial networks (GANs) have achieved significant success in
generating real-valued data. However, the discrete nature of text hinders the
application of GAN to text-generation tasks. Instead of using the standard GAN
objective, we propose to improve text-generation GAN via a novel approach
inspired by optimal transport. Specifically, we consider matching the latent
feature distributions of real and synthetic sentences using a novel metric,
termed the feature-mover's distance (FMD). This formulation leads to a highly
discriminative critic and easy-to-optimize objective, overcoming the
mode-collapsing and brittle-training problems in existing methods. Extensive
experiments are conducted on a variety of tasks to evaluate the proposed model
empirically, including unconditional text generation, style transfer from
non-parallel text, and unsupervised cipher cracking. The proposed model yields
superior performance, demonstrating wide applicability and effectiveness."
"This paper describes our competing system to enter the MEDIQA-2019
competition. We use a multi-source transfer learning approach to transfer the
knowledge from MT-DNN and SciBERT to natural language understanding tasks in
the medical domain. For transfer learning fine-tuning, we use multi-task
learning on NLI, RQE and QA tasks on general and medical domains to improve
performance. The proposed methods are proved effective for natural language
understanding in the medical domain, and we rank the first place on the QA
task."
"Dependency distance minimization (DDm) is a word order principle favouring
the placement of syntactically related words close to each other in sentences.
Massive evidence of the principle has been reported for more than a decade with
the help of syntactic dependency treebanks where long sentences abound.
However, it has been predicted theoretically that the principle is more likely
to be beaten in short sequences by the principle of surprisal minimization
(predictability maximization). Here we introduce a simple binomial test to
verify such a hypothesis. In short sentences, we find anti-DDm for some
languages from different families. Our analysis of the syntactic dependency
structures suggests that anti-DDm is produced by star trees."
"Multiple entities in a document generally exhibit complex inter-sentence
relations, and cannot be well handled by existing relation extraction (RE)
methods that typically focus on extracting intra-sentence relations for single
entity pairs. In order to accelerate the research on document-level RE, we
introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with
three features: (1) DocRED annotates both named entities and relations, and is
the largest human-annotated dataset for document-level RE from plain text; (2)
DocRED requires reading multiple sentences in a document to extract entities
and infer their relations by synthesizing all information of the document; (3)
along with the human-annotated data, we also offer large-scale distantly
supervised data, which enables DocRED to be adopted for both supervised and
weakly supervised scenarios. In order to verify the challenges of
document-level RE, we implement recent state-of-the-art methods for RE and
conduct a thorough evaluation of these methods on DocRED. Empirical results
show that DocRED is challenging for existing RE methods, which indicates that
document-level RE remains an open problem and requires further efforts. Based
on the detailed analysis on the experiments, we discuss multiple promising
directions for future research."
"In this paper, we propose two novel methods for domain adaptation for the
attention-only neural machine translation (NMT) model, i.e., the Transformer.
Our methods focus on training a single translation model for multiple domains
by either learning domain specialized hidden state representations or predictor
biases for each domain. We combine our methods with a previously proposed
black-box method called mixed fine tuning, which is known to be highly
effective for domain adaptation. In addition, we incorporate multilingualism
into the domain adaptation framework. Experiments show that multilingual
multi-domain adaptation can significantly improve both resource-poor in-domain
and resource-rich out-of-domain translations, and the combination of our
methods with mixed fine tuning achieves the best performance."
"The effect of translationese has been studied in the field of machine
translation (MT), mostly with respect to training data. We study in depth the
effect of translationese on test data, using the test sets from the last three
editions of WMT's news shared task, containing 17 translation directions. We
show evidence that (i) the use of translationese in test sets results in
inflated human evaluation scores for MT systems; (ii) in some cases system
rankings do change and (iii) the impact translationese has on a translation
direction is inversely correlated to the translation quality attainable by
state-of-the-art MT systems for that direction."
"Textual conversational agent or chatbots' development gather tremendous
traction from both academia and industries in recent years. Nowadays, chatbots
are widely used as an agent to communicate with a human in some services such
as booking assistant, customer service, and also a personal partner. The
biggest challenge in building chatbot is to build a humanizing machine to
improve user engagement. Some studies show that emotion is an important aspect
to humanize machine, including chatbot. In this paper, we will provide a
systematic review of approaches in building an emotionally-aware chatbot (EAC).
As far as our knowledge, there is still no work focusing on this area. We
propose three research question regarding EAC studies. We start with the
history and evolution of EAC, then several approaches to build EAC by previous
studies, and some available resources in building EAC. Based on our
investigation, we found that in the early development, EAC exploits a simple
rule-based approach while now most of EAC use neural-based approach. We also
notice that most of EAC contain emotion classifier in their architecture, which
utilize several available affective resources. We also predict that the
development of EAC will continue to gain more and more attention from scholars,
noted by some recent studies propose new datasets for building EAC in various
languages."
"This paper describes the systems that we submitted to the WMT19 Machine
Translation robustness task. This task aims to improve MT's robustness to noise
found on social media, like informal language, spelling mistakes and other
orthographic variations. The organizers provide parallel data extracted from a
social media website in two language pairs: French-English and Japanese-English
(in both translation directions). The goal is to obtain the best scores on
unseen test sets from the same source, according to automatic metrics (BLEU)
and human evaluation. We proposed one single and one ensemble system for each
translation direction. Our ensemble models ranked first in all language pairs,
according to BLEU evaluation. We discuss the pre-processing choices that we
made, and present our solutions for robustness to noise and domain adaptation."
"We propose a practical scheme to train a single multilingual sequence
labeling model that yields state of the art results and is small and fast
enough to run on a single CPU. Starting from a public multilingual BERT
checkpoint, our final model is 6x smaller and 27x faster, and has higher
accuracy than a state-of-the-art multilingual baseline. We show that our model
especially outperforms on low-resource languages, and works on codemixed input
text without being explicitly trained on codemixed examples. We showcase the
effectiveness of our method by reporting on part-of-speech tagging and
morphological prediction on 70 treebanks and 48 languages."
"Generating text from graph-based data, such as Abstract Meaning
Representation (AMR), is a challenging task due to the inherent difficulty in
how to properly encode the structure of a graph with labeled edges. To address
this difficulty, we propose a novel graph-to-sequence model that encodes
different but complementary perspectives of the structural information
contained in the AMR graph. The model learns parallel top-down and bottom-up
representations of nodes capturing contrasting views of the graph. We also
investigate the use of different node message passing strategies, employing
different state-of-the-art graph encoders to compute node representations based
on incoming and outgoing perspectives. In our experiments, we demonstrate that
the dual graph representation leads to improvements in AMR-to-text generation,
achieving state-of-the-art results on two AMR datasets."
"Vector averaging remains one of the most popular sentence embedding methods
in spite of its obvious disregard for syntactic structure. While more complex
sequential or convolutional networks potentially yield superior classification
performance, the improvements in classification accuracy are typically mediocre
compared to the simple vector averaging. As an efficient alternative, we
propose the use of discrete cosine transform (DCT) to compress word sequences
in an order-preserving manner. The lower order DCT coefficients represent the
overall feature patterns in sentences, which results in suitable embeddings for
tasks that could benefit from syntactic features. Our results in semantic
probing tasks demonstrate that DCT embeddings indeed preserve more syntactic
information compared with vector averaging. With practically equivalent
complexity, the model yields better overall performance in downstream
classification tasks that correlate with syntactic features, which illustrates
the capacity of DCT to preserve word order information."
"Neural network language model (NNLM) is an essential component of industrial
ASR systems. One important challenge of training an NNLM is to leverage between
scaling the learning process and handling big data. Conventional approaches
such as block momentum provides a blockwise model update filtering (BMUF)
process and achieves almost linear speedups with no performance degradation for
speech recognition. However, it needs to calculate the model average from all
computing nodes (e.g., GPUs) and when the number of computing nodes is large,
the learning suffers from the severe communication latency. As a consequence,
BMUF is not suitable under restricted network conditions. In this paper, we
present a decentralized BMUF process, in which the model is split into
different components, each of which is updated by communicating to some
randomly chosen neighbor nodes with the same component, followed by a BMUF-like
process. We apply this method to several LSTM language modeling tasks.
Experimental results show that our approach achieves consistently better
performance than conventional BMUF. In particular, we obtain a lower perplexity
than the single-GPU baseline on the wiki-text-103 benchmark using 4 GPUs. In
addition, no performance degradation is observed when scaling to 8 and 16 GPUs."
"Contextualized word embeddings (CWE) such as provided by ELMo (Peters et al.,
2018), Flair NLP (Akbik et al., 2018), or BERT (Devlin et al., 2019) are a
major recent innovation in NLP. CWEs provide semantic vector representations of
words depending on their respective context. Their advantage over static word
embeddings has been shown for a number of tasks, such as text classification,
sequence tagging, or machine translation. Since vectors of the same word type
can vary depending on the respective context, they implicitly provide a model
for word sense disambiguation (WSD). We introduce a simple but effective
approach to WSD using a nearest neighbor classification on CWEs. We compare the
performance of different CWE models for the task and can report improvements
above the current state of the art for two standard WSD benchmark datasets. We
further show that the pre-trained BERT model is able to place polysemic words
into distinct 'sense' regions of the embedding space, while ELMo and Flair NLP
do not seem to possess this ability."
"We introduce a classification scheme for detecting political bias in long
text content such as newspaper opinion articles. Obtaining long text data and
annotations at sufficient scale for training is difficult, but it is relatively
easy to extract political polarity from tweets through their authorship. We
train on tweets and perform inference on articles. Universal sentence encoders
and other existing methods that aim to address this domain-adaptation scenario
deliver inaccurate and inconsistent predictions on articles, which we show is
due to a difference in opinion concentration between tweets and articles. We
propose a two-step classification scheme that uses a neutral detector trained
on tweets to remove neutral sentences from articles in order to align opinion
concentration and therefore improve accuracy on that domain. Our implementation
is available for public use at https://knowbias.ml."
"We analyse coreference phenomena in three neural machine translation systems
trained with different data settings with or without access to explicit intra-
and cross-sentential anaphoric information. We compare system performance on
two different genres: news and TED talks. To do this, we manually annotate (the
possibly incorrect) coreference chains in the MT outputs and evaluate the
coreference chain translations. We define an error typology that aims to go
further than pronoun translation adequacy and includes types such as incorrect
word selection or missing words. The features of coreference chains in
automatic translations are also compared to those of the source texts and human
translations. The analysis shows stronger potential translationese effects in
machine translated outputs than in human translations."
"In English semantic similarity tasks, classic word embedding-based approaches
explicitly model pairwise ""interactions"" between the word representations of a
sentence pair. Transformer-based pretrained language models disregard this
notion, instead modeling pairwise word interactions globally and implicitly
through their self-attention mechanism. In this paper, we hypothesize that
introducing an explicit, constrained pairwise word interaction mechanism to
pretrained language models improves their effectiveness on semantic similarity
tasks. We validate our hypothesis using BERT on four tasks in semantic textual
similarity and answer sentence selection. We demonstrate consistent
improvements in quality by adding an explicit pairwise word interaction module
to BERT."
"Despite the success of neural machine translation (NMT), simultaneous neural
machine translation (SNMT), the task of translating in real time before a full
sentence has been observed, remains challenging due to the syntactic structure
difference and simultaneity requirements. In this paper, we propose a general
framework for adapting neural machine translation to translate simultaneously.
Our framework contains two parts: prefix translation that utilizes a
consecutive NMT model to translate source prefixes and a stopping criterion
that determines when to stop the prefix translation. Experiments on three
translation corpora and two language pairs show the efficacy of the proposed
framework on balancing the quality and latency in adapting NMT to perform
simultaneous translation."
"The words-as-classifiers model of grounded lexical semantics learns a
semantic fitness score between physical entities and the words that are used to
denote those entities. In this paper, we explore how such a model can
incrementally perform composition and how the model can be unified with a
distributional representation. For the latter, we leverage the classifier
coefficients as an embedding. For composition, we leverage the underlying
mechanics of three different classifier types (i.e., logistic regression,
decision trees, and multi-layer perceptrons) to arrive at a several systematic
approaches to composition unique to each classifier including both denotational
and connotational methods of composition. We compare these approaches to each
other and to prior work in a visual reference resolution task using the refCOCO
dataset. Our results demonstrate the need to expand upon existing composition
strategies and bring together grounded and distributional representations."
"Recently, the character-word lattice structure has been proved to be
effective for Chinese named entity recognition (NER) by incorporating the word
information. However, since the lattice structure is complex and dynamic, most
existing lattice-based models are hard to fully utilize the parallel
computation of GPUs and usually have a low inference-speed. In this paper, we
propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the
lattice structure into a flat structure consisting of spans. Each span
corresponds to a character or latent word and its position in the original
lattice. With the power of Transformer and well-designed position encoding,
FLAT can fully leverage the lattice information and has an excellent
parallelization ability. Experiments on four datasets show FLAT outperforms
other lexicon-based models in performance and efficiency."
"This paper describes our method for the task of Semantic Question Similarity
in Arabic in the workshop on NLP Solutions for Under-Resourced Languages
(NSURL). The aim is to build a model that is able to detect similar semantic
questions in the Arabic language for the provided dataset. Different methods of
determining questions similarity are explored in this work. The proposed models
achieved high F1-scores, which range from (88% to 96%). Our official best
result is produced from the ensemble model of using a pre-trained multilingual
BERT model with different random seeds with 95.924% F1-Score, which ranks the
first among nine participants teams."
"We present an approach to generating topics using a model trained only for
document title generation, with zero examples of topics given during training.
We leverage features that capture the relevance of a candidate span in a
document for the generation of a title for that document. The output is a
weighted collection of the phrases that are most relevant for describing the
document and distinguishing it within a corpus, without requiring access to the
rest of the corpus. We conducted a double-blind trial in which human annotators
scored the quality of our machine-generated topics along with original
human-written topics associated with news articles from The Guardian and The
Huffington Post. The results show that our zero-shot model generates topic
labels for news documents that are on average equal to or higher quality than
those written by humans, as judged by humans."
"Named Entity Recognition (NER) is a crucial upstream task in Natural Language
Processing (NLP). Traditional tag scheme approaches offer a single recognition
that does not meet the needs of many downstream tasks such as coreference
resolution. Meanwhile, Tag scheme approaches ignore the continuity of entities.
Inspired by one-stage object detection models in computer vision (CV), this
paper proposes a new no-tag scheme, the Whole-Aware Detection, which makes NER
an object detection task. Meanwhile, this paper presents a novel model, Entity
Candidate Network (ECNet), and a specific convolution network, Adaptive Context
Convolution Network (ACCN), to fuse multi-scale contexts and encode entity
information at each position. ECNet identifies the full span of a named entity
and its type at each position based on Entity Loss. Furthermore, ECNet is
regulable between the highest precision and the highest recall, while the tag
scheme approaches are not. Experimental results on the CoNLL 2003 English
dataset and the WNUT 2017 dataset show that ECNet outperforms other previous
state-of-the-art methods."
"Recent advances in multilingual dependency parsing have brought the idea of a
truly universal parser closer to reality. However, cross-language interference
and restrained model capacity remain major obstacles. To address this, we
propose a novel multilingual task adaptation approach based on contextual
parameter generation and adapter modules. This approach enables to learn
adapters via language embeddings while sharing model parameters across
languages. It also allows for an easy but effective integration of existing
linguistic typology features into the parsing network. The resulting parser,
UDapter, outperforms strong monolingual and multilingual baselines on the
majority of both high-resource and low-resource (zero-shot) languages, showing
the success of the proposed adaptation approach. Our in-depth analyses show
that soft parameter sharing via typological features is key to this success."
"Many valid translations exist for a given sentence, yet machine translation
(MT) is trained with a single reference translation, exacerbating data sparsity
in low-resource settings. We introduce Simulated Multiple Reference Training
(SMRT), a novel MT training method that approximates the full space of possible
translations by sampling a paraphrase of the reference sentence from a
paraphraser and training the MT model to predict the paraphraser's distribution
over possible tokens. We demonstrate the effectiveness of SMRT in low-resource
settings when translating to English, with improvements of 1.2 to 7.0 BLEU. We
also find SMRT is complementary to back-translation."
"TACRED (Zhang et al., 2017) is one of the largest, most widely used
crowdsourced datasets in Relation Extraction (RE). But, even with recent
advances in unsupervised pre-training and knowledge enhanced neural RE, models
still show a high error rate. In this paper, we investigate the questions: Have
we reached a performance ceiling or is there still room for improvement? And
how do crowd annotations, dataset, and models contribute to this error rate? To
answer these questions, we first validate the most challenging 5K examples in
the development and test sets using trained annotators. We find that label
errors account for 8% absolute F1 test error, and that more than 50% of the
examples need to be relabeled. On the relabeled test set the average F1 score
of a large baseline model set improves from 62.1 to 70.1. After validation, we
analyze misclassifications on the challenging instances, categorize them into
linguistically motivated error groups, and verify the resulting error
hypotheses on three state-of-the-art RE models. We show that two groups of
ambiguous relations are responsible for most of the remaining errors and that
models may adopt shallow heuristics on the dataset when entities are not
masked."
"Neural models for response generation produce responses that are semantically
plausible but not necessarily factually consistent with facts describing the
speaker's persona. These models are trained with fully supervised learning
where the objective function barely captures factual consistency. We propose to
fine-tune these models by reinforcement learning and an efficient reward
function that explicitly captures the consistency between a response and
persona facts as well as semantic plausibility. Our automatic and human
evaluations on the PersonaChat corpus confirm that our approach increases the
rate of responses that are factually consistent with persona facts over its
supervised counterpart while retaining the language quality of responses."
"Medical entity linking is the task of identifying and standardizing medical
concepts referred to in an unstructured text. Most of the existing methods
adopt a three-step approach of (1) detecting mentions, (2) generating a list of
candidate concepts, and finally (3) picking the best concept among them. In
this paper, we probe into alleviating the problem of overgeneration of
candidate concepts in the candidate generation module, the most under-studied
component of medical entity linking. For this, we present MedType, a fully
modular system that prunes out irrelevant candidate concepts based on the
predicted semantic type of an entity mention. We incorporate MedType into five
off-the-shelf toolkits for medical entity linking and demonstrate that it
consistently improves entity linking performance across several benchmark
datasets. To address the dearth of annotated training data for medical entity
linking, we present WikiMed and PubMedDS, two large-scale medical entity
linking datasets, and demonstrate that pre-training MedType on these datasets
further improves entity linking performance. We make our source code and
datasets publicly available for medical entity linking research."
"Despite their large-scale coverage, cross-domain knowledge graphs invariably
suffer from inherent incompleteness and sparsity. Link prediction can alleviate
this by inferring a target entity, given a source entity and a query relation.
Recent embedding-based approaches operate in an uninterpretable latent semantic
vector space of entities and relations, while path-based approaches operate in
the symbolic space, making the inference process explainable. However, these
approaches typically consider static snapshots of the knowledge graphs,
severely restricting their applicability for evolving knowledge graphs with
newly emerging entities. To overcome this issue, we propose an inductive
representation learning framework that is able to learn representations of
previously unseen entities. Our method finds reasoning paths between source and
target entities, thereby making the link prediction for unseen entities
interpretable and providing support evidence for the inferred link."
"While state-of-the-art neural network models continue to achieve lower
perplexity scores on language modeling benchmarks, it remains unknown whether
optimizing for broad-coverage predictive performance leads to human-like
syntactic knowledge. Furthermore, existing work has not provided a clear
picture about the model properties required to produce proper syntactic
generalizations. We present a systematic evaluation of the syntactic knowledge
of neural language models, testing 20 combinations of model types and data
sizes on a set of 34 English-language syntactic test suites. We find
substantial differences in syntactic generalization performance by model
architecture, with sequential models underperforming other architectures.
Factorially manipulating model architecture and training dataset size (1M--40M
words), we find that variability in syntactic generalization performance is
substantially greater by architecture than by dataset size for the corpora
tested in our experiments. Our results also reveal a dissociation between
perplexity and syntactic generalization performance."
"The interpretation of parasitic gaps is an ostensible case of non-linearity
in natural language composition. Existing categorial analyses, both in the
typelogical and in the combinatory traditions, rely on explicit forms of
syntactic copying. We identify two types of parasitic gapping where the
duplication of semantic content can be confined to the lexicon. Parasitic gaps
in adjuncts are analysed as forms of generalized coordination with a
polymorphic type schema for the head of the adjunct phrase. For parasitic gaps
affecting arguments of the same predicate, the polymorphism is associated with
the lexical item that introduces the primary gap. Our analysis is formulated in
terms of Lambek calculus extended with structural control modalities. A
compositional translation relates syntactic types and derivations to the
interpreting compact closed category of finite dimensional vector spaces and
linear maps with Frobenius algebras over it. When interpreted over the
necessary semantic spaces, the Frobenius algebras provide the tools to model
the proposed instances of lexical polymorphism."
"It is well-understood that different algorithms, training processes, and
corpora produce different word embeddings. However, less is known about the
relation between different embedding spaces, i.e. how far different sets of
embeddings deviate from each other. In this paper, we propose a novel metric
called Relative pairwise inner Product Distance (RPD) to quantify the distance
between different sets of word embeddings. This metric has a unified scale for
comparing different sets of word embeddings. Based on the properties of RPD, we
study the relations of word embeddings of different algorithms systematically
and investigate the influence of different training processes and corpora. The
results shed light on the poorly understood word embeddings and justify RPD as
a measure of the distance of embedding spaces."
"Training data for text classification is often limited in practice,
especially for applications with many output classes or involving many related
classification problems. This means classifiers must generalize from limited
evidence, but the manner and extent of generalization is task dependent.
Current practice primarily relies on pre-trained word embeddings to map words
unseen in training to similar seen ones. Unfortunately, this squishes many
components of meaning into highly restricted capacity. Our alternative begins
with sparse pre-trained representations derived from unlabeled parsed corpora;
based on the available training data, we select features that offers the
relevant generalizations. This produces task-specific semantic vectors; here,
we show that a feed-forward network over these vectors is especially effective
in low-data scenarios, compared to existing state-of-the-art methods. By
further pairing this network with a convolutional neural network, we keep this
edge in low data scenarios and remain competitive when using full training
sets."
"Automatic phenotype concept recognition from unstructured text remains a
challenging task in biomedical text mining research. Previous works that
address the task typically use dictionary-based matching methods, which can
achieve high precision but suffer from lower recall. Recently, machine
learning-based methods have been proposed to identify biomedical concepts,
which can recognize more unseen concept synonyms by automatic feature learning.
However, most methods require large corpora of manually annotated data for
model training, which is difficult to obtain due to the high cost of human
annotation. In this paper, we propose PhenoTagger, a hybrid method that
combines both dictionary and machine learning-based methods to recognize Human
Phenotype Ontology (HPO) concepts in unstructured biomedical text. We first use
all concepts and synonyms in HPO to construct a dictionary, which is then used
to automatically build a distantly supervised training dataset for machine
learning. Next, a cutting-edge deep learning model is trained to classify each
candidate phrase (n-gram from input sentence) into a corresponding concept
label. Finally, the dictionary and machine learning-based prediction results
are combined for improved performance. Our method is validated with two HPO
corpora, and the results show that PhenoTagger compares favorably to previous
methods. In addition, to demonstrate the generalizability of our method, we
retrained PhenoTagger using the disease ontology MEDIC for disease concept
recognition to investigate the effect of training on different ontologies.
Experimental results on the NCBI disease corpus show that PhenoTagger without
requiring manually annotated training data achieves competitive performance as
compared with state-of-the-art supervised methods."
"Despite being the seventh most widely spoken language in the world, Bengali
has received much less attention in machine translation literature due to being
low in resources. Most publicly available parallel corpora for Bengali are not
large enough; and have rather poor quality, mostly because of incorrect
sentence alignments resulting from erroneous sentence segmentation, and also
because of a high volume of noise present in them. In this work, we build a
customized sentence segmenter for Bengali and propose two novel methods for
parallel corpus creation on low-resource setups: aligner ensembling and batch
filtering. With the segmenter and the two methods combined, we compile a
high-quality Bengali-English parallel corpus comprising of 2.75 million
sentence pairs, more than 2 million of which were not available before.
Training on neural models, we achieve an improvement of more than 9 BLEU score
over previous approaches to Bengali-English machine translation. We also
evaluate on a new test set of 1000 pairs made with extensive quality control.
We release the segmenter, parallel corpus, and the evaluation set, thus
elevating Bengali from its low-resource status. To the best of our knowledge,
this is the first ever large scale study on Bengali-English machine
translation. We believe our study will pave the way for future research on
Bengali-English machine translation as well as other low-resource languages.
Our data and code are available at https://github.com/csebuetnlp/banglanmt."
"This paper presents DWIE, the 'Deutsche Welle corpus for Information
Extraction', a newly created multi-task dataset that combines four main
Information Extraction (IE) annotation subtasks: (i) Named Entity Recognition
(NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv)
Entity Linking. DWIE is conceived as an entity-centric dataset that describes
interactions and properties of conceptual entities on the level of the complete
document. This contrasts with currently dominant mention-driven approaches that
start from the detection and classification of named entity mentions in
individual sentences. Further, DWIE presented two main challenges when building
and evaluating IE models for it. First, the use of traditional mention-level
evaluation metrics for NER and RE tasks on entity-centric DWIE dataset can
result in measurements dominated by predictions on more frequently mentioned
entities. We tackle this issue by proposing a new entity-driven metric that
takes into account the number of mentions that compose each of the predicted
and ground truth entities. Second, the document-level multi-task annotations
require the models to transfer information between entity mentions located in
different parts of the document, as well as between different tasks, in a joint
learning setting. To realize this, we propose to use graph-based neural message
passing techniques between document-level mention spans. Our experiments show
an improvement of up to 5.5 F1 percentage points when incorporating neural
graph propagation into our joint model. This demonstrates DWIE's potential to
stimulate further research in graph neural networks for representation learning
in multi-task IE. We make DWIE publicly available at
https://github.com/klimzaporojets/DWIE."
"Span extraction is an essential problem in machine reading comprehension.
Most of the existing algorithms predict the start and end positions of an
answer span in the given corresponding context by generating two probability
vectors. In this paper, we propose a novel approach that extends the
probability vector to a probability matrix. Such a matrix can cover more
start-end position pairs. Precisely, to each possible start index, the method
always generates an end probability vector. Besides, we propose a
sampling-based training strategy to address the computational cost and memory
issue in the matrix training phase. We evaluate our method on SQuAD 1.1 and
three other question answering benchmarks. Leveraging the most competitive
models BERT and BiDAF as the backbone, our proposed approach can get consistent
improvements in all datasets, demonstrating the effectiveness of the proposed
method."
"In neural machine translation (NMT), monolingual data in the target language
are usually exploited through a method so-called ""back-translation"" to
synthesize additional training parallel data. The synthetic data have been
shown helpful to train better NMT, especially for low-resource language pairs
and domains. Nonetheless, large monolingual data in the target domains or
languages are not always available to generate large synthetic parallel data.
In this work, we propose a new method to generate large synthetic parallel data
leveraging very small monolingual data in a specific domain. We fine-tune a
pre-trained GPT-2 model on such small in-domain monolingual data and use the
resulting model to generate a large amount of synthetic in-domain monolingual
data. Then, we perform back-translation, or forward translation, to generate
synthetic in-domain parallel data. Our preliminary experiments on three
language pairs and five domains show the effectiveness of our method to
generate fully synthetic but useful in-domain parallel data for improving NMT
in all configurations. We also show promising results in extreme adaptation for
personalized NMT."
"Recent progress in pretraining language models on large corpora has resulted
in large performance gains on many NLP tasks. These large models acquire
linguistic knowledge during pretraining, which helps to improve performance on
downstream tasks via fine-tuning. To assess what kind of knowledge is acquired,
language models are commonly probed by querying them with `fill in the blank'
style cloze questions. Existing probing datasets mainly focus on knowledge
about relations between words and entities. We introduce WDLMPro (Word
Definition Language Model Probing) to evaluate word understanding directly
using dictionary definitions of words. In our experiments, three popular
pretrained language models struggle to match words and their definitions. This
indicates that they understand many words poorly and that our new probing task
is a difficult challenge that could help guide research on LMs in the future."
"Automatic comment generation is a special and challenging task to verify the
model ability on news content comprehension and language generation. Comments
not only convey salient and interesting information in news articles, but also
imply various and different reader characteristics which we treat as the
essential clues for diversity. However, most of the comment generation
approaches only focus on saliency information extraction, while the
reader-aware factors implied by comments are neglected. To address this issue,
we propose a unified reader-aware topic modeling and saliency information
detection framework to enhance the quality of generated comments. For
reader-aware topic modeling, we design a variational generative clustering
algorithm for latent semantic learning and topic mining from reader comments.
For saliency information detection, we introduce Bernoulli distribution
estimating on news content to select saliency information. The obtained topic
representations as well as the selected saliency information are incorporated
into the decoder to generate diversified and informative comments. Experimental
results on three datasets show that our framework outperforms existing baseline
methods in terms of both automatic metrics and human evaluation. The potential
ethical issues are also discussed in detail."
"By using a trigram model and fine-tuning a pretrained BERT model for sequence
classification, we show that machine translation and human translation can be
classified with an accuracy above chance level, which suggests that machine
translation and human translation are different in a systematic way. The
classification accuracy of machine translation is much higher than of human
translation. We show that this may be explained by the difference in lexical
diversity between machine translation and human translation. If machine
translation has independent patterns from human translation, automatic metrics
which measure the deviation of machine translation from human translation may
conflate difference with quality. Our experiment with two different types of
automatic metrics shows correlation with the result of the classification task.
Therefore, we suggest the difference in lexical diversity between machine
translation and human translation be given more attention in machine
translation evaluation."
"Recent years have witnessed great progress on building emotional chatbots.
Tremendous methods have been proposed for chatbots to generate responses with
given emotions. However, the emotion changes of the user during the
conversation has not been fully explored. In this work, we study the problem of
positive emotion elicitation, which aims to generate responses that can elicit
positive emotion of the user, in human-machine conversation. We propose a
weakly supervised Emotion Eliciting Machine (EEM) to address this problem.
Specifically, we first collect weak labels of user emotion status changes in a
conversion based on a pre-trained emotion classifier. Then we propose a dual
encoder-decoder structure to model the generation of responses in both positive
and negative side based on the changes of the user's emotion status in the
conversation. An emotion eliciting factor is introduced on top of the dual
structure to balance the positive and negative emotional impacts on the
generated response during emotion elicitation. The factor also provides a
fine-grained controlling manner for emotion elicitation. Experimental results
on a large real-world dataset show that EEM outperforms the existing models in
generating responses with positive emotion elicitation."
"Pre-trained contextualized language models (PrLMs) have led to strong
performance gains in downstream natural language understanding tasks. However,
PrLMs can still be easily fooled by adversarial word substitution, which is one
of the most challenging textual adversarial attack methods. Existing defence
approaches suffer from notable performance loss and complexities. Thus, this
paper presents a compact and performance-preserved framework, Anomaly Detection
with Frequency-Aware Randomization (ADFAR). In detail, we design an auxiliary
anomaly detection classifier and adopt a multi-task learning procedure, by
which PrLMs are able to distinguish adversarial input samples. Then, in order
to defend adversarial word substitution, a frequency-aware randomization
process is applied to those recognized adversarial input samples. Empirical
results show that ADFAR significantly outperforms those newly proposed defense
methods over various tasks with much higher inference speed. Remarkably, ADFAR
does not impair the overall performance of PrLMs. The code is available at
https://github.com/LilyNLP/ADFAR"
"In computational psycholinguistics, various language models have been
evaluated against human reading behavior (e.g., eye movement) to build
human-like computational models. However, most previous efforts have focused
almost exclusively on English, despite the recent trend towards linguistic
universal within the general community. In order to fill the gap, this paper
investigates whether the established results in computational psycholinguistics
can be generalized across languages. Specifically, we re-examine an established
generalization -- the lower perplexity a language model has, the more
human-like the language model is -- in Japanese with typologically different
structures from English. Our experiments demonstrate that this established
generalization exhibits a surprising lack of universality; namely, lower
perplexity is not always human-like. Moreover, this discrepancy between English
and Japanese is further explored from the perspective of (non-)uniform
information density. Overall, our results suggest that a cross-lingual
evaluation will be necessary to construct human-like computational models."
"Language understanding must identify the logical connections between events
in a discourse, but core events are often unstated due to their commonsense
nature. This paper fills in these missing events by generating precondition
events. Precondition generation can be framed as a sequence-to-sequence
problem: given a target event, generate a possible precondition. However, in
most real-world scenarios, an event can have several preconditions, requiring
diverse generation -- a challenge for standard seq2seq approaches. We propose
DiP, a Diverse Precondition generation system that can generate unique and
diverse preconditions. DiP uses a generative process with three components --
an event sampler, a candidate generator, and a post-processor. The event
sampler provides control codes (precondition triggers) which the candidate
generator uses to focus its generation. Unlike other conditional generation
systems, DiP automatically generates control codes without training on diverse
examples. Analysis against baselines reveals that DiP improves the diversity of
preconditions significantly while also generating more preconditions."
"Lexically constrained machine translation allows the user to manipulate the
output sentence by enforcing the presence or absence of certain words and
phrases. Although current approaches can enforce terms to appear in the
translation, they often struggle to make the constraint word form agree with
the rest of the generated output. Our manual analysis shows that 46% of the
errors in the output of a baseline constrained model for English to Czech
translation are related to agreement. We investigate mechanisms to allow neural
machine translation to infer the correct word inflection given lemmatized
constraints. In particular, we focus on methods based on training the model
with constraints provided as part of the input sequence. Our experiments on the
English-Czech language pair show that this approach improves the translation of
constrained terms in both automatic and manual evaluation by reducing errors in
agreement. Our approach thus eliminates inflection errors, without introducing
new errors or decreasing the overall quality of the translation."
"In Contract Life-cycle Management (CLM), managing and tracking the master
agreements and their associated amendments is essential, in order to be kept
informed with different due dates and obligations. An automatic solution can
facilitate the daily jobs and improve the efficiency of legal practitioners. In
this paper, we propose an approach based on machine learning (ML) and Natural
Language Processing (NLP) to detect the amendment relationship between two
documents. The algorithm takes two PDF documents preprocessed by OCR (Optical
Character Recognition) and NER (Named Entity Recognition) as input, and then it
builds the features of each document pair and classifies the relationship. We
experimented with different configurations on a dataset consisting of 1124
pairs of contract-amendment documents in English and French. The best result
obtained a F1-score of 91%, which outperformed 23% compared to a
heuristic-based baseline."
"It is presented here a machine learning-based (ML) natural language
processing (NLP) approach capable to automatically recognize and extract
categorical and numerical parameters from a corpus of articles. The approach
(named a.RIX) operates with a concomitant/interchangeable use of ML models such
as neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes
classifiers (NBC), and a pattern recognition model using regular expression
(REGEX). A corpus of 7,873 scientific articles dealing with natural products
(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine
automatically extracts categorical and numerical parameters such as (i) the
plant species from which active molecules are extracted, (ii) the
microorganisms species for which active molecules can act against, and (iii)
the values of minimum inhibitory concentration (MIC) against these
microorganisms. The parameters are extracted without part-of-speech tagging
(POS) and named entity recognition (NER) approaches (i.e. without the need of
text annotation), and the models training is performed with unsupervised
approaches. In this way, a.RIX can be essentially used on articles from any
scientific field. Finally, it can potentially make obsolete the current article
reviewing process in some areas, especially those in which machine learning
models capture texts structure, text semantics, and latent knowledge."
"Complex reasoning aims to draw a correct inference based on complex rules. As
a hallmark of human intelligence, it involves a degree of explicit reading
comprehension, interpretation of logical knowledge and complex rule
application. In this paper, we take a step forward in complex reasoning by
systematically studying the three challenging and domain-general tasks of the
Law School Admission Test (LSAT), including analytical reasoning, logical
reasoning and reading comprehension. We propose a hybrid reasoning system to
integrate these three tasks and achieve impressive overall performance on the
LSAT tests. The experimental results demonstrate that our system endows itself
a certain complex reasoning ability, especially the fundamental reading
comprehension and challenging logical reasoning capacities. Further analysis
also shows the effectiveness of combining the pre-trained models with the
task-specific reasoning module, and integrating symbolic knowledge into
discrete interpretable reasoning steps in complex reasoning. We further shed a
light on the potential future directions, like unsupervised symbolic knowledge
extraction, model interpretability, few-shot learning and comprehensive
benchmark for complex reasoning."
"Sentence semantic matching requires an agent to determine the semantic
relation between two sentences, which is widely used in various natural
language tasks, such as Natural Language Inference (NLI), Paraphrase
Identification (PI), and so on. Much recent progress has been made in this
area, especially attention-based methods and pre-trained language model based
methods. However, most of these methods focus on all the important parts in
sentences in a static way and only emphasize how important the words are to the
query, inhibiting the ability of attention mechanism. In order to overcome this
problem and boost the performance of attention mechanism, we propose a novel
dynamic re-read attention, which can pay close attention to one small region of
sentences at each step and re-read the important parts for better sentence
representations. Based on this attention variation, we develop a novel Dynamic
Re-read Network (DRr-Net) for sentence semantic matching. Moreover, selecting
one small region in dynamic re-read attention seems insufficient for sentence
semantics, and employing pre-trained language models as input encoders will
introduce incomplete and fragile representation problems. To this end, we
extend DRrNet to Locally-Aware Dynamic Re-read Attention Net (LadRa-Net), in
which local structure of sentences is employed to alleviate the shortcoming of
Byte-Pair Encoding (BPE) in pre-trained language models and boost the
performance of dynamic reread attention. Extensive experiments on two popular
sentence semantic matching tasks demonstrate that DRr-Net can significantly
improve the performance of sentence semantic matching. Meanwhile, LadRa-Net is
able to achieve better performance by considering the local structures of
sentences. In addition, it is exceedingly interesting that some discoveries in
our experiments are consistent with some findings of psychological research."
"Can we construct a neural model that is inductively biased towards learning
human languages? Motivated by this question, we aim at constructing an
informative prior over neural weights, in order to adapt quickly to held-out
languages in the task of character-level language modeling. We infer this
distribution from a sample of typologically diverse training languages via
Laplace approximation. The use of such a prior outperforms baseline models with
an uninformative prior (so-called ""fine-tuning"") in both zero-shot and few-shot
settings. This shows that the prior is imbued with universal phonological
knowledge. Moreover, we harness additional language-specific side information
as distant supervision for held-out languages. Specifically, we condition
language models on features from typological databases, by concatenating them
to hidden states or generating weights with hyper-networks. These features
appear beneficial in the few-shot setting, but not in the zero-shot setting.
Since the paucity of digital texts affects the majority of the world's
languages, we hope that these findings will help broaden the scope of
applications for language technology."
"We present Viola, an open-domain dialogue system for spoken conversation that
uses a topic-agnostic dialogue manager based on a simple generate-and-rank
approach. Leveraging recent advances of generative dialogue systems powered by
large language models, Viola fetches a batch of response candidates from
various neural dialogue models trained with different datasets and
knowledge-grounding inputs. Additional responses originating from
template-based generators are also considered, depending on the user's input
and detected entities. The hand-crafted generators build on a dynamic knowledge
graph injected with rich content that is crawled from the web and automatically
processed on a daily basis. Viola's response ranker is a fine-tuned polyencoder
that chooses the best response given the dialogue history. While dedicated
annotations for the polyencoder alone can indirectly steer it away from
choosing problematic responses, we add rule-based safety nets to detect neural
degeneration and a dedicated classifier to filter out offensive content. We
analyze conversations that Viola took part in for the Alexa Prize Socialbot
Grand Challenge 4 and discuss the strengths and weaknesses of our approach.
Lastly, we suggest future work with a focus on curating conversation data
specifcially for socialbots that will contribute towards a more robust
data-driven socialbot."
"This paper presents Self-correcting Encoding (Secoco), a framework that
effectively deals with input noise for robust neural machine translation by
introducing self-correcting predictors. Different from previous robust
approaches, Secoco enables NMT to explicitly correct noisy inputs and delete
specific errors simultaneously with the translation decoding process. Secoco is
able to achieve significant improvements over strong baselines on two
real-world test sets and a benchmark WMT dataset with good interpretability. We
will make our code and dataset publicly available soon."
"We consider the task of document-level entity linking (EL), where it is
important to make consistent decisions for entity mentions over the full
document jointly. We aim to leverage explicit ""connections"" among mentions
within the document itself: we propose to join the EL task with that of
coreference resolution (coref). This is complementary to related works that
exploit either (i) implicit document information (e.g., latent relations among
entity mentions, or general language models) or (ii) connections between the
candidate links (e.g, as inferred from the external knowledge base).
Specifically, we cluster mentions that are linked via coreference, and enforce
a single EL for all of the clustered mentions together. The latter constraint
has the added benefit of increased coverage by joining EL candidate lists for
the thus clustered mentions. We formulate the coref+EL problem as a structured
prediction task over directed trees and use a globally normalized model to
solve it. Experimental results on two datasets show a boost of up to +5%
F1-score on both coref and EL tasks, compared to their standalone counterparts.
For a subset of hard cases, with individual mentions lacking the correct EL in
their candidate entity list, we obtain a +50% increase in accuracy."
"Text augmentation techniques are widely used in text classification problems
to improve the performance of classifiers, especially in low-resource
scenarios. Whilst lots of creative text augmentation methods have been
designed, they augment the text in a non-selective manner, which means the less
important or noisy words have the same chances to be augmented as the
informative words, and thereby limits the performance of augmentation. In this
work, we systematically summarize three kinds of role keywords, which have
different functions for text classification, and design effective methods to
extract them from the text. Based on these extracted role keywords, we propose
STA (Selective Text Augmentation) to selectively augment the text, where the
informative, class-indicating words are emphasized but the irrelevant or noisy
words are diminished. Extensive experiments on four English and Chinese text
classification benchmark datasets demonstrate that STA can substantially
outperform the non-selective text augmentation methods."
"Human-designed rules are widely used to build industry applications. However,
it is infeasible to maintain thousands of such hand-crafted rules. So it is
very important to integrate the rule knowledge into neural networks to build a
hybrid model that achieves better performance. Specifically, the human-designed
rules are formulated as Regular Expressions (REs), from which the equivalent
Minimal Deterministic Finite Automatons (MDFAs) are constructed. We propose to
use the MDFA as an intermediate model to capture the matched RE patterns as
rule-based features for each input sentence and introduce these additional
features into neural networks. We evaluate the proposed method on the ATIS
intent classification task. The experiment results show that the proposed
method achieves the best performance compared to neural networks and four other
methods that combine REs and neural networks when the training dataset is
relatively small."
"Datasets and methods for cross-document coreference resolution (CDCR) focus
on events or entities with strict coreference relations. They lack, however,
annotating and resolving coreference mentions with more abstract or loose
relations that may occur when news articles report about controversial and
polarized events. Bridging and loose coreference relations trigger associations
that may lead to exposing news readers to bias by word choice and labeling. For
example, coreferential mentions of ""direct talks between U.S. President Donald
Trump and Kim"" such as ""an extraordinary meeting following months of heated
rhetoric"" or ""great chance to solve a world problem"" form a more positive
perception of this event. A step towards bringing awareness of bias by word
choice and labeling is the reliable resolution of coreferences with high
lexical diversity. We propose an unsupervised method named XCoref, which is a
CDCR method that capably resolves not only previously prevalent entities, such
as persons, e.g., ""Donald Trump,"" but also abstractly defined concepts, such as
groups of persons, ""caravan of immigrants,"" events and actions, e.g., ""marching
to the U.S. border."" In an extensive evaluation, we compare the proposed XCoref
to a state-of-the-art CDCR method and a previous method TCA that resolves such
complex coreference relations and find that XCoref outperforms these methods.
Outperforming an established CDCR model shows that the new CDCR models need to
be evaluated on semantically complex mentions with more loose coreference
relations to indicate their applicability of models to resolve mentions in the
""wild"" of political news articles."
"Neural Machine Translation (NMT) models are strong enough to convey semantic
and syntactic information from the source language to the target language.
However, these models are suffering from the need for a large amount of data to
learn the parameters. As a result, for languages with scarce data, these models
are at risk of underperforming. We propose to augment attention based neural
network with reordering information to alleviate the lack of data. This
augmentation improves the translation quality for both English to Persian and
Persian to English by up to 6% BLEU absolute over the baseline models."
"Structured distributions, i.e. distributions over combinatorial spaces, are
commonly used to learn latent probabilistic representations from observed data.
However, scaling these models is bottlenecked by the high computational and
memory complexity with respect to the size of the latent representations.
Common models such as Hidden Markov Models (HMMs) and Probabilistic
Context-Free Grammars (PCFGs) require time and space quadratic and cubic in the
number of hidden states respectively. This work demonstrates a simple approach
to reduce the computational and memory complexity of a large class of
structured models. We show that by viewing the central inference step as a
matrix-vector product and using a low-rank constraint, we can trade off model
expressivity and speed via the rank. Experiments with neural parameterized
structured models for language modeling, polyphonic music modeling,
unsupervised grammar induction, and video modeling show that our approach
matches the accuracy of standard models at large state spaces while providing
practical speedups."
"Document-level relation extraction is to extract relation facts from a
document consisting of multiple sentences, in which pronoun crossed sentences
are a ubiquitous phenomenon against a single sentence. However, most of the
previous works focus more on mentions coreference resolution except for
pronouns, and rarely pay attention to mention-pronoun coreference and capturing
the relations. To represent multi-sentence features by pronouns, we imitate the
reading process of humans by leveraging coreference information when
dynamically constructing a heterogeneous graph to enhance semantic information.
Since the pronoun is notoriously ambiguous in the graph, a mention-pronoun
coreference resolution is introduced to calculate the affinity between pronouns
and corresponding mentions, and the noise suppression mechanism is proposed to
reduce the noise caused by pronouns. Experiments on the public dataset, DocRED,
DialogRE and MPDD, show that Coref-aware Doc-level Relation Extraction based on
Graph Inference Network outperforms the state-of-the-art."
"A rapidly growing body of research on compositional generalization
investigates the ability of a semantic parser to dynamically recombine
linguistic elements seen in training into unseen sequences. We present a
systematic comparison of sequence-to-sequence models and models guided by
compositional principles on the recent COGS corpus (Kim and Linzen, 2020).
Though seq2seq models can perform well on lexical tasks, they perform with
near-zero accuracy on structural generalization tasks that require novel
syntactic structures; this holds true even when they are trained to predict
syntax instead of semantics. In contrast, compositional models achieve
near-perfect accuracy on structural generalization; we present new results
confirming this from the AM parser (Groschwitz et al., 2021). Our findings show
structural generalization is a key measure of compositional generalization and
requires models that are aware of complex structure."
"Few-shot table-to-text generation is a task of composing fluent and faithful
sentences to convey table content using limited data. Despite many efforts
having been made towards generating impressive fluent sentences by fine-tuning
powerful pre-trained language models, the faithfulness of generated content
still needs to be improved. To this end, this paper proposes a novel approach
Attend, Memorize and Generate (called AMG), inspired by the text generation
process of humans. In particular, AMG (1) attends over the multi-granularity of
context using a novel strategy based on table slot level and traditional
token-by-token level attention to exploit both the table structure and natural
linguistic information; (2) dynamically memorizes the table slot allocation
states; and (3) generates faithful sentences according to both the context and
memory allocation states. Comprehensive experiments with human evaluation on
three domains (i.e., humans, songs, and books) of the Wiki dataset show that
our model can generate higher qualified texts when compared with several
state-of-the-art baselines, in both fluency and faithfulness."
"With the high prevalence of offensive language against minorities in social
media, counter-hate speeches (CHS) generation is considered an automatic way of
tackling this challenge. The CHS is supposed to appear as a third voice to
educate people and keep the social [red lines bold] without limiting the
principles of freedom of speech. In this paper, we review the most important
research in the past and present with a main focus on methodologies, collected
datasets and statistical analysis CHS's impact on social media. The CHS
generation is based on the optimistic assumption that any attempt to intervene
the hate speech in social media can play a positive role in this context.
Beyond that, previous works ignored the investigation of the sequence of
comments before and after the CHS. However, the positive impact is not
guaranteed, as shown in some previous works. To the best of our knowledge, no
attempt has been made to survey the related work to compare the past research
in terms of CHS's impact on social media. We take the first step in this
direction by providing a comprehensive review on related works and categorizing
them based on different factors including impact, methodology, data source,
etc."
"Most of the open-domain dialogue models tend to perform poorly in the setting
of long-term human-bot conversations. The possible reason is that they lack the
capability of understanding and memorizing long-term dialogue history
information. To address this issue, we present a novel task of Long-term Memory
Conversation (LeMon) and then build a new dialogue dataset DuLeMon and a
dialogue generation framework with Long-Term Memory (LTM) mechanism (called
PLATO-LTM). This LTM mechanism enables our system to accurately extract and
continuously update long-term persona memory without requiring multiple-session
dialogue datasets for model training. To our knowledge, this is the first
attempt to conduct real-time dynamic management of persona information of both
parties, including the user and the bot. Results on DuLeMon indicate that
PLATO-LTM can significantly outperform baselines in terms of long-term dialogue
consistency, leading to better dialogue engagingness."
"Predicate entailment detection is a crucial task for question-answering from
text, where previous work has explored unsupervised learning of entailment
graphs from typed open relation triples. In this paper, we present the first
pipeline for building Chinese entailment graphs, which involves a novel
high-recall open relation extraction (ORE) method and the first Chinese
fine-grained entity typing dataset under the FIGER type ontology. Through
experiments on the Levy-Holt dataset, we verify the strength of our Chinese
entailment graph, and reveal the cross-lingual complementarity: on the parallel
Levy-Holt dataset, an ensemble of Chinese and English entailment graphs
outperforms both monolingual graphs, and raises unsupervised SOTA by 4.7 AUC
points."
"With the development of pre-trained language models, remarkable success has
been witnessed in dialogue understanding (DU). However, current DU approaches
usually employ independent models for each distinct DU task without considering
shared knowledge across different DU tasks. In this paper, we propose a unified
generative dialogue understanding framework, named {\em UniDU}, to achieve
effective information exchange across diverse DU tasks. Here, we reformulate
all DU tasks into a unified prompt-based generative model paradigm. More
importantly, a novel model-agnostic multi-task training strategy (MATS) is
introduced to dynamically adapt the weights of diverse tasks for best knowledge
sharing during training, based on the nature and available data of each task.
Experiments on ten DU datasets covering five fundamental DU tasks show that the
proposed UniDU framework largely outperforms task-specific well-designed
methods on all tasks. MATS also reveals the knowledge-sharing structure of
these tasks. Finally, UniDU obtains promising performance in the unseen
dialogue domain, showing the great potential for generalization."
"Morphological and syntactic changes in word usage (as captured, e.g., by
grammatical profiles) have been shown to be good predictors of a word's meaning
change. In this work, we explore whether large pre-trained contextualised
language models, a common tool for lexical semantic change detection, are
sensitive to such morphosyntactic changes. To this end, we first compare the
performance of grammatical profiles against that of a multilingual neural
language model (XLM-R) on 10 datasets, covering 7 languages, and then combine
the two approaches in ensembles to assess their complementarity. Our results
show that ensembling grammatical profiles with XLM-R improves semantic change
detection performance for most datasets and languages. This indicates that
language models do not fully cover the fine-grained morphological and syntactic
signals that are explicitly represented in grammatical profiles.
  An interesting exception are the test sets where the time spans under
analysis are much longer than the time gap between them (for example,
century-long spans with a one-year gap between them). Morphosyntactic change is
slow so grammatical profiles do not detect in such cases. In contrast, language
models, thanks to their access to lexical information, are able to detect fast
topical changes."
"Multilingual pre-trained language models (PLMs) have demonstrated impressive
performance on several downstream tasks for both high-resourced and
low-resourced languages. However, there is still a large performance drop for
languages unseen during pre-training, especially African languages. One of the
most effective approaches to adapt to a new language is \textit{language
adaptive fine-tuning} (LAFT) -- fine-tuning a multilingual PLM on monolingual
texts of a language using the pre-training objective. However, adapting to a
target language individually takes a large disk space and limits the
cross-lingual transfer abilities of the resulting models because they have been
specialized for a single language. In this paper, we perform
\textit{multilingual adaptive fine-tuning} on 17 most-resourced African
languages and three other high-resource languages widely spoken on the African
continent to encourage cross-lingual transfer learning. To further specialize
the multilingual PLM, we removed vocabulary tokens from the embedding layer
that corresponds to non-African writing scripts before MAFT, thus reducing the
model size by around 50%. Our evaluation on two multilingual PLMs (AfriBERTa
and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment
classification) shows that our approach is competitive to applying LAFT on
individual languages while requiring significantly less disk space.
Additionally, we show that our adapted PLM also improves the zero-shot
cross-lingual transfer abilities of parameter efficient fine-tuning methods."
"We study few-shot debugging of transformer based natural language
understanding models, using recently popularized test suites to not just
diagnose but correct a problem. Given a few debugging examples of a certain
phenomenon, and a held-out test set of the same phenomenon, we aim to maximize
accuracy on the phenomenon at a minimal cost of accuracy on the original test
set. We examine several methods that are faster than full epoch retraining. We
introduce a new fast method, which samples a few in-danger examples from the
original training set. Compared to fast methods using parameter distance
constraints or Kullback-Leibler divergence, we achieve superior original
accuracy for comparable debugging accuracy."
"In text classification tasks, useful information is encoded in the label
names. Label semantic aware systems have leveraged this information for
improved text classification performance during fine-tuning and prediction.
However, use of label-semantics during pre-training has not been extensively
explored. We therefore propose Label Semantic Aware Pre-training (LSAP) to
improve the generalization and data efficiency of text classification systems.
LSAP incorporates label semantics into pre-trained generative models (T5 in our
case) by performing secondary pre-training on labeled sentences from a variety
of domains. As domain-general pre-training requires large amounts of data, we
develop a filtering and labeling pipeline to automatically create
sentence-label pairs from unlabeled text. We perform experiments on intent
(ATIS, Snips, TOPv2) and topic classification (AG News, Yahoo! Answers). LSAP
obtains significant accuracy improvements over state-of-the-art models for
few-shot text classification while maintaining performance comparable to state
of the art in high-resource settings."
"Although adapting pre-trained language models with few examples has shown
promising performance on text classification, there is a lack of understanding
of where the performance gain comes from. In this work, we propose to answer
this question by interpreting the adaptation behavior using post-hoc
explanations from model predictions. By modeling feature statistics of
explanations, we discover that (1) without fine-tuning, pre-trained models
(e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although
few-shot fine-tuning can mitigate the prediction bias and demonstrate promising
prediction performance, our analysis shows models gain performance improvement
by capturing non-task-related features (e.g. stop words) or shallow data
patterns (e.g. lexical overlaps). These observations alert that pursuing model
performance with fewer examples may incur pathological prediction behavior,
which requires further sanity check on model predictions and careful design in
model evaluations in few-shot fine-tuning."
"The detection and extraction of abbreviations from unstructured texts can
help to improve the performance of Natural Language Processing tasks, such as
machine translation and information retrieval. However, in terms of publicly
available datasets, there is not enough data for training
deep-neural-networks-based models to the point of generalising well over data.
This paper presents PLOD, a large-scale dataset for abbreviation detection and
extraction that contains 160k+ segments automatically annotated with
abbreviations and their long forms. We performed manual validation over a set
of instances and a complete automatic validation for this dataset. We then used
it to generate several baseline models for detecting abbreviations and long
forms. The best models achieved an F1-score of 0.92 for abbreviations and 0.89
for detecting their corresponding long forms. We release this dataset along
with our code and all the models publicly in
https://github.com/surrey-nlp/PLOD-AbbreviationDetection"
"This paper aims to present how the application of Natural Language Processing
(NLP) and data augmentation techniques can improve the performance of a neural
network for better detection of fake news in the Portuguese language. Fake news
is one of the main controversies during the growth of the internet in the last
decade. Verifying what is fact and what is false has proven to be a difficult
task, while the dissemination of false news is much faster, which leads to the
need for the creation of tools that, automated, assist in the process of
verification of what is fact and what is false. In order to bring a solution,
an experiment was developed with neural network using news, real and fake,
which were never seen by artificial intelligence (AI). There was a significant
performance in the news classification after the application of the mentioned
techniques."
"Although sequence-to-sequence models often achieve good performance in
semantic parsing for i.i.d. data, their performance is still inferior in
compositional generalization. Several data augmentation methods have been
proposed to alleviate this problem. However, prior work only leveraged
superficial grammar or rules for data augmentation, which resulted in limited
improvement. We propose to use subtree substitution for compositional data
augmentation, where we consider subtrees with similar semantic functions as
exchangeable. Our experiments showed that such augmented data led to
significantly better performance on SCAN and GeoQuery, and reached new SOTA on
compositional split of GeoQuery."
"Automatically associating social media posts with topics is an important
prerequisite for effective search and recommendation on many social media
platforms. However, topic classification of such posts is quite challenging
because of (a) a large topic space (b) short text with weak topical cues, and
(c) multiple topic associations per post. In contrast to most prior work which
only focuses on post classification into a small number of topics ($10$-$20$),
we consider the task of large-scale topic classification in the context of
Twitter where the topic space is $10$ times larger with potentially multiple
topic associations per Tweet. We address the challenges above by proposing a
novel neural model, CTM that (a) supports a large topic space of $300$ topics
and (b) takes a holistic approach to tweet content modeling -- leveraging
multi-modal content, author context, and deeper semantic cues in the Tweet. Our
method offers an effective way to classify Tweets into topics at scale by
yielding superior performance to other approaches (a relative lift of
$\mathbf{20}\%$ in median average precision score) and has been successfully
deployed in production at Twitter."
"Hierarchical text classification aims to leverage label hierarchy in
multi-label text classification. Existing methods encode label hierarchy in a
global view, where label hierarchy is treated as the static hierarchical
structure containing all labels. Since global hierarchy is static and
irrelevant to text samples, it makes these methods hard to exploit hierarchical
information. Contrary to global hierarchy, local hierarchy as a structured
labels hierarchy corresponding to each text sample. It is dynamic and relevant
to text samples, which is ignored in previous methods. To exploit global and
local hierarchies,we propose Hierarchy-guided BERT with Global and Local
hierarchies (HBGL), which utilizes the large-scale parameters and prior
language knowledge of BERT to model both global and local
hierarchies.Moreover,HBGL avoids the intentional fusion of semantic and
hierarchical modules by directly modeling semantic and hierarchical information
with BERT.Compared with the state-of-the-art method HGCLR,our method achieves
significant improvement on three benchmark datasets."
"We release our synthetic parallel paraphrase corpus across 17 languages:
Arabic, Catalan, Czech, German, English, Spanish, Estonian, French, Hindi,
Indonesian, Italian, Dutch, Romanian, Russian, Swedish, Vietnamese, and
Chinese. Our method relies only on monolingual data and a neural machine
translation system to generate paraphrases, hence simple to apply. We generate
multiple translation samples using beam search and choose the most lexically
diverse pair according to their sentence BLEU. We compare our generated corpus
with the \texttt{ParaBank2}. According to our evaluation, our synthetic
paraphrase pairs are semantically similar and lexically diverse."
"Microblogs have become a social platform for people to express their emotions
in real-time, and it is a trend to analyze user emotional tendencies from the
information on Microblogs. The dynamic features of emojis can affect the
sentiment polarity of microblog texts. Since existing models seldom consider
the diversity of emoji sentiment polarity,the paper propose a microblog
sentiment classification model based on ALBERT-FAET. We obtain text embedding
via ALBERT pretraining model and learn the inter-emoji embedding with an
attention-based LSTM network. In addition, a fine-grained attention mechanism
is proposed to capture the word-level interactions between plain text and
emoji. Finally, we concatenate these features and feed them into a CNN
classifier to predict the sentiment labels of the microblogs. To verify the
effectiveness of the model and the fine-grained attention network, we conduct
comparison experiments and ablation experiments. The comparison experiments
show that the model outperforms previous methods in three evaluation indicators
(accuracy, precision, and recall) and the model can significantly improve
sentiment classification. The ablation experiments show that compared with
ALBERT-AET, the proposed model ALBERT-FAET is better in the metrics, indicating
that the fine-grained attention network can understand the diversified
information of emoticons."
"Recently, for few-shot or even zero-shot learning, the new paradigm
""pre-train, prompt, and predict"" has achieved remarkable achievements compared
with the ""pre-train, fine-tune"" paradigm. After the success of prompt-based
GPT-3, a series of masked language model (MLM)-based (e.g., BERT, RoBERTa)
prompt learning methods became popular and widely used. However, another
efficient pre-trained discriminative model, ELECTRA, has probably been
neglected. In this paper, we attempt to accomplish several NLP tasks in the
zero-shot scenario using a novel our proposed replaced token detection
(RTD)-based prompt learning method. Experimental results show that ELECTRA
model based on RTD-prompt learning achieves surprisingly state-of-the-art
zero-shot performance. Numerically, compared to MLM-RoBERTa-large and
MLM-BERT-large, our RTD-ELECTRA-large has an average of about 8.4% and 13.7%
improvement on all 15 tasks. Especially on the SST-2 task, our
RTD-ELECTRA-large achieves an astonishing 90.1% accuracy without any training
data. Overall, compared to the pre-trained masked language models, the
pre-trained replaced token detection model performs better in zero-shot
learning. The source code is available at:
https://github.com/nishiwen1214/RTD-ELECTRA."
"Task generalization has been a long standing challenge in Natural Language
Processing (NLP). Recent research attempts to improve the task generalization
ability of pre-trained language models by mapping NLP tasks into human-readable
prompted forms. However, these approaches require laborious and inflexible
manual collection of prompts, and different prompts on the same downstream task
may receive unstable performance. We propose Unified Schema Prompt, a flexible
and extensible prompting method, which automatically customizes the learnable
prompts for each task according to the task input schema. It models the shared
knowledge between tasks, while keeping the characteristics of different task
schema, and thus enhances task generalization ability. The schema prompt takes
the explicit data structure of each task to formulate prompts so that little
human effort is involved. To test the task generalization ability of schema
prompt at scale, we conduct schema prompt-based multitask pre-training on a
wide variety of general NLP tasks. The framework achieves strong zero-shot and
few-shot generalization performance on 16 unseen downstream tasks from 8 task
types (e.g., QA, NLI, etc). Furthermore, comprehensive analyses demonstrate the
effectiveness of each component in the schema prompt, its flexibility in task
compositionality, and its ability to improve performance under a full-data
fine-tuning setting."
"Multiword expressions (MWEs) present groups of words in which the meaning of
the whole is not derived from the meaning of its parts. The task of processing
MWEs is crucial in many natural language processing (NLP) applications,
including machine translation and terminology extraction. Therefore, detecting
MWEs is a popular research theme. In this paper, we explore state-of-the-art
neural transformers in the task of detecting MWEs.We empirically evaluate
several transformer models in the dataset for SemEval-2016 Task 10: Detecting
Minimal Semantic Units and their Meanings (DiMSUM). We show that transformer
models outperform the previous neural models based on long short-term memory
(LSTM). The code and pre-trained model will be made freely available to the
community."
"This paper proposes a simple yet effective interpolation-based data
augmentation approach termed DoubleMix, to improve the robustness of models in
text classification. DoubleMix first leverages a couple of simple augmentation
operations to generate several perturbed samples for each training data, and
then uses the perturbed data and original data to carry out a two-step
interpolation in the hidden space of neural models. Concretely, it first mixes
up the perturbed data to a synthetic sample and then mixes up the original data
and the synthetic perturbed data. DoubleMix enhances models' robustness by
learning the ""shifted"" features in hidden space. On six text classification
benchmark datasets, our approach outperforms several popular text augmentation
methods including token-level, sentence-level, and hidden-level data
augmentation techniques. Also, experiments in low-resource settings show our
approach consistently improves models' performance when the training data is
scarce. Extensive ablation studies and case studies confirm that each component
of our approach contributes to the final performance and show that our approach
exhibits superior performance on challenging counterexamples. Additionally,
visual analysis shows that text features generated by our approach are highly
interpretable. Our code for this paper can be found at
https://github.com/declare-lab/DoubleMix.git."
"Recent neural supervised topic segmentation models achieve distinguished
superior effectiveness over unsupervised methods, with the availability of
large-scale training corpora sampled from Wikipedia. These models may, however,
suffer from limited robustness and transferability caused by exploiting simple
linguistic cues for prediction, but overlooking more important inter-sentential
topical consistency. To address this issue, we present a discourse-aware neural
topic segmentation model with the injection of above-sentence discourse
dependency structures to encourage the model make topic boundary prediction
based more on the topical consistency between sentences. Our empirical study on
English evaluation datasets shows that injecting above-sentence discourse
structures to a neural topic segmenter with our proposed strategy can
substantially improve its performances on intra-domain and out-of-domain data,
with little increase of model's complexity."
"Current language models have been criticised for learning language from text
alone without connection between words and their meaning. Consequently,
multimodal training has been proposed as a way for creating models with better
language understanding by providing the lacking connection. We focus on
pre-trained multimodal vision-and-language (VL) models for which there already
are some results on their language understanding capabilities. An unresolved
issue with evaluating the linguistic skills of these models, however, is that
there is no established method for adapting them to text-only input without
out-of-distribution uncertainty. To find the best approach, we investigate and
compare seven possible methods for adapting three different pre-trained VL
models to text-only input. Our evaluations on both GLUE and Visual Property
Norms (VPN) show that care should be put into adapting VL models to zero-shot
text-only tasks, while the models are less sensitive to how we adapt them to
non-zero-shot tasks. We also find that the adaptation methods perform
differently for different models and that unimodal model counterparts perform
on par with the VL models regardless of adaptation, indicating that current VL
models do not necessarily gain better language understanding from their
multimodal training."
"This work investigates the challenge of learning and reasoning for
Commonsense Question Answering given an external source of knowledge in the
form of a knowledge graph (KG). We propose a novel graph neural network
architecture, called Dynamic Relevance Graph Network (DRGN). DRGN operates on a
given KG subgraph based on the question and answers entities and uses the
relevance scores between the nodes to establish new edges dynamically for
learning node representations in the graph network. This explicit usage of
relevance as graph edges has the following advantages, a) the model can exploit
the existing relationships, re-scale the node weights, and influence the way
the neighborhood nodes' representations are aggregated in the KG subgraph, b)
It potentially recovers the missing edges in KG that are needed for reasoning.
Moreover, as a byproduct, our model improves handling the negative questions
due to considering the relevance between the question node and the graph
entities. Our proposed approach shows competitive performance on two QA
benchmarks, CommonsenseQA and OpenbookQA, compared to the state-of-the-art
published results."
"An increasing number of people now rely on online platforms to meet their
health information needs. Thus identifying inconsistent or conflicting textual
health information has become a safety-critical task. Health advice data poses
a unique challenge where information that is accurate in the context of one
diagnosis can be conflicting in the context of another. For example, people
suffering from diabetes and hypertension often receive conflicting health
advice on diet. This motivates the need for technologies which can provide
contextualized, user-specific health advice. A crucial step towards
contextualized advice is the ability to compare health advice statements and
detect if and how they are conflicting. This is the task of health conflict
detection (HCD). Given two pieces of health advice, the goal of HCD is to
detect and categorize the type of conflict. It is a challenging task, as (i)
automatically identifying and categorizing conflicts requires a deeper
understanding of the semantics of the text, and (ii) the amount of available
data is quite limited.
  In this study, we are the first to explore HCD in the context of pre-trained
language models. We find that DeBERTa-v3 performs best with a mean F1 score of
0.68 across all experiments. We additionally investigate the challenges posed
by different conflict types and how synthetic data improves a model's
understanding of conflict-specific semantics. Finally, we highlight the
difficulty in collecting real health conflicts and propose a human-in-the-loop
synthetic data augmentation approach to expand existing HCD datasets. Our HCD
training dataset is over 2x bigger than the existing HCD dataset and is made
publicly available on Github."
"We define a novel concept called extended word alignment in order to improve
post-editing assistance efficiency. Based on extended word alignment, we
further propose a novel task called refined word-level QE that outputs refined
tags and word-level correspondences. Compared to original word-level QE, the
new task is able to directly point out editing operations, thus improves
efficiency. To extract extended word alignment, we adopt a supervised method
based on mBERT. To solve refined word-level QE, we firstly predict original QE
tags by training a regression model for sequence tagging based on mBERT and
XLM-R. Then, we refine original word tags with extended word alignment. In
addition, we extract source-gap correspondences, meanwhile, obtaining gap tags.
Experiments on two language pairs show the feasibility of our method and give
us inspirations for further improvement."
"Span-based nested named-entity recognition (NER) has a cubic-time complexity
using a variant of the CYK algorithm. We show that by adding a supplementary
structural constraint on the search space, nested NER has a quadratic-time
complexity, that is the same asymptotic complexity than the non-nested case.
The proposed algorithm covers a large part of three standard English benchmarks
and delivers comparable experimental results."
"The following paper presents a project focused on the research and creation
of a new Automatic Speech Recognition (ASR) based in the Chukchi language.
There is no one complete corpus of the Chukchi language, so most of the work
consisted in collecting audio and texts in the Chukchi language from open
sources and processing them. We managed to collect 21:34:23 hours of audio
recordings and 112,719 sentences (or 2,068,273 words) of text in the Chukchi
language. The XLSR model was trained on the obtained data, which showed good
results even with a small amount of data. Besides the fact that the Chukchi
language is a low-resource language, it is also polysynthetic, which
significantly complicates any automatic processing. Thus, the usual WER metric
for evaluating ASR becomes less indicative for a polysynthetic language.
However, the CER metric showed good results. The question of metrics for
polysynthetic languages remains open."
"Automated fault diagnosis can facilitate diagnostics assistance, speedier
troubleshooting, and better-organised logistics. Currently, AI-based
prognostics and health management in the automotive industry ignore the textual
descriptions of the experienced problems or symptoms. With this study, however,
we show that a multilingual pre-trained Transformer can effectively classify
the textual claims from a large company with vehicle fleets, despite the task's
challenging nature due to the 38 languages and 1,357 classes involved. Overall,
we report an accuracy of more than 80% for high-frequency classes and above 60%
for above-low-frequency classes, bringing novel evidence that multilingual
classification can benefit automotive troubleshooting management."
"Multi-dimensional evaluation is the dominant paradigm for human evaluation in
Natural Language Generation (NLG), i.e., evaluating the generated text from
multiple explainable dimensions, such as coherence and fluency. However,
automatic evaluation in NLG is still dominated by similarity-based metrics, and
we lack a reliable framework for a more comprehensive evaluation of advanced
models. In this paper, we propose a unified multi-dimensional evaluator UniEval
for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task,
and by guiding the model with different questions, we can use one evaluator to
evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean
QA format, we are able to introduce an intermediate learning phase that enables
UniEval to incorporate external knowledge from multiple related tasks and gain
further improvement. Experiments on three typical NLG tasks show that UniEval
correlates substantially better with human judgments than existing metrics.
Specifically, compared to the top-performing unified evaluators, UniEval
achieves a 23% higher correlation on text summarization, and over 43% on
dialogue response generation. Also, UniEval demonstrates a strong zero-shot
learning ability for unseen evaluation dimensions and tasks. Source code, data
and all pre-trained evaluators are available on our GitHub repository
(https://github.com/maszhongming/UniEval)."
"This paper describes Tencent AI Lab - Shanghai Jiao Tong University
(TAL-SJTU) Low-Resource Translation systems for the WMT22 shared task. We
participate in the general translation task on
English$\Leftrightarrow$Livonian. Our system is based on M2M100 with novel
techniques that adapt it to the target language pair. (1) Cross-model word
embedding alignment: inspired by cross-lingual word embedding alignment, we
successfully transfer a pre-trained word embedding to M2M100, enabling it to
support Livonian. (2) Gradual adaptation strategy: we exploit Estonian and
Latvian as auxiliary languages for many-to-many translation training and then
adapt to English-Livonian. (3) Data augmentation: to enlarge the parallel data
for English-Livonian, we construct pseudo-parallel data with Estonian and
Latvian as pivot languages. (4) Fine-tuning: to make the most of all available
data, we fine-tune the model with the validation set and online
back-translation, further boosting the performance. In model evaluation: (1) We
find that previous work underestimated the translation performance of Livonian
due to inconsistent Unicode normalization, which may cause a discrepancy of up
to 14.9 BLEU score. (2) In addition to the standard validation set, we also
employ round-trip BLEU to evaluate the models, which we find more appropriate
for this task. Finally, our unconstrained system achieves BLEU scores of 17.0
and 30.4 for English to/from Livonian."
"The lack of wide coverage datasets annotated with everyday metaphorical
expressions for languages other than English is striking. This means that most
research on supervised metaphor detection has been published only for that
language. In order to address this issue, this work presents the first corpus
annotated with naturally occurring metaphors in Spanish large enough to develop
systems to perform metaphor detection. The presented dataset, CoMeta, includes
texts from various domains, namely, news, political discourse, Wikipedia and
reviews. In order to label CoMeta, we apply the MIPVU method, the guidelines
most commonly used to systematically annotate metaphor on real data. We use our
newly created dataset to provide competitive baselines by fine-tuning several
multilingual and monolingual state-of-the-art large language models.
Furthermore, by leveraging the existing VUAM English data in addition to
CoMeta, we present the, to the best of our knowledge, first cross-lingual
experiments on supervised metaphor detection. Finally, we perform a detailed
error analysis that explores the seemingly high transfer of everyday metaphor
across these two languages and datasets."
"This paper describes our system submitted to Dialogue Robot Competition 2022.
Our proposed system is a combined model of rule-based and generation-based
dialog systems. The system utilizes HyperCLOVA, a Japanese foundation model,
not only to generate responses but also summarization, search information, etc.
We also used our original speech recognition system, which was fine-tuned for
this dialog task. As a result, our system ranked second in the preliminary
round and moved on to the finals."
"This paper investigates how hate speech varies in systematic ways according
to the identities it targets. Across multiple hate speech datasets annotated
for targeted identities, we find that classifiers trained on hate speech
targeting specific identity groups struggle to generalize to other targeted
identities. This provides empirical evidence for differences in hate speech by
target identity; we then investigate which patterns structure this variation.
We find that the targeted demographic category (e.g. gender/sexuality or
race/ethnicity) appears to have a greater effect on the language of hate speech
than does the relative social power of the targeted identity group. We also
find that words associated with hate speech targeting specific identities often
relate to stereotypes, histories of oppression, current social movements, and
other social contexts specific to identities. These experiments suggest the
importance of considering targeted identity, as well as the social contexts
associated with these identities, in automated hate speech classification."
"Building pretrained language models is considered expensive and
data-intensive, but must we increase dataset size to achieve better
performance? We propose an alternative to larger training sets by automatically
identifying smaller yet domain-representative subsets. We extend Cynical Data
Selection, a statistical sentence scoring method that conditions on a
representative target domain corpus. As an example, we treat the OntoNotes
corpus as a target domain and pretrain a RoBERTa-like encoder from a cynically
selected subset of the Pile. On both perplexity and across several downstream
tasks in the target domain, it consistently outperforms random selection with
20x less data, 3x fewer training iterations, and 2x less estimated cloud
compute cost, validating the recipe of automatic document selection for LM
pretraining."
"As one of the challenging NLP tasks, designing math word problem (MWP)
solvers has attracted increasing research attention for the past few years. In
previous work, models designed by taking into account the properties of the
binary tree structure of mathematical expressions at the output side have
achieved better performance. However, the expressions corresponding to a MWP
are often diverse (e.g., $n_1+n_2 \times n_3-n_4$, $n_3\times n_2-n_4+n_1$,
etc.), and so are the corresponding binary trees, which creates difficulties in
model learning due to the non-deterministic output space. In this paper, we
propose the Structure-Unified M-Tree Coding Solver (SUMC-Solver), which applies
a tree with any M branches (M-tree) to unify the output structures. To learn
the M-tree, we use a mapping to convert the M-tree into the M-tree codes, where
codes store the information of the paths from tree root to leaf nodes and the
information of leaf nodes themselves, and then devise a Sequence-to-Code
(seq2code) model to generate the codes. Experimental results on the widely used
MAWPS and Math23K datasets have demonstrated that SUMC-Solver not only
outperforms several state-of-the-art models under similar experimental settings
but also performs much better under low-resource conditions."
"Construction Grammar (CxG) is a paradigm from cognitive linguistics
emphasising the connection between syntax and semantics. Rather than rules that
operate on lexical items, it posits constructions as the central building
blocks of language, i.e., linguistic units of different granularity that
combine syntax and semantics. As a first step towards assessing the
compatibility of CxG with the syntactic and semantic knowledge demonstrated by
state-of-the-art pretrained language models (PLMs), we present an investigation
of their capability to classify and understand one of the most commonly studied
constructions, the English comparative correlative (CC). We conduct experiments
examining the classification accuracy of a syntactic probe on the one hand and
the models' behaviour in a semantic application task on the other, with BERT,
RoBERTa, and DeBERTa as the example PLMs. Our results show that all three
investigated PLMs are able to recognise the structure of the CC but fail to use
its meaning. While human-like performance of PLMs on many NLP tasks has been
alleged, this indicates that PLMs still suffer from substantial shortcomings in
central domains of linguistic knowledge."
"Document-level relation extraction (DocRE) aims to identify semantic labels
among entities within a single document. One major challenge of DocRE is to dig
decisive details regarding a specific entity pair from long text. However, in
many cases, only a fraction of text carries required information, even in the
manually labeled supporting evidence. To better capture and exploit instructive
information, we propose a novel expLicit syntAx Refinement and Subsentence
mOdeliNg based framework (LARSON). By introducing extra syntactic information,
LARSON can model subsentences of arbitrary granularity and efficiently screen
instructive ones. Moreover, we incorporate refined syntax into text
representations which further improves the performance of LARSON. Experimental
results on three benchmark datasets (DocRED, CDR, and GDA) demonstrate that
LARSON significantly outperforms existing methods."
"In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain
compact BERT-style language model dubbed SKDBERT. In each iteration, SKD
samples a teacher model from a pre-defined teacher ensemble, which consists of
multiple teacher models with multi-level capacities, to transfer knowledge into
student model in an one-to-one manner. Sampling distribution plays an important
role in SKD. We heuristically present three types of sampling distributions to
assign appropriate probabilities for multi-level teacher models. SKD has two
advantages: 1) it can preserve the diversities of multi-level teacher models
via stochastically sampling single teacher model in each iteration, and 2) it
can also improve the efficacy of knowledge distillation via multi-level teacher
models when large capacity gap exists between the teacher model and the student
model. Experimental results on GLUE benchmark show that SKDBERT reduces the
size of a BERT$_{\rm BASE}$ model by 40% while retaining 99.5% performances of
language understanding and being 100% faster."
"This work aims to employ natural language generation (NLG) to rapidly
generate items for English language learning applications: this requires both
language models capable of generating fluent, high-quality English, and to
control the output of the generation to match the requirements of the relevant
items. We experiment with deep pretrained models for this task, developing
novel methods for controlling items for factors relevant in language learning:
diverse sentences for different proficiency levels and argument structure to
test grammar. Human evaluation demonstrates high grammatically scores for all
models (3.4 and above out of 4), and higher length (24%) and complexity (9%)
over the baseline for the advanced proficiency model. Our results show that we
can achieve strong performance while adding additional control to ensure
diverse, tailored content for individual users."
"We adapt Lee et al.'s (2018) span-based entity coreference model to the task
of end-to-end discourse deixis resolution in dialogue, specifically by
proposing extensions to their model that exploit task-specific characteristics.
The resulting model, dd-utt, achieves state-of-the-art results on the four
datasets in the CODI-CRAC 2021 shared task."
"Cross-lingual transfer learning without labeled target language data or
parallel text has been surprisingly effective in zero-shot cross-lingual
classification, question answering, unsupervised machine translation, etc.
However, some recent publications have claimed that domain mismatch prevents
cross-lingual transfer, and their results show that unsupervised bilingual
lexicon induction (UBLI) and unsupervised neural machine translation (UNMT) do
not work well when the underlying monolingual corpora come from different
domains (e.g., French text from Wikipedia but English text from UN
proceedings). In this work, we show that a simple initialization regimen can
overcome much of the effect of domain mismatch in cross-lingual transfer. We
pre-train word and contextual embeddings on the concatenated domain-mismatched
corpora, and use these as initializations for three tasks: MUSE UBLI, UN
Parallel UNMT, and the SemEval 2017 cross-lingual word similarity task. In all
cases, our results challenge the conclusions of prior work by showing that
proper initialization can recover a large portion of the losses incurred by
domain mismatch."
"Machine learning enables the development of new, supplemental, and empowering
tools that can either expand existing technologies or invent new ones. In
education, space exists for a tool that supports generic student course review
formats to organize and recapitulate students' views on the pedagogical
practices to which they are exposed. Often, student opinions are gathered with
a general comment section that solicits their feelings towards their courses
without polling specifics about course contents. Herein, we show a novel
approach to summarizing and organizing students' opinions via analyzing their
sentiment towards a course as a function of the language/vocabulary used to
convey their opinions about a class and its contents. This analysis is derived
from their responses to a general comment section encountered at the end of
post-course review surveys. This analysis, accomplished with Python, LaTeX, and
Google's Natural Language API, allows for the conversion of unstructured text
data into both general and topic-specific sub-reports that convey students'
views in a unique, novel way."
"Since the meaning representations are detailed and accurate annotations which
express fine-grained sequence-level semtantics, it is usually hard to train
discriminative semantic parsers via Maximum Likelihood Estimation (MLE) in an
autoregressive fashion. In this paper, we propose a semantic-aware contrastive
learning algorithm, which can learn to distinguish fine-grained meaning
representations and take the overall sequence-level semantic into
consideration. Specifically, a multi-level online sampling algorithm is
proposed to sample confusing and diverse instances. Three semantic-aware
similarity functions are designed to accurately measure the distance between
meaning representations as a whole. And a ranked contrastive loss is proposed
to pull the representations of the semantic-identical instances together and
push negative instances away. Experiments on two standard datasets show that
our approach achieves significant improvements over MLE baselines and gets
state-of-the-art performances by simply applying semantic-aware contrastive
learning on a vanilla Seq2Seq model."
"Selecting a birth control method is a complex healthcare decision. While
birth control methods provide important benefits, they can also cause
unpredictable side effects and be stigmatized, leading many people to seek
additional information online, where they can find reviews, advice, hypotheses,
and experiences of other birth control users. However, the relationships
between their healthcare concerns, sensemaking activities, and online settings
are not well understood. We gather texts about birth control shared on Twitter,
Reddit, and WebMD -- platforms with different affordances, moderation, and
audiences -- to study where and how birth control is discussed online. Using a
combination of topic modeling and hand annotation, we identify and characterize
the dominant sensemaking practices across these platforms, and we create
lexicons to draw comparisons across birth control methods and side effects. We
use these to measure variations from survey reports of side effect experiences
and method usage. Our findings characterize how online platforms are used to
make sense of difficult healthcare choices and highlight unmet needs of birth
control users."
"The rise in hateful and offensive language directed at other users is one of
the adverse side effects of the increased use of social networking platforms.
This could make it difficult for human moderators to review tagged comments
filtered by classification systems. To help address this issue, we present the
ViHOS (Vietnamese Hate and Offensive Spans) dataset, the first human-annotated
corpus containing 26k spans on 11k comments. We also provide definitions of
hateful and offensive spans in Vietnamese comments as well as detailed
annotation guidelines. Besides, we conduct experiments with various
state-of-the-art models. Specifically, XLM-R$_{Large}$ achieved the best
F1-scores in Single span detection and All spans detection, while
PhoBERT$_{Large}$ obtained the highest in Multiple spans detection. Finally,
our error analysis demonstrates the difficulties in detecting specific types of
spans in our data for future research.
  Disclaimer: This paper contains real comments that could be considered
profane, offensive, or abusive."
"Table-based reasoning has shown remarkable progress in combining deep models
with discrete reasoning, which requires reasoning over both free-form natural
language (NL) questions and structured tabular data. However, previous
table-based reasoning solutions usually suffer from significant performance
degradation on huge evidence (tables). In addition, most existing methods
struggle to reason over complex questions since the required information is
scattered in different places. To alleviate the above challenges, we exploit
large language models (LLMs) as decomposers for effective table-based
reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence
(a small table) to mitigate the interference of useless information for table
reasoning; and (ii) decompose complex questions into simpler sub-questions for
text reasoning. Specifically, we first use the LLMs to break down the evidence
(tables) involved in the current question, retaining the relevant evidence and
excluding the remaining irrelevant evidence from the huge table. In addition,
we propose a ""parsing-execution-filling"" strategy to alleviate the
hallucination dilemma of the chain of thought by decoupling logic and numerical
computation in each step. Extensive experiments show that our method can
effectively leverage decomposed evidence and questions and outperforms the
strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,
our model outperforms human performance for the first time on the TabFact
dataset."
"Text generation from Abstract Meaning Representation (AMR) has substantially
benefited from the popularized Pretrained Language Models (PLMs). Myriad
approaches have linearized the input graph as a sequence of tokens to fit the
PLM tokenization requirements. Nevertheless, this transformation jeopardizes
the structural integrity of the graph and is therefore detrimental to its
resulting representation. To overcome this issue, Ribeiro et al. have recently
proposed StructAdapt, a structure-aware adapter which injects the input graph
connectivity within PLMs using Graph Neural Networks (GNNs). In this paper, we
investigate the influence of Relative Position Embeddings (RPE) on AMR-to-Text,
and, in parallel, we examine the robustness of StructAdapt. Through ablation
studies, graph attack and link prediction, we reveal that RPE might be
partially encoding input graphs. We suggest further research regarding the role
of RPE will provide valuable insights for Graph-to-Text generation."
"Table pretrain-then-finetune paradigm has been proposed and employed at a
rapid pace after the success of pre-training in the natural language domain.
Despite the promising findings in tabular pre-trained language models (TPLMs),
there is an input gap between pre-training and fine-tuning phases. For
instance, TPLMs jointly pre-trained with table and text input could be
effective for tasks also with table-text joint input like table question
answering, but it may fail for tasks with only tables or text as input such as
table retrieval. To this end, we propose UTP, an approach that dynamically
supports three types of multi-modal inputs: table-text, table, and text.
Specifically, UTP is pre-trained with two strategies: (1) We first utilize a
universal mask language modeling objective on each kind of input, enforcing the
model to adapt various inputs. (2) We then present Cross-Modal Contrastive
Regularization (CMCR), which utilizes contrastive learning to encourage the
consistency between table-text cross-modality representations via unsupervised
instance-wise training signals during pre-training. By these means, the
resulting model not only bridges the input gap between pre-training and
fine-tuning but also advances in the alignment of table and text. Extensive
results show UTP achieves superior results on uni-modal input tasks (e.g.,
table retrieval) and cross-modal input tasks (e.g., table question answering)."
"To advance Chinese financial natural language processing (NLP), we introduce
BBT-FinT5, a new Chinese financial pre-training language model based on the T5
model. To support this effort, we have built BBT-FinCorpus, a large-scale
financial corpus with approximately 300GB of raw text from four different
sources. In general domain NLP, comprehensive benchmarks like GLUE and
SuperGLUE have driven significant advancements in language model pre-training
by enabling head-to-head comparisons among models. Drawing inspiration from
these benchmarks, we propose BBT-CFLEB, a Chinese Financial Language
understanding and generation Evaluation Benchmark, which includes six datasets
covering both understanding and generation tasks. Our aim is to facilitate
research in the development of NLP within the Chinese financial domain. Our
model, corpus and benchmark are released at
https://github.com/ssymmetry/BBT-FinCUGE-Applications. Our work belongs to the
Big Bang Transformer (BBT), a large-scale pre-trained language model project."
"We introduce Directional Stimulus Prompting, a novel framework for guiding
black-box large language models (LLMs) toward specific desired outputs. Instead
of directly adjusting LLMs, our method employs a small tunable policy model
(e.g., T5) to generate an auxiliary directional stimulus prompt for each input
instance. These directional stimulus prompts act as nuanced, instance-specific
hints and clues to guide LLMs in generating desired outcomes, such as including
specific keywords in the generated summary. Our approach sidesteps the
challenges of direct LLM tuning by optimizing the policy model to explore
directional stimulus prompts that align LLMs with desired behaviors. The policy
model can be optimized through 1) supervised fine-tuning using labeled data and
2) reinforcement learning from offline or online rewards based on the LLM's
output. We assess our method across summarization, dialogue response
generation, and chain-of-thought reasoning tasks. Our experiments demonstrate
that the framework consistently improves LLMs' (e.g., ChatGPT, Codex,
InstructGPT) performance on these supervised tasks using minimal labeled data.
Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances
ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully
supervised start-of-the-art models. Additionally, the instance-specific
chain-of-thought prompt generated by our approach improves InstructGPT's
reasoning accuracy compared to human-crafted or automatically generated
prompts. The code and data are publicly available at
\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}."
"Prompt tuning, in which a base pretrained model is adapted to each task via
conditioning on learned prompt vectors, has emerged as a promising approach for
efficiently adapting large language models to multiple downstream tasks.
However, existing methods typically learn soft prompt vectors from scratch, and
it has not been clear how to exploit the rich cross-task knowledge with prompt
vectors in a multitask learning setting. We propose multitask prompt tuning
(MPT), which first learns a single transferable prompt by distilling knowledge
from multiple task-specific source prompts. We then learn multiplicative low
rank updates to this shared prompt to efficiently adapt it to each downstream
target task. Extensive experiments on 23 NLP datasets demonstrate that our
proposed approach outperforms the state-of-the-art methods, including the full
finetuning baseline in some cases, despite only tuning 0.035% as many
task-specific parameters."
"Recently, many studies incorporate external knowledge into character-level
feature based models to improve the performance of Chinese relation extraction.
However, these methods tend to ignore the internal information of the Chinese
character and cannot filter out the noisy information of external knowledge. To
address these issues, we propose a mixture-of-view-experts framework (MoVE) to
dynamically learn multi-view features for Chinese relation extraction. With
both the internal and external knowledge of Chinese characters, our framework
can better capture the semantic information of Chinese characters. To
demonstrate the effectiveness of the proposed framework, we conduct extensive
experiments on three real-world datasets in distinct domains. Experimental
results show consistent and significant superiority and robustness of our
proposed framework. Our code and dataset will be released at:
https://gitee.com/tmg-nudt/multi-view-of-expert-for-chineserelation-extraction"
"The paper looks at the role of large language models in academic knowledge
creation based on a scoping review (2018 to January 2023) of how researchers
have previously used the language model GPT to assist in the performance of
academic knowledge creation tasks beyond data analysis. These tasks include
writing, editing, reviewing, dataset creation and curation, which have been
difficult to perform using earlier ML tools. Based on a synthesis of these
papers, this study identifies pathways for a future academic research landscape
that incorporates wider usage of large language models based on the current
modes of adoption in published articles as a Co-Writer, Research Assistant and
Respondent."
"Vulnerability to lexical perturbation is a critical weakness of automatic
evaluation metrics for image captioning. This paper proposes Perturbation
Robust Multi-Lingual CLIPScore(PR-MCS), which exhibits robustness to such
perturbations, as a novel reference-free image captioning metric applicable to
multiple languages. To achieve perturbation robustness, we fine-tune the text
encoder of CLIP with our language-agnostic method to distinguish the perturbed
text from the original text. To verify the robustness of PR-MCS, we introduce a
new fine-grained evaluation dataset consisting of detailed captions, critical
objects, and the relationships between the objects for 3, 000 images in five
languages. In our experiments, PR-MCS significantly outperforms baseline
metrics in capturing lexical noise of all various perturbation types in all
five languages, proving that PR-MCS is highly robust to lexical perturbations."
"Multilingual language models have pushed state-of-the-art in cross-lingual
NLP transfer. The majority of zero-shot cross-lingual transfer, however, use
one and the same massively multilingual transformer (e.g., mBERT or XLM-R) to
transfer to all target languages, irrespective of their typological,
etymological, and phylogenetic relations to other languages. In particular,
readily available data and models of resource-rich sibling languages are often
ignored. In this work, we empirically show, in a case study for Faroese -- a
low-resource language from a high-resource language family -- that by
leveraging the phylogenetic information and departing from the
'one-size-fits-all' paradigm, one can improve cross-lingual transfer to
low-resource languages. In particular, we leverage abundant resources of other
Scandinavian languages (i.e., Danish, Norwegian, Swedish, and Icelandic) for
the benefit of Faroese. Our evaluation results show that we can substantially
improve the transfer performance to Faroese by exploiting data and models of
closely-related high-resource languages. Further, we release a new web corpus
of Faroese and Faroese datasets for named entity recognition (NER), semantic
text similarity (STS), and new language models trained on all Scandinavian
languages."
"Recent psycholinguistic studies have drawn conflicting conclusions about the
relationship between the quality of a language model and the ability of its
surprisal estimates to predict human reading times, which has been speculated
to be due to the large gap in both the amount of training data and model
capacity across studies. The current work aims to consolidate these findings by
evaluating surprisal estimates from Transformer-based language model variants
that vary systematically in the amount of training data and model capacity on
their ability to predict human reading times. The results show that surprisal
estimates from most variants with contemporary model capacities provide the
best fit after seeing about two billion training tokens, after which they begin
to diverge from humanlike expectations. Additionally, newly-trained smaller
model variants reveal a 'tipping point' at convergence, after which the
decrease in language model perplexity begins to result in poorer fits to human
reading times. These results suggest that the massive amount of training data
is mainly responsible for the poorer fit achieved by surprisal from larger
pre-trained language models, and that a certain degree of model capacity is
necessary for Transformer-based language models to capture humanlike
expectations."
"Machine Translation (MT) has greatly advanced over the years due to the
developments in deep neural networks. However, the emergence of Large Language
Models (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MT
domain. In this context, we believe that the future of MT is intricately tied
to the capabilities of LLMs. These models not only offer vast linguistic
understandings but also bring innovative methodologies, such as prompt-based
techniques, that have the potential to further elevate MT. In this paper, we
provide an overview of the significant enhancements in MT that are influenced
by LLMs and advocate for their pivotal role in upcoming MT research and
implementations. We highlight several new MT directions, emphasizing the
benefits of LLMs in scenarios such as Long-Document Translation, Stylized
Translation, and Interactive Translation. Additionally, we address the
important concern of privacy in LLM-driven MT and suggest essential
privacy-preserving strategies. By showcasing practical instances, we aim to
demonstrate the advantages that LLMs offer, particularly in tasks like
translating extended documents. We conclude by emphasizing the critical role of
LLMs in guiding the future evolution of MT and offer a roadmap for future
exploration in the sector."
"Cross-lingual and cross-domain knowledge alignment without sufficient
external resources is a fundamental and crucial task for fusing irregular data.
As the element-wise fusion process aiming to discover equivalent objects from
different knowledge graphs (KGs), entity alignment (EA) has been attracting
great interest from industry and academic research recent years. Most of
existing EA methods usually explore the correlation between entities and
relations through neighbor nodes, structural information and external
resources. However, the complex intrinsic interactions among triple elements
and role information are rarely modeled in these methods, which may lead to the
inadequate illustration for triple. In addition, external resources are usually
unavailable in some scenarios especially cross-lingual and cross-domain
applications, which reflects the little scalability of these methods. To tackle
the above insufficiency, a novel universal EA framework (OTIEA) based on
ontology pair and role enhancement mechanism via triple-aware attention is
proposed in this paper without introducing external resources. Specifically, an
ontology-enhanced triple encoder is designed via mining intrinsic correlations
and ontology pair information instead of independent elements. In addition, the
EA-oriented representations can be obtained in triple-aware entity decoder by
fusing role diversity. Finally, a bidirectional iterative alignment strategy is
deployed to expand seed entity pairs. The experimental results on three
real-world datasets show that our framework achieves a competitive performance
compared with baselines."
"Large language models (LMs) beyond a certain scale, demonstrate the emergent
capability of generating free-text rationales for their predictions via
chain-of-thought (CoT) prompting. While CoT can yield dramatically improved
performance, such gains are only observed for sufficiently large LMs. Even more
concerning, there is little guarantee that the generated rationales are
consistent with LM's predictions or faithfully justify the decisions. In this
work, we propose a faithful knowledge distillation method to learn a small,
self-consistent CoT model from a teacher model that is orders of magnitude
larger. To form better supervision, we elicit rationales supporting the gold
answers from a large LM (teacher) by contrastive decoding, which encourages the
teacher to generate tokens that become more plausible only when the answer is
considered. To ensure faithful distillation, we use the teacher-generated
rationales to learn a student LM with a counterfactual reasoning objective,
which prevents the student from ignoring the rationales to make inconsistent
predictions. Experiments show that, while yielding comparable end-task
performance, our method can generate CoT rationales that are more faithful than
baselines do. Further analysis suggests that such a model respects the
rationales more when making decisions; thus, we can improve its performance
more by refining its rationales."
"Automated dialogue or conversational systems are anthropomorphised by
developers and personified by users. While a degree of anthropomorphism may be
inevitable due to the choice of medium, conscious and unconscious design
choices can guide users to personify such systems to varying degrees.
Encouraging users to relate to automated systems as if they were human can lead
to high risk scenarios caused by over-reliance on their outputs. As a result,
natural language processing researchers have investigated the factors that
induce personification and develop resources to mitigate such effects. However,
these efforts are fragmented, and many aspects of anthropomorphism have yet to
be explored. In this paper, we discuss the linguistic factors that contribute
to the anthropomorphism of dialogue systems and the harms that can arise,
including reinforcing gender stereotypes and notions of acceptable language. We
recommend that future efforts towards developing dialogue systems take
particular care in their design, development, release, and description; and
attend to the many linguistic cues that can elicit personification by users."
"Most research in Relation Extraction (RE) involves the English language,
mainly due to the lack of multi-lingual resources. We propose Multi-CrossRE,
the broadest multi-lingual dataset for RE, including 26 languages in addition
to English, and covering six text domains. Multi-CrossRE is a machine
translated version of CrossRE (Bassignana and Plank, 2022), with a sub-portion
including more than 200 sentences in seven diverse languages checked by native
speakers. We run a baseline model over the 26 new datasets and--as sanity
check--over the 26 back-translations to English. Results on the back-translated
data are consistent with the ones on the original English CrossRE, indicating
high quality of the translation and the resulting dataset."
"Relation Extraction (RE) remains a challenging task, especially when
considering realistic out-of-domain evaluations. One of the main reasons for
this is the limited training size of current RE datasets: obtaining
high-quality (manually annotated) data is extremely expensive and cannot
realistically be repeated for each new domain. An intermediate training step on
data from related tasks has shown to be beneficial across many NLP
tasks.However, this setup still requires supplementary annotated data, which is
often not available. In this paper, we investigate intermediate pre-training
specifically for RE. We exploit the affinity between syntactic structure and
semantic RE, and identify the syntactic relations which are closely related to
RE by being on the shortest dependency path between two entities. We then take
advantage of the high accuracy of current syntactic parsers in order to
automatically obtain large amounts of low-cost pre-training data. By
pre-training our RE model on the relevant syntactic relations, we are able to
outperform the baseline in five out of six cross-domain setups, without any
additional annotated data."
"State-of-the-art target-oriented opinion word extraction (TOWE) models
typically use BERT-based text encoders that operate on the word level, along
with graph convolutional networks (GCNs) that incorporate syntactic information
extracted from syntax trees. These methods achieve limited gains with GCNs and
have difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to
be effective at representing rare words or words with insufficient context
information. To address this issue, this work trades syntax trees for BERT
wordpieces by entirely removing the GCN component from the methods'
architectures. To enhance TOWE performance, we tackle the issue of aspect
representation loss during encoding. Instead of solely utilizing a sentence as
the input, we use a sentence-aspect pair. Our relatively simple approach
achieves state-of-the-art results on benchmark datasets and should serve as a
strong baseline for further research."
"Formulating selective information needs results in queries that implicitly
specify set operations, such as intersection, union, and difference. For
instance, one might search for ""shorebirds that are not sandpipers"" or
""science-fiction films shot in England"". To study the ability of retrieval
systems to meet such information needs, we construct QUEST, a dataset of 3357
natural language queries with implicit set operations, that map to a set of
entities corresponding to Wikipedia documents. The dataset challenges models to
match multiple constraints mentioned in queries with corresponding evidence in
documents and correctly perform various set operations. The dataset is
constructed semi-automatically using Wikipedia category names. Queries are
automatically composed from individual categories, then paraphrased and further
validated for naturalness and fluency by crowdworkers. Crowdworkers also assess
the relevance of entities based on their documents and highlight attribution of
query constraints to spans of document text. We analyze several modern
retrieval systems, finding that they often struggle on such queries. Queries
involving negation and conjunction are particularly challenging and systems are
further challenged with combinations of these operations."
"Unsupervised clustering is widely used to explore large corpora, but existing
formulations neither consider the users' goals nor explain clusters' meanings.
We propose a new task formulation, ""Goal-Driven Clustering with Explanations""
(GoalEx), which represents both the goal and the explanations as free-form
language descriptions. For example, to categorize the errors made by a
summarization system, the input to GoalEx is a corpus of annotator-written
comments for system-generated summaries and a goal description ""cluster the
comments based on why the annotators think the summary is imperfect.''; the
outputs are text clusters each with an explanation (""this cluster mentions that
the summary misses important context information.""), which relates to the goal
and precisely explain which comments should (not) belong to a cluster. To
tackle GoalEx, we prompt a language model with ""[corpus subset] + [goal] +
Brainstorm a list of explanations each representing a cluster.""; then we
classify whether each sample belongs to a cluster based on its explanation;
finally, we use integer linear programming to select a subset of candidate
clusters to cover most samples while minimizing overlaps. Under both automatic
and human evaluation on corpora with or without labels, our method produces
more accurate and goal-related explanations than prior methods. We release our
data and implementation at https://github.com/ZihanWangKi/GoalEx."
"Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training
are emerging as the next big revolution in natural language processing and
understanding. These CtB-LLMs are democratizing access to trainable Very
Large-Language Models (VLLMs) and, thus, may represent the building blocks of
many NLP systems solving downstream tasks. Hence, a little or a large bias in
CtB-LLMs may cause huge harm. In this paper, we performed a large investigation
of the bias of three families of CtB-LLMs, and we showed that debiasing
techniques are effective and usable. Indeed, according to current tests, the
LLaMA and the OPT families have an important bias in gender, race, religion,
and profession. In contrast to the analysis for other LLMs, we discovered that
bias depends not on the number of parameters but on the perplexity. Finally,
the debiasing of OPT using LoRA reduces bias up to 4.12 points in the
normalized stereotype score."
"Grammatical error correction (GEC) is a well-explored problem in English with
many existing models and datasets. However, research on GEC in morphologically
rich languages has been limited due to challenges such as data scarcity and
language complexity. In this paper, we present the first results on Arabic GEC
using two newly developed Transformer-based pretrained sequence-to-sequence
models. We also define the task of multi-class Arabic grammatical error
detection (GED) and present the first results on multi-class Arabic GED. We
show that using GED information as an auxiliary input in GEC models improves
GEC performance across three datasets spanning different genres. Moreover, we
also investigate the use of contextual morphological preprocessing in aiding
GEC systems. Our models achieve SOTA results on two Arabic GEC shared task
datasets and establish a strong benchmark on a recently created dataset. We
make our code, data, and pretrained models publicly available."
"Diffusion-based language models are emerging as a promising alternative to
autoregressive LMs: they approach the competence of autoregressive LMs while
offering nuanced controllability at inference time. While autoregressive LMs
have benefited immensely from scaling and instruction-based learning, existing
studies of diffusion LMs have been conducted on a smaller scale. Starting with
a recently proposed diffusion model SSD-LM, in this work we first explore
methods to scale it from 0.4B to 13B parameters, proposing techniques to
improve its training and inference efficiency, and to finetune the model to
follow instructions. Armed with a more powerful, general purpose diffusion LM,
we introduce the primary contribution of this work -- SSD-2 -- an approach to
easily ensemble at inference time a large general-purpose diffusion LM with
smaller, but specialized and contextualized diffusion LMs. We show that SSD-2
facilitates novel ensembles with 100x smaller models that can be customized and
deployed by individual users. We find that compared to autoregressive models,
the collaboration between diffusion LMs is more effective, leading to
higher-quality model responses due to their ability to dynamically incorporate
bi-directional contexts."
"Evidence plays a crucial role in automated fact-checking. When verifying
real-world claims, existing fact-checking systems either assume the evidence
sentences are given or use the search snippets returned by the search engine.
Such methods ignore the challenges of collecting evidence and may not provide
sufficient information to verify real-world claims. Aiming at building a better
fact-checking system, we propose to incorporate full text from source documents
as evidence and introduce two enriched datasets. The first one is a
multilingual dataset, while the second one is monolingual (English). We further
develop a latent variable model to jointly extract evidence sentences from
documents and perform claim verification. Experiments indicate that including
source documents can provide sufficient contextual clues even when gold
evidence sentences are not annotated. The proposed system is able to achieve
significant improvements upon best-reported models under different settings."
"Backdoor attacks are an insidious security threat against machine learning
models. Adversaries can manipulate the predictions of compromised models by
inserting triggers into the training phase. Various backdoor attacks have been
devised which can achieve nearly perfect attack success without affecting model
predictions for clean inputs. Means of mitigating such vulnerabilities are
underdeveloped, especially in natural language processing. To fill this gap, we
introduce IMBERT, which uses either gradients or self-attention scores derived
from victim models to self-defend against backdoor attacks at inference time.
Our empirical studies demonstrate that IMBERT can effectively identify up to
98.5% of inserted triggers. Thus, it significantly reduces the attack success
rate while attaining competitive accuracy on the clean dataset across
widespread insertion-based attacks compared to two baselines. Finally, we show
that our approach is model-agnostic, and can be easily ported to several
pre-trained transformer models."
"As large language models, such as GPT, continue to advance the capabilities
of natural language processing (NLP), the question arises: does the problem of
correction still persist? This paper investigates the role of correction in the
context of large language models by conducting two experiments. The first
experiment focuses on correction as a standalone task, employing few-shot
learning techniques with GPT-like models for error correction. The second
experiment explores the notion of correction as a preparatory task for other
NLP tasks, examining whether large language models can tolerate and perform
adequately on texts containing certain levels of noise or errors. By addressing
these experiments, we aim to shed light on the significance of correction in
the era of large language models and its implications for various NLP
applications."
"Recent computational approaches for combating online hate speech involve the
automatic generation of counter narratives by adapting Pretrained
Transformer-based Language Models (PLMs) with human-curated data. This process,
however, can produce in-domain overfitting, resulting in models generating
acceptable narratives only for hatred similar to training data, with little
portability to other targets or to real-world toxic language. This paper
introduces novel attention regularization methodologies to improve the
generalization capabilities of PLMs for counter narratives generation.
Overfitting to training-specific terms is then discouraged, resulting in more
diverse and richer narratives. We experiment with two attention-based
regularization techniques on a benchmark English dataset. Regularized models
produce better counter narratives than state-of-the-art approaches in most
cases, both in terms of automatic metrics and human evaluation, especially when
hateful targets are not present in the training data. This work paves the way
for better and more flexible counter-speech generation models, a task for which
datasets are highly challenging to produce."
"The development of highly fluent large language models (LLMs) has prompted
increased interest in assessing their reasoning and problem-solving
capabilities. We investigate whether several LLMs can solve a classic type of
deductive reasoning problem from the cognitive science literature. The tested
LLMs have limited abilities to solve these problems in their conventional form.
We performed follow up experiments to investigate if changes to the
presentation format and content improve model performance. We do find
performance differences between conditions; however, they do not improve
overall performance. Moreover, we find that performance interacts with
presentation format and content in unexpected ways that differ from human
performance. Overall, our results suggest that LLMs have unique reasoning
biases that are only partially predicted from human reasoning performance and
the human-generated language corpora that informs them."
"There is increasing evidence that human activity in general, and narrative in
particular, can be treated as a dynamical system in the physics sense; a system
whose evolution is described by an action integral, such that the average of
all possible paths from point A to point B is given by the extremum of the
action. We create by construction three such paths by averaging about 500
different narratives, and we show that the average path is consistent with an
action principle."
"In recent years, large language models (LLMs) have shown remarkable
capabilities at scale, particularly at generating text conditioned on a prompt.
In our work, we investigate the use of LLMs to augment training data of small
language models~(SLMs) with automatically generated counterfactual~(CF)
instances -- i.e. minimally altered inputs -- in order to improve
out-of-domain~(OOD) performance of SLMs in the extractive question
answering~(QA) setup. We show that, across various LLM generators, such data
augmentation consistently enhances OOD performance and improves model
calibration for both confidence-based and rationale-augmented calibrator
models. Furthermore, these performance improvements correlate with higher
diversity of CF instances in terms of their surface form and semantic content.
Finally, we show that CF augmented models which are easier to calibrate also
exhibit much lower entropy when assigning importance, indicating that
rationale-augmented calibrators prefer concise explanations."
"Modern language models often exhibit powerful but brittle behavior, leading
to the development of larger and more diverse benchmarks to reliably assess
their behavior. Here, we suggest that model performance can be benchmarked and
elucidated with much smaller evaluation sets. We first show that in six popular
language classification benchmarks, model confidence in the correct class on
many pairs of points is strongly correlated across models. We build upon this
phenomenon to propose Anchor Point Selection, a technique to select small
subsets of datasets that capture model behavior across the entire dataset.
Anchor points reliably rank models: across 87 diverse language model-prompt
pairs, evaluating models using 1-30 anchor points outperforms uniform sampling
and other baselines at accurately ranking models. Moreover, just several anchor
points can be used to estimate model per-class predictions on all other points
in a dataset with low mean absolute error, sufficient for gauging where the
model is likely to fail. Lastly, we present Anchor Point Maps for visualizing
these insights and facilitating comparisons of the performance of different
models on various regions within the dataset distribution."
"Named Entity Recognition (NER) aims to extract and classify entity mentions
in the text into pre-defined types (e.g., organization or person name).
Recently, many works have been proposed to shape the NER as a machine reading
comprehension problem (also termed MRC-based NER), in which entity recognition
is achieved by answering the formulated questions related to pre-defined entity
types through MRC, based on the contexts. However, these works ignore the label
dependencies among entity types, which are critical for precisely recognizing
named entities. In this paper, we propose to incorporate the label dependencies
among entity types into a multi-task learning framework for better MRC-based
NER. We decompose MRC-based NER into multiple tasks and use a self-attention
module to capture label dependencies. Comprehensive experiments on both nested
NER and flat NER datasets are conducted to validate the effectiveness of the
proposed Multi-NER. Experimental results show that Multi-NER can achieve better
performance on all datasets."
"Large language models (LLMs) have demonstrated dominating performance in many
NLP tasks, especially on generative tasks. However, they often fall short in
some information extraction tasks, particularly those requiring domain-specific
knowledge, such as Biomedical Named Entity Recognition (NER). In this paper,
inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER
step-by-step: break down the NER task into entity span extraction and entity
type determination. Additionally, for entity type determination, we inject
entity knowledge to address the problem that LLM's lack of domain knowledge
when predicting entity category. Experimental results show a significant
improvement in our two-step BioNER approach compared to previous few-shot LLM
baseline. Additionally, the incorporation of external knowledge significantly
enhances entity category determination performance."
"Recently, the development of open-source large language models (LLMs) has
advanced rapidly. Nevertheless, due to data constraints, the capabilities of
most open-source LLMs are primarily focused on English. To address this issue,
we introduce the concept of $\textit{chat vector}$ to equip pre-trained
language models with instruction following and human value alignment via simple
model arithmetic. The chat vector is derived by subtracting the weights of a
pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model
(e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained
model's weights, we can endow the model with chat capabilities in new languages
without the need for further training. Our empirical studies demonstrate the
superior efficacy of the chat vector from three different aspects: instruction
following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase
the adaptability of our approach, we extend our experiments to encompass
various languages, base models, and chat vectors. The results underscore the
chat vector's simplicity, effectiveness, and wide applicability, making it a
compelling solution for efficiently enabling conversational capabilities in
pre-trained language models. Our code is available at
https://github.com/aqweteddy/ChatVector."
"Unlike alphabetic languages, Chinese spelling and pronunciation are
different. Both characters and pinyin take an important role in Chinese
language understanding. In Chinese NLP tasks, we almost adopt characters or
words as model input, and few works study how to use pinyin. However, pinyin is
essential in many scenarios, such as error correction and fault tolerance for
ASR-introduced errors. Most of these errors are caused by the same or similar
pronunciation words, and we refer to this type of error as SSP(the same or
similar pronunciation) errors for short. In this work, we explore various ways
of using pinyin in pretraining models and propose a new pretraining method
called PmBERT. Our method uses characters and pinyin in parallel for
pretraining. Through delicate pretraining tasks, the characters and pinyin
representation are fused, which can enhance the error tolerance for SSP errors.
We do comprehensive experiments and ablation tests to explore what makes a
robust phonetic enhanced Chinese language model. The experimental results on
both the constructed noise-added dataset and the public error-correction
dataset demonstrate that our model is more robust compared to SOTA models."
"Collection of annotated dialogs for training task-oriented dialog systems
have been one of the key bottlenecks in improving current models. While dialog
response generation has been widely studied on the agent side, it is not
evident if similar generative models can be used to generate a large variety
of, and often unexpected, user inputs that real dialog systems encounter in
practice. Existing data augmentation techniques such as paraphrase generation
do not take the dialog context into consideration. In this paper, we develop a
novel dialog augmentation model that generates a user turn, conditioning on
full dialog context. Additionally, with a new prompt design for language model,
and output re-ranking, the dialogs generated from our model can be directly
used to train downstream dialog systems. On common benchmark datasets MultiWoZ
and SGD, we show that our dialog augmentation model generates high quality
dialogs and improves dialog success rate by as much as $8\%$ over baseline."
"Multilingual language models enable zero-shot cross-lingual transfer
(ZS-XLT): fine-tuned on sizable source-language task data, they perform the
task in target languages without labeled instances. The effectiveness of ZS-XLT
hinges on the linguistic proximity between languages and the amount of
pretraining data for a language. Because of this, model selection based on
source-language validation is unreliable: it picks model snapshots with
suboptimal target-language performance. As a remedy, some work optimizes ZS-XLT
by extensively tuning hyperparameters: the follow-up work then routinely
struggles to replicate the original results. Other work searches over narrower
hyperparameter grids, reporting substantially lower performance. In this work,
we therefore propose an unsupervised evaluation protocol for ZS-XLT that
decouples performance maximization from hyperparameter tuning. As a robust and
more transparent alternative to extensive hyperparameter tuning, we propose to
accumulatively average snapshots from different runs into a single model. We
run broad ZS-XLT experiments on both higher-level semantic tasks (NLI,
extractive QA) and a lower-level token classification task (NER) and find that
conventional model selection based on source-language validation quickly
plateaus to suboptimal ZS-XLT performance. On the other hand, our accumulative
run-by-run averaging of models trained with different hyperparameters boosts
ZS-XLT performance and closely correlates with ""oracle"" ZS-XLT, i.e., model
selection based on target-language validation performance."
"Contrastive explanations, where one decision is explained in contrast to
another, are supposed to be closer to how humans explain a decision than
non-contrastive explanations, where the decision is not necessarily referenced
to an alternative. This claim has never been empirically validated. We analyze
four English text-classification datasets (SST2, DynaSent, BIOS and
DBpedia-Animals). We fine-tune and extract explanations from three different
models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three
post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore
collect and release human rationale annotations for a subset of 100 samples
from the BIOS dataset for contrastive and non-contrastive settings. A
cross-comparison between model-based rationales and human annotations, both in
contrastive and non-contrastive settings, yields a high agreement between the
two settings for models as well as for humans. Moreover, model-based
explanations computed in both settings align equally well with human
rationales. Thus, we empirically find that humans do not necessarily explain in
a contrastive manner.9 pages, long paper at ACL 2022 proceedings."
"Question generation is a widely used data augmentation approach with
extensive applications, and extracting qualified candidate answers from context
passages is a critical step for most question generation systems. However,
existing methods for candidate answer extraction are reliant on linguistic
rules or annotated data that face the partial annotation issue and challenges
in generalization. To overcome these limitations, we propose a novel
unsupervised candidate answer extraction approach that leverages the inherent
structure of context passages through a Differentiable Masker-Reconstructor
(DMR) Model with the enforcement of self-consistency for picking up salient
information tokens. We curated two datasets with exhaustively-annotated answers
and benchmark a comprehensive set of supervised and unsupervised candidate
answer extraction methods. We demonstrate the effectiveness of the DMR model by
showing its performance is superior among unsupervised methods and comparable
to supervised methods."
"Conversational Machine Reading (CMR) requires answering a user's initial
question through multi-turn dialogue interactions based on a given document.
Although there exist many effective methods, they largely neglected the
alignment between the document and the user-provided information, which
significantly affects the intermediate decision-making and subsequent follow-up
question generation. To address this issue, we propose a pipeline framework
that (1) aligns the aforementioned two sides in an explicit way, (2)makes
decisions using a lightweight many-to-many entailment reasoning module, and (3)
directly generates follow-up questions based on the document and previously
asked questions. Our proposed method achieves state-of-the-art in
micro-accuracy and ranks the first place on the public leaderboard of the CMR
benchmark dataset ShARC."
"Large language models (LLMs) are a promising avenue for machine translation
(MT). However, current LLM-based MT systems are brittle: their effectiveness
highly depends on the choice of few-shot examples and they often require extra
post-processing due to overgeneration. Alternatives such as finetuning on
translation instructions are computationally expensive and may weaken
in-context learning capabilities, due to overspecialization. In this paper, we
provide a closer look at this problem. We start by showing that adapter-based
finetuning with LoRA matches the performance of traditional finetuning while
reducing the number of training parameters by a factor of 50. This method also
outperforms few-shot prompting and eliminates the need for post-processing or
in-context examples. However, we show that finetuning generally degrades
few-shot performance, hindering adaptation capabilities. Finally, to obtain the
best of both worlds, we propose a simple approach that incorporates few-shot
examples during finetuning. Experiments on 10 language pairs show that our
proposed approach recovers the original few-shot capabilities while keeping the
added benefits of finetuning."
"This paper addresses the problem of generating questions from a given context
and an answer, specifically focusing on questions that require multi-hop
reasoning across an extended context. Previous studies have suggested that key
phrase selection is essential for question generation (QG), yet it is still
challenging to connect such disjointed phrases into meaningful questions,
particularly for long context. To mitigate this issue, we propose MultiFactor,
a novel QG framework based on multi-level content planning. Specifically,
MultiFactor includes two components: FA-model, which simultaneously selects key
phrases and generates full answers, and Q-model which takes the generated full
answer as an additional input to generate questions. Here, full answer
generation is introduced to connect the short answer with the selected key
phrases, thus forming an answer-aware summary to facilitate QG. Both FA-model
and Q-model are formalized as simple-yet-effective Phrase-Enhanced
Transformers, our joint model for phrase selection and text generation.
Experimental results show that our method outperforms strong baselines on two
popular QG datasets. Our code is available at
https://github.com/zeaver/MultiFactor."
"Table-based question answering (TableQA) is an important task in natural
language processing, which requires comprehending tables and employing various
reasoning ways to answer the questions. This paper introduces TableQAKit, the
first comprehensive toolkit designed specifically for TableQA. The toolkit
designs a unified platform that includes plentiful TableQA datasets and
integrates popular methods of this task as well as large language models
(LLMs). Users can add their datasets and methods according to the friendly
interface. Also, pleasantly surprised using the modules in this toolkit
achieves new SOTA on some datasets. Finally, \tableqakit{} also provides an
LLM-based TableQA Benchmark for evaluating the role of LLMs in TableQA.
TableQAKit is open-source with an interactive interface that includes visual
operations, and comprehensive data for ease of use."
"We introduce NoteChat, a novel cooperative multi-agent framework leveraging
Large Language Models (LLMs) to generate patient-physician dialogues. NoteChat
embodies the principle that an ensemble of role-specific LLMs, through
structured role-play and strategic prompting, can perform their assigned roles
more effectively. The synergy among these role-playing LLMs results in a
cohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a
benchmark dataset for patient-physician dialogues-note pairs, shows that models
trained with the augmented synthetic patient-physician dialogues by NoteChat
outperforms other state-of-the-art models for generating clinical notes. Our
comprehensive automatic and human evaluation demonstrates that NoteChat
substantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to
22.78% by domain experts in generating superior synthetic patient-physician
dialogues based on clinical notes. NoteChat has the potential to engage
patients directly and help clinical documentation, a leading cause of physician
burnout."
"Understanding sentence meanings and updating information states appropriately
across time -- what we call ""situational understanding"" (SU) -- is a critical
ability for human-like AI agents. SU is essential in particular for chat
models, such as ChatGPT, to enable consistent, coherent, and effective dialogue
between humans and AI. Previous works have identified certain SU limitations in
non-chatbot Large Language models (LLMs), but the extent and causes of these
limitations are not well understood, and capabilities of current chat-based
models in this domain have not been explored. In this work we tackle these
questions, proposing a novel synthetic environment for SU testing which allows
us to do controlled and systematic testing of SU in chat-oriented models,
through assessment of models' ability to track and enumerate environment
states. Our environment also allows for close analysis of dynamics of model
performance, to better understand underlying causes for performance patterns.
We apply our test to ChatGPT, the state-of-the-art chatbot, and find that
despite the fundamental simplicity of the task, the model's performance
reflects an inability to retain correct environment states across time. Our
follow-up analyses suggest that performance degradation is largely because
ChatGPT has non-persistent in-context memory (although it can access the full
dialogue history) and it is susceptible to hallucinated updates -- including
updates that artificially inflate accuracies. Our findings suggest overall that
ChatGPT is not currently equipped for robust tracking of situation states, and
that trust in the impressive dialogue performance of ChatGPT comes with risks.
We release the codebase for reproducing our test environment, as well as all
prompts and API responses from ChatGPT, at
https://github.com/yangalan123/SituationalTesting."
"Existing discourse formalisms use different taxonomies of discourse
relations, which require expert knowledge to understand, posing a challenge for
annotation and automatic classification. We show that discourse relations can
be effectively captured by some simple cognitively inspired dimensions proposed
by Sanders et al.(2018). Our experiments on cross-framework discourse relation
classification (PDTB & RST) demonstrate that it is possible to transfer
knowledge of discourse relations for one framework to another framework by
means of these dimensions, in spite of differences in discourse segmentation of
the two frameworks. This manifests the effectiveness of these dimensions in
characterizing discourse relations across frameworks. Ablation studies reveal
that different dimensions influence different types of discourse relations. The
patterns can be explained by the role of dimensions in characterizing and
distinguishing different relations. We also report our experimental results on
automatic prediction of these dimensions."
"In natural language processing, interactive text-based games serve as a test
bed for interactive AI systems. Prior work has proposed to play text-based
games by acting based on discrete knowledge graphs constructed by the Discrete
Graph Updater (DGU) to represent the game state from the natural language
description. While DGU has shown promising results with high interpretability,
it suffers from lower knowledge graph accuracy due to its lack of temporality
and limited generalizability to complex environments with objects with the same
label. In order to address DGU's weaknesses while preserving its high
interpretability, we propose the Temporal Discrete Graph Updater (TDGU), a
novel neural network model that represents dynamic knowledge graphs as a
sequence of timestamped graph events and models them using a temporal point
based graph neural network. Through experiments on the dataset collected from a
text-based game TextWorld, we show that TDGU outperforms the baseline DGU. We
further show the importance of temporal information for TDGU's performance
through an ablation study and demonstrate that TDGU has the ability to
generalize to more complex environments with objects with the same label. All
the relevant code can be found at
\url{https://github.com/yukw777/temporal-discrete-graph-updater}."
"In-context learning (ICL) ability has emerged with the increasing scale of
large language models (LLMs), enabling them to learn input-label mappings from
demonstrations and perform well on downstream tasks. However, under the
standard ICL setting, LLMs may sometimes neglect query-related information in
demonstrations, leading to incorrect predictions. To address this limitation,
we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to
explore the power of ICL in open-domain question answering, an important form
in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract
query-related knowledge from demonstrations, then concatenates the knowledge to
prompt LLMs in a more explicit way. Furthermore, we track the source of this
knowledge to identify specific examples, and introduce a Hint-related Example
Retriever (HER) to select informative examples for enhanced demonstrations. We
evaluate HICL with HER on 3 open-domain QA benchmarks, and observe average
performance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM
score and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting."
"The present study asks if ChatGPT4, the version of ChatGPT which uses the
language model GPT4, can successfully solve introductory linguistic exams.
Previous exam questions of an Introduction to Linguistics course at a German
university are used to test this. The exam questions were fed into ChatGPT4
with only minimal preprocessing. The results show that the language model is
very successful in the interpretation even of complex and nested tasks. It
proved surprisingly successful in the task of broad phonetic transcription, but
performed less well in the analysis of morphemes and phrases. In simple cases
it performs sufficiently well, but rarer cases, particularly with missing
one-to-one correspondence, are currently treated with mixed results. The model
is not yet able to deal with visualisations, such as the analysis or generation
of syntax trees. More extensive preprocessing, which translates these tasks
into text data, allow the model to also solve these tasks successfully."
"Translation quality estimation (TQE) is the task of predicting translation
quality without reference translations. Due to the enormous cost of creating
training data for TQE, only a few translation directions can benefit from
supervised training. To address this issue, unsupervised TQE methods have been
studied. In this paper, we extensively investigate the usefulness of synthetic
TQE data and pre-trained multilingual encoders in unsupervised sentence-level
TQE, both of which have been proven effective in the supervised training
scenarios. Our experiment on WMT20 and WMT21 datasets revealed that this
approach can outperform other unsupervised TQE methods on high- and
low-resource translation directions in predicting post-editing effort and human
evaluation score, and some zero-resource translation directions in predicting
post-editing effort."
"This paper reports on the use of prompt engineering and GPT-3.5 for
biomedical query-focused multi-document summarisation. Using GPT-3.5 and
appropriate prompts, our system achieves top ROUGE-F1 results in the task of
obtaining short-paragraph-sized answers to biomedical questions in the 2023
BioASQ Challenge (BioASQ 11b). This paper confirms what has been observed in
other domains: 1) Prompts that incorporated few-shot samples generally improved
on their counterpart zero-shot variants; 2) The largest improvement was
achieved by retrieval augmented generation. The fact that these prompts allow
our top runs to rank within the top two runs of BioASQ 11b demonstrate the
power of using adequate prompts for Large Language Models in general, and
GPT-3.5 in particular, for query-focused summarisation."
"Federated learning (FL) enables multiple participants to collaboratively
train machine learning models using decentralized data sources, alleviating
privacy concerns that arise from directly sharing local data. However, the lack
of model privacy protection in FL becomes an unneglectable challenge,
especially when people want to federally finetune models based on a proprietary
large language model. In this study, we propose a novel FL training approach
that accomplishes information exchange among participants via tunable soft
prompts. These soft prompts, updated and transmitted between the server and
clients, assume the role of the global model parameters and serve as messengers
to deliver useful knowledge from the local data and global model. As the global
model itself is not required to be shared and the local training is conducted
based on an auxiliary model with fewer parameters than the global model, the
proposed approach provides protection for the global model while reducing
communication and computation costs in FL. Extensive experiments show the
effectiveness of the proposed approach compared to several baselines. We have
released the source code at
\url{https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp}."
"The dynamic nature of knowledge in an ever-changing world presents challenges
for language models trained on static data; the model in the real world often
requires not only acquiring new knowledge but also overwriting outdated
information into updated ones. To study the ability of language models for
these time-dependent dynamics in human language, we introduce a novel task,
EvolvingQA, a temporally evolving question-answering benchmark designed for
training and evaluating LMs on an evolving Wikipedia database. The construction
of EvolvingQA is automated with our pipeline using large language models. We
uncover that existing continual learning baselines suffer from updating and
removing outdated knowledge. Our analysis suggests that models fail to rectify
knowledge due to small weight gradients. In addition, we elucidate that
language models particularly struggle to reflect the change of numerical or
temporal information. Our work aims to model the dynamic nature of real-world
information, suggesting faithful evaluations of the evolution-adaptability of
language models."
"While recent advancements in large language models (LLMs) bring us closer to
achieving artificial general intelligence, the question persists: Do LLMs truly
understand language, or do they merely mimic comprehension through pattern
recognition? This study seeks to explore this question through the lens of
syntax, a crucial component of sentence comprehension. Adopting a natural
language question-answering (Q&A) scheme, we craft questions targeting nine
syntactic knowledge points that are most closely related to sentence
comprehension. Experiments conducted on 24 LLMs suggest that most have a
limited grasp of syntactic knowledge, exhibiting notable discrepancies across
different syntactic knowledge points. In particular, questions involving
prepositional phrase attachment pose the greatest challenge, whereas those
concerning adjectival modifier and indirect object are relatively easier for
LLMs to handle. Furthermore, a case study on the training dynamics of the LLMs
reveals that the majority of syntactic knowledge is learned during the initial
stages of training, hinting that simply increasing the number of training
tokens may not be the `silver bullet' for improving the comprehension ability
of LLMs."
"To combat the misuse of Large Language Models (LLMs), many recent studies
have presented LLM-generated-text detectors with promising performance. When
users instruct LLMs to generate texts, the instruction can include different
constraints depending on the user's need. However, most recent studies do not
cover such diverse instruction patterns when creating datasets for LLM
detection. In this paper, we reveal that even task-oriented constraints --
constraints that would naturally be included in an instruction and are not
related to detection-evasion -- cause existing powerful detectors to have a
large variance in detection performance. We focus on student essay writing as a
realistic domain and manually create task-oriented constraints based on several
factors for essay quality. Our experiments show that the standard deviation
(SD) of current detector performance on texts generated by an instruction with
such a constraint is significantly larger (up to an SD of 14.4 F1-score) than
that by generating texts multiple times or paraphrasing the instruction. We
also observe an overall trend where the constraints can make LLM detection more
challenging than without them. Finally, our analysis indicates that the high
instruction-following ability of LLMs fosters the large impact of such
constraints on detection performance."
"Crossword puzzles are one of the most popular word games, played in different
languages all across the world, where riddle style can vary significantly from
one country to another. Automated crossword resolution is challenging, and
typical solvers rely on large databases of previously solved crosswords. In
this work, we extend WebCrow 2.0, an automatic crossword solver, to French,
making it the first program for crossword solving in the French language. To
cope with the lack of a large repository of clue-answer crossword data, WebCrow
2.0 exploits multiple modules, called experts, that retrieve candidate answers
from heterogeneous resources, such as the web, knowledge graphs, and linguistic
rules. We compared WebCrow's performance against humans in two different
challenges. Despite the limited amount of past crosswords, French WebCrow was
competitive, actually outperforming humans in terms of speed and accuracy, thus
proving its capabilities to generalize to new languages."
"Transformers have become the gold standard for many natural language
processing tasks and, in particular, for multi-hop question answering (MHQA).
This task includes processing a long document and reasoning over the multiple
parts of it. The landscape of MHQA approaches can be classified into two
primary categories. The first group focuses on extracting supporting evidence,
thereby constraining the QA model's context to predicted facts. Conversely, the
second group relies on the attention mechanism of the long input encoding model
to facilitate multi-hop reasoning. However, attention-based token
representations lack explicit global contextual information to connect
reasoning steps. To address these issues, we propose GEMFormer, a two-stage
method that first collects relevant information over the entire document to the
memory and then combines it with local context to solve the task. Our
experimental results show that fine-tuning a pre-trained model with
memory-augmented input, including the most certain global elements, improves
the model's performance on three MHQA datasets compared to the baseline. We
also found that the global explicit memory contains information from supporting
facts required for the correct answer."
"Foodborne illnesses significantly impact public health. Deep learning
surveillance applications using social media data aim to detect early warning
signals. However, labeling foodborne illness-related tweets for model training
requires extensive human resources, making it challenging to collect a
sufficient number of high-quality labels for tweets within a limited budget.
The severe class imbalance resulting from the scarcity of foodborne
illness-related tweets among the vast volume of social media further
exacerbates the problem. Classifiers trained on a class-imbalanced dataset are
biased towards the majority class, making accurate detection difficult. To
overcome these challenges, we propose EGAL, a deep learning framework for
foodborne illness detection that uses small expert-labeled tweets augmented by
crowdsourced-labeled and massive unlabeled data. Specifically, by leveraging
tweets labeled by experts as a reward set, EGAL learns to assign a weight of
zero to incorrectly labeled tweets to mitigate their negative influence. Other
tweets receive proportionate weights to counter-balance the unbalanced class
distribution. Extensive experiments on real-world \textit{TWEET-FID} data show
that EGAL outperforms strong baseline models across different settings,
including varying expert-labeled set sizes and class imbalance ratios. A case
study on a multistate outbreak of Salmonella Typhimurium infection linked to
packaged salad greens demonstrates how the trained model captures relevant
tweets offering valuable outbreak insights. EGAL, funded by the U.S. Department
of Agriculture (USDA), has the potential to be deployed for real-time analysis
of tweet streaming, contributing to foodborne illness outbreak surveillance
efforts."
"The vast majority of today's large language models (LLMs) are
English-centric, having been pretrained predominantly on English text. Yet, in
order to meet user expectations, models need to be able to respond
appropriately in multiple languages once deployed in downstream applications.
This requires strong cross-lingual transfer abilities. In this work, we
investigate the minimal amount of multilinguality required during finetuning to
elicit cross-lingual generalisation in English-centric LLMs. In experiments
across four LLMs, we find that multilingual instruction tuning with as few as
two to three languages is both necessary and sufficient to elicit effective
cross-lingual generalisation, with the limiting factor being the degree to
which a target language is seen during pretraining. Evaluations on five
different tasks further reveal that multilingual instruction tuning is most
beneficial for generative tasks that assume input/output language agreement,
such as in chat settings, while being of less importance for highly structured
classification-style tasks. Our code and data is available at
https://github.com/ZurichNLP/multilingual-instruction-tuning."
"Abbreviation expansion is a strategy used to speed up communication by
limiting the amount of typing and using a language model to suggest expansions.
Here we look at personalizing a Large Language Model's (LLM) suggestions based
on prior conversations to enhance the relevance of predictions, particularly
when the user data is small (~1000 samples). Specifically, we compare
fine-tuning, prompt-tuning, and retrieval augmented generation of expanded text
suggestions for abbreviated inputs. Our case study with a deployed 8B parameter
LLM on a real user living with ALS, and experiments on movie character
personalization indicates that (1) customization may be necessary in some
scenarios and prompt-tuning generalizes well to those, (2) fine-tuning on
in-domain data (with as few as 600 samples) still shows some gains, however (3)
retrieval augmented few-shot selection also outperforms fine-tuning. (4)
Parameter efficient tuning allows for efficient and scalable personalization.
For prompt-tuning, we also find that initializing the learned ""soft-prompts"" to
user relevant concept tokens leads to higher accuracy than random
initialization."
"Moderate-sized large language models (LLMs) -- those with 7B or 13B
parameters -- exhibit promising machine translation (MT) performance. However,
even the top-performing 13B LLM-based translation models, like ALMA, does not
match the performance of state-of-the-art conventional encoder-decoder
translation models or larger-scale LLMs such as GPT-4. In this study, we bridge
this performance gap. We first assess the shortcomings of supervised
fine-tuning for LLMs in the MT task, emphasizing the quality issues present in
the reference data, despite being human-generated. Then, in contrast to SFT
which mimics reference translations, we introduce Contrastive Preference
Optimization (CPO), a novel approach that trains models to avoid generating
adequate but not perfect translations. Applying CPO to ALMA models with only
22K parallel sentences and 12M parameters yields significant improvements. The
resulting model, called ALMA-R, can match or exceed the performance of the WMT
competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets."
"Accurately gauging the confidence level of Large Language Models' (LLMs)
predictions is pivotal for their reliable application. However, LLMs are often
uncalibrated inherently and elude conventional calibration techniques due to
their proprietary nature and massive scale. In this work, we explore the
potential of deriving confidence from the distribution of multiple randomly
sampled model generations, via three measures of consistency. We perform an
extensive evaluation across various open and closed-source models on nine
reasoning datasets. Results show that consistency-based calibration methods
outperform existing post-hoc approaches. Meanwhile, we find that factors such
as intermediate explanations, model scaling, and larger sample sizes enhance
calibration, while instruction-tuning makes calibration more difficult.
Moreover, confidence scores obtained from consistency have the potential to
enhance model performance. Finally, we offer practical guidance on choosing
suitable consistency metrics for calibration, tailored to the characteristics
of various LMs."
"We study existing approaches to leverage off-the-shelf Natural Language
Inference (NLI) models for the evaluation of summary faithfulness and argue
that these are sub-optimal due to the granularity level considered for premises
and hypotheses. That is, the smaller content unit considered as hypothesis is a
sentence and premises are made up of a fixed number of document sentences. We
propose a novel approach, namely InFusE, that uses a variable premise size and
simplifies summary sentences into shorter hypotheses. Departing from previous
studies which focus on single short document summarisation, we analyse NLI
based faithfulness evaluation for diverse summarisation tasks. We introduce
DiverSumm, a new benchmark comprising long form summarisation (long documents
and summaries) and diverse summarisation tasks (e.g., meeting and
multi-document summarisation). In experiments, InFusE obtains superior
performance across the different summarisation tasks. Our code and data are
available at https://github.com/HJZnlp/infuse."
"Model editing has emerged as a cost-effective strategy to update knowledge
stored in language models. However, model editing can have unintended
consequences after edits are applied: information unrelated to the edits can
also be changed, and other general behaviors of the model can be wrongly
altered. In this work, we investigate how model editing methods unexpectedly
amplify model biases post-edit. We introduce a novel benchmark dataset,
Seesaw-CF, for measuring bias-related harms of model editing and conduct the
first in-depth investigation of how different weight-editing methods impact
model bias. Specifically, we focus on biases with respect to demographic
attributes such as race, geographic origin, and gender, as well as qualitative
flaws in long-form texts generated by edited language models. We find that
edited models exhibit, to various degrees, more biased behavior as they become
less confident in attributes for Asian, African, and South American subjects.
Furthermore, edited models amplify sexism and xenophobia in text generations
while remaining seemingly coherent and logical. Finally, editing facts about
place of birth, country of citizenship, or gender have particularly negative
effects on the model's knowledge about unrelated features like field of work."
"This paper introduces a novel annotation framework for the fine-grained
modeling of Noun Phrases' (NPs) genericity in natural language. The framework
is designed to be simple and intuitive, making it accessible to non-expert
annotators and suitable for crowd-sourced tasks. Drawing from theoretical and
cognitive literature on genericity, this framework is grounded in established
linguistic theory. Through a pilot study, we created a small but crucial
annotated dataset of 324 sentences, serving as a foundation for future
research. To validate our approach, we conducted an evaluation comparing our
continuous annotations with existing binary annotations on the same dataset,
demonstrating the framework's effectiveness in capturing nuanced aspects of
genericity. Our work offers a practical resource for linguists, providing a
first annotated dataset and an annotation scheme designed to build
real-language datasets that can be used in studies on the semantics of
genericity, and NLP practitioners, contributing to the development of
commonsense knowledge repositories valuable in enhancing various NLP
applications."
"Geoparsing is the task of estimating the latitude and longitude (coordinates)
of location expressions in texts. Geoparsing must deal with the ambiguity of
the expressions that indicate multiple locations with the same notation. For
evaluating geoparsing systems, several corpora have been proposed in previous
work. However, these corpora are small-scale and suffer from the coverage of
location expressions on general domains. In this paper, we propose Wikipedia
Hyperlink-based Location Linking (WHLL), a novel method to construct a
large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages
hyperlinks in Wikipedia to annotate multiple location expressions with
coordinates. With this method, we constructed the WHLL corpus, a new
large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles,
each containing about 7.8 unique location expressions. 45.6% of location
expressions are ambiguous and refer to more than one location with the same
notation. In each article, location expressions of the article title and those
hyperlinks to other articles are assigned with coordinates. By utilizing
hyperlinks, we can accurately assign location expressions with coordinates even
with ambiguous location expressions in the texts. Experimental results show
that there remains room for improvement by disambiguating location expressions."
"Labeling corpora constitutes a bottleneck to create models for new tasks or
domains. Large language models mitigate the issue with automatic corpus
labeling methods, particularly for categorical annotations. Some NLP tasks such
as emotion intensity prediction, however, require text regression, but there is
no work on automating annotations for continuous label assignments. Regression
is considered more challenging than classification: The fact that humans
perform worse when tasked to choose values from a rating scale lead to
comparative annotation methods, including best-worst scaling. This raises the
question if large language model-based annotation methods show similar
patterns, namely that they perform worse on rating scale annotation tasks than
on comparative annotation tasks. To study this, we automate emotion intensity
predictions and compare direct rating scale predictions, pairwise comparisons
and best-worst scaling. We find that the latter shows the highest reliability.
A transformer regressor fine-tuned on these data performs nearly on par with a
model trained on the original manual annotations."
"Recent advancements highlight the success of instruction tuning with large
language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical
reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as
incorrect, missing, and redundant steps in CoT generation leading to
inaccuracies in answer predictions. To alleviate this problem, we propose a
dual instruction tuning strategy to meticulously model mathematical reasoning
from both forward and reverse directions. This involves introducing the
Intermediate Reasoning State Prediction task (forward reasoning) and the
Instruction Reconstruction task (reverse reasoning) to enhance the LLMs'
understanding and execution of instructions. Training instances for these tasks
are constructed based on existing mathematical instruction tuning datasets.
Subsequently, LLMs undergo multi-task fine-tuning using both existing
mathematical instructions and the newly created data. Comprehensive experiments
validate the effectiveness and domain generalization of the dual instruction
tuning strategy across various mathematical reasoning tasks."
"Unsupervised constrained text generation aims to generate text under a given
set of constraints without any supervised data. Current state-of-the-art
methods stochastically sample edit positions and actions, which may cause
unnecessary search steps. In this paper, we propose PMCTG to improve
effectiveness by searching for the best edit position and action in each step.
Specifically, PMCTG extends perturbed masking technique to effectively search
for the most incongruent token to edit. Then it introduces four multi-aspect
scoring functions to select edit action to further reduce search difficulty.
Since PMCTG does not require supervised data, it could be applied to different
generation tasks. We show that under the unsupervised setting, PMCTG achieves
new state-of-the-art results in two representative tasks, namely
keywords-to-sentence generation and paraphrasing."
"Searching dependency graphs and manipulating them can be a time consuming and
challenging task to get right. We document Semgrex, a system for searching
dependency graphs, and introduce Ssurgeon, a system for manipulating the output
of Semgrex. The compact language used by these systems allows for easy command
line or API processing of dependencies. Additionally, integration with publicly
released toolkits in Java and Python allows for searching text relations and
attributes over natural text."
"Computational historical linguistics seeks to systematically understand
processes of sound change, including during periods at which little to no
formal recording of language is attested. At the same time, few computational
resources exist which deeply explore phonological and morphological connections
between proto-languages and their descendants. This is particularly true for
the family of Italic languages. To assist historical linguists in the study of
Italic sound change, we introduce the Proto-Italic to Latin (PILA) dataset,
which consists of roughly 3,000 pairs of forms from Proto-Italic and Latin. We
provide a detailed description of how our dataset was created and organized.
Then, we exhibit PILA's value in two ways. First, we present baseline results
for PILA on a pair of traditional computational historical linguistics tasks.
Second, we demonstrate PILA's capability for enhancing other
historical-linguistic datasets through a dataset compatibility study."
"Instruction tuning has been proven effective in enhancing zero-shot
generalization across various tasks and in improving the performance of
specific tasks. For task-specific improvements, strategically selecting and
training on related tasks that provide meaningful supervision is crucial, as
this approach enhances efficiency and prevents performance degradation from
learning irrelevant tasks. In this light, we introduce a simple yet effective
task selection method that leverages instruction information alone to identify
relevant tasks, optimizing instruction tuning for specific tasks. Our method is
significantly more efficient than traditional approaches, which require complex
measurements of pairwise transferability between tasks or the creation of data
samples for the target task. Additionally, by aligning the model with the
unique instructional template style of the meta-dataset, we enhance its ability
to granularly discern relevant tasks, leading to improved overall performance.
Experimental results demonstrate that training on a small set of tasks, chosen
solely based on the instructions, results in substantial improvements in
performance on benchmarks such as P3, Big-Bench, NIV2, and Big-Bench Hard.
Significantly, these improvements surpass those achieved by prior task
selection methods, highlighting the superiority of our approach."
"Language models now constitute essential tools for improving efficiency for
many professional tasks such as writing, coding, or learning. For this reason,
it is imperative to identify inherent biases. In the field of Natural Language
Processing, five sources of bias are well-identified: data, annotation,
representation, models, and research design. This study focuses on biases
related to geographical knowledge. We explore the connection between geography
and language models by highlighting their tendency to misrepresent spatial
information, thus leading to distortions in the representation of geographical
distances. This study introduces four indicators to assess these distortions,
by comparing geographical and semantic distances. Experiments are conducted
from these four indicators with ten widely used language models. Results
underscore the critical necessity of inspecting and rectifying spatial biases
in language models to ensure accurate and equitable representations."
"Perceptions of hate can vary greatly across cultural contexts. Hate speech
(HS) datasets, however, have traditionally been developed by language. This
hides potential cultural biases, as one language may be spoken in different
countries home to different cultures. In this work, we evaluate cultural bias
in HS datasets by leveraging two interrelated cultural proxies: language and
geography. We conduct a systematic survey of HS datasets in eight languages and
confirm past findings on their English-language bias, but also show that this
bias has been steadily decreasing in the past few years. For three
geographically-widespread languages -- English, Arabic and Spanish -- we then
leverage geographical metadata from tweets to approximate geo-cultural contexts
by pairing language and country information. We find that HS datasets for these
languages exhibit a strong geo-cultural bias, largely overrepresenting a
handful of countries (e.g., US and UK for English) relative to their prominence
in both the broader social media population and the general population speaking
these languages. Based on these findings, we formulate recommendations for the
creation of future HS datasets."
"We introduce Holmes, a new benchmark designed to assess language models (LMs)
linguistic competence - their unconscious understanding of linguistic
phenomena. Specifically, we use classifier-based probing to examine LMs'
internal representations regarding distinct linguistic phenomena (e.g.,
part-of-speech tagging). As a result, we meet recent calls to disentangle LMs'
linguistic competence from other cognitive abilities, such as following
instructions in prompting-based evaluations. Composing Holmes, we review over
270 probing studies and include more than 200 datasets to assess syntax,
morphology, semantics, reasoning, and discourse phenomena. Analyzing over 50
LMs reveals that, aligned with known trends, their linguistic competence
correlates with model size. However, surprisingly, model architecture and
instruction tuning also significantly influence performance, particularly in
morphology and syntax. Finally, we propose FlashHolmes, a streamlined version
that reduces the computation load while maintaining high-ranking precision."
"In this work we investigate the capability of Graph Attention Network for
extracting aspect and opinion terms. Aspect and opinion term extraction is
posed as a token-level classification task akin to named entity recognition. We
use the dependency tree of the input query as additional feature in a Graph
Attention Network along with the token and part-of-speech features. We show
that the dependency structure is a powerful feature that in the presence of a
CRF layer substantially improves the performance and generates the best result
on the commonly used datasets from SemEval 2014, 2015 and 2016. We experiment
with additional layers like BiLSTM and Transformer in addition to the CRF
layer. We also show that our approach works well in the presence of multiple
aspects or sentiments in the same query and it is not necessary to modify the
dependency tree based on a single aspect as was the original application for
sentiment classification."
"Compared to standard language model (LM) pretraining (i.e., from scratch),
Knowledge Distillation (KD) entails an additional forward pass through a
teacher model that is typically substantially larger than the target student
model. As such, KD in LM pretraining materially slows down throughput of
pretraining instances vis-a-vis pretraining from scratch. Scaling laws of LM
pretraining suggest that smaller models can close the gap to larger
counterparts if trained on more data (i.e., processing more tokens)-and under a
fixed computation budget, smaller models are able be process more data than
larger models. We thus hypothesize that KD might, in fact, be suboptimal to
pretraining from scratch for obtaining smaller LMs, when appropriately
accounting for the compute budget. To test this, we compare pretraining from
scratch against several KD strategies for masked language modeling (MLM) in a
fair experimental setup, with respect to amount of computation as well as
pretraining data. Downstream results on GLUE, however, do not confirm our
hypothesis: while pretraining from scratch performs comparably to ordinary KD
under a fixed computation budget, more sophisticated KD strategies, namely
TinyBERT (Jiao et al., 2020) and MiniLM (Wang et al., 2023), outperform it by a
notable margin. We further find that KD yields larger gains over pretraining
from scratch when the data must be repeated under the fixed computation budget."
"Customer service is how companies interface with their customers. It can
contribute heavily towards the overall customer satisfaction. However,
high-quality service can become expensive, creating an incentive to make it as
cost efficient as possible and prompting most companies to utilize AI-powered
assistants, or ""chat bots"". On the other hand, human-to-human interaction is
still desired by customers, especially when it comes to complex scenarios such
as disputes and sensitive topics like bill payment.
  This raises the bar for customer service agents. They need to accurately
understand the customer's question or concern, identify a solution that is
acceptable yet feasible (and within the company's policy), all while handling
multiple conversations at once.
  In this work, we introduce ""Ask Me Anything"" (AMA) as an add-on feature to an
agent-facing customer service interface. AMA allows agents to ask questions to
a large language model (LLM) on demand, as they are handling customer
conversations -- the LLM provides accurate responses in real-time, reducing the
amount of context switching the agent needs. In our internal experiments, we
find that agents using AMA versus a traditional search experience spend
approximately 10% fewer seconds per conversation containing a search,
translating to millions of dollars of savings annually. Agents that used the
AMA feature provided positive feedback nearly 80% of the time, demonstrating
its usefulness as an AI-assisted feature for customer care."
"The rapid emergence of generative Language Models (LMs) has led to growing
concern about the impacts that their unexamined adoption may have on the social
well-being of diverse user groups. Meanwhile, LMs are increasingly being
adopted in K-20 schools and one-on-one student settings with minimal
investigation of potential harms associated with their deployment. Motivated in
part by real-world/everyday use cases (e.g., an AI writing assistant) this
paper explores the potential psychosocial harms of stories generated by five
leading LMs in response to open-ended prompting. We extend findings of
stereotyping harms analyzing a total of 150K 100-word stories related to
student classroom interactions. Examining patterns in LM-generated character
demographics and representational harms (i.e., erasure, subordination, and
stereotyping) we highlight particularly egregious vignettes, illustrating the
ways LM-generated outputs may influence the experiences of users with
marginalized and minoritized identities, and emphasizing the need for a
critical understanding of the psychosocial impacts of generative AI tools when
deployed and utilized in diverse social contexts."
"Detecting stereotypes and biases in Large Language Models (LLMs) is crucial
for enhancing fairness and reducing adverse impacts on individuals or groups
when these models are applied. Traditional methods, which rely on embedding
spaces or are based on probability metrics, fall short in revealing the nuanced
and implicit biases present in various contexts. To address this challenge, we
propose the FairMonitor framework and adopt a static-dynamic detection method
for a comprehensive evaluation of stereotypes and biases in LLMs. The static
component consists of a direct inquiry test, an implicit association test, and
an unknown situation test, including 10,262 open-ended questions with 9
sensitive factors and 26 educational scenarios. And it is effective for
evaluating both explicit and implicit biases. Moreover, we utilize the
multi-agent system to construst the dynamic scenarios for detecting subtle
biases in more complex and realistic setting. This component detects the biases
based on the interaction behaviors of LLMs across 600 varied educational
scenarios. The experimental results show that the cooperation of static and
dynamic methods can detect more stereotypes and biased in LLMs."
"Scientific document summarization has been a challenging task due to the long
structure of the input text. The long input hinders the simultaneous effective
modeling of both global high-order relations between sentences and local
intra-sentence relations which is the most critical step in extractive
summarization. However, existing methods mostly focus on one type of relation,
neglecting the simultaneous effective modeling of both relations, which can
lead to insufficient learning of semantic representations. In this paper, we
propose HAESum, a novel approach utilizing graph neural networks to locally and
globally model documents based on their hierarchical discourse structure.
First, intra-sentence relations are learned using a local heterogeneous graph.
Subsequently, a novel hypergraph self-attention layer is introduced to further
enhance the characterization of high-order inter-sentence relations. We
validate our approach on two benchmark datasets, and the experimental results
demonstrate the effectiveness of HAESum and the importance of considering
hierarchical structures in modeling long scientific documents. Our code will be
available at \url{https://github.com/MoLICHENXI/HAESum}"
"Text-to-Table aims to generate structured tables to convey the key
information from unstructured documents. Existing text-to-table datasets are
typically oriented English, limiting the research in non-English languages.
Meanwhile, the emergence of large language models (LLMs) has shown great
success as general task solvers in multi-lingual settings (e.g., ChatGPT),
theoretically enabling text-to-table in other languages. In this paper, we
propose a Chinese text-to-table dataset, CT-Eval, to benchmark LLMs on this
task. Our preliminary analysis of English text-to-table datasets highlights two
key factors for dataset construction: data diversity and data hallucination.
Inspired by this, the CT-Eval dataset selects a popular Chinese
multidisciplinary online encyclopedia as the source and covers 28 domains to
ensure data diversity. To minimize data hallucination, we first train an LLM to
judge and filter out the task samples with hallucination, then employ human
annotators to clean the hallucinations in the validation and testing sets.
After this process, CT-Eval contains 88.6K task samples. Using CT-Eval, we
evaluate the performance of open-source and closed-source LLMs. Our results
reveal that zero-shot LLMs (including GPT-4) still have a significant
performance gap compared with human judgment. Furthermore, after fine-tuning,
open-source LLMs can significantly improve their text-to-table ability,
outperforming GPT-4 by a large margin. In short, CT-Eval not only helps
researchers evaluate and quickly understand the Chinese text-to-table ability
of existing LLMs but also serves as a valuable resource to significantly
improve the text-to-table performance of LLMs."
"Enterprise retrieval augmented generation (RAG) offers a highly flexible
framework for combining powerful large language models (LLMs) with internal,
possibly temporally changing, documents. In RAG, documents are first chunked.
Relevant chunks are then retrieved for a user query, which are passed as
context to a synthesizer LLM to generate the query response. However, the
retrieval step can limit performance, as incorrect chunks can lead the
synthesizer LLM to generate a false response. This work applies a zero-shot
adaptation of standard dense retrieval steps for more accurate chunk recall.
Specifically, a chunk is first decomposed into atomic statements. A set of
synthetic questions are then generated on these atoms (with the chunk as the
context). Dense retrieval involves finding the closest set of synthetic
questions, and associated chunks, to the user query. It is found that retrieval
with the atoms leads to higher recall than retrieval with chunks. Further
performance gain is observed with retrieval using the synthetic questions
generated over the atoms. Higher recall at the retrieval step enables higher
performance of the enterprise LLM using the RAG pipeline."
"Self-correction is an approach to improving responses from large language
models (LLMs) by refining the responses using LLMs during inference. Prior work
has proposed various self-correction frameworks using different sources of
feedback, including self-evaluation and external feedback. However, there is
still no consensus on the question of when LLMs can correct their own mistakes,
as recent studies also report negative results. In this work, we critically
survey broad papers and discuss the conditions required for successful
self-correction. We first find that prior studies often do not define their
research questions in detail and involve impractical frameworks or unfair
evaluations that over-evaluate self-correction. To tackle these issues, we
categorize research questions in self-correction research and provide a
checklist for designing appropriate experiments. Our critical survey based on
the newly categorized research questions shows that (1) no prior work
demonstrates successful self-correction with feedback from prompted LLMs,
except for studies in tasks that are exceptionally suited for self-correction,
(2) self-correction works well in tasks that can use reliable external
feedback, and (3) large-scale fine-tuning enables self-correction."
"Chinese Spelling Check (CSC) aims to detect and correct potentially
misspelled characters in Chinese sentences. Naturally, it involves the
detection and correction subtasks, which interact with each other dynamically.
Such interactions are bi-directional, i.e., the detection result would help
reduce the risk of over-correction and under-correction while the knowledge
learnt from correction would help prevent false detection. Current CSC
approaches are of two types: correction-only or single-directional
detection-to-correction interactive frameworks. Nonetheless, they overlook the
bi-directional interactions between detection and correction. This paper aims
to fill the gap by proposing a Bi-directional Detector-Corrector framework for
CSC (Bi-DCSpell). Notably, Bi-DCSpell contains separate detection and
correction encoders, followed by a novel interactive learning module
facilitating bi-directional feature interactions between detection and
correction to improve each other's representation learning. Extensive
experimental results demonstrate a robust correction performance of Bi-DCSpell
on widely used benchmarking datasets while possessing a satisfactory detection
ability."
"New Intent Discovery (NID) aims at detecting known and previously undefined
categories of user intent by utilizing limited labeled and massive unlabeled
data. Most prior works often operate under the unrealistic assumption that the
distribution of both familiar and new intent classes is uniform, overlooking
the skewed and long-tailed distributions frequently encountered in real-world
scenarios. To bridge the gap, our work introduces the imbalanced new intent
discovery (i-NID) task, which seeks to identify familiar and novel intent
categories within long-tailed distributions. A new benchmark (ImbaNID-Bench)
comprised of three datasets is created to simulate the real-world long-tail
distributions. ImbaNID-Bench ranges from broad cross-domain to specific
single-domain intent categories, providing a thorough representation of
practical use cases. Besides, a robust baseline model ImbaNID is proposed to
achieve cluster-friendly intent representations. It includes three stages:
model pre-training, generation of reliable pseudo-labels, and robust
representation learning that strengthens the model performance to handle the
intricacies of real-world data distributions. Our extensive experiments on
previous benchmarks and the newly established benchmark demonstrate the
superior performance of ImbaNID in addressing the i-NID task, highlighting its
potential as a powerful baseline for uncovering and categorizing user intents
in imbalanced and long-tailed
distributions\footnote{\url{https://github.com/Zkdc/i-NID}}."
"Large Language Models (LLMs) often do not perform well on queries that
require the aggregation of information across texts. To better evaluate this
setting and facilitate modeling efforts, we introduce TACT - Text And
Calculations through Tables, a dataset crafted to evaluate LLMs' reasoning and
computational abilities using complex instructions. TACT contains challenging
instructions that demand stitching information scattered across one or more
texts, and performing complex integration on this information to generate the
answer. We construct this dataset by leveraging an existing dataset of texts
and their associated tables. For each such tables, we formulate new queries,
and gather their respective answers. We demonstrate that all contemporary LLMs
perform poorly on this dataset, achieving an accuracy below 38%. To pinpoint
the difficulties and thoroughly dissect the problem, we analyze model
performance across three components: table-generation, Pandas
command-generation, and execution. Unexpectedly, we discover that each
component presents substantial challenges for current LLMs. These insights lead
us to propose a focused modeling framework, which we refer to as IE as a tool.
Specifically, we propose to add ""tools"" for each of the above steps, and
implement each such tool with few-shot prompting. This approach shows an
improvement over existing prompting techniques, offering a promising direction
for enhancing model capabilities in these tasks."
"This study explores the sycophantic tendencies of Large Language Models
(LLMs), where these models tend to provide answers that match what users want
to hear, even if they are not entirely correct. The motivation behind this
exploration stems from the common behavior observed in individuals searching
the internet for facts with partial or misleading knowledge. Similar to using
web search engines, users may recall fragments of misleading keywords and
submit them to an LLM, hoping for a comprehensive response. Our empirical
analysis of several LLMs shows the potential danger of these models amplifying
misinformation when presented with misleading keywords. Additionally, we
thoroughly assess four existing hallucination mitigation strategies to reduce
LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of
these strategies for generating factually correct statements. Furthermore, our
analyses delve into knowledge-probing experiments on factual keywords and
different categories of sycophancy mitigation."
"Large language models (LLMs) exhibit robust capabilities in text generation
and comprehension, mimicking human behavior and exhibiting synthetic
personalities. However, some LLMs have displayed offensive personality,
propagating toxic discourse. Existing literature neglects the origin and
evolution of LLM personalities, as well as the effective personality control.
To fill these gaps, our study embarked on a comprehensive investigation into
LLM personality control. We investigated several typical methods to influence
LLMs, including three training methods: Continual Pre-training, Supervised
Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF), along
with inference phase considerations (prompts). Our investigation revealed a
hierarchy of effectiveness in control: Prompt > SFT > RLHF > Continual
Pre-train. Notably, SFT exhibits a higher control success rate compared to
prompt induction. While prompts prove highly effective, we found that
prompt-induced personalities are less robust than those trained, making them
more prone to showing conflicting personalities under reverse personality
prompt induction. Besides, harnessing the strengths of both SFT and prompt, we
proposed $\underline{\text{P}}$rompt $\underline{\text{I}}$nduction post
$\underline{\text{S}}$upervised $\underline{\text{F}}$ine-tuning (PISF), which
emerges as the most effective and robust strategy for controlling LLMs'
personality, displaying high efficacy, high success rates, and high robustness.
Even under reverse personality prompt induction, LLMs controlled by PISF still
exhibit stable and robust personalities."
"Most large language models (LLMs) are sensitive to prompts, and another
synonymous expression or a typo may lead to unexpected results for the model.
Composing an optimal prompt for a specific demand lacks theoretical support and
relies entirely on human experimentation, which poses a considerable obstacle
to popularizing generative artificial intelligence. However, there is no
systematic analysis of the stability of LLMs in resisting prompt perturbations
in real-world scenarios. In this work, we propose to evaluate the ease-of-use
of LLMs and construct E-Bench, simulating the actual situation of human use
from synonymous perturbation (including paraphrasing, simplification, and
colloquialism) and typographical perturbation (such as typing). On this basis,
we also discuss the combination of these two types of perturbation and analyze
the main reasons for performance degradation. Experimental results indicate
that with the increase of model size, although the ease-of-use are
significantly improved, there is still a long way to go to build a sufficiently
user-friendly model."
"Static word embeddings are ubiquitous in computational social science
applications and contribute to practical decision-making in a variety of fields
including law and healthcare. However, assessing the statistical uncertainty in
downstream conclusions drawn from word embedding statistics has remained
challenging. When using only point estimates for embeddings, researchers have
no streamlined way of assessing the degree to which their model selection
criteria or scientific conclusions are subject to noise due to sparsity in the
underlying data used to generate the embeddings. We introduce a method to
obtain approximate, easy-to-use, and scalable reconstruction error variance
estimates for GloVe (Pennington et al., 2014), one of the most widely used word
embedding models, using an analytical approximation to a multivariate normal
model. To demonstrate the value of embeddings with variance (GloVe-V), we
illustrate how our approach enables principled hypothesis testing in core word
embedding tasks, such as comparing the similarity between different word pairs
in vector space, assessing the performance of different models, and analyzing
the relative degree of ethnic or gender bias in a corpus using different word
lists."
"Emotion-Cause analysis has attracted the attention of researchers in recent
years. However, most existing datasets are limited in size and number of
emotion categories. They often focus on extracting parts of the document that
contain the emotion cause and fail to provide more abstractive, generalizable
root cause. To bridge this gap, we introduce a large-scale dataset of emotion
causes, derived from 9.8 million cleaned tweets over 15 years. We describe our
curation process, which includes a comprehensive pipeline for data gathering,
cleaning, labeling, and validation, ensuring the dataset's reliability and
richness. We extract emotion labels and provide abstractive summarization of
the events causing emotions. The final dataset comprises over 700,000 tweets
with corresponding emotion-cause pairs spanning 48 emotion classes, validated
by human evaluators. The novelty of our dataset stems from its broad spectrum
of emotion classes and the abstractive emotion cause that facilitates the
development of an emotion-cause knowledge graph for nuanced reasoning. Our
dataset will enable the design of emotion-aware systems that account for the
diverse emotional responses of different people for the same event."
"Synthetic data has been proposed as a solution to address the issue of
high-quality data scarcity in the training of large language models (LLMs).
Studies have shown that synthetic data can effectively improve the performance
of LLMs on downstream benchmarks. However, despite its potential benefits, our
analysis suggests that there may be inherent flaws in synthetic data. The
uniform format of synthetic data can lead to pattern overfitting and cause
significant shifts in the output distribution, thereby reducing the model's
instruction-following capabilities. Our work delves into these specific flaws
associated with question-answer (Q-A) pairs, a prevalent type of synthetic
data, and presents a method based on unlearning techniques to mitigate these
flaws. The empirical results demonstrate the effectiveness of our approach,
which can reverse the instruction-following issues caused by pattern
overfitting without compromising performance on benchmarks at relatively low
cost. Our work has yielded key insights into the effective use of synthetic
data, aiming to promote more robust and efficient LLM training."
"We present an open-source web service for Czech morphosyntactic analysis. The
system combines a deep learning model with rescoring by a high-precision
morphological dictionary at inference time. We show that our hybrid method
surpasses two competitive baselines: While the deep learning model ensures
generalization for out-of-vocabulary words and better disambiguation, an
improvement over an existing morphological analyser MorphoDiTa, at the same
time, the deep learning model benefits from inference-time guidance of a
manually curated morphological dictionary. We achieve 50% error reduction in
lemmatization and 58% error reduction in POS tagging over MorphoDiTa, while
also offering dependency parsing. The model is trained on one of the currently
largest Czech morphosyntactic corpora, the PDT-C 1.0, with the trained models
available at https://hdl.handle.net/11234/1-5293. We provide the tool as a web
service deployed at https://lindat.mff.cuni.cz/services/udpipe/. The source
code is available at GitHub (https://github.com/ufal/udpipe/tree/udpipe-2),
along with a Python client for a simple use. The documentation for the models
can be found at https://ufal.mff.cuni.cz/udpipe/2/models#czech_pdtc1.0_model."
"In-context learning (ICL) has been instrumental in adapting Large Language
Models (LLMs) to downstream tasks using correct input-output examples. Recent
advances have attempted to improve model performance through principles derived
from mistakes, yet these approaches suffer from lack of customization and
inadequate error coverage. To address these limitations, we propose Retrieved
In-Context Principles (RICP), a novel teacher-student framework. In RICP, the
teacher model analyzes mistakes from the student model to generate reasons and
insights for preventing similar mistakes. These mistakes are clustered based on
their underlying reasons for developing task-level principles, enhancing the
error coverage of principles. During inference, the most relevant mistakes for
each question are retrieved to create question-level principles, improving the
customization of the provided guidance. RICP is orthogonal to existing
prompting methods and does not require intervention from the teacher model
during inference. Experimental results across seven reasoning benchmarks reveal
that RICP effectively enhances performance when applied to various prompting
strategies."
"The current prevalence of conspiracy theories on the internet is a
significant issue, tackled by many computational approaches. However, these
approaches fail to recognize the relevance of distinguishing between texts
which contain a conspiracy theory and texts which are simply critical and
oppose mainstream narratives. Furthermore, little attention is usually paid to
the role of inter-group conflict in oppositional narratives. We contribute by
proposing a novel topic-agnostic annotation scheme that differentiates between
conspiracies and critical texts, and that defines span-level categories of
inter-group conflict. We also contribute with the multilingual
XAI-DisInfodemics corpus (English and Spanish), which contains a high-quality
annotation of Telegram messages related to COVID-19 (5,000 messages per
language). We also demonstrate the feasibility of an NLP-based automatization
by performing a range of experiments that yield strong baseline solutions.
Finally, we perform an analysis which demonstrates that the promotion of
intergroup conflict and the presence of violence and anger are key aspects to
distinguish between the two types of oppositional narratives, i.e., conspiracy
vs. critical."
"Automatic question generation (QG) serves a wide range of purposes, such as
augmenting question-answering (QA) corpora, enhancing chatbot systems, and
developing educational materials. Despite its importance, most existing
datasets predominantly focus on English, resulting in a considerable gap in
data availability for other languages. Cross-lingual transfer for QG (XLT-QG)
addresses this limitation by allowing models trained on high-resource language
datasets to generate questions in low-resource languages. In this paper, we
propose a simple and efficient XLT-QG method that operates without the need for
monolingual, parallel, or labeled data in the target language, utilizing a
small language model. Our model, trained solely on English QA datasets, learns
interrogative structures from a limited set of question exemplars, which are
then applied to generate questions in the target language. Experimental results
show that our method outperforms several XLT-QG baselines and achieves
performance comparable to GPT-3.5-turbo across different languages.
Additionally, the synthetic data generated by our model proves beneficial for
training multilingual QA models. With significantly fewer parameters than large
language models and without requiring additional training for target languages,
our approach offers an effective solution for QG and QA tasks across various
languages."
"The context window of large language models (LLMs) has been extended
significantly in recent years. However, while the context length that the LLM
can process has grown, the capability of the model to accurately reason over
that context degrades noticeably. This occurs because modern LLMs often become
overwhelmed by the vast amount of information in the context; when answering
questions, the model must identify and reason over relevant evidence sparsely
distributed throughout the text. To alleviate the challenge of long-context
reasoning, we develop a retrieve-then-reason framework, enabling LLMs to reason
over relevant evidence collected during an intermediate retrieval step. We find
that modern LLMs struggle to accurately retrieve relevant facts and instead,
often hallucinate ""retrieved facts"", resulting in flawed reasoning and the
production of incorrect answers. To address these issues, we introduce ALR$^2$,
a method that augments the long-context reasoning capability of LLMs via an
explicit two-stage procedure, i.e., aligning LLMs with the objectives of both
retrieval and reasoning. We demonstrate the efficacy of ALR$^2$ for mitigating
performance degradation in long-context reasoning tasks. Through extensive
experiments on long-context QA benchmarks, we find our method to outperform
competitive baselines by large margins, achieving at least 8.4 and 7.9 EM gains
on the long-context versions of HotpotQA and SQuAD datasets, respectively."
"Remote communication through video or audio conferences has become more
popular than ever because of the worldwide pandemic. These events, therefore,
have provoked the development of systems for automatic minuting of spoken
language leading to AutoMin 2021 challenge. The following paper illustrates the
results of the research that team MTS has carried out while participating in
the Automatic Minutes challenge. In particular, in this paper we analyze
existing approaches to text and speech summarization, propose an unsupervised
summarization technique based on clustering and provide a pipeline that
includes an adapted automatic speech recognition block able to run on real-life
recordings. The proposed unsupervised technique outperforms pre-trained
summarization models on the automatic minuting task with Rouge 1, Rouge 2 and
Rouge L values of 0.21, 0.02 and 0.2 on the dev set, with Rouge 1, Rouge 2,
Rouge L, Adequacy, Grammatical correctness and Fluency values of 0.180, 0.035,
0.098, 1.857, 2.304, 1.911 on the test set accordingly"
"In this paper, we present the MetricX-24 submissions to the WMT24 Metrics
Shared Task and provide details on the improvements we made over the previous
version of MetricX. Our primary submission is a hybrid reference-based/-free
metric, which can score a translation irrespective of whether it is given the
source segment, the reference, or both. The metric is trained on previous WMT
data in a two-stage fashion, first on the DA ratings only, then on a mixture of
MQM and DA ratings. The training set in both stages is augmented with synthetic
examples that we created to make the metric more robust to several common
failure modes, such as fluent but unrelated translation, or undertranslation.
We demonstrate the benefits of the individual modifications via an ablation
study, and show a significant performance increase over MetricX-23 on the WMT23
MQM ratings, as well as our new synthetic challenge set."
"How to defend large language models (LLMs) from generating toxic content is
an important research area. Yet, most research focused on various model
training techniques to remediate LLMs by updating their weights. A typical
related research area is safety alignment. This however is often costly and
tedious and can expose the model to even more problems such as catastrophic
forgetting if the trainings are not carefully handled by experienced NLP
practitioners. We thus propose a simple yet effective and novel algorithm,
namely \textbf{Tox}ic Subword \textbf{Prun}ing (ToxPrune) to prune the subword
contained by the toxic words from BPE in trained LLMs. In contrast to the
previous work that demonstrates pruning BPE tokens as harmful to the task of
machine translation, we surprisingly found its usefulness in preventing toxic
content from being generated on LLMs. Fortunately, our findings suggest that
ToxPrune simultaneously improves the toxic language model NSFW-3B on the task
of dialogue response generation obviously. We surprisingly found that ToxPrune
can even obviously improve official Llama-3.1-6B in the metric of dialogue
diversity. Extensive automatic results and human evaluation indicate that
ToxPrune could be helpful for both remediating toxic LLMs and improving
non-toxic LLMs on the task of dialogue response generation.\footnote{We plan to
release the resources to facilitate future work.}"
"The dissertation addresses the design of parsing grammars for automatic
surface-syntactic analysis of unconstrained English text. It consists of a
summary and three articles. {\it Morphological disambiguation} documents a
grammar for morphological (or part-of-speech) disambiguation of English, done
within the Constraint Grammar framework proposed by Fred Karlsson. The
disambiguator seeks to discard those of the alternative morphological analyses
proposed by the lexical analyser that are contextually illegitimate. The 1,100
constraints express some 23 general, essentially syntactic statements as
restrictions on the linear order of morphological tags. The error rate of the
morphological disambiguator is about ten times smaller than that of another
state-of-the-art probabilistic disambiguator, given that both are allowed to
leave some of the hardest ambiguities unresolved. This accuracy suggests the
viability of the grammar-based approach to natural language parsing, thus also
contributing to the more general debate concerning the viability of
probabilistic vs.\ linguistic techniques. {\it Experiments with heuristics}
addresses the question of how to resolve those ambiguities that survive the
morphological disambiguator. Two approaches are presented and empirically
evaluated: (i) heuristic disambiguation constraints and (ii) techniques for
learning from the fully disambiguated part of the corpus and then applying this
information to resolving remaining ambiguities."
"TECHDOC is an implemented system demonstrating the feasibility of generating
multilingual technical documents on the basis of a language-independent
knowledge base. Its application domain is user and maintenance instructions,
which are produced from underlying plan structures representing the activities,
the participating objects with their properties, relations, and so on. This
paper gives a brief outline of the system architecture and discusses some
recent developments in the project: the addition of actual event simulation in
the KB, steps towards a document authoring tool, and a multimodal user
interface. (slightly corrected version of a paper to appear in: COLING 94,
Proceedings)"
"Various methods have been proposed for aligning texts in two or more
languages such as the Canadian Parliamentary Debates(Hansards). Some of these
methods generate a bilingual lexicon as a by-product. We present an alternative
alignment strategy which we call K-vec, that starts by estimating the lexicon.
For example, it discovers that the English word ""fisheries"" is similar to the
French ""pe^ches"" by noting that the distribution of ""fisheries"" in the English
text is similar to the distribution of ""pe^ches"" in the French. K-vec does not
depend on sentence boundaries."
"We discuss implementation issues of MARIE-1, a mostly symbolic parser fully
implemented, and MARIE-2, a more statistical parser partially implemented. They
address a corpus of 100,000 picture captions. We argue that the mixed approach
of MARIE-2 should be better for this corpus because its algorithms (not data)
are simpler."
"The paper presents a constraint based semantic formalism for HPSG. The
advantages of the formlism are shown with respect to a grammar for a fragment
of German that deals with (i) quantifier scope ambiguities triggered by
scrambling and/or movement and (ii) ambiguities that arise from the
collective/distributive distinction of plural NPs. The syntax-semantics
interface directly implements syntactic conditions on quantifier scoping and
distributivity. The construction of semantic representations is guided by
general principles governing the interaction between syntax and semantics. Each
of these principles acts as a constraint to narrow down the set of possible
interpretations of a sentence. Meanings of ambiguous sentences are represented
by single partial representations (so-called U(nderspecified) D(iscourse)
R(epresentation) S(tructure)s) to which further constraints can be added
monotonically to gain more information about the content of a sentence. There
is no need to build up a large number of alternative representations of the
sentence which are then filtered by subsequent discourse and world knowledge.
The advantage of UDRSs is not only that they allow for monotonic incremental
interpretation but also that they are equipped with truth conditions and a
proof theory that allows for inferences to be drawn directly on structures
where quantifier scope is not resolved."
"Natural language understanding applications such as interactive planning and
face-to-face translation require extensive inferencing. Many of these
inferences are based on the meaning of particular open class words. Providing a
representation that can support such lexically-based inferences is a primary
concern of lexical semantics. The representation language of first order logic
has well-understood semantics and a multitude of inferencing systems have been
implemented for it. Thus it is a prime candidate to serve as a lexical
semantics representation. However, we argue that FOL, although a good starting
point, needs to be extended before it can efficiently and concisely support all
the lexically-based inferences needed."
"Nominalization is a highly productive phenomena in most languages. The
process of nominalization ejects a verb from its syntactic role into a nominal
position. The original verb is often replaced by a semantically emptied support
verb (e.g., ""make a proposal""). The choice of a support verb for a given
nominalization is unpredictable, causing a problem for language learners as
well as for natural language processing systems. We present here a method of
discovering support verbs from an untagged corpus via low-level syntactic
processing and comparison of arguments attached to verbal forms and potential
nominalized forms. The result of the process is a list of potential support
verbs for the nominalized form of a given predicate."
"Since Austin introduced the term ``infelicity'', the linguistic literature
has been flooded with its use, but no formal or computational explanation has
been given for it. This thesis provides one for those infelicities that occur
when a pragmatic inference is cancelled.
  Our contribution assumes the existence of a finer grained taxonomy with
respect to pragmatic inferences. It is shown that if one wants to account for
the natural language expressiveness, one should distinguish between pragmatic
inferences that are felicitous to defeat and pragmatic inferences that are
infelicitously defeasible. Thus, it is shown that one should consider at least
three types of information: indefeasible, felicitously defeasible, and
infelicitously defeasible. The cancellation of the last of these determines the
pragmatic infelicities.
  A new formalism has been devised to accommodate the three levels of
information, called ``stratified logic''. Within it, we are able to express
formally notions such as ``utterance U presupposes P'' or ``utterance U is
infelicitous''. Special attention is paid to the implications that our work has
in solving some well-known existential philosophical puzzles. The formalism
yields an algorithm for computing interpretations for utterances, for
determining their associated presuppositions, and for signalling infelicitous
utterances that has been implemented in Common Lisp. The algorithm applies
equally to simple and complex utterances and sequences of utterances."
"Writing specifications for computer programs is not easy since one has to
take into account the disparate conceptual worlds of the application domain and
of software development. To bridge this conceptual gap we propose controlled
natural language as a declarative and application-specific specification
language. Controlled natural language is a subset of natural language that can
be accurately and efficiently processed by a computer, but is expressive enough
to allow natural usage by non-specialists. Specifications in controlled natural
language are automatically translated into Prolog clauses, hence become formal
and executable. The translation uses a definite clause grammar (DCG) enhanced
by feature structures. Inter-text references of the specification, e.g.
anaphora, are resolved with the help of discourse representation theory (DRT).
The generated Prolog clauses are added to a knowledge base. We have implemented
a prototypical specification system that successfully processes the
specification of a simple automated teller machine."
"This is an interim report discussing possible guidelines for the assessment
and evaluation of projects developing speech and language systems. It was
prepared at the request of the European Commission DG XIII by an ad hoc study
group, and is now being made available in the form in which it was submitted to
the Commission. However, the report is not an official European Commission
document, and does not reflect European Commission policy, official or
otherwise.
  After a discussion of terminology, the report focusses on combining
user-centred and technology-centred assessment, and on how meaningful
comparisons can be made of a variety of systems performing different tasks for
different domains. The report outlines the kind of infra-structure that might
be required to support comparative assessment and evaluation of heterogenous
projects, and also the results of a questionnaire concerning different
approaches to evaluation."
"Grice's maxims of conversation [Grice 1975] are framed as directives to be
followed by a speaker of the language. This paper argues that, when considered
from the point of view of natural language generation, such a characterisation
is rather misleading, and that the desired behaviour falls out quite naturally
if we view language generation as a goal-oriented process. We argue this
position with particular regard to the generation of referring expressions."
"In this paper I present ongoing work on the data-oriented parsing (DOP)
model. In previous work, DOP was tested on a cleaned-up set of analyzed
part-of-speech strings from the Penn Treebank, achieving excellent test
results. This left, however, two important questions unanswered: (1) how does
DOP perform if tested on unedited data, and (2) how can DOP be used for parsing
word strings that contain unknown words? This paper addresses these questions.
We show that parse results on unedited data are worse than on cleaned-up data,
although very competitive if compared to other models. As to the parsing of
word strings, we show that the hardness of the problem does not so much depend
on unknown words, but on previously unseen lexical categories of known words.
We give a novel method for parsing these words by estimating the probabilities
of unknown subtrees. The method is of general interest since it shows that good
performance can be obtained without the use of a part-of-speech tagger. To the
best of our knowledge, our method outperforms other statistical parsers tested
on Penn Treebank word strings."
"We present a novel approach to lexical error recovery on textual input. An
advanced robust tokenizer has been implemented that can not only correct
spelling mistakes, but also recover from segmentation errors. Apart from the
orthographic considerations taken, the tokenizer also makes use of linguistic
expectations extracted from a training corpus. The idea is to arrange Hidden
Markov Models (HMM) in multiple layers where the HMMs in each layer are
responsible for different aspects of the processing of the input. We report on
experimental evaluations with alternative probabilistic language models to
guide the lexical error recovery process."
"This paper proposes an analysis of classifiers into four major types: UNIT,
METRIC, GROUP and SPECIES, based on properties of both Japanese and English.
The analysis makes possible a uniform and straightforward treatment of noun
phrases headed by classifiers in Japanese-to-English machine translation, and
has been implemented in the MT system ALT-J/E. Although the analysis is based
on the characteristics of, and differences between, Japanese and English, it is
shown to be also applicable to the unrelated language Thai."
"This paper presents a new view of Explanation-Based Learning (EBL) of natural
language parsing. Rather than employing EBL for specializing parsers by
inferring new ones, this paper suggests employing EBL for learning how to
reduce ambiguity only partially.
  The present method consists of an EBL algorithm for learning partial-parsers,
and a parsing algorithm which combines partial-parsers with existing
``full-parsers"". The learned partial-parsers, implementable as Cascades of
Finite State Transducers (CFSTs), recognize and combine constituents
efficiently, prohibiting spurious overgeneration. The parsing algorithm
combines a learned partial-parser with a given full-parser such that the role
of the full-parser is limited to combining the constituents, recognized by the
partial-parser, and to recognizing unrecognized portions of the input sentence.
Besides the reduction of the parse-space prior to disambiguation, the present
method provides a way for refining existing disambiguation models that learn
stochastic grammars from tree-banks.
  We exhibit encouraging empirical results using a pilot implementation:
parse-space is reduced substantially with minimal loss of coverage. The speedup
gain for disambiguation models is exemplified by experiments with the DOP
model."
"It is known that the classification performance of Support Vector Machine
(SVM) can be conveniently affected by the different parameters of the kernel
tricks and the regularization parameter, C. Thus, in this article, we propose a
study in order to find the suitable kernel with which SVM may achieve good
generalization performance as well as the parameters to use. We need to analyze
the behavior of the SVM classifier when these parameters take very small or
very large values. The study is conducted for a multi-class vowel recognition
using the TIMIT corpus. Furthermore, for the experiments, we used different
feature representations such as MFCC and PLP. Finally, a comparative study was
done to point out the impact of the choice of the parameters, kernel trick and
feature representations on the performance of the SVM classifier"
"The hierarchy of classical Chinese poetry has been broadly acknowledged by a
number of studies in Chinese literature. However, quantitative investigations
about the evolutionary linkages of classical Chinese poetry are limited. The
primary goal of this study is to provide quantitative evidence of the
evolutionary linkages, with emphasis on character usage, among different period
genres of classical Chinese poetry. Specifically, various statistical analyses
are performed to find and compare the patterns of character usage in the poems
of nine period genres, including shi jing, chu ci, Han shi , Jin shi, Tang shi,
Song shi, Yuan shi, Ming shi, and Qing shi. The result of analysis indicates
that each of nine period genres has unique patterns of character usage, with
some Chinese characters that are preferably used in the poems of a particular
period genre. The analysis on the general pattern of character preference
implies a decreasing trend in the use of Chinese characters that rarely occur
in modern Chinese literature along the timeline of dynastic types of classical
Chinese poetry. The phylogenetic analysis based on the distance matrix suggests
that the evolutionary linkages of different types of classical Chinese poetry
are congruent with their chronological order, suggesting that character
frequencies contain phylogenetic information that is useful for inferring
evolutionary linkages among various types of classical Chinese poetry. The
estimated phylogenetic tree identifies four groups (shi jing, chu ci), (Han
shi, Jin shi), (Tang shi, Song shi, Yuan shi), and (Ming shi, Qing shi). The
statistical analyses conducted in this study can be generalized to analyze the
data sets of general Chinese literature. Such analyses can provide quantitative
insights about the evolutionary linkages of general Chinese literature."
"In this work we address the issue of generic automated disease incidence
monitoring on twitter. We employ an ontology of disease related concepts and
use it to obtain a conceptual representation of tweets. Unlike previous key
word based systems and topic modeling approaches, our ontological approach
allows us to apply more stringent criteria for determining which messages are
relevant such as spatial and temporal characteristics whilst giving a stronger
guarantee that the resulting models will perform well on new data that may be
lexically divergent. We achieve this by training learners on concepts rather
than individual words. For training we use a dataset containing mentions of
influenza and Listeria and use the learned models to classify datasets
containing mentions of an arbitrary selection of other diseases. We show that
our ontological approach achieves good performance on this task using a variety
of Natural Language Processing Techniques. We also show that word vectors can
be learned directly from our concepts to achieve even better results."
"Recent approaches to the Automatic Post-Editing (APE) research have shown
that better results are obtained by multi-source models, which jointly encode
both source (src) and machine translation output (mt) to produce post-edited
sentence (pe). Along this trend, we present a new multi-source APE model based
on the Transformer. To construct effective joint representations, our model
internally learns to incorporate src context into mt representation. With this
approach, we achieve a significant improvement over baseline systems, as well
as the state-of-the-art multi-source APE model. Moreover, to demonstrate the
capability of our model to incorporate src context, we show that the word
alignment of the unknown MT system is successfully captured in our encoding
results."
"We propose a simple yet effective approach for improving Korean word
representations using additional linguistic annotation (i.e. Hanja). We employ
cross-lingual transfer learning in training word representations by leveraging
the fact that Hanja is closely related to Chinese. We evaluate the intrinsic
quality of representations learned through our approach using the word analogy
and similarity tests. In addition, we demonstrate their effectiveness on
several downstream tasks, including a novel Korean news headline generation
task."
"We propose procedures for evaluating and strengthening contextual embedding
alignment and show that they are useful in analyzing and improving multilingual
BERT. In particular, after our proposed alignment procedure, BERT exhibits
significantly improved zero-shot performance on XNLI compared to the base
model, remarkably matching pseudo-fully-supervised translate-train models for
Bulgarian and Greek. Further, to measure the degree of alignment, we introduce
a contextual version of word retrieval and show that it correlates well with
downstream zero-shot transfer. Using this word retrieval task, we also analyze
BERT and find that it exhibits systematic deficiencies, e.g. worse alignment
for open-class parts-of-speech and word pairs written in different scripts,
that are corrected by the alignment procedure. These results support contextual
alignment as a useful concept for understanding large multilingual pre-trained
models."
"Machine learning based language models have recently made significant
progress, which introduces a danger to spread misinformation. To combat this
potential danger, several methods have been proposed for detecting text written
by these language models. This paper presents two classes of black-box attacks
on these detectors, one which randomly replaces characters with homoglyphs, and
the other a simple scheme to purposefully misspell words. The homoglyph and
misspelling attacks decrease a popular neural text detector's recall on neural
text from 97.44% to 0.26% and 22.68%, respectively. Results also indicate that
the attacks are transferable to other neural text detectors."
"Deep learning (DL) workloads are moving towards accelerators for faster
processing and lower cost. Modern DL accelerators are good at handling the
large-scale multiply-accumulate operations that dominate DL workloads; however,
it is challenging to make full use of the compute power of an accelerator since
the data must be properly staged in a software-managed scratchpad memory.
Failing to do so can result in significant performance loss. This paper
proposes a systematic approach which leverages the polyhedral model to analyze
all operators of a DL model together to minimize the number of memory accesses.
Experiments show that our approach can substantially reduce the impact of
memory accesses required by common neural-network models on a homegrown AWS
machine-learning inference chip named Inferentia, which is available through
Amazon EC2 Inf1 instances."
"Information content (IC) based measures for finding semantic similarity is
gaining preferences day by day. Semantics of concepts can be highly
characterized by information theory. The conventional way for calculating IC is
based on the probability of appearance of concepts in corpora. Due to data
sparseness and corpora dependency issues of those conventional approaches, a
new corpora independent intrinsic IC calculation measure has evolved. In this
paper, we mainly focus on such intrinsic IC model and several topological
aspects of the underlying ontology. Accuracy of intrinsic IC calculation and
semantic similarity measure rely on these aspects deeply. Based on these
analysis we propose an information theoretic framework which comprises an
intrinsic IC calculator and a semantic similarity model. Our approach is
compared with state of the art semantic similarity measures based on corpora
dependent IC calculation as well as intrinsic IC based methods using several
benchmark data set. We also compare our model with the related Edge based,
Feature based and Distributional approaches. Experimental results show that our
intrinsic IC model gives high correlation value when applied to different
semantic similarity models. Our proposed semantic similarity model also
achieves significant results when embedded with some state of the art IC models
including ours."
"Large-scale datasets for natural language inference are created by presenting
crowd workers with a sentence (premise), and asking them to generate three new
sentences (hypotheses) that it entails, contradicts, or is logically neutral
with respect to. We show that, in a significant portion of such data, this
protocol leaves clues that make it possible to identify the label by looking
only at the hypothesis, without observing the premise. Specifically, we show
that a simple text categorization model can correctly classify the hypothesis
alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams
et. al, 2017). Our analysis reveals that specific linguistic phenomena such as
negation and vagueness are highly correlated with certain inference classes.
Our findings suggest that the success of natural language inference models to
date has been overestimated, and that the task remains a hard open problem."
"Many vision and language tasks require commonsense reasoning beyond
data-driven image and natural language processing. Here we adopt Visual
Question Answering (VQA) as an example task, where a system is expected to
answer a question in natural language about an image. Current state-of-the-art
systems attempted to solve the task using deep neural architectures and
achieved promising performance. However, the resulting systems are generally
opaque and they struggle in understanding questions for which extra knowledge
is required. In this paper, we present an explicit reasoning layer on top of a
set of penultimate neural network based systems. The reasoning layer enables
reasoning and answering questions where additional knowledge is required, and
at the same time provides an interpretable interface to the end users.
Specifically, the reasoning layer adopts a Probabilistic Soft Logic (PSL) based
engine to reason over a basket of inputs: visual relations, the semantic parse
of the question, and background ontological knowledge from word2vec and
ConceptNet. Experimental analysis of the answers and the key evidential
predicates generated on the VQA dataset validate our approach."
"Automatically generated fake restaurant reviews are a threat to online review
systems. Recent research has shown that users have difficulties in detecting
machine-generated fake reviews hiding among real restaurant reviews. The method
used in this work (char-LSTM ) has one drawback: it has difficulties staying in
context, i.e. when it generates a review for specific target entity, the
resulting review may contain phrases that are unrelated to the target, thus
increasing its detectability. In this work, we present and evaluate a more
sophisticated technique based on neural machine translation (NMT) with which we
can generate reviews that stay on-topic. We test multiple variants of our
technique using native English speakers on Amazon Mechanical Turk. We
demonstrate that reviews generated by the best variant have almost optimal
undetectability (class-averaged F-score 47%). We conduct a user study with
skeptical users and show that our method evades detection more frequently
compared to the state-of-the-art (average evasion 3.2/4 vs 1.5/4) with
statistical significance, at level {\alpha} = 1% (Section 4.3). We develop very
effective detection tools and reach average F-score of 97% in classifying
these. Although fake reviews are very effective in fooling people, effective
automatic detection is still feasible."
"This paper provides a holistic study of how stock prices vary in their
response to financial disclosures across different topics. Thereby, we
specifically shed light into the extensive amount of filings for which no a
priori categorization of their content exists. For this purpose, we utilize an
approach from data mining - namely, latent Dirichlet allocation - as a means of
topic modeling. This technique facilitates our task of automatically
categorizing, ex ante, the content of more than 70,000 regulatory 8-K filings
from U.S. companies. We then evaluate the subsequent stock market reaction. Our
empirical evidence suggests a considerable discrepancy among various types of
news stories in terms of their relevance and impact on financial markets. For
instance, we find a statistically significant abnormal return in response to
earnings results and credit rating, but also for disclosures regarding business
strategy, the health sector, as well as mergers and acquisitions. Our results
yield findings that benefit managers, investors and policy-makers by indicating
how regulatory filings should be structured and the topics most likely to
precede changes in stock valuations."
"Demand Response (DR) schemes are effective tools to maintain a dynamic
balance in energy markets with higher integration of fluctuating renewable
energy sources. DR schemes can be used to harness residential devices'
flexibility and to utilize it to achieve social and financial objectives.
However, existing DR schemes suffer from low user participation as they fail at
taking into account the users' requirements. First, DR schemes are highly
demanding for the users, as users need to provide direct information, e.g. via
surveys, on their energy consumption preferences. Second, the user utility
models based on these surveys are hard-coded and do not adapt over time. Third,
the existing scheduling techniques require the users to input their energy
requirements on a daily basis. As an alternative, this paper proposes a DR
scheme for user-oriented direct load-control of residential appliances
operations. Instead of relying on user surveys to evaluate the user utility, we
propose an online data-driven approach for estimating user utility functions,
purely based on available load consumption data, that adaptively models the
users' preference over time. Our scheme is based on a day-ahead scheduling
technique that transparently prescribes the users with optimal device operation
schedules that take into account both financial benefits and user-perceived
quality of service. To model day-ahead user energy demand and flexibility, we
propose a probabilistic approach for generating flexibility models under
uncertainty. Results on both real-world and simulated datasets show that our DR
scheme can provide significant financial benefits while preserving the
user-perceived quality of service."
"This paper contributes to cross-lingual image annotation and retrieval in
terms of data and baseline methods. We propose COCO-CN, a novel dataset
enriching MS-COCO with manually written Chinese sentences and tags. For more
effective annotation acquisition, we develop a recommendation-assisted
collective annotation system, automatically providing an annotator with several
tags and sentences deemed to be relevant with respect to the pictorial content.
Having 20,342 images annotated with 27,218 Chinese sentences and 70,993 tags,
COCO-CN is currently the largest Chinese-English dataset that provides a
unified and challenging platform for cross-lingual image tagging, captioning
and retrieval. We develop conceptually simple yet effective methods per task
for learning from cross-lingual resources. Extensive experiments on the three
tasks justify the viability of the proposed dataset and methods. Data and code
are publicly available at https://github.com/li-xirong/coco-cn"
"Domain specific information retrieval process has been a prominent and
ongoing research in the field of natural language processing. Many researchers
have incorporated different techniques to overcome the technical and domain
specificity and provide a mature model for various domains of interest. The
main bottleneck in these studies is the heavy coupling of domain experts, that
makes the entire process to be time consuming and cumbersome. In this study, we
have developed three novel models which are compared against a golden standard
generated via the on line repositories provided, specifically for the legal
domain. The three different models incorporated vector space representations of
the legal domain, where document vector generation was done in two different
mechanisms and as an ensemble of the above two. This study contains the
research being carried out in the process of representing legal case documents
into different vector spaces, whilst incorporating semantic word measures and
natural language processing techniques. The ensemble model built in this study,
shows a significantly higher accuracy level, which indeed proves the need for
incorporation of domain specific semantic similarity measures into the
information retrieval process. This study also shows, the impact of varying
distribution of the word similarity measures, against varying document vector
dimensions, which can lead to improvements in the process of legal information
retrieval."
"Neural Network based models have been state-of-the-art models for various
Natural Language Processing tasks, however, the input and output dimension
problem in the networks has still not been fully resolved, especially in text
generation tasks (e.g. Machine Translation, Text Summarization), in which input
and output both have huge sizes of vocabularies. Therefore, input-output
embedding weight sharing has been introduced and adopted widely, which remains
to be improved. Based on linear algebra and statistical theories, this paper
locates the shortcoming of existed input-output embedding weight sharing
method, then raises methods for improving input-output weight shared embedding,
among which methods of normalization of embedding weight matrices show best
performance. These methods are nearly computational cost-free, can get combined
with other embedding techniques, and show good effectiveness when applied on
state-of-the-art Neural Network models. For Transformer-big models, the
normalization techniques can get at best 0.6 BLEU improvement compared to the
original version of model on WMT'16 En-De dataset, and similar BLEU
improvements on IWSLT 14' datasets. For DynamicConv models, 0.5 BLEU
improvement can be attained on WMT'16 En-De dataset, and 0.41 BLEU improvement
on IWSLT 14' De-En translation task is achieved."
"The amount of user generated contents from various social medias allows
analyst to handle a wide view of conversations on several topics related to
their business. Nevertheless keeping up-to-date with this amount of information
is not humanly feasible. Automatic Summarization then provides an interesting
mean to digest the dynamics and the mass volume of contents. In this paper, we
address the issue of tweets summarization which remains scarcely explored. We
propose to automatically generated summaries of Micro-Blogs conversations
dealing with public figures E-Reputation. These summaries are generated using
key-word queries or sample tweet and offer a focused view of the whole
Micro-Blog network. Since state-of-the-art is lacking on this point we conduct
and evaluate our experiments over the multilingual CLEF RepLab Topic-Detection
dataset according to an experimental evaluation process."
"Given the advantage and recent success of English character-level and
subword-unit models in several NLP tasks, we consider the equivalent modeling
problem for Chinese. Chinese script is logographic and many Chinese logograms
are composed of common substructures that provide semantic, phonetic and
syntactic hints. In this work, we propose to explicitly incorporate the visual
appearance of a character's glyph in its representation, resulting in a novel
glyph-aware embedding of Chinese characters. Being inspired by the success of
convolutional neural networks in computer vision, we use them to incorporate
the spatio-structural patterns of Chinese glyphs as rendered in raw pixels. In
the context of two basic Chinese NLP tasks of language modeling and word
segmentation, the model learns to represent each character's task-relevant
semantic and syntactic information in the character-level embedding."
"Variation in speech is often quantified by comparing phonetic transcriptions
of the same utterance. However, manually transcribing speech is time-consuming
and error prone. As an alternative, therefore, we investigate the extraction of
acoustic embeddings from several self-supervised neural models. We use these
representations to compute word-based pronunciation differences between
non-native and native speakers of English, and between Norwegian dialect
speakers. For comparison with several earlier studies, we evaluate how well
these differences match human perception by comparing them with available human
judgements of similarity. We show that speech representations extracted from a
specific type of neural model (i.e. Transformers) lead to a better match with
human perception than two earlier approaches on the basis of phonetic
transcriptions and MFCC-based acoustic features. We furthermore find that
features from the neural models can generally best be extracted from one of the
middle hidden layers than from the final layer. We also demonstrate that neural
speech representations not only capture segmental differences, but also
intonational and durational differences that cannot adequately be represented
by a set of discrete symbols used in phonetic transcriptions."
"Developments in weakly supervised and self-supervised models could enable
speech technology in low-resource settings where full transcriptions are not
available. We consider whether keyword localisation is possible using two forms
of weak supervision where location information is not provided explicitly. In
the first, only the presence or absence of a word is indicated, i.e. a
bag-of-words (BoW) labelling. In the second, visual context is provided in the
form of an image paired with an unlabelled utterance; a model then needs to be
trained in a self-supervised fashion using the paired data. For keyword
localisation, we adapt a saliency-based method typically used in the vision
domain. We compare this to an existing technique that performs localisation as
a part of the network architecture. While the saliency-based method is more
flexible (it can be applied without architectural restrictions), we identify a
critical limitation when using it for keyword localisation. Of the two forms of
supervision, the visually trained model performs worse than the BoW-trained
model. We show qualitatively that the visually trained model sometimes locate
semantically related words, but this is not consistent. While our results show
that there is some signal allowing for localisation, it also calls for other
localisation methods better matched to these forms of weak supervision."
"As machine learning algorithms are more widely deployed in healthcare, the
question of algorithmic fairness becomes more critical to examine. Our work
seeks to identify and understand disparities in a deployed model that
classifies doctor-patient conversations into sections of a medical SOAP note.
We employ several metrics to measure disparities in the classifier performance,
and find small differences in a portion of the disadvantaged groups. A deeper
analysis of the language in these conversations and further stratifying the
groups suggests these differences are related to and often attributable to the
type of medical appointment (e.g., psychiatric vs. internist). Our findings
stress the importance of understanding the disparities that may exist in the
data itself and how that affects a model's ability to equally distribute
benefits."
"On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take
a slot-filling approach by building several dedicated models for each type of
slots. Such modularized systems are not only complex butalso of limited
capacity for capturing inter-dependencies among SQL clauses. To solve these
problems, this paper proposes a novel extraction-linking approach, where a
unified extractor recognizes all types of slot mentions appearing in the
question sentence before a linker maps the recognized columns to the table
schema to generate executable SQL queries. Trained with automatically generated
annotations, the proposed method achieves the first place on the WikiSQL
benchmark."
"Verb Sense Disambiguation is a well-known task in NLP, the aim is to find the
correct sense of a verb in a sentence. Recently, this problem has been extended
in a multimodal scenario, by exploiting both textual and visual features of
ambiguous verbs leading to a new problem, the Visual Verb Sense Disambiguation
(VVSD). Here, the sense of a verb is assigned considering the content of an
image paired with it rather than a sentence in which the verb appears.
Annotating a dataset for this task is more complex than textual disambiguation,
because assigning the correct sense to a pair of $<$image, verb$>$ requires
both non-trivial linguistic and visual skills. In this work, differently from
the literature, the VVSD task will be performed in a transductive
semi-supervised learning (SSL) setting, in which only a small amount of labeled
information is required, reducing tremendously the need for annotated data. The
disambiguation process is based on a graph-based label propagation method which
takes into account mono or multimodal representations for $<$image, verb$>$
pairs. Experiments have been carried out on the recently published dataset
VerSe, the only available dataset for this task. The achieved results
outperform the current state-of-the-art by a large margin while using only a
small fraction of labeled samples per sense. Code available:
https://github.com/GiBg1aN/TVVSD."
"As mobile devices are becoming ubiquitous, regularly interacting with a
variety of user interfaces (UIs) is a common aspect of daily life for many
people. To improve the accessibility of these devices and to enable their usage
in a variety of settings, building models that can assist users and accomplish
tasks through the UI is vitally important. However, there are several
challenges to achieve this. First, UI components of similar appearance can have
different functionalities, making understanding their function more important
than just analyzing their appearance. Second, domain-specific features like
Document Object Model (DOM) in web pages and View Hierarchy (VH) in mobile
applications provide important signals about the semantics of UI elements, but
these features are not in a natural language format. Third, owing to a large
diversity in UIs and absence of standard DOM or VH representations, building a
UI understanding model with high coverage requires large amounts of training
data.
  Inspired by the success of pre-training based approaches in NLP for tackling
a variety of problems in a data-efficient way, we introduce a new pre-trained
UI representation model called ActionBert. Our methodology is designed to
leverage visual, linguistic and domain-specific features in user interaction
traces to pre-train generic feature representations of UIs and their
components. Our key intuition is that user actions, e.g., a sequence of clicks
on different UI components, reveals important information about their
functionality. We evaluate the proposed model on a wide variety of downstream
tasks, ranging from icon classification to UI component retrieval based on its
natural language description. Experiments show that the proposed ActionBert
model outperforms multi-modal baselines across all downstream tasks by up to
15.5%."
"In social settings, much of human behavior is governed by unspoken rules of
conduct. For artificial systems to be fully integrated into social
environments, adherence to such norms is a central prerequisite. We investigate
whether contemporary NLG models can function as behavioral priors for systems
deployed in social settings by generating action hypotheses that achieve
predefined goals under moral constraints. Moreover, we examine if models can
anticipate likely consequences of (im)moral actions, or explain why certain
actions are preferable by generating relevant norms. For this purpose, we
introduce 'Moral Stories', a crowd-sourced dataset of structured, branching
narratives for the study of grounded, goal-oriented social reasoning. Finally,
we propose decoding strategies that effectively combine multiple expert models
to significantly improve the quality of generated actions, consequences, and
norms compared to strong baselines, e.g. though abductive reasoning."
"In this paper, we establish Fog Index (FI) as a text filter to locate the
sentences in texts that contain connected biomedical concepts of interest. To
do so, we have used 24 random papers each containing four pairs of connected
concepts. For each pair, we categorize sentences based on whether they contain
both, any or none of the concepts. We then use FI to measure difficulty of the
sentences of each category and find that sentences containing both of the
concepts have low readability. We rank sentences of a text according to their
FI and select 30 percent of the most difficult sentences. We use an association
matrix to track the most frequent pairs of concepts in them. This matrix
reports that the first filter produces some pairs that hold almost no
connections. To remove these unwanted pairs, we use the Equally Weighted
Harmonic Mean of their Positive Predictive Value (PPV) and Sensitivity as a
second filter. Experimental results demonstrate the effectiveness of our
method."
"One of the challenges for text analysis in medical domains is analyzing
large-scale medical documents. As a consequence, finding relevant documents has
become more difficult. One of the popular methods to retrieve information based
on discovering the themes in the documents is topic modeling. The themes in the
documents help to retrieve documents on the same topic with and without a
query. In this paper, we present a novel approach to topic modeling using fuzzy
clustering. To evaluate our model, we experiment with two text datasets of
medical documents. The evaluation metrics carried out through document
classification and document modeling show that our model produces better
performance than LDA, indicating that fuzzy set theory can improve the
performance of topic models in medical domains."
"The Int_reg-problem of a combinatorial problem P asks, given a
nondeterministic automaton M as input, whether the language L(M) accepted by M
contains any positive instance of the problem P. We consider the
Int_reg-problem for a number of different graph problems and give general
criteria that give decision procedures for these Int_reg-problems. To achieve
this goal, we consider a natural graph encoding so that the language of all
graph encodings is regular. Then, we draw the connection between classical
pumping- and interchange-arguments from the field of formal language theory
with the graph operations induced on the encoded graph. Our techniques apply
among others to the Int_reg-problem of well-known graph problems like Vertex
Cover and Independent Set, as well as to subgraph problems, graph-edit problems
and graph-partitioning problems, including coloring problems."
"On 23rd June 2016, 51.9% of British voters voted to leave the European Union,
triggering a process and events that have led to the United Kingdom leaving the
EU, an event that has become known as 'Brexit'. In this piece of research, we
investigate the effects of this entire process on the currency markets,
specifically the GBP/EUR exchange rate. Financial markets are known to be
sensitive to news articles and media, and the aim of this research is to
evaluate the magnitude of impact of relevant events, as well as whether the
impact was positive or negative for the GBP."
"We explore the task of Video Object Grounding (VOG), which grounds objects in
videos referred to in natural language descriptions. Previous methods apply
image grounding based algorithms to address VOG, fail to explore the object
relation information and suffer from limited generalization. Here, we
investigate the role of object relations in VOG and propose a novel framework
VOGNet to encode multi-modal object relations via self-attention with relative
position encoding. To evaluate VOGNet, we propose novel contrasting sampling
methods to generate more challenging grounding input samples, and construct a
new dataset called ActivityNet-SRL (ASRL) based on existing caption and
grounding datasets. Experiments on ASRL validate the need of encoding object
relations in VOG, and our VOGNet outperforms competitive baselines by a
significant margin."
"With the growing popularity of deep-learning based NLP models, comes a need
for interpretable systems. But what is interpretability, and what constitutes a
high-quality interpretation? In this opinion piece we reflect on the current
state of interpretability evaluation research. We call for more clearly
differentiating between different desired criteria an interpretation should
satisfy, and focus on the faithfulness criteria. We survey the literature with
respect to faithfulness evaluation, and arrange the current approaches around
three assumptions, providing an explicit form to how faithfulness is ""defined""
by the community. We provide concrete guidelines on how evaluation of
interpretation methods should and should not be conducted. Finally, we claim
that the current binary definition for faithfulness sets a potentially
unrealistic bar for being considered faithful. We call for discarding the
binary notion of faithfulness in favor of a more graded one, which we believe
will be of greater practical utility."
"Many studies have applied reinforcement learning to train a dialog policy and
show great promise these years. One common approach is to employ a user
simulator to obtain a large number of simulated user experiences for
reinforcement learning algorithms. However, modeling a realistic user simulator
is challenging. A rule-based simulator requires heavy domain expertise for
complex tasks, and a data-driven simulator requires considerable data and it is
even unclear how to evaluate a simulator. To avoid explicitly building a user
simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which
regards both the system and the user as the dialog agents. Two agents interact
with each other and are jointly learned simultaneously. The method uses the
actor-critic framework to facilitate pretraining and improve scalability. We
also propose Hybrid Value Network for the role-aware reward decomposition to
integrate role-specific domain knowledge of each agent in the task-oriented
dialog. Results show that our method can successfully build a system policy and
a user policy simultaneously, and two agents can achieve a high task success
rate through conversational interaction."
"In Transformer-based neural machine translation (NMT), the positional
encoding mechanism helps the self-attention networks to learn the source
representation with order dependency, which makes the Transformer-based NMT
achieve state-of-the-art results for various translation tasks. However,
Transformer-based NMT only adds representations of positions sequentially to
word vectors in the input sentence and does not explicitly consider reordering
information in this sentence. In this paper, we first empirically investigate
the relationship between source reordering information and translation
performance. The empirical findings show that the source input with the target
order learned from the bilingual parallel dataset can substantially improve
translation performance. Thus, we propose a novel reordering method to
explicitly model this reordering information for the Transformer-based NMT. The
empirical results on the WMT14 English-to-German, WAT ASPEC
Japanese-to-English, and WMT17 Chinese-to-English translation tasks show the
effectiveness of the proposed approach."
"Despite the continuing efforts to improve the engagingness and consistency of
chit-chat dialogue systems, the majority of current work simply focus on
mimicking human-like responses, leaving understudied the aspects of modeling
understanding between interlocutors. The research in cognitive science,
instead, suggests that understanding is an essential signal for a high-quality
chit-chat conversation. Motivated by this, we propose P^2 Bot, a
transmitter-receiver based framework with the aim of explicitly modeling
understanding. Specifically, P^2 Bot incorporates mutual persona perception to
enhance the quality of personalized dialogue generation. Experiments on a large
public dataset, Persona-Chat, demonstrate the effectiveness of our approach,
with a considerable boost over the state-of-the-art baselines across both
automatic metrics and human evaluations."
"Evaluating Visual Dialogue, the task of answering a sequence of questions
relating to a visual input, remains an open research challenge. The current
evaluation scheme of the VisDial dataset computes the ranks of ground-truth
answers in predefined candidate sets, which Massiceti et al. (2018) show can be
susceptible to the exploitation of dataset biases. This scheme also does little
to account for the different ways of expressing the same answer--an aspect of
language that has been well studied in NLP. We propose a revised evaluation
scheme for the VisDial dataset leveraging metrics from the NLP literature to
measure consensus between answers generated by the model and a set of relevant
answers. We construct these relevant answer sets using a simple and effective
semi-supervised method based on correlation, which allows us to automatically
extend and scale sparse relevance annotations from humans to the entire
dataset. We release these sets and code for the revised evaluation scheme as
DenseVisDial, and intend them to be an improvement to the dataset in the face
of its existing constraints and design choices."
"Natural language understanding (NLU) converts sentences into structured
semantic forms. The paucity of annotated training samples is still a
fundamental challenge of NLU. To solve this data sparsity problem, previous
work based on semi-supervised learning mainly focuses on exploiting unlabeled
sentences. In this work, we introduce a dual task of NLU, semantic-to-sentence
generation (SSG), and propose a new framework for semi-supervised NLU with the
corresponding dual model. The framework is composed of dual pseudo-labeling and
dual learning method, which enables an NLU model to make full use of data
(labeled and unlabeled) through a closed-loop of the primal and dual tasks. By
incorporating the dual task, the framework can exploit pure semantic forms as
well as unlabeled sentences, and further improve the NLU and SSG models
iteratively in the closed-loop. The proposed approaches are evaluated on two
public datasets (ATIS and SNIPS). Experiments in the semi-supervised setting
show that our methods can outperform various baselines significantly, and
extensive ablation studies are conducted to verify the effectiveness of our
framework. Finally, our method can also achieve the state-of-the-art
performance on the two datasets in the supervised setting. Our code is
available at \url{https://github.com/rhythmcao/slu-dual-learning.git}."
"Named entity recognition is an important task in natural language processing.
It is very well studied for rich language, but still under explored for
low-resource languages. The main reason is that the existing techniques
required a lot of annotated data to reach good performance. Recently, a new
distributional representation of words has been proposed to project named
entities from a rich language to a low-resource one. This representation has
been coupled to a neural network in order to project named entities from
English to Ewondo, a Bantu language spoken in Cameroon. Although the proposed
method reached appreciable results, the size of the used neural network was too
large compared to the size of the dataset. Furthermore the impact of the model
parameters has not been studied. In this paper, we show experimentally that the
same results can be obtained using a smaller neural network. We also emphasize
the parameters that are highly correlated to the network performance. This work
is a step forward to build a reliable and robust network architecture for named
entity projection in low resource languages."
"Pretraining from unlabelled web videos has quickly become the de-facto means
of achieving high performance on many video understanding tasks. Features are
learned via prediction of grounded relationships between visual content and
automatic speech recognition (ASR) tokens. However, prior pretraining work has
been limited to only instructional videos; a priori, we expect this domain to
be relatively ""easy:"" speakers in instructional videos will often reference the
literal objects/actions being depicted. We ask: can similar models be trained
on more diverse video corpora? And, if so, what types of videos are ""grounded""
and what types are not? We fit a representative pretraining model to the
diverse YouTube8M dataset, and study its success and failure cases. We find
that visual-textual grounding is indeed possible across previously unexplored
video categories, and that pretraining on a more diverse set results in
representations that generalize to both non-instructional and instructional
domains."
"Identification of Fake News plays a prominent role in the ongoing pandemic,
impacting multiple aspects of day-to-day life. In this work we present a
solution to the shared task titled COVID19 Fake News Detection in English,
scoring the 50th place amongst 168 submissions. The solution was within 1.5% of
the best performing solution. The proposed solution employs a heterogeneous
representation ensemble, adapted for the classification task via an additional
neural classification head comprised of multiple hidden layers. The paper
consists of detailed ablation studies further displaying the proposed method's
behavior and possible implications. The solution is freely available.
\url{https://gitlab.com/boshko.koloski/covid19-fake-news}"
"As an essential component of human cognition, cause-effect relations appear
frequently in text, and curating cause-effect relations from text helps in
building causal networks for predictive tasks. Existing causality extraction
techniques include knowledge-based, statistical machine learning(ML)-based, and
deep learning-based approaches. Each method has its advantages and weaknesses.
For example, knowledge-based methods are understandable but require extensive
manual domain knowledge and have poor cross-domain applicability. Statistical
machine learning methods are more automated because of natural language
processing (NLP) toolkits. However, feature engineering is labor-intensive, and
toolkits may lead to error propagation. In the past few years, deep learning
techniques attract substantial attention from NLP researchers because of its'
powerful representation learning ability and the rapid increase in
computational resources. Their limitations include high computational costs and
a lack of adequate annotated training data. In this paper, we conduct a
comprehensive survey of causality extraction. We initially introduce primary
forms existing in the causality extraction: explicit intra-sentential
causality, implicit causality, and inter-sentential causality. Next, we list
benchmark datasets and modeling assessment methods for causal relation
extraction. Then, we present a structured overview of the three techniques with
their representative systems. Lastly, we highlight existing open challenges
with their potential directions."
"This paper describes our proposed system for the AAAI-CAD21 shared task:
Predicting Emphasis in Presentation Slides. In this specific task, given the
contents of a slide we are asked to predict the degree of emphasis to be laid
on each word in the slide. We propose 2 approaches to this problem including a
BiLSTM-ELMo approach and a transformers based approach based on RoBERTa and
XLNet architectures. We achieve a score of 0.518 on the evaluation leaderboard
which ranks us 3rd and 0.543 on the post-evaluation leaderboard which ranks us
1st at the time of writing the paper."
"Machine understanding of user utterances in conversational systems is of
utmost importance for enabling engaging and meaningful conversations with
users. Entity Linking (EL) is one of the means of text understanding, with
proven efficacy for various downstream tasks in information retrieval. In this
paper, we study entity linking for conversational systems. To develop a better
understanding of what EL in a conversational setting entails, we analyze a
large number of dialogues from existing conversational datasets and annotate
references to concepts, named entities, and personal entities using
crowdsourcing. Based on the annotated dialogues, we identify the main
characteristics of conversational entity linking. Further, we report on the
performance of traditional EL systems on our Conversational Entity Linking
dataset, ConEL, and present an extension to these methods to better fit the
conversational setting. The resources released with this paper include
annotated datasets, detailed descriptions of crowdsourcing setups, as well as
the annotations produced by various EL systems. These new resources allow for
an investigation of how the role of entities in conversations is different from
that in documents or isolated short text utterances like queries and tweets,
and complement existing conversational datasets."
"We present the first large scale corpus for entity resolution in email
conversations (CEREC). The corpus consists of 6001 email threads from the Enron
Email Corpus containing 36,448 email messages and 60,383 entity coreference
chains. The annotation is carried out as a two-step process with minimal manual
effort. Experiments are carried out for evaluating different features and
performance of four baselines on the created corpus. For the task of mention
identification and coreference resolution, a best performance of 59.2 F1 is
reported, highlighting the room for improvement. An in-depth qualitative and
quantitative error analysis is presented to understand the limitations of the
baselines considered."
"Pre-trained language models (PrLMs) have demonstrated superior performance
due to their strong ability to learn universal language representations from
self-supervised pre-training. However, even with the help of the powerful
PrLMs, it is still challenging to effectively capture task-related knowledge
from dialogue texts which are enriched by correlations among speaker-aware
utterances. In this work, we present SPIDER, Structural Pre-traIned DialoguE
Reader, to capture dialogue exclusive features. To simulate the dialogue-like
features, we propose two training objectives in addition to the original LM
objectives: 1) utterance order restoration, which predicts the order of the
permuted utterances in dialogue context; 2) sentence backbone regularization,
which regularizes the model to improve the factual correctness of summarized
subject-verb-object triplets. Experimental results on widely used dialogue
benchmarks verify the effectiveness of the newly introduced self-supervised
tasks."
"Multi-head attention plays a crucial role in the recent success of
Transformer models, which leads to consistent performance improvements over
conventional attention in various applications. The popular belief is that this
effectiveness stems from the ability of jointly attending multiple positions.
In this paper, we first demonstrate that jointly attending multiple positions
is not a unique feature of multi-head attention, as multi-layer single-head
attention also attends multiple positions and is more effective. Then, we
suggest the main advantage of the multi-head attention is the training
stability, since it has less number of layers than the single-head attention,
when attending the same number of positions. For example, 24-layer 16-head
Transformer (BERT-large) and 384-layer single-head Transformer has the same
total attention head number and roughly the same model size, while the
multi-head one is significantly shallower. Meanwhile, we show that, with recent
advances in deep learning, we can successfully stabilize the training of the
384-layer Transformer. As the training difficulty is no longer a bottleneck,
substantially deeper single-head Transformer achieves consistent performance
improvements without tuning hyper-parameters."
"Recent multilingual pre-trained language models have achieved remarkable
zero-shot performance, where the model is only finetuned on one source language
and directly evaluated on target languages. In this work, we propose a
self-learning framework that further utilizes unlabeled data of target
languages, combined with uncertainty estimation in the process to select
high-quality silver labels. Three different uncertainties are adapted and
analyzed specifically for the cross lingual transfer: Language
Heteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty
(EVI). We evaluate our framework with uncertainties on two cross-lingual tasks
including Named Entity Recognition (NER) and Natural Language Inference (NLI)
covering 40 languages in total, which outperforms the baselines significantly
by 10 F1 on average for NER and 2.5 accuracy score for NLI."
"A recent variation of Transformer, Performer, scales Transformer to longer
sequences with a linear attention mechanism. However, it is not compatible with
relative position encoding, which has advantages over absolute position
encoding. In this paper, we discuss possible ways to add relative position
encoding to Performer. Based on the analysis, we propose PermuteFormer, a
Performer-based model with relative position encoding that scales linearly on
long sequences. PermuteFormer applies position-dependent transformation on
queries and keys to encode positional information into the attention module.
This transformation is carefully crafted so that the final output of
self-attention is not affected by absolute positions of tokens. PermuteFormer
introduces negligible computational overhead by design that it runs as fast as
Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long
sequences, as well as WikiText-103, a language modeling dataset. The
experiments show that PermuteFormer uniformly improves the performance of
Performer with almost no computational overhead and outperforms vanilla
Transformer on most of the tasks."
"Using prompts to utilize language models to perform various downstream tasks,
also known as prompt-based learning or prompt-learning, has lately gained
significant success in comparison to the pre-train and fine-tune paradigm.
Nonetheless, virtually all prompt-based methods are token-level, meaning they
all utilize GPT's left-to-right language model or BERT's masked language model
to perform cloze-style tasks. In this paper, we attempt to accomplish several
NLP tasks in the zero-shot scenario using a BERT original pre-training task
abandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unlike
token-level techniques, our sentence-level prompt-based method NSP-BERT does
not need to fix the length of the prompt or the position to be predicted,
allowing it to handle tasks such as entity linking with ease. Based on the
characteristics of NSP-BERT, we offer several quick building templates for
various downstream tasks. We suggest a two-stage prompt method for word sense
disambiguation tasks in particular. Our strategies for mapping the labels
significantly enhance the model's performance on sentence pair tasks. On the
FewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most of
these tasks and comes close to the few-shot methods."
"For many business applications, we often seek to analyze sentiments
associated with any arbitrary aspects of commercial products, despite having a
very limited amount of labels or even without any labels at all. However,
existing aspect target sentiment classification (ATSC) models are not trainable
if annotated datasets are not available. Even with labeled data, they fall
short of reaching satisfactory performance. To address this, we propose simple
approaches that better solve ATSC with natural language prompts, enabling the
task under zero-shot cases and enhancing supervised settings, especially for
few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop
domain, our method of reformulating ATSC as an NLI task outperforms supervised
SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.
Moreover, we demonstrate that our prompts could handle implicitly stated
aspects as well: our models reach about 77% accuracy on detecting sentiments
for aspect categories (e.g., food), which do not necessarily appear within the
text, even though we trained the models only with explicitly mentioned aspect
terms (e.g., fajitas) from just 16 reviews - while the accuracy of the
no-prompt baseline is only around 65%."
"In open-domain question answering, a model receives a text question as input
and searches for the correct answer using a large evidence corpus. The
retrieval step is especially difficult as typical evidence corpora have
\textit{millions} of documents, each of which may or may not have the correct
answer to the question. Very recently, dense models have replaced sparse
methods as the de facto retrieval method. Rather than focusing on lexical
overlap to determine similarity, dense methods build an encoding function that
captures semantic similarity by learning from a small collection of
question-answer or question-context pairs. In this paper, we investigate dense
retrieval models in the context of open-domain question answering across
different input distributions. To do this, first we introduce an entity-rich
question answering dataset constructed from Wikidata facts and demonstrate
dense models are unable to generalize to unseen input question distributions.
Second, we perform analyses aimed at better understanding the source of the
problem and propose new training techniques to improve out-of-domain
performance on a wide variety of datasets. We encourage the field to further
investigate the creation of a single, universal dense retrieval model that
generalizes well across all input distributions."
"State-of-the-art NLP models can adopt shallow heuristics that limit their
generalization capability (McCoy et al., 2019). Such heuristics include lexical
overlap with the training set in Named-Entity Recognition (Taill\'e et al.,
2020) and Event or Type heuristics in Relation Extraction (Rosenman et al.,
2020). In the more realistic end-to-end RE setting, we can expect yet another
heuristic: the mere retention of training relation triples. In this paper, we
propose several experiments confirming that retention of known facts is a key
factor of performance on standard benchmarks. Furthermore, one experiment
suggests that a pipeline model able to use intermediate type representations is
less prone to over-rely on retention."
"Textual Question Answering (QA) aims to provide precise answers to user's
questions in natural language using unstructured data. One of the most popular
approaches to this goal is machine reading comprehension(MRC). In recent years,
many novel datasets and evaluation metrics based on classical MRC tasks have
been proposed for broader textual QA tasks. In this paper, we survey 47 recent
textual QA benchmark datasets and propose a new taxonomy from an application
point of view. In addition, We summarize 8 evaluation metrics of textual QA
tasks. Finally, we discuss current trends in constructing textual QA benchmarks
and suggest directions for future work."
"In this article, we tackle the math word problem, namely, automatically
answering a mathematical problem according to its textual description. Although
recent methods have demonstrated their promising results, most of these methods
are based on template-based generation scheme which results in limited
generalization capability. To this end, we propose a novel human-like
analogical learning method in a recall and learn manner. Our proposed framework
is composed of modules of memory, representation, analogy, and reasoning, which
are designed to make a new exercise by referring to the exercises learned in
the past. Specifically, given a math word problem, the model first retrieves
similar questions by a memory module and then encodes the unsolved problem and
each retrieved question using a representation module. Moreover, to solve the
problem in a way of analogy, an analogy module and a reasoning module with a
copy mechanism are proposed to model the interrelationship between the problem
and each retrieved question. Extensive experiments on two well-known datasets
show the superiority of our proposed algorithm as compared to other
state-of-the-art competitors from both overall performance comparison and
micro-scope studies."
"Pruning aims to reduce the number of parameters while maintaining performance
close to the original network. This work proposes a novel
\emph{self-distillation} based pruning strategy, whereby the representational
similarity between the pruned and unpruned versions of the same network is
maximized. Unlike previous approaches that treat distillation and pruning
separately, we use distillation to inform the pruning criteria, without
requiring a separate student network as in knowledge distillation. We show that
the proposed {\em cross-correlation objective for self-distilled pruning}
implicitly encourages sparse solutions, naturally complementing magnitude-based
pruning criteria. Experiments on the GLUE and XGLUE benchmarks show that
self-distilled pruning increases mono- and cross-lingual language model
performance. Self-distilled pruned models also outperform smaller Transformers
with an equal number of parameters and are competitive against (6 times) larger
distilled networks. We also observe that self-distillation (1) maximizes class
separability, (2) increases the signal-to-noise ratio, and (3) converges faster
after pruning steps, providing further insights into why self-distilled pruning
improves generalization."
"An overwhelmingly large amount of knowledge in the materials domain is
generated and stored as text published in peer-reviewed scientific literature.
Recent developments in natural language processing, such as bidirectional
encoder representations from transformers (BERT) models, provide promising
tools to extract information from these texts. However, direct application of
these models in the materials domain may yield suboptimal results as the models
themselves may not be trained on notations and jargon that are specific to the
domain. Here, we present a materials-aware language model, namely, MatSciBERT,
which is trained on a large corpus of scientific literature published in the
materials domain. We further evaluate the performance of MatSciBERT on three
downstream tasks, namely, abstract classification, named entity recognition,
and relation extraction, on different materials datasets. We show that
MatSciBERT outperforms SciBERT, a language model trained on science corpus, on
all the tasks. Further, we discuss some of the applications of MatSciBERT in
the materials domain for extracting information, which can, in turn, contribute
to materials discovery or optimization. Finally, to make the work accessible to
the larger materials community, we make the pretrained and finetuned weights
and the models of MatSciBERT freely accessible."
"In this work, we show a novel method for neural machine translation (NMT),
using a denoising diffusion probabilistic model (DDPM), adjusted for textual
data, following recent advances in the field. We show that it's possible to
translate sentences non-autoregressively using a diffusion model conditioned on
the source sentence. We also show that our model is able to translate between
pairs of languages unseen during training (zero-shot learning)."
"Information extraction (IE) from documents is an intensive area of research
with a large set of industrial applications. Current state-of-the-art methods
focus on scanned documents with approaches combining computer vision, natural
language processing and layout representation. We propose to challenge the
usage of computer vision in the case where both token style and visual
representation are available (i.e native PDF documents). Our experiments on
three real-world complex datasets demonstrate that using token style attributes
based embedding instead of a raw visual embedding in LayoutLM model is
beneficial. Depending on the dataset, such an embedding yields an improvement
of 0.18% to 2.29% in the weighted F1-score with a decrease of 30.7% in the
final number of trainable parameters of the model, leading to an improvement in
both efficiency and effectiveness."
"Nowadays, Knowledge graphs (KGs) have been playing a pivotal role in
AI-related applications. Despite the large sizes, existing KGs are far from
complete and comprehensive. In order to continuously enrich KGs, automatic
knowledge construction and update mechanisms are usually utilized, which
inevitably bring in plenty of noise. However, most existing knowledge graph
embedding (KGE) methods assume that all the triple facts in KGs are correct,
and project both entities and relations into a low-dimensional space without
considering noise and knowledge conflicts. This will lead to low-quality and
unreliable representations of KGs. To this end, in this paper, we propose a
general multi-task reinforcement learning framework, which can greatly
alleviate the noisy data problem. In our framework, we exploit reinforcement
learning for choosing high-quality knowledge triples while filtering out the
noisy ones. Also, in order to take full advantage of the correlations among
semantically similar relations, the triple selection processes of similar
relations are trained in a collective way with multi-task learning. Moreover,
we extend popular KGE models TransE, DistMult, ConvE and RotatE with the
proposed framework. Finally, the experimental validation shows that our
approach is able to enhance existing KGE models and can provide more robust
representations of KGs in noisy scenarios."
"Speech summarization, which generates a text summary from speech, can be
achieved by combining automatic speech recognition (ASR) and text summarization
(TS). With this cascade approach, we can exploit state-of-the-art models and
large training datasets for both subtasks, i.e., Transformer for ASR and
Bidirectional Encoder Representations from Transformers (BERT) for TS. However,
ASR errors directly affect the quality of the output summary in the cascade
approach. We propose a cascade speech summarization model that is robust to ASR
errors and that exploits multiple hypotheses generated by ASR to attenuate the
effect of ASR errors on the summary. We investigate several schemes to combine
ASR hypotheses. First, we propose using the sum of sub-word embedding vectors
weighted by their posterior values provided by an ASR system as an input to a
BERT-based TS system. Then, we introduce a more general scheme that uses an
attention-based fusion module added to a pre-trained BERT module to align and
combine several ASR hypotheses. Finally, we perform speech summarization
experiments on the How2 dataset and a newly assembled TED-based dataset that we
will release with this paper. These experiments show that retraining the
BERT-based TS system with these schemes can improve summarization performance
and that the attention-based fusion module is particularly effective."
"Automatic question answering is an important yet challenging task in
E-commerce given the millions of questions posted by users about the product
that they are interested in purchasing. Hence, there is a great demand for
automatic answer generation systems that provide quick responses using related
information about the product. There are three sources of knowledge available
for answering a user posted query, they are reviews, duplicate or similar
questions, and specifications. Effectively utilizing these information sources
will greatly aid us in answering complex questions. However, there are two main
challenges present in exploiting these sources: (i) The presence of irrelevant
information and (ii) the presence of ambiguity of sentiment present in reviews
and similar questions. Through this work we propose a novel pipeline (MSQAP)
that utilizes the rich information present in the aforementioned sources by
separately performing relevancy and ambiguity prediction before generating a
response.
  Experimental results show that our relevancy prediction model (BERT-QA)
outperforms all other variants and has an improvement of 12.36% in F1 score
compared to the BERT-base baseline. Our generation model (T5-QA) outperforms
the baselines in all content preservation metrics such as BLEU, ROUGE and has
an average improvement of 35.02% in ROUGE and 198.75% in BLEU compared to the
highest performing baseline (HSSC-q). Human evaluation of our pipeline shows us
that our method has an overall improvement in accuracy of 30.7% over the
generation model (T5-QA), resulting in our full pipeline-based approach (MSQAP)
providing more accurate answers. To the best of our knowledge, this is the
first work in the e-commerce domain that automatically generates natural
language answers combining the information present in diverse sources such as
specifications, similar questions, and reviews data."
"Enhancing the diversity of sentences to describe video contents is an
important problem arising in recent video captioning research. In this paper,
we explore this problem from a novel perspective of customizing video captions
by imitating exemplar sentence syntaxes. Specifically, given a video and any
syntax-valid exemplar sentence, we introduce a new task of Syntax Customized
Video Captioning (SCVC) aiming to generate one caption which not only
semantically describes the video contents but also syntactically imitates the
given exemplar sentence. To tackle the SCVC task, we propose a novel video
captioning model, where a hierarchical sentence syntax encoder is firstly
designed to extract the syntactic structure of the exemplar sentence, then a
syntax conditioned caption decoder is devised to generate the syntactically
structured caption expressing video semantics. As there is no available syntax
customized groundtruth video captions, we tackle such a challenge by proposing
a new training strategy, which leverages the traditional pairwise video
captioning data and our collected exemplar sentences to accomplish the model
learning. Extensive experiments, in terms of semantic, syntactic, fluency, and
diversity evaluations, clearly demonstrate our model capability to generate
syntax-varied and semantics-coherent video captions that well imitate different
exemplar sentences with enriched diversities."
"Prompting shows promising results in few-shot scenarios. However, its
strength for multilingual/cross-lingual problems has not been fully exploited.
Zhao and Sch\""utze (2021) made initial explorations in this direction by
presenting that cross-lingual prompting outperforms cross-lingual finetuning.
In this paper, we conduct an empirical exploration on the effect of each
component in cross-lingual prompting and derive language-agnostic Universal
Prompting, which helps alleviate the discrepancies between source-language
training and target-language inference. Based on this, we propose DPA, a dual
prompt augmentation framework, aiming at relieving the data scarcity issue in
few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54%
with only 16 English training examples per class, significantly better than
34.99% of finetuning. Our code is available at
https://github.com/DAMO-NLP-SG/DPA."
"Databases for OLTP are often the backbone for applications such as hotel room
or cinema ticket booking applications. However, developing a conversational
agent (i.e., a chatbot-like interface) to allow end-users to interact with an
application using natural language requires both immense amounts of training
data and NLP expertise. This motivates CAT, which can be used to easily create
conversational agents for transactional databases. The main idea is that, for a
given OLTP database, CAT uses weak supervision to synthesize the required
training data to train a state-of-the-art conversational agent, allowing users
to interact with the OLTP database. Furthermore, CAT provides an out-of-the-box
integration of the resulting agent with the database. As a major difference to
existing conversational agents, agents synthesized by CAT are data-aware. This
means that the agent decides which information should be requested from the
user based on the current data distributions in the database, which typically
results in markedly more efficient dialogues compared with non-data-aware
agents. We publish the code for CAT as open source."
"We propose Speculative Decoding (SpecDec), for the first time ever, to
formally study exploiting the idea of speculative execution to accelerate
autoregressive (AR) decoding. Speculative Decoding has two innovations:
Spec-Drafter -- an independent model specially optimized for efficient and
accurate drafting -- and Spec-Verification -- a reliable method for verifying
the drafted tokens efficiently in the decoding paradigm. Experimental results
on various seq2seq tasks including machine translation and abstractive
summarization show our approach can achieve around $5\times$ speedup for the
popular Transformer architectures with comparable generation quality to beam
search decoding, refreshing the impression that the draft-then-verify paradigm
introduces only $1.4\times$$\sim$$2\times$ speedup. In addition to the
remarkable speedup, we also demonstrate 3 additional advantages of SpecDec,
revealing its practical value for accelerating generative models in real-world
applications. Our models and codes are available at
https://github.com/hemingkx/SpecDec."
"A long-term ambition of information seeking QA systems is to reason over
multi-modal contexts and generate natural answers to user queries. Today,
memory intensive pre-trained language models are adapted to downstream tasks
such as QA by fine-tuning the model on QA data in a specific modality like
unstructured text or structured tables. To avoid training such memory-hungry
models while utilizing a uniform architecture for each modality,
parameter-efficient adapters add and train small task-specific bottle-neck
layers between transformer layers. In this work, we study parameter-efficient
abstractive QA in encoder-decoder models over structured tabular data and
unstructured textual data using only 1.5% additional parameters for each
modality. We also ablate over adapter layers in both encoder and decoder
modules to study the efficiency-performance trade-off and demonstrate that
reducing additional trainable parameters down to 0.7%-1.0% leads to comparable
results. Our models out-perform current state-of-the-art models on tabular QA
datasets such as Tablesum and FeTaQA, and achieve comparable performance on a
textual QA dataset such as NarrativeQA using significantly less trainable
parameters than fine-tuning."
"Legal judgment prediction (LJP) applies Natural Language Processing (NLP)
techniques to predict judgment results based on fact descriptions
automatically. Recently, large-scale public datasets and advances in NLP
research have led to increasing interest in LJP. Despite a clear gap between
machine and human performance, impressive results have been achieved in various
benchmark datasets. In this paper, to address the current lack of comprehensive
survey of existing LJP tasks, datasets, models and evaluations, (1) we analyze
31 LJP datasets in 6 languages, present their construction process and define a
classification method of LJP with 3 different attributes; (2) we summarize 14
evaluation metrics under four categories for different outputs of LJP tasks;
(3) we review 12 legal-domain pretrained models in 3 languages and highlight 3
major research directions for LJP; (4) we show the state-of-art results for 8
representative datasets from different court cases and discuss the open
challenges. This paper can provide up-to-date and comprehensive reviews to help
readers understand the status of LJP. We hope to facilitate both NLP
researchers and legal professionals for further joint efforts in this problem."
"Pruning is an effective way to reduce the huge inference cost of Transformer
models. However, prior work on pruning Transformers requires retraining the
models. This can add high training cost and high complexity to model
deployment, making it difficult to use in many practical situations. To address
this, we propose a fast post-training pruning framework for Transformers that
does not require any retraining. Given a resource constraint and a sample
dataset, our framework automatically prunes the Transformer model using
structured sparsity methods. To retain high accuracy without retraining, we
introduce three novel techniques: (i) a lightweight mask search algorithm that
finds which heads and filters to prune based on the Fisher information; (ii)
mask rearrangement that complements the search algorithm; and (iii) mask tuning
that reconstructs the output activations for each layer. We apply our method to
BERT-base and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD
benchmarks. Our framework achieves up to 2.0x reduction in FLOPs and 1.56x
speedup in inference latency, while maintaining < 1% loss in accuracy.
Importantly, our framework prunes Transformers in less than 3 minutes on a
single GPU, which is over two orders of magnitude faster than existing pruning
approaches that retrain the models."
"The application of Deep Learning and Natural Language based ChatBots are
growing rapidly in recent years. They are used in many fields like customer
support, reservation system and as personal assistant. The Enterprises are
using such ChatBots to serve their customers in a better and efficient manner.
Even after such technological advancement, the expert advice does not reach the
farmers on timely manner. The farmers are still largely dependent on their
peers knowledge in solving the problems they face in their field. These
technologies have not been effectively used to give the required information to
farmers on timely manner. This project aims to implement a closed domain
ChatBot for the field of Agriculture Farmers Assistant. Farmers can have
conversation with the Chatbot and get the expert advice in their field. Farmers
Assistant is based on RASA Open Source Framework. The Chatbot identifies the
intent and entity from user utterances and retrieve the remedy from the
database and share it with the user. We tested the Bot with existing data and
it showed promising results."
"The logical negation property (LNP), which implies generating different
predictions for semantically opposite inputs, is an important property that a
trustworthy language model must satisfy. However, much recent evidence shows
that large-size pre-trained language models (PLMs) do not satisfy this
property. In this paper, we perform experiments using probing tasks to assess
PLM's LNP understanding. Unlike previous studies that only examined negation
expressions, we expand the boundary of the investigation to lexical semantics.
Through experiments, we observe that PLMs violate the LNP frequently. To
alleviate the issue, we propose a novel intermediate training task, names
meaning-matching, designed to directly learn a meaning-text correspondence,
instead of relying on the distributional hypothesis. Through multiple
experiments, we find that the task enables PLMs to learn lexical semantic
information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm
that it is a safe intermediate task that guarantees a similar or better
performance of downstream tasks. Finally, we observe that our proposed approach
outperforms our previous counterparts despite its time and resource efficiency."
"Recent literature has seen growing interest in using black-box strategies
like CheckList for testing the behavior of NLP models. Research on white-box
testing has developed a number of methods for evaluating how thoroughly the
internal behavior of deep models is tested, but they are not applicable to NLP
models. We propose a set of white-box testing methods that are customized for
transformer-based NLP models. These include Mask Neuron Coverage (MNCOVER) that
measures how thoroughly the attention layers in models are exercised during
testing. We show that MNCOVER can refine testing suites generated by CheckList
by substantially reduce them in size, for more than 60\% on average, while
retaining failing tests -- thereby concentrating the fault detection power of
the test suite. Further we show how MNCOVER can be used to guide CheckList
input generation, evaluate alternative NLP testing methods, and drive data
augmentation to improve accuracy."
"This paper investigates different pretraining approaches to spoken language
identification. The paper is based on our submission to the Oriental Language
Recognition 2021 Challenge. We participated in two tracks of the challenge:
constrained and unconstrained language recognition. For the constrained track,
we first trained a Conformer-based encoder-decoder model for multilingual
automatic speech recognition (ASR), using the provided training data that had
transcripts available. The shared encoder of the multilingual ASR model was
then finetuned for the language identification task. For the unconstrained
task, we relied on both externally available pretrained models as well as
external data: the multilingual XLSR-53 wav2vec2.0 model was finetuned on the
VoxLingua107 corpus for the language recognition task, and finally finetuned on
the provided target language training data, augmented with CommonVoice data.
Our primary metric $C_{\rm avg}$ values on the Test set are 0.0079 for the
constrained task and 0.0119 for the unconstrained task which resulted in the
second place in both rankings. In post-evaluation experiments, we study the
amount of target language data needed for training an accurate backend model,
the importance of multilingual pretraining data, and compare different models
as finetuning starting points."
"Many important questions (e.g. ""How to eat healthier?"") require conversation
to establish context and explore in depth. However, conversational question
answering (ConvQA) systems have long been stymied by scarce training data that
is expensive to collect. To address this problem, we propose a new technique
for synthetically generating diverse and high-quality dialog data: dialog
inpainting. Our approach takes the text of any document and transforms it into
a two-person dialog between the writer and an imagined reader: we treat
sentences from the article as utterances spoken by the writer, and then use a
dialog inpainter to predict what the imagined reader asked or said in between
each of the writer's utterances. By applying this approach to passages from
Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets
totalling 19 million diverse information-seeking dialogs -- 1,000x larger than
the largest existing ConvQA dataset. Furthermore, human raters judge the answer
adequacy and conversationality of WikiDialog to be as good or better than
existing manually-collected datasets. Using our inpainted data to pre-train
ConvQA retrieval systems, we significantly advance state-of-the-art across
three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains
on standard evaluation metrics."
"Predicting the subsequent event for an existing event context is an important
but challenging task, as it requires understanding the underlying relationship
between events. Previous methods propose to retrieve relational features from
event graph to enhance the modeling of event correlation. However, the sparsity
of event graph may restrict the acquisition of relevant graph information, and
hence influence the model performance. To address this issue, we consider
automatically building of event graph using a BERT model. To this end, we
incorporate an additional structured variable into BERT to learn to predict the
event connections in the training process. Hence, in the test process, the
connection relationship for unseen events can be predicted by the structured
variable. Results on two event prediction tasks: script event prediction and
story ending prediction, show that our approach can outperform state-of-the-art
baseline methods."
"Building conversational agents that can have natural and knowledge-grounded
interactions with humans requires understanding user utterances. Entity Linking
(EL) is an effective and widely used method for understanding natural language
text and connecting it to external knowledge. It is, however, shown that
existing EL methods developed for annotating documents are suboptimal for
conversations, where personal entities (e.g., ""my cars"") and concepts are
essential for understanding user utterances. In this paper, we introduce a
collection and a tool for entity linking in conversations. We collect EL
annotations for 1327 conversational utterances, consisting of links to named
entities, concepts, and personal entities. The dataset is used for training our
toolkit for conversational entity linking, CREL. Unlike existing EL methods,
CREL is developed to identify both named entities and concepts. It also
utilizes coreference resolution techniques to identify personal entities and
references to the explicit entity mentions in the conversations. We compare
CREL with state-of-the-art techniques and show that it outperforms all existing
baselines."
"Generating counterfactual test-cases is an important backbone for testing NLP
models and making them as robust and reliable as traditional software. In
generating the test-cases, a desired property is the ability to control the
test-case generation in a flexible manner to test for a large variety of
failure cases and to explain and repair them in a targeted manner. In this
direction, significant progress has been made in the prior works by manually
writing rules for generating controlled counterfactuals. However, this approach
requires heavy manual supervision and lacks the flexibility to easily introduce
new controls. Motivated by the impressive flexibility of the plug-and-play
approach of PPLM, we propose bringing the framework of plug-and-play to
counterfactual test case generation task. We introduce CASPer, a plug-and-play
counterfactual generation framework to generate test cases that satisfy goal
attributes on demand. Our plug-and-play model can steer the test case
generation process given any attribute model without requiring
attribute-specific training of the model. In experiments, we show that CASPer
effectively generates counterfactual text that follow the steering provided by
an attribute model while also being fluent, diverse and preserving the original
content. We also show that the generated counterfactuals from CASPer can be
used for augmenting the training data and thereby fixing and making the test
model more robust."
"Few-shot learning (FSL) is an emergent paradigm of learning that attempts to
learn to reason with low sample complexity to mimic the way humans learn,
generalise and extrapolate from only a few seen examples. While FSL attempts to
mimic these human characteristics, fundamentally, the task of FSL as
conventionally formulated using meta-learning with episodic-based training does
not in actuality align with how humans acquire and reason with knowledge. FSL
with episodic training, while only requires $K$ instances of each test class,
still requires a large number of labelled training instances from disjoint
classes. In this paper, we introduce the novel task of constrained few-shot
learning (CFSL), a special case of FSL where $M$, the number of instances of
each training class is constrained such that $M \leq K$ thus applying a similar
restriction during FSL training and test. We propose a method for CFSL
leveraging Cat2Vec using a novel categorical contrastive loss inspired by
cognitive theories such as fuzzy trace theory and prototype theory."
"Many complex problems are naturally understood in terms of symbolic concepts.
For example, our concept of ""cat"" is related to our concepts of ""ears"" and
""whiskers"" in a non-arbitrary way. Fodor (1998) proposes one theory of
concepts, which emphasizes symbolic representations related via constituency
structures. Whether neural networks are consistent with such a theory is open
for debate. We propose unit tests for evaluating whether a system's behavior is
consistent with several key aspects of Fodor's criteria. Using a simple visual
concept learning task, we evaluate several modern neural architectures against
this specification. We find that models succeed on tests of groundedness,
modularlity, and reusability of concepts, but that important questions about
causality remain open. Resolving these will require new methods for analyzing
models' internal states."
"We present a bi-encoder framework for named entity recognition (NER), which
applies contrastive learning to map candidate text spans and entity types into
the same vector representation space. Prior work predominantly approaches NER
as sequence labeling or span classification. We instead frame NER as a
representation learning problem that maximizes the similarity between the
vector representations of an entity mention and its type. This makes it easy to
handle nested and flat NER alike, and can better leverage noisy
self-supervision signals. A major challenge to this bi-encoder formulation for
NER lies in separating non-entity spans from entity mentions. Instead of
explicitly labeling all non-entity spans as the same class $\texttt{Outside}$
($\texttt{O}$) as in most prior methods, we introduce a novel dynamic
thresholding loss. Experiments show that our method performs well in both
supervised and distantly supervised settings, for nested and flat NER alike,
establishing new state of the art across standard datasets in the general
domain (e.g., ACE2004, ACE2005) and high-value verticals such as biomedicine
(e.g., GENIA, NCBI, BC5CDR, JNLPBA). We release the code at
github.com/microsoft/binder."
"Quantization is an effective technique to reduce memory footprint, inference
latency, and power consumption of deep learning models. However, existing
quantization methods suffer from accuracy degradation compared to
full-precision (FP) models due to the errors introduced by coarse gradient
estimation through non-differentiable quantization layers. The existence of
sharp local minima in the loss landscapes of overparameterized models (e.g.,
Transformers) tends to aggravate such performance penalty in low-bit (2, 4
bits) settings. In this work, we propose sharpness- and quantization-aware
training (SQuAT), which would encourage the model to converge to flatter minima
while performing quantization-aware training. Our proposed method alternates
training between sharpness objective and step-size objective, which could
potentially let the model learn the most suitable parameter update magnitude to
reach convergence near-flat minima. Extensive experiments show that our method
can consistently outperform state-of-the-art quantized BERT models under 2, 3,
and 4-bit settings on GLUE benchmarks by 1%, and can sometimes even outperform
full precision (32-bit) models. Our experiments on empirical measurement of
sharpness also suggest that our method would lead to flatter minima compared to
other quantization methods."
"Detecting out-of-distribution (OOD) instances is significant for the safe
deployment of NLP models. Among recent textual OOD detection works based on
pretrained language models (PLMs), distance-based methods have shown superior
performance. However, they estimate sample distance scores in the last-layer
CLS embedding space and thus do not make full use of linguistic information
underlying in PLMs. To address the issue, we propose to boost OOD detection by
deriving more holistic sentence embeddings. On the basis of the observations
that token averaging and layer combination contribute to improving OOD
detection, we propose a simple embedding approach named Avg-Avg, which averages
all token representations from each intermediate layer as the sentence
embedding and significantly surpasses the state-of-the-art on a comprehensive
suite of benchmarks by a 9.33% FAR95 margin. Furthermore, our analysis
demonstrates that it indeed helps preserve general linguistic knowledge in
fine-tuned PLMs and substantially benefits detecting background shifts. The
simple yet effective embedding method can be applied to fine-tuned PLMs with
negligible extra costs, providing a free gain in OOD detection. Our code is
available at https://github.com/lancopku/Avg-Avg."
"In this paper, we present a characteristic extraction algorithm and the
Multi-domain Image Characteristics Dataset of characteristic-tagged images to
simulate the way a human brain classifies cross-domain information and
generates insight. The intent was to identify prominent characteristics in data
and use this identification mechanism to auto-generate insight from data in
other unseen domains. An information extraction algorithm is proposed which is
a combination of Variational Autoencoders (VAEs) and Capsule Networks. Capsule
Networks are used to decompose images into their individual features and VAEs
are used to explore variations on these decomposed features. Thus, making the
model robust in recognizing characteristics from variations of the data. A
noteworthy point is that the algorithm uses efficient hierarchical decoding of
data which helps in richer output interpretation. Noticing a dearth in the
number of datasets that contain visible characteristics in images belonging to
various domains, the Multi-domain Image Characteristics Dataset was created and
made publicly available. It consists of thousands of images across three
domains. This dataset was created with the intent of introducing a new
benchmark for fine-grained characteristic recognition tasks in the future."
"Why do bilingual speakers code-switch (mix their two languages)? Among the
several theories that attempt to explain this natural and ubiquitous
phenomenon, the Triggering Hypothesis relates code-switching to the presence of
lexical triggers, specifically cognates and proper names, adjacent to the
switch point. We provide a fuller, more nuanced and refined exploration of the
triggering hypothesis, based on five large datasets in three language pairs,
reflecting both spoken and written bilingual interactions. Our results show
that words that are assumed to reside in a mental lexicon shared by both
languages indeed trigger code-switching; that the tendency to switch depends on
the distance of the trigger from the switch point; and on whether the trigger
precedes or succeeds the switch; but not on the etymology of the trigger words.
We thus provide strong, robust, evidence-based confirmation to several
hypotheses on the relationships between lexical triggers and code-switching."
"We continue the investigation into the power of smaller Transformer-based
language models as initiated by \textbf{TinyStories} -- a 10 million parameter
model that can produce coherent English -- and the follow-up work on
\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance
close to the state-of-the-art. The latter work proposed to use existing Large
Language Models (LLMs) to generate ``textbook quality"" data as a way to enhance
the learning process compared to traditional web data. We follow the
``Textbooks Are All You Need"" approach, focusing this time on common sense
reasoning in natural language, and create a new 1.3 billion parameter model
named \textbf{phi-1.5}, with performance on natural language tasks comparable
to models 5x larger, and surpassing most non-frontier LLMs on more complex
reasoning tasks such as grade-school mathematics and basic coding. More
generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs,
both good -- such as the ability to ``think step by step"" or perform some
rudimentary in-context learning -- and bad, including hallucinations and the
potential for toxic and biased generations -- encouragingly though, we are
seeing improvement on that front thanks to the absence of web data. We
open-source \textbf{phi-1.5} to promote further research on these urgent
topics."
"In our increasingly interconnected digital world, social media platforms have
emerged as powerful channels for the dissemination of hate speech and offensive
content. This work delves into the domain of hate speech detection, placing
specific emphasis on three low-resource Indian languages: Bengali, Assamese,
and Gujarati. The challenge is framed as a text classification task, aimed at
discerning whether a tweet contains offensive or non-offensive content.
Leveraging the HASOC 2023 datasets, we fine-tuned pre-trained BERT and SBERT
models to evaluate their effectiveness in identifying hate speech. Our findings
underscore the superiority of monolingual sentence-BERT models, particularly in
the Bengali language, where we achieved the highest ranking. However, the
performance in Assamese and Gujarati languages signifies ongoing opportunities
for enhancement. Our goal is to foster inclusive online spaces by countering
hate speech proliferation."
"Topic models are popular statistical tools for detecting latent semantic
topics in a text corpus. They have been utilized in various applications across
different fields. However, traditional topic models have some limitations,
including insensitivity to user guidance, sensitivity to the amount and quality
of data, and the inability to adapt learned topics from one corpus to another.
To address these challenges, this paper proposes a neural topic model,
TopicAdapt, that can adapt relevant topics from a related source corpus and
also discover new topics in a target corpus that are absent in the source
corpus. The proposed model offers a promising approach to improve topic
modeling performance in practical scenarios. Experiments over multiple datasets
from diverse domains show the superiority of the proposed model against the
state-of-the-art topic models."
"We study continual event extraction, which aims to extract incessantly
emerging event information while avoiding forgetting. We observe that the
semantic confusion on event types stems from the annotations of the same text
being updated over time. The imbalance between event types even aggravates this
issue. This paper proposes a novel continual event extraction model with
semantic confusion rectification. We mark pseudo labels for each sentence to
alleviate semantic confusion. We transfer pivotal knowledge between current and
previous models to enhance the understanding of event types. Moreover, we
encourage the model to focus on the semantics of long-tailed event types by
leveraging other associated types. Experimental results show that our model
outperforms state-of-the-art baselines and is proficient in imbalanced
datasets."
"Large language models (LLMs) are typically evaluated on the basis of
task-based benchmarks such as MMLU. Such benchmarks do not examine responsible
behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+
context where social stereotypes may result in variation in LGBTI+ terminology.
Therefore, domain-specific lexicons or dictionaries may be useful as a
representative list of words against which the LLM's behaviour needs to be
evaluated. This paper presents a methodology for evaluation of LLMs using an
LGBTI+ lexicon in Indian languages. The methodology consists of four steps:
formulating NLP tasks relevant to the expected behaviour, creating prompts that
test LLMs, using the LLMs to obtain the output and, finally, manually
evaluating the results. Our qualitative analysis shows that the three LLMs we
experiment on are unable to detect underlying hateful content. Similarly, we
observe limitations in using machine translation as means to evaluate natural
language understanding in languages other than English. The methodology
presented in this paper can be useful for LGBTI+ lexicons in other languages as
well as other domain-specific lexicons. The work done in this paper opens
avenues for responsible behaviour of LLMs, as demonstrated in the context of
prevalent social perception of the LGBTI+ community."
"Morality is a fundamental aspect of human behavior and ethics, influencing
how we interact with each other and the world around us. When faced with a
moral dilemma, a person's ability to make clear moral judgments can be clouded.
Due to many factors such as personal biases, emotions and situational factors
people can find it difficult to decide their best course of action. The
AmITheAsshole (AITA) subreddit is a forum on the social media platform Reddit
that helps people get clarity and objectivity on their predicaments. In the
forum people post anecdotes about moral dilemmas they are facing in their
lives, seeking validation for their actions or advice on how to navigate the
situation from the community. The morality of the actions in each post is
classified based on the collective opinion of the community into mainly two
labels, ""Not The Asshole"" (NTA) and ""You Are The Asshole"" (YTA). This project
aims to generate comments with moral reasoning for stories with moral dilemmas
using the AITA subreddit as a dataset. While past literature has explored the
classification of posts into labels (Alhassan et al., 2022), the generation of
comments remains a novel and challenging task. It involves understanding the
complex social and ethical considerations in each situation. To address this
challenge, we will leverage the vast amount of data on the forum with the goal
of generating coherent comments that align with the norms and values of the
AITA community. In this endeavor, we aim to evaluate state-of-the-art seq2seq
text generation models for their ability to make moral judgments similarly to
humans, ultimately producing concise comments providing clear moral stances and
advice for the poster."
"As the scaling of Large Language Models (LLMs) has dramatically enhanced
their capabilities, there has been a growing focus on the alignment problem to
ensure their responsible and ethical use. While existing alignment efforts
predominantly concentrate on universal values such as the HHH principle, the
aspect of culture, which is inherently pluralistic and diverse, has not
received adequate attention. This work introduces a new benchmark, CDEval,
aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by
incorporating both GPT-4's automated generation and human verification,
covering six cultural dimensions across seven domains. Our comprehensive
experiments provide intriguing insights into the culture of mainstream LLMs,
highlighting both consistencies and variations across different dimensions and
domains. The findings underscore the importance of integrating cultural
considerations in LLM development, particularly for applications in diverse
cultural settings. Through CDEval, we aim to broaden the horizon of LLM
alignment research by including cultural dimensions, thus providing a more
holistic framework for the future development and evaluation of LLMs. This
benchmark serves as a valuable resource for cultural studies in LLMs, paving
the way for more culturally aware and sensitive models."
"The formidable capacity for zero- or few-shot decision-making in language
agents encourages us to pose a compelling question: Can language agents be
alternatives to PPO agents in traditional sequential decision-making tasks? To
investigate this, we first take environments collected in OpenAI Gym as our
testbeds and ground them to textual environments that construct the TextGym
simulator. This allows for straightforward and efficient comparisons between
PPO agents and language agents, given the widespread adoption of OpenAI Gym. To
ensure a fair and effective benchmarking, we introduce $5$ levels of scenario
for accurate domain-knowledge controlling and a unified RL-inspired framework
for language agents. Additionally, we propose an innovative
explore-exploit-guided language (EXE) agent to solve tasks within TextGym.
Through numerical experiments and ablation studies, we extract valuable
insights into the decision-making capabilities of language agents and make a
preliminary evaluation of their potential to be alternatives to PPO in
classical sequential decision-making problems. This paper sheds light on the
performance of language agents and paves the way for future research in this
exciting domain. Our code is publicly available
at~\url{https://github.com/mail-ecnu/Text-Gym-Agents}."
"Query-focused summarization (QFS) aims to provide a summary of a single
document/multi documents that can satisfy the information needs of a given
query. It is useful for various real-world applications, such as abstractive
snippet generation or more recent retrieval augmented generation (RAG). A
prototypical QFS pipeline consists of a retriever (sparse or dense retrieval)
and a generator (usually a large language model). However, applying large
language models (LLM) potentially leads to hallucinations, especially when the
evidence contradicts the prior belief of LLMs. There has been growing interest
in developing new decoding methods to improve generation quality and reduce
hallucination. In this work, we conduct a large-scale reproducibility study on
one recently proposed decoding method -- Context-aware Decoding (CAD). In
addition to replicating CAD's experiments on news summarization datasets, we
include experiments on QFS datasets, and conduct more rigorous analysis on
computational complexity and hyperparameter sensitivity. Experiments with eight
different language models show that performance-wise, CAD improves QFS quality
by (1) reducing factuality errors/hallucinations while (2) mostly retaining the
match of lexical patterns, measured by ROUGE scores, while also at a cost of
increased inference-time FLOPs and reduced decoding speed. The code
implementation based on Huggingface Library is made available
https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs"
"The primary objective of simultaneous machine translation (SiMT) is to
minimize latency while preserving the quality of the final translation. Drawing
inspiration from CPU branch prediction techniques, we propose incorporating
branch prediction techniques in SiMT tasks to reduce translation latency.
Specifically, we utilize a language model as a branch predictor to predict
potential branch directions, namely, future source words. Subsequently, we
utilize the predicted source words to decode the output in advance. When the
actual source word deviates from the predicted source word, we use the real
source word to decode the output again, replacing the predicted output. To
further reduce computational costs, we share the parameters of the encoder and
the branch predictor, and utilize a pre-trained language model for
initialization. Our proposed method can be seamlessly integrated with any SiMT
model. Extensive experimental results demonstrate that our approach can improve
translation quality and latency at the same time. Our code is available at
https://github.com/YinAoXiong/simt_branch_predictor ."
"This study aims to acquire knowledge for creating very large language models
that are immune to hallucinations. Hallucinations in contemporary large
language models are often attributed to a misunderstanding of real-world social
relationships. Therefore, I hypothesize that very large language models capable
of thoroughly grasping all these relationships will be free from
hallucinations. Additionally, I propose that certain types of equivariant
language models are adept at learning and understanding these relationships.
Building on this, I have developed a specialized cross-entropy error function
to create a hallucination scale for language models, which measures their
extent of equivariance acquisition. Utilizing this scale, I tested language
models for their ability to acquire character-level equivariance. In
particular, I introduce and employ a novel technique based on T5 (Text To Text
Transfer Transformer) that efficiently understands permuted input texts without
the need for explicit dictionaries to convert token IDs (integers) to texts
(strings). This T5 model demonstrated a moderate ability to acquire
character-level equivariance. Additionally, I discovered scale laws that can
aid in developing hallucination-free language models at the character level.
This methodology can be extended to assess equivariance acquisition at the word
level, paving the way for very large language models that can comprehensively
understand relationships and, consequently, avoid hallucinations."
"The revolution of natural language processing via large language models has
motivated its use in multidisciplinary areas that include social sciences and
humanities and more specifically, comparative religion. Sentiment analysis
provides a mechanism to study the emotions expressed in text. Recently,
sentiment analysis has been used to study and compare translations of the
Bhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we
use sentiment analysis for studying selected chapters of the Bible. These
chapters are known as the Sermon on the Mount. We utilize a pre-trained
language model for sentiment analysis by reviewing five translations of the
Sermon on the Mount, which include the King James version, the New
International Version, the New Revised Standard Version, the Lamsa Version, and
the Basic English Version. We provide a chapter-by-chapter and verse-by-verse
comparison using sentiment and semantic analysis and review the major
sentiments expressed. Our results highlight the varying sentiments across the
chapters and verses. We found that the vocabulary of the respective
translations is significantly different. We detected different levels of
humour, optimism, and empathy in the respective chapters that were used by
Jesus to deliver his message."
"The capabilities of the most recent language models have increased the
interest in integrating them into real-world applications. However, the fact
that these models generate plausible, yet incorrect text poses a constraint
when considering their use in several domains. Healthcare is a prime example of
a domain where text-generative trustworthiness is a hard requirement to
safeguard patient well-being. In this paper, we present Physio, a chat-based
application for physical rehabilitation. Physio is capable of making an initial
diagnosis while citing reliable health sources to support the information
provided. Furthermore, drawing upon external knowledge databases, Physio can
recommend rehabilitation exercises and over-the-counter medication for symptom
relief. By combining these features, Physio can leverage the power of
generative models for language processing while also conditioning its response
on dependable and verifiable sources. A live demo of Physio is available at
https://physio.inesctec.pt."
"Visual question answering (VQA) is a task where an image is given, and a
series of questions are asked about the image. To build an efficient VQA
algorithm, a large amount of QA data is required which is very expensive.
Generating synthetic QA pairs based on templates is a practical way to obtain
data. However, VQA models trained on those data do not perform well on complex,
human-written questions. To address this issue, we propose a new method called
{\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a
sequence of QA interactions between a large language model and a VQA model
trained on synthetic data to reason and derive logical answers for
human-written questions. We tested the effectiveness of CoQAH on two types of
human-written VQA datasets for 3D-rendered and chest X-ray images and found
that it achieved state-of-the-art accuracy in both types of data. Notably,
CoQAH outperformed general vision-language models, VQA models, and medical
foundation models with no finetuning."
"LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as
Prompt Optimizers to self-reflect and refine prompts, has shown promising
performance in recent studies. Despite the success, the underlying mechanism of
this approach remains unexplored, and the true effectiveness of LLMs as Prompt
Optimizers requires further validation. In this work, we conducted a
comprehensive study to uncover the actual mechanism of LLM-based Prompt
Optimization. Our findings reveal that the LLM optimizers struggle to identify
the true causes of errors during reflection, tending to be biased by their own
prior knowledge rather than genuinely reflecting on the errors. Furthermore,
even when the reflection is semantically valid, the LLM optimizers often fail
to generate appropriate prompts for the target models with a single prompt
refinement step, partly due to the unpredictable behaviors of the target
models. Based on the observations, we introduce a new ""Automatic Behavior
Optimization"" paradigm, which directly optimizes the target model's behavior in
a more controllable manner. We hope our study can inspire new directions for
automatic prompt optimization development."
"We investigate the mechanism of in-context learning (ICL) on sentence
classification tasks with semantically-unrelated labels (""foo""/""bar""). We find
intervening in only 1\% heads (named ""in-context heads"") significantly affects
ICL accuracy from 87.6\% to 24.4\%. To understand this phenomenon, we analyze
the value-output vectors in these heads and discover that the vectors at each
label position contain substantial information about the corresponding labels.
Furthermore, we observe that the prediction shift from ""foo"" to ""bar"" is due to
the respective reduction and increase in these heads' attention scores at ""foo""
and ""bar"" positions. Therefore, we propose a hypothesis for ICL: in in-context
heads, the value-output matrices extract label features, while the query-key
matrices compute the similarity between the features at the last position and
those at each label position. The query and key matrices can be considered as
two towers that learn the similarity metric between the last position's
features and each demonstration at label positions. Using this hypothesis, we
explain the majority label bias and recency bias in ICL and propose two methods
to reduce these biases by 22\% and 17\%, respectively."
"Given the generational gap in available hardware between lay practitioners
and the most endowed institutions, LLMs are becoming increasingly inaccessible
as they grow in size. Whilst many approaches have been proposed to compress
LLMs to make their resource consumption manageable, these methods themselves
tend to be resource intensive, putting them out of the reach of the very user
groups they target. In this work, we explore the problem of structured pruning
of LLMs using only forward passes. We seek to empower practitioners to prune
models so large that their available hardware has just enough memory to run
inference. We develop Bonsai, a gradient-free, perturbative pruning method
capable of delivering small, fast, and accurate pruned models.
  We observe that Bonsai outputs pruned models that (i) outperform those
generated by more expensive gradient-based structured pruning methods, and (ii)
are twice as fast (with comparable accuracy) as those generated by
semi-structured pruning methods requiring comparable resources as Bonsai. We
also leverage Bonsai to produce a new sub-2B model using a single A6000 that
yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM
leaderboard."
"Instruction tuning (IT) is widely used to teach pretrained large language
models (LLMs) to follow arbitrary instructions, but is under-studied in
multilingual settings. In this work, we conduct a systematic study of zero-shot
cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only
data and then tested on user prompts in other languages. We advocate for the
importance of evaluating various aspects of model responses in multilingual
instruction following and investigate the influence of different model
configuration choices. We find that cross-lingual transfer does happen
successfully in IT even if all stages of model training are English-centric,
but only if multiliguality is taken into account in hyperparameter tuning and
with large enough IT data. English-trained LLMs are capable of generating
correct-language, comprehensive and helpful responses in other languages, but
suffer from low factuality and may occasionally have fluency errors."
"In pursuit of more inclusive Vision-Language Models (VLMs), this study
introduces a Large Multilingual Multimodal Model called PALO. PALO offers
visual reasoning capabilities in 10 major languages, including English,
Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese,
that span a total of ~5B people (65% of the world population). Our approach
involves a semi-automated translation approach to adapt the multimodal
instruction dataset from English to the target languages using a fine-tuned
Large Language Model, thereby ensuring high linguistic fidelity while allowing
scalability due to minimal manual effort. The incorporation of diverse
instruction sets helps us boost overall performance across multiple languages
especially those that are underrepresented like Hindi, Arabic, Bengali, and
Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B
parameters) to show the generalization and scalability where we observe
substantial improvements compared to strong baselines. We also propose the
first multilingual multimodal benchmark for the forthcoming approaches to
evaluate their vision-language reasoning capabilities across languages. Code:
https://github.com/mbzuai-oryx/PALO."
"Theory of Mind (ToM) is the cognitive capability to perceive and ascribe
mental states to oneself and others. Recent research has sparked a debate over
whether large language models (LLMs) exhibit a form of ToM. However, existing
ToM evaluations are hindered by challenges such as constrained scope,
subjective judgment, and unintended contamination, yielding inadequate
assessments. To address this gap, we introduce ToMBench with three key
characteristics: a systematic evaluation framework encompassing 8 tasks and 31
abilities in social cognition, a multiple-choice question format to support
automated and unbiased evaluation, and a build-from-scratch bilingual inventory
to strictly avoid data leakage. Based on ToMBench, we conduct extensive
experiments to evaluate the ToM performance of 10 popular LLMs across tasks and
abilities. We find that even the most advanced LLMs like GPT-4 lag behind human
performance by over 10% points, indicating that LLMs have not achieved a
human-level theory of mind yet. Our aim with ToMBench is to enable an efficient
and effective evaluation of LLMs' ToM capabilities, thereby facilitating the
development of LLMs with inherent social intelligence."
"Since internet technologies have advanced, one of the primary factors in
company development is customer happiness. Online platforms have become
prominent places for sharing reviews. Twitter is one of these platforms where
customers frequently post their thoughts. Reviews of flights on these platforms
have become a concern for the airline business. A positive review can help the
company grow, while a negative one can quickly ruin its revenue and reputation.
So it's vital for airline businesses to examine the feedback and experiences of
their customers and enhance their services to remain competitive. But studying
thousands of tweets and analyzing them to find the satisfaction of the customer
is quite a difficult task. This tedious process can be made easier by using a
machine learning approach to analyze tweets to determine client satisfaction
levels. Some work has already been done on this strategy to automate the
procedure using machine learning and deep learning techniques. However, they
are all purely concerned with assessing the text's sentiment. In addition to
the text, the tweet also includes the time, location, username, airline name,
and so on. This additional information can be crucial for improving the model's
outcome. To provide a machine learning based solution, this work has broadened
its perspective to include these qualities. And it has come as no surprise that
the additional features beyond text sentiment analysis produce better outcomes
in machine learning based models."
"Across the dynamic business landscape today, enterprises face an
ever-increasing range of challenges. These include the constantly evolving
regulatory environment, the growing demand for personalization within software
applications, and the heightened emphasis on governance. In response to these
multifaceted demands, large enterprises have been adopting automation that
spans from the optimization of core business processes to the enhancement of
customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a
pivotal element of modern software systems. In this context, data plays an
indispensable role. AI-centric software systems based on supervised learning
and operating at an industrial scale require large volumes of training data to
perform effectively. Moreover, the incorporation of generative AI has led to a
growing demand for adequate evaluation benchmarks. Our experience in this field
has revealed that the requirement for large datasets for training and
evaluation introduces a host of intricate challenges. This book chapter
explores the evolving landscape of Software Engineering (SE) in general, and
Requirements Engineering (RE) in particular, in this era marked by AI
integration. We discuss challenges that arise while integrating Natural
Language Processing (NLP) and generative AI into enterprise-critical software
systems. The chapter provides practical insights, solutions, and examples to
equip readers with the knowledge and tools necessary for effectively building
solutions with NLP at their cores. We also reflect on how these text
data-centric tasks sit together with the traditional RE process. We also
highlight new RE tasks that may be necessary for handling the increasingly
important text data-centricity involved in developing software systems."
"Despite superior reasoning prowess demonstrated by Large Language Models
(LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails
around the internal mechanisms of the models that facilitate CoT generation.
This work investigates the neural sub-structures within LLMs that manifest CoT
reasoning from a mechanistic point of view. From an analysis of Llama-2 7B
applied to multistep reasoning over fictional ontologies, we demonstrate that
LLMs deploy multiple parallel pathways of answer generation for step-by-step
reasoning. These parallel pathways provide sequential answers from the input
question context as well as the generated CoT. We observe a functional rift in
the middle layers of the LLM. Token representations in the initial half remain
strongly biased towards the pretraining prior, with the in-context prior taking
over in the later half. This internal phase shift manifests in different
functional components: attention heads that write the answer token appear in
the later half, attention heads that move information along ontological
relationships appear in the initial half, and so on. To the best of our
knowledge, this is the first attempt towards mechanistic investigation of CoT
reasoning in LLMs."
"In recent years,the entire field of Natural Language Processing (NLP) has
enjoyed amazing novel results achieving almost human-like performance on a
variety of tasks. Legal NLP domain has also been part of this process, as it
has seen an impressive growth. However, general-purpose models are not readily
applicable for legal domain. Due to the nature of the domain (e.g. specialized
vocabulary, long documents) specific models and methods are often needed for
Legal NLP. In this work we investigate both specialized and general models for
predicting the final ruling of a legal case, task known as Legal Judgment
Prediction (LJP). We particularly focus on methods to extend to sequence length
of Transformer-based models to better understand the long documents present in
legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating
from 2 sources with significantly different sizes and document lengths, show
that specialized models and handling long texts are critical for a good
performance."
"Benchmarking is the de-facto standard for evaluating LLMs, due to its speed,
replicability and low cost. However, recent work has pointed out that the
majority of the open source benchmarks available today have been contaminated
or leaked into LLMs, meaning that LLMs have access to test data during
pretraining and/or fine-tuning. This raises serious concerns about the validity
of benchmarking studies conducted so far and the future of evaluation using
benchmarks. To solve this problem, we propose Private Benchmarking, a solution
where test datasets are kept private and models are evaluated without revealing
the test data to the model. We describe various scenarios (depending on the
trust placed on model owners or dataset owners), and present solutions to avoid
data contamination using private benchmarking. For scenarios where the model
weights need to be kept private, we describe solutions from confidential
computing and cryptography that can aid in private benchmarking. We build an
end-to-end system, TRUCE, that enables such private benchmarking showing that
the overheads introduced to protect models and benchmark are negligible (in the
case of confidential computing) and tractable (when cryptographic security is
required). Finally, we also discuss solutions to the problem of benchmark
dataset auditing, to ensure that private benchmarks are of sufficiently high
quality."
"Topic modelling was mostly dominated by Bayesian graphical models during the
last decade. With the rise of transformers in Natural Language Processing,
however, several successful models that rely on straightforward clustering
approaches in transformer-based embedding spaces have emerged and consolidated
the notion of topics as clusters of embedding vectors. We propose the
Transformer-Representation Neural Topic Model (TNTM), which combines the
benefits of topic representations in transformer-based embedding spaces and
probabilistic modelling. Therefore, this approach unifies the powerful and
versatile notion of topics based on transformer embeddings with fully
probabilistic modelling, as in models such as Latent Dirichlet Allocation
(LDA). We utilize the variational autoencoder (VAE) framework for improved
inference speed and modelling flexibility. Experimental results show that our
proposed model achieves results on par with various state-of-the-art approaches
in terms of embedding coherence while maintaining almost perfect topic
diversity. The corresponding source code is available at
https://github.com/ArikReuter/TNTM."
"This paper presents a comprehensive survey of research works on the topic of
form understanding in the context of scanned documents. We delve into recent
advancements and breakthroughs in the field, highlighting the significance of
language models and transformers in solving this challenging task. Our research
methodology involves an in-depth analysis of popular documents and forms of
understanding of trends over the last decade, enabling us to offer valuable
insights into the evolution of this domain. Focusing on cutting-edge models, we
showcase how transformers have propelled the field forward, revolutionizing
form-understanding techniques. Our exploration includes an extensive
examination of state-of-the-art language models designed to effectively tackle
the complexities of noisy scanned documents. Furthermore, we present an
overview of the latest and most relevant datasets, which serve as essential
benchmarks for evaluating the performance of selected models. By comparing and
contrasting the capabilities of these models, we aim to provide researchers and
practitioners with useful guidance in choosing the most suitable solutions for
their specific form understanding tasks."
"While the progression of Large Language Models (LLMs) has notably propelled
financial analysis, their application has largely been confined to singular
language realms, leaving untapped the potential of bilingual Chinese-English
capacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating
the ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis.
ICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated
and original English datasets, enriching the breadth and depth of bilingual
financial modeling. It provides unrestricted access to diverse model variants,
a substantial compilation of diverse cross-lingual and multi-modal instruction
data, and an evaluation benchmark with expert annotations, comprising 10 NLP
tasks, 20 bilingual specific tasks, totaling 95k datasets. Our thorough
evaluation emphasizes the advantages of incorporating these bilingual datasets,
especially in translation tasks and utilizing original English data, enhancing
both linguistic flexibility and analytical acuity in financial contexts.
Notably, ICE-INTENT distinguishes itself by showcasing significant enhancements
over conventional LLMs and existing financial LLMs in bilingual milieus,
underscoring the profound impact of robust bilingual data on the accuracy and
efficacy of financial NLP."
"Self-admitted technical debt (SATD) refers to a form of technical debt in
which developers explicitly acknowledge and document the existence of technical
shortcuts, workarounds, or temporary solutions within the codebase. Over recent
years, researchers have manually labeled datasets derived from various software
development artifacts: source code comments, messages from the issue tracker
and pull request sections, and commit messages. These datasets are designed for
training, evaluation, performance validation, and improvement of machine
learning and deep learning models to accurately identify SATD instances.
However, class imbalance poses a serious challenge across all the existing
datasets, particularly when researchers are interested in categorizing the
specific types of SATD. In order to address the scarcity of labeled data for
SATD \textit{identification} (i.e., whether an instance is SATD or not) and
\textit{categorization} (i.e., which type of SATD is being classified) in
existing datasets, we share the \textit{SATDAUG} dataset, an augmented version
of existing SATD datasets, including source code comments, issue tracker, pull
requests, and commit messages. These augmented datasets have been balanced in
relation to the available artifacts and provide a much richer source of labeled
data for training machine learning or deep learning models."
"Two approaches have emerged to input images into large language models
(LLMs). The first is to caption images into natural language. The second is to
map image feature embeddings into the domain of the LLM and pass the mapped
embeddings directly to the LLM. The majority of recent few-shot multimodal work
reports performance using architectures that employ variations of one of these
two approaches. But they overlook an important comparison between them. We
design a controlled and focused experiment to compare these two approaches to
few-shot visual question answering (VQA) with LLMs. Our findings indicate that
for Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to
the LLM embedding space does not guarantee improved performance over using
image captions. In the zero-shot regime, we find using textual image captions
is better. In the few-shot regimes, how the in-context examples are selected
determines which is better."
"This paper focuses on task-agnostic prompt compression for better
generalizability and efficiency. Considering the redundancy in natural
language, existing approaches compress prompts by removing tokens or lexical
units according to their information entropy obtained from a causal language
model such as LLaMa-7B. The challenge is that information entropy may be a
suboptimal compression metric: (i) it only leverages unidirectional context and
may fail to capture all essential information needed for prompt compression;
(ii) it is not aligned with the prompt compression objective.
  To address these issues, we propose a data distillation procedure to derive
knowledge from an LLM to compress prompts without losing crucial information,
and meantime, introduce an extractive text compression dataset. We formulate
prompt compression as a token classification problem to guarantee the
faithfulness of the compressed prompt to the original one, and use a
Transformer encoder as the base architecture to capture all essential
information for prompt compression from the full bidirectional context. Our
approach leads to lower latency by explicitly learning the compression
objective with smaller models such as XLM-RoBERTa-large and mBERT.
  We evaluate our method on both in-domain and out-of-domain datasets,
including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its
small size, our model shows significant performance gains over strong baselines
and demonstrates robust generalization ability across different LLMs.
Additionally, our model is 3x-6x faster than existing prompt compression
methods, while accelerating the end-to-end latency by 1.6x-2.9x with
compression ratios of 2x-5x. Our code is available at
https://aka.ms/LLMLingua-2."
"In the context of the rising interest in code language models (code LMs) and
vulnerability detection, we study the effectiveness of code LMs for detecting
vulnerabilities. Our analysis reveals significant shortcomings in existing
vulnerability datasets, including poor data quality, low label accuracy, and
high duplication rates, leading to unreliable model performance in realistic
vulnerability detection scenarios. Additionally, the evaluation methods used
with these datasets are not representative of real-world vulnerability
detection.
  To address these challenges, we introduce PrimeVul, a new dataset for
training and evaluating code LMs for vulnerability detection. PrimeVul
incorporates a novel set of data labeling techniques that achieve comparable
label accuracy to human-verified benchmarks while significantly expanding the
dataset. It also implements a rigorous data de-duplication and chronological
data splitting strategy to mitigate data leakage issues, alongside introducing
more realistic evaluation metrics and settings. This comprehensive approach
aims to provide a more accurate assessment of code LMs' performance in
real-world conditions.
  Evaluating code LMs on PrimeVul reveals that existing benchmarks
significantly overestimate the performance of these models. For instance, a
state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on
PrimeVul. Attempts to improve performance through advanced training techniques
and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin
to random guessing in the most stringent settings. These findings underscore
the considerable gap between current capabilities and the practical
requirements for deploying code LMs in security roles, highlighting the need
for more innovative research in this domain."
"The embedding-based retrieval (EBR) approach is widely used in mainstream
search engine retrieval systems and is crucial in recent retrieval-augmented
methods for eliminating LLM illusions. However, existing EBR models often face
the ""semantic drift"" problem and insufficient focus on key information, leading
to a low adoption rate of retrieval results in subsequent steps. This issue is
especially noticeable in real-time search scenarios, where the various
expressions of popular events on the Internet make real-time retrieval heavily
reliant on crucial event information. To tackle this problem, this paper
proposes a novel approach called EER, which enhances real-time retrieval
performance by improving the dual-encoder model of traditional EBR. We
incorporate contrastive learning to accompany pairwise learning for encoder
optimization. Furthermore, to strengthen the focus on critical event
information in events, we include a decoder module after the document encoder,
introduce a generative event triplet extraction scheme based on prompt-tuning,
and correlate the events with query encoder optimization through comparative
learning. This decoder module can be removed during inference. Extensive
experiments demonstrate that EER can significantly improve the real-time search
retrieval performance. We believe that this approach will provide new
perspectives in the field of information retrieval. The codes and dataset are
available at https://github.com/open-event-hub/Event-enhanced_Retrieval ."
"Despite the remarkable achievements of language models (LMs) across a broad
spectrum of tasks, their propensity for generating toxic outputs remains a
prevalent concern. Current solutions involving finetuning or auxiliary models
usually require extensive computational resources, hindering their practicality
in large language models (LLMs). In this paper, we propose DeStein, a novel
method that detoxifies LMs by applying representation engineering in activation
spaces with lower resource and time costs. Specifically, we derive
detoxification vectors from self-induced, universal steering pairs through
arithmetic operations in activation spaces. During inference, detoxification is
achieved by fusing the detoxification vectors with the original representations
in a head-wise manner. Empirical results demonstrate that our method
significantly outperforms previous state-of-the-art approaches on various
metrics, while also maintaining satisfactory generation quality and diversity.
We further validate the practicality and scalability of DeStein with a series
of white-box LLMs. The method is open-sourced at
https://github.com/LizLizLi/DeStein. Warning: Some example model outputs may
contain highly offensive or disturbing text."
"Formulas are the language of communication between humans and nature. It is
an important research topic of artificial intelligence to find expressions from
observed data to reflect the relationship between each variable in the data,
which is called a symbolic regression problem. The existing symbolic regression
methods directly generate expressions according to the given observation data,
and we cannot require the algorithm to generate expressions that meet specific
requirements according to the known prior knowledge. For example, the
expression needs to contain $\sin$ or be symmetric, and so on. Even if it can,
it often requires very complex operations, which is very inconvenient. In this
paper, based on multi-modal large language models, we propose MLLM-SR, a
conversational symbolic regression method that can generate expressions that
meet the requirements simply by describing the requirements with natural
language instructions. By experimenting on the Nguyen dataset, we can
demonstrate that MLLM-SR leads the state-of-the-art baselines in fitting
performance. More notably, we experimentally demonstrate that MLLM-SR can well
understand the prior knowledge we add to the natural language instructions.
Moreover, the addition of prior knowledge can effectively guide MLLM-SR to
generate correct expressions."
"To extend the context length of Transformer-based large language models
(LLMs) and improve comprehension capabilities, we often face limitations due to
computational resources and bounded memory storage capacity. This work
introduces a method called Recurrent Context Compression (RCC), designed to
efficiently expand the context window length of LLMs within constrained storage
space. We also investigate the issue of poor model responses when both
instructions and context are compressed in downstream tasks, and propose an
instruction reconstruction method to mitigate this problem. We validated the
effectiveness of our approach on multiple tasks, achieving a compression rate
of up to 32x on text reconstruction tasks with a BLEU4 score close to 0.95, and
nearly 100\% accuracy on a passkey retrieval task with a sequence length of 1M.
Finally, our method demonstrated competitive performance in long-text
question-answering tasks compared to non-compressed methods, while
significantly saving storage resources in long-text inference tasks. Our code,
models, and demo are available at https://github.com/WUHU-G/RCC_Transformer"
"We introduce a language competition model that is based on the
Abrams-Strogatz model and incorporates the effects of memory and learning in
the language shift dynamics. On a coarse grained time scale, the effects of
memory and learning can be expressed as thresholds on the speakers fractions of
the competing languages. In its simplest form, the resulting model is exactly
solvable. Besides the consensus on one of the two languages, the model
describes additional equilibrium states that are not present in the
Abrams-Strogatz model: a stable dynamical coexistence of the two languages and
a frozen state coinciding with the initial state. We show numerically that
these results are preserved for threshold functions of a more general shape.
The comparison of the model predictions with historical datasets demonstrates
that while the Abrams-Strogatz model fails to describe some relevant language
competition situations, the proposed model provides a good fitting."
"Automated Essay Scoring (AES) holds significant promise in the field of
education, helping educators to mark larger volumes of essays and provide
timely feedback. However, Arabic AES research has been limited by the lack of
publicly available essay data. This study introduces AR-AES, an Arabic AES
benchmark dataset comprising 2046 undergraduate essays, including gender
information, scores, and transparent rubric-based evaluation guidelines,
providing comprehensive insights into the scoring process. These essays come
from four diverse courses, covering both traditional and online exams.
Additionally, we pioneer the use of AraBERT for AES, exploring its performance
on different question types. We find encouraging results, particularly for
Environmental Chemistry and source-dependent essay questions. For the first
time, we examine the scale of errors made by a BERT-based AES system, observing
that 96.15 percent of the errors are within one point of the first human
marker's prediction, on a scale of one to five, with 79.49 percent of
predictions matching exactly. In contrast, additional human markers did not
exceed 30 percent exact matches with the first marker, with 62.9 percent within
one mark. These findings highlight the subjectivity inherent in essay grading,
and underscore the potential for current AES technology to assist human markers
to grade consistently across large classes."
"Large language models (LLMs) and multimodal models (MMs) have exhibited
impressive capabilities in various domains, particularly in general language
understanding and visual reasoning. However, these models, trained on massive
data, may not be finely optimized for specific tasks triggered by instructions.
Continual instruction tuning is crucial to adapt a large model to evolving
tasks and domains, ensuring their effectiveness and relevance across a wide
range of applications. In the context of continual instruction tuning, where
models are sequentially trained on different tasks, catastrophic forgetting can
occur, leading to performance degradation on previously learned tasks. This
work addresses the catastrophic forgetting in continual instruction learning
through a switching mechanism for routing computations to parameter-efficient
tuned models. We demonstrate the effectiveness of our method through
experiments on continual instruction tuning of different natural language
generation tasks and vision-language tasks. We also showcase the advantages of
our proposed method in terms of efficiency, scalability, portability, and
privacy preservation."
"Recently, large language models (LLMs) have demonstrated excellent
performance in understanding human instructions and generating code, which has
inspired researchers to explore the feasibility of generating RTL code with
LLMs. However, the existing approaches to fine-tune LLMs on RTL codes typically
are conducted on fixed datasets, which do not fully stimulate the capability of
LLMs and require large amounts of reference data. To mitigate these issues , we
introduce a simple yet effective iterative training paradigm named ITERTL.
During each iteration, samples are drawn from the model trained in the previous
cycle. Then these new samples are employed for training in this loop. Through
this iterative approach, the distribution mismatch between the model and the
training samples is reduced. Additionally, the model is thus enabled to explore
a broader generative space and receive more comprehensive feedback. Theoretical
analyses are conducted to investigate the mechanism of the effectiveness.
Experimental results show the model trained through our proposed approach can
compete with and even outperform the state-of-the-art (SOTA) open-source model
with nearly 37\% reference samples, achieving remarkable 42.9\% and 62.2\%
pass@1 rate on two VerilogEval evaluation datasets respectively. While using
the same amount of reference samples, our method can achieved a relative
improvement of 16.9\% and 12.5\% in pass@1 compared to the non-iterative
method. This study facilitates the application of LLMs for generating RTL code
in practical scenarios with limited data."
"Using Large Language Models (LLMs) to generate synthetic data for model
training has become increasingly popular in recent years. While LLMs are
capable of producing realistic training data, the effectiveness of data
generation is influenced by various factors, including the choice of prompt,
task complexity, and the quality, quantity, and diversity of the generated
data. In this work, we focus exclusively on using synthetic data for text
classification tasks. Specifically, we use natural language understanding (NLU)
models trained on synthetic data to assess the quality of synthetic data from
different generation approaches. This work provides an empirical analysis of
the impact of these factors and offers recommendations for better data
generation practices."
"Solving grid puzzles involves a significant amount of logical reasoning.
Hence, it is a good domain to evaluate the reasoning capability of a model
which can then guide us to improve the reasoning ability of models. However,
most existing works evaluate only the final predicted answer of a puzzle,
without delving into an in-depth analysis of the LLMs' reasoning chains (such
as where they falter) or providing any finer metrics to evaluate them. Since
LLMs may rely on simple heuristics or artifacts to predict the final answer, it
is crucial to evaluate the generated reasoning chain beyond overall correctness
measures, for accurately evaluating the reasoning abilities of LLMs. To this
end, we first develop GridPuzzle, an evaluation dataset comprising 274
grid-based puzzles with different complexities. Second, we propose a new error
taxonomy derived from manual analysis of reasoning chains from LLMs including
GPT-4, Claude-3, Gemini, Mistral, and Llama-2. Then, we develop an LLM-based
framework for large-scale subjective evaluation (i.e., identifying errors) and
an objective metric, PuzzleEval, to evaluate the correctness of reasoning
chains. Evaluating reasoning chains from LLMs leads to several interesting
findings. We further show that existing prompting methods used for enhancing
models' reasoning abilities do not improve performance on GridPuzzle. This
highlights the importance of understanding fine-grained errors and presents a
challenge for future research to enhance LLMs' puzzle-solving abilities by
developing methods that address these errors. Data and source code are
available at https://github.com/Mihir3009/GridPuzzle."
"Warning: This paper may contain texts with uncomfortable content.
  Large Language Models (LLMs) have achieved remarkable performance in various
tasks, including those involving multimodal data like speech. However, these
models often exhibit biases due to the nature of their training data. Recently,
more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent
need to address these biases. This study introduces Spoken Stereoset, a dataset
specifically designed to evaluate social biases in SLLMs. By examining how
different models respond to speech from diverse demographic groups, we aim to
identify these biases. Our experiments reveal significant insights into their
performance and bias levels. The findings indicate that while most models show
minimal bias, some still exhibit slightly stereotypical or anti-stereotypical
tendencies."
